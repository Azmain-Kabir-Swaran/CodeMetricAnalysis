{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FSDirConcatOp.java",
  "functionName": "concat",
  "functionId": "concat___fsd-FSDirectory__pc-FSPermissionChecker__target-String__srcs-String[]__logRetryCache-boolean",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java",
  "functionStartLine": 51,
  "functionEndLine": 83,
  "numCommitsSeen": 909,
  "timeTaken": 36032,
  "changeHistory": [
    "84a1321f6aa0af6895564a7c47f8f264656f0294",
    "9b90e52f1ec22c18cd535af2a569defcef65b093",
    "9d175853b0170683ad5f21d9bcdeaac49fe89e04",
    "e363417e7b7abdd5d149f303f729ecf3e95ef8f3",
    "2848db814a98b83e7546f65a2751e56fb5b2dbe0",
    "65f2a4ee600dfffa5203450261da3c1989de25a9",
    "475c6b4978045d55d1ebcea69cc9a2f24355aca2",
    "8caf537afabc70b0c74e0a29aea0cc2935ecb162",
    "57dec288070f903931771485d6424317b20551aa",
    "e98529858edeed11c4f900b0db30d7e4eb2ab1ec",
    "78b9321539f973c7a1da5ce14acb49cdab41737a",
    "4c87a27ad851ffaa3cc3e2074a9ef7073b5a164a",
    "8c7a7e619699386f9e6991842558d78aa0c8053d",
    "e2a618e1cc3fb99115547af6540932860dc6766e",
    "cdb292f44caff9763631d9e9bcd69c375a7cddea",
    "d174f574bafcfefc635c64a47f258b1ce5d5c84e",
    "ba2ee1d7fb91462c861169224d250d2d90bec3a6",
    "ad06a087131d69d173d8e03dce5c97650a530f2e",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "5d5b1c6c10c66c6a17b483a3e1a98d59d3d0bdee",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "84a1321f6aa0af6895564a7c47f8f264656f0294": "Ymultichange(Yparameterchange,Ybodychange)",
    "9b90e52f1ec22c18cd535af2a569defcef65b093": "Yreturntypechange",
    "9d175853b0170683ad5f21d9bcdeaac49fe89e04": "Ybodychange",
    "e363417e7b7abdd5d149f303f729ecf3e95ef8f3": "Ybodychange",
    "2848db814a98b83e7546f65a2751e56fb5b2dbe0": "Ybodychange",
    "65f2a4ee600dfffa5203450261da3c1989de25a9": "Ybodychange",
    "475c6b4978045d55d1ebcea69cc9a2f24355aca2": "Ybodychange",
    "8caf537afabc70b0c74e0a29aea0cc2935ecb162": "Ymultichange(Ymovefromfile,Yreturntypechange,Ymodifierchange,Yexceptionschange,Ybodychange,Yrename,Yparameterchange)",
    "57dec288070f903931771485d6424317b20551aa": "Ybodychange",
    "e98529858edeed11c4f900b0db30d7e4eb2ab1ec": "Ybodychange",
    "78b9321539f973c7a1da5ce14acb49cdab41737a": "Ybodychange",
    "4c87a27ad851ffaa3cc3e2074a9ef7073b5a164a": "Ybodychange",
    "8c7a7e619699386f9e6991842558d78aa0c8053d": "Ymultichange(Yparameterchange,Ybodychange)",
    "e2a618e1cc3fb99115547af6540932860dc6766e": "Ybodychange",
    "cdb292f44caff9763631d9e9bcd69c375a7cddea": "Ymultichange(Yparameterchange,Ybodychange)",
    "d174f574bafcfefc635c64a47f258b1ce5d5c84e": "Ybodychange",
    "ba2ee1d7fb91462c861169224d250d2d90bec3a6": "Ybodychange",
    "ad06a087131d69d173d8e03dce5c97650a530f2e": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "5d5b1c6c10c66c6a17b483a3e1a98d59d3d0bdee": "Ymodifierchange",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "84a1321f6aa0af6895564a7c47f8f264656f0294": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-13136. Avoid taking FSN lock while doing group member lookup for FSD permission check. Contributed by Xiaoyu Yao.\n",
      "commitDate": "22/02/18 11:32 AM",
      "commitName": "84a1321f6aa0af6895564a7c47f8f264656f0294",
      "commitAuthor": "Xiaoyu Yao",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-13136. Avoid taking FSN lock while doing group member lookup for FSD permission check. Contributed by Xiaoyu Yao.\n",
          "commitDate": "22/02/18 11:32 AM",
          "commitName": "84a1321f6aa0af6895564a7c47f8f264656f0294",
          "commitAuthor": "Xiaoyu Yao",
          "commitDateOld": "08/02/18 8:59 AM",
          "commitNameOld": "8faf0b50d435039f69ea35f592856ca04d378809",
          "commitAuthorOld": "Xiaoyu Yao",
          "daysBetweenCommits": 14.11,
          "commitsBetweenForRepo": 74,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,34 +1,33 @@\n-  static FileStatus concat(FSDirectory fsd, String target, String[] srcs,\n-    boolean logRetryCache) throws IOException {\n+  static FileStatus concat(FSDirectory fsd, FSPermissionChecker pc,\n+      String target, String[] srcs, boolean logRetryCache) throws IOException {\n     validatePath(target, srcs);\n     assert srcs !\u003d null;\n     if (FSDirectory.LOG.isDebugEnabled()) {\n       FSDirectory.LOG.debug(\"concat {} to {}\", Arrays.toString(srcs), target);\n     }\n-    FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n     final INodesInPath targetIIP \u003d fsd.resolvePath(pc, target, DirOp.WRITE);\n     // write permission for the target\n     if (fsd.isPermissionEnabled()) {\n       fsd.checkPathAccess(pc, targetIIP, FsAction.WRITE);\n     }\n \n     // check the target\n     verifyTargetFile(fsd, target, targetIIP);\n     // check the srcs\n     INodeFile[] srcFiles \u003d verifySrcFiles(fsd, srcs, targetIIP, pc);\n \n     if(NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" +\n           Arrays.toString(srcs) + \" to \" + target);\n     }\n \n     long timestamp \u003d now();\n     fsd.writeLock();\n     try {\n       unprotectedConcat(fsd, targetIIP, srcFiles, timestamp);\n     } finally {\n       fsd.writeUnlock();\n     }\n     fsd.getEditLog().logConcat(target, srcs, timestamp, logRetryCache);\n     return fsd.getAuditFileInfo(targetIIP);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static FileStatus concat(FSDirectory fsd, FSPermissionChecker pc,\n      String target, String[] srcs, boolean logRetryCache) throws IOException {\n    validatePath(target, srcs);\n    assert srcs !\u003d null;\n    if (FSDirectory.LOG.isDebugEnabled()) {\n      FSDirectory.LOG.debug(\"concat {} to {}\", Arrays.toString(srcs), target);\n    }\n    final INodesInPath targetIIP \u003d fsd.resolvePath(pc, target, DirOp.WRITE);\n    // write permission for the target\n    if (fsd.isPermissionEnabled()) {\n      fsd.checkPathAccess(pc, targetIIP, FsAction.WRITE);\n    }\n\n    // check the target\n    verifyTargetFile(fsd, target, targetIIP);\n    // check the srcs\n    INodeFile[] srcFiles \u003d verifySrcFiles(fsd, srcs, targetIIP, pc);\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" +\n          Arrays.toString(srcs) + \" to \" + target);\n    }\n\n    long timestamp \u003d now();\n    fsd.writeLock();\n    try {\n      unprotectedConcat(fsd, targetIIP, srcFiles, timestamp);\n    } finally {\n      fsd.writeUnlock();\n    }\n    fsd.getEditLog().logConcat(target, srcs, timestamp, logRetryCache);\n    return fsd.getAuditFileInfo(targetIIP);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java",
          "extendedDetails": {
            "oldValue": "[fsd-FSDirectory, target-String, srcs-String[], logRetryCache-boolean]",
            "newValue": "[fsd-FSDirectory, pc-FSPermissionChecker, target-String, srcs-String[], logRetryCache-boolean]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-13136. Avoid taking FSN lock while doing group member lookup for FSD permission check. Contributed by Xiaoyu Yao.\n",
          "commitDate": "22/02/18 11:32 AM",
          "commitName": "84a1321f6aa0af6895564a7c47f8f264656f0294",
          "commitAuthor": "Xiaoyu Yao",
          "commitDateOld": "08/02/18 8:59 AM",
          "commitNameOld": "8faf0b50d435039f69ea35f592856ca04d378809",
          "commitAuthorOld": "Xiaoyu Yao",
          "daysBetweenCommits": 14.11,
          "commitsBetweenForRepo": 74,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,34 +1,33 @@\n-  static FileStatus concat(FSDirectory fsd, String target, String[] srcs,\n-    boolean logRetryCache) throws IOException {\n+  static FileStatus concat(FSDirectory fsd, FSPermissionChecker pc,\n+      String target, String[] srcs, boolean logRetryCache) throws IOException {\n     validatePath(target, srcs);\n     assert srcs !\u003d null;\n     if (FSDirectory.LOG.isDebugEnabled()) {\n       FSDirectory.LOG.debug(\"concat {} to {}\", Arrays.toString(srcs), target);\n     }\n-    FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n     final INodesInPath targetIIP \u003d fsd.resolvePath(pc, target, DirOp.WRITE);\n     // write permission for the target\n     if (fsd.isPermissionEnabled()) {\n       fsd.checkPathAccess(pc, targetIIP, FsAction.WRITE);\n     }\n \n     // check the target\n     verifyTargetFile(fsd, target, targetIIP);\n     // check the srcs\n     INodeFile[] srcFiles \u003d verifySrcFiles(fsd, srcs, targetIIP, pc);\n \n     if(NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" +\n           Arrays.toString(srcs) + \" to \" + target);\n     }\n \n     long timestamp \u003d now();\n     fsd.writeLock();\n     try {\n       unprotectedConcat(fsd, targetIIP, srcFiles, timestamp);\n     } finally {\n       fsd.writeUnlock();\n     }\n     fsd.getEditLog().logConcat(target, srcs, timestamp, logRetryCache);\n     return fsd.getAuditFileInfo(targetIIP);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static FileStatus concat(FSDirectory fsd, FSPermissionChecker pc,\n      String target, String[] srcs, boolean logRetryCache) throws IOException {\n    validatePath(target, srcs);\n    assert srcs !\u003d null;\n    if (FSDirectory.LOG.isDebugEnabled()) {\n      FSDirectory.LOG.debug(\"concat {} to {}\", Arrays.toString(srcs), target);\n    }\n    final INodesInPath targetIIP \u003d fsd.resolvePath(pc, target, DirOp.WRITE);\n    // write permission for the target\n    if (fsd.isPermissionEnabled()) {\n      fsd.checkPathAccess(pc, targetIIP, FsAction.WRITE);\n    }\n\n    // check the target\n    verifyTargetFile(fsd, target, targetIIP);\n    // check the srcs\n    INodeFile[] srcFiles \u003d verifySrcFiles(fsd, srcs, targetIIP, pc);\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" +\n          Arrays.toString(srcs) + \" to \" + target);\n    }\n\n    long timestamp \u003d now();\n    fsd.writeLock();\n    try {\n      unprotectedConcat(fsd, targetIIP, srcFiles, timestamp);\n    } finally {\n      fsd.writeUnlock();\n    }\n    fsd.getEditLog().logConcat(target, srcs, timestamp, logRetryCache);\n    return fsd.getAuditFileInfo(targetIIP);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java",
          "extendedDetails": {}
        }
      ]
    },
    "9b90e52f1ec22c18cd535af2a569defcef65b093": {
      "type": "Yreturntypechange",
      "commitMessage": "HDFS-11641. Reduce cost of audit logging by using FileStatus instead of HdfsFileStatus. Contributed by Daryn Sharp.\n",
      "commitDate": "16/05/17 9:28 AM",
      "commitName": "9b90e52f1ec22c18cd535af2a569defcef65b093",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "24/10/16 3:14 PM",
      "commitNameOld": "9d175853b0170683ad5f21d9bcdeaac49fe89e04",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 203.76,
      "commitsBetweenForRepo": 1218,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,34 +1,34 @@\n-  static HdfsFileStatus concat(FSDirectory fsd, String target, String[] srcs,\n+  static FileStatus concat(FSDirectory fsd, String target, String[] srcs,\n     boolean logRetryCache) throws IOException {\n     validatePath(target, srcs);\n     assert srcs !\u003d null;\n     if (FSDirectory.LOG.isDebugEnabled()) {\n       FSDirectory.LOG.debug(\"concat {} to {}\", Arrays.toString(srcs), target);\n     }\n     FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n     final INodesInPath targetIIP \u003d fsd.resolvePath(pc, target, DirOp.WRITE);\n     // write permission for the target\n     if (fsd.isPermissionEnabled()) {\n       fsd.checkPathAccess(pc, targetIIP, FsAction.WRITE);\n     }\n \n     // check the target\n     verifyTargetFile(fsd, target, targetIIP);\n     // check the srcs\n     INodeFile[] srcFiles \u003d verifySrcFiles(fsd, srcs, targetIIP, pc);\n \n     if(NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" +\n           Arrays.toString(srcs) + \" to \" + target);\n     }\n \n     long timestamp \u003d now();\n     fsd.writeLock();\n     try {\n       unprotectedConcat(fsd, targetIIP, srcFiles, timestamp);\n     } finally {\n       fsd.writeUnlock();\n     }\n     fsd.getEditLog().logConcat(target, srcs, timestamp, logRetryCache);\n     return fsd.getAuditFileInfo(targetIIP);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static FileStatus concat(FSDirectory fsd, String target, String[] srcs,\n    boolean logRetryCache) throws IOException {\n    validatePath(target, srcs);\n    assert srcs !\u003d null;\n    if (FSDirectory.LOG.isDebugEnabled()) {\n      FSDirectory.LOG.debug(\"concat {} to {}\", Arrays.toString(srcs), target);\n    }\n    FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n    final INodesInPath targetIIP \u003d fsd.resolvePath(pc, target, DirOp.WRITE);\n    // write permission for the target\n    if (fsd.isPermissionEnabled()) {\n      fsd.checkPathAccess(pc, targetIIP, FsAction.WRITE);\n    }\n\n    // check the target\n    verifyTargetFile(fsd, target, targetIIP);\n    // check the srcs\n    INodeFile[] srcFiles \u003d verifySrcFiles(fsd, srcs, targetIIP, pc);\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" +\n          Arrays.toString(srcs) + \" to \" + target);\n    }\n\n    long timestamp \u003d now();\n    fsd.writeLock();\n    try {\n      unprotectedConcat(fsd, targetIIP, srcFiles, timestamp);\n    } finally {\n      fsd.writeUnlock();\n    }\n    fsd.getEditLog().logConcat(target, srcs, timestamp, logRetryCache);\n    return fsd.getAuditFileInfo(targetIIP);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java",
      "extendedDetails": {
        "oldValue": "HdfsFileStatus",
        "newValue": "FileStatus"
      }
    },
    "9d175853b0170683ad5f21d9bcdeaac49fe89e04": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10997. Reduce number of path resolving methods. Contributed by Daryn Sharp.\n",
      "commitDate": "24/10/16 3:14 PM",
      "commitName": "9d175853b0170683ad5f21d9bcdeaac49fe89e04",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "10/12/15 11:55 PM",
      "commitNameOld": "e363417e7b7abdd5d149f303f729ecf3e95ef8f3",
      "commitAuthorOld": "Uma Mahesh",
      "daysBetweenCommits": 318.6,
      "commitsBetweenForRepo": 2181,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,35 +1,34 @@\n   static HdfsFileStatus concat(FSDirectory fsd, String target, String[] srcs,\n     boolean logRetryCache) throws IOException {\n     validatePath(target, srcs);\n     assert srcs !\u003d null;\n     if (FSDirectory.LOG.isDebugEnabled()) {\n       FSDirectory.LOG.debug(\"concat {} to {}\", Arrays.toString(srcs), target);\n     }\n-    final INodesInPath targetIIP \u003d fsd.getINodesInPath4Write(target);\n+    FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n+    final INodesInPath targetIIP \u003d fsd.resolvePath(pc, target, DirOp.WRITE);\n     // write permission for the target\n-    FSPermissionChecker pc \u003d null;\n     if (fsd.isPermissionEnabled()) {\n-      pc \u003d fsd.getPermissionChecker();\n       fsd.checkPathAccess(pc, targetIIP, FsAction.WRITE);\n     }\n \n     // check the target\n     verifyTargetFile(fsd, target, targetIIP);\n     // check the srcs\n     INodeFile[] srcFiles \u003d verifySrcFiles(fsd, srcs, targetIIP, pc);\n \n     if(NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" +\n           Arrays.toString(srcs) + \" to \" + target);\n     }\n \n     long timestamp \u003d now();\n     fsd.writeLock();\n     try {\n       unprotectedConcat(fsd, targetIIP, srcFiles, timestamp);\n     } finally {\n       fsd.writeUnlock();\n     }\n     fsd.getEditLog().logConcat(target, srcs, timestamp, logRetryCache);\n     return fsd.getAuditFileInfo(targetIIP);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static HdfsFileStatus concat(FSDirectory fsd, String target, String[] srcs,\n    boolean logRetryCache) throws IOException {\n    validatePath(target, srcs);\n    assert srcs !\u003d null;\n    if (FSDirectory.LOG.isDebugEnabled()) {\n      FSDirectory.LOG.debug(\"concat {} to {}\", Arrays.toString(srcs), target);\n    }\n    FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n    final INodesInPath targetIIP \u003d fsd.resolvePath(pc, target, DirOp.WRITE);\n    // write permission for the target\n    if (fsd.isPermissionEnabled()) {\n      fsd.checkPathAccess(pc, targetIIP, FsAction.WRITE);\n    }\n\n    // check the target\n    verifyTargetFile(fsd, target, targetIIP);\n    // check the srcs\n    INodeFile[] srcFiles \u003d verifySrcFiles(fsd, srcs, targetIIP, pc);\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" +\n          Arrays.toString(srcs) + \" to \" + target);\n    }\n\n    long timestamp \u003d now();\n    fsd.writeLock();\n    try {\n      unprotectedConcat(fsd, targetIIP, srcFiles, timestamp);\n    } finally {\n      fsd.writeUnlock();\n    }\n    fsd.getEditLog().logConcat(target, srcs, timestamp, logRetryCache);\n    return fsd.getAuditFileInfo(targetIIP);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java",
      "extendedDetails": {}
    },
    "e363417e7b7abdd5d149f303f729ecf3e95ef8f3": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9472. concat() API does not give proper exception messages on ./reserved relative path (Rakesh R via umamahesh)\n",
      "commitDate": "10/12/15 11:55 PM",
      "commitName": "e363417e7b7abdd5d149f303f729ecf3e95ef8f3",
      "commitAuthor": "Uma Mahesh",
      "commitDateOld": "13/10/15 11:03 AM",
      "commitNameOld": "da16c9b3b40f9cbef0ea7d8cffc4c2c77fd1c447",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 58.58,
      "commitsBetweenForRepo": 444,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,38 +1,35 @@\n   static HdfsFileStatus concat(FSDirectory fsd, String target, String[] srcs,\n     boolean logRetryCache) throws IOException {\n-    Preconditions.checkArgument(!target.isEmpty(), \"Target file name is empty\");\n-    Preconditions.checkArgument(srcs !\u003d null \u0026\u0026 srcs.length \u003e 0,\n-      \"No sources given\");\n+    validatePath(target, srcs);\n     assert srcs !\u003d null;\n     if (FSDirectory.LOG.isDebugEnabled()) {\n       FSDirectory.LOG.debug(\"concat {} to {}\", Arrays.toString(srcs), target);\n     }\n-\n     final INodesInPath targetIIP \u003d fsd.getINodesInPath4Write(target);\n     // write permission for the target\n     FSPermissionChecker pc \u003d null;\n     if (fsd.isPermissionEnabled()) {\n       pc \u003d fsd.getPermissionChecker();\n       fsd.checkPathAccess(pc, targetIIP, FsAction.WRITE);\n     }\n \n     // check the target\n     verifyTargetFile(fsd, target, targetIIP);\n     // check the srcs\n     INodeFile[] srcFiles \u003d verifySrcFiles(fsd, srcs, targetIIP, pc);\n \n     if(NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" +\n           Arrays.toString(srcs) + \" to \" + target);\n     }\n \n     long timestamp \u003d now();\n     fsd.writeLock();\n     try {\n       unprotectedConcat(fsd, targetIIP, srcFiles, timestamp);\n     } finally {\n       fsd.writeUnlock();\n     }\n     fsd.getEditLog().logConcat(target, srcs, timestamp, logRetryCache);\n     return fsd.getAuditFileInfo(targetIIP);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static HdfsFileStatus concat(FSDirectory fsd, String target, String[] srcs,\n    boolean logRetryCache) throws IOException {\n    validatePath(target, srcs);\n    assert srcs !\u003d null;\n    if (FSDirectory.LOG.isDebugEnabled()) {\n      FSDirectory.LOG.debug(\"concat {} to {}\", Arrays.toString(srcs), target);\n    }\n    final INodesInPath targetIIP \u003d fsd.getINodesInPath4Write(target);\n    // write permission for the target\n    FSPermissionChecker pc \u003d null;\n    if (fsd.isPermissionEnabled()) {\n      pc \u003d fsd.getPermissionChecker();\n      fsd.checkPathAccess(pc, targetIIP, FsAction.WRITE);\n    }\n\n    // check the target\n    verifyTargetFile(fsd, target, targetIIP);\n    // check the srcs\n    INodeFile[] srcFiles \u003d verifySrcFiles(fsd, srcs, targetIIP, pc);\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" +\n          Arrays.toString(srcs) + \" to \" + target);\n    }\n\n    long timestamp \u003d now();\n    fsd.writeLock();\n    try {\n      unprotectedConcat(fsd, targetIIP, srcFiles, timestamp);\n    } finally {\n      fsd.writeUnlock();\n    }\n    fsd.getEditLog().logConcat(target, srcs, timestamp, logRetryCache);\n    return fsd.getAuditFileInfo(targetIIP);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java",
      "extendedDetails": {}
    },
    "2848db814a98b83e7546f65a2751e56fb5b2dbe0": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3689. Add support for variable length block. Contributed by Jing Zhao.\n",
      "commitDate": "27/01/15 12:58 PM",
      "commitName": "2848db814a98b83e7546f65a2751e56fb5b2dbe0",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "18/12/14 11:25 AM",
      "commitNameOld": "65f2a4ee600dfffa5203450261da3c1989de25a9",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 40.06,
      "commitsBetweenForRepo": 212,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,137 +1,38 @@\n-  static HdfsFileStatus concat(\n-    FSDirectory fsd, String target, String[] srcs,\n+  static HdfsFileStatus concat(FSDirectory fsd, String target, String[] srcs,\n     boolean logRetryCache) throws IOException {\n     Preconditions.checkArgument(!target.isEmpty(), \"Target file name is empty\");\n     Preconditions.checkArgument(srcs !\u003d null \u0026\u0026 srcs.length \u003e 0,\n       \"No sources given\");\n     assert srcs !\u003d null;\n-\n-    FSDirectory.LOG.debug(\"concat {} to {}\", Arrays.toString(srcs), target);\n-    // We require all files be in the same directory\n-    String trgParent \u003d\n-      target.substring(0, target.lastIndexOf(Path.SEPARATOR_CHAR));\n-    for (String s : srcs) {\n-      String srcParent \u003d s.substring(0, s.lastIndexOf(Path.SEPARATOR_CHAR));\n-      if (!srcParent.equals(trgParent)) {\n-        throw new IllegalArgumentException(\n-           \"Sources and target are not in the same directory\");\n-      }\n+    if (FSDirectory.LOG.isDebugEnabled()) {\n+      FSDirectory.LOG.debug(\"concat {} to {}\", Arrays.toString(srcs), target);\n     }\n \n-    final INodesInPath trgIip \u003d fsd.getINodesInPath4Write(target);\n+    final INodesInPath targetIIP \u003d fsd.getINodesInPath4Write(target);\n     // write permission for the target\n+    FSPermissionChecker pc \u003d null;\n     if (fsd.isPermissionEnabled()) {\n-      FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n-      fsd.checkPathAccess(pc, trgIip, FsAction.WRITE);\n-\n-      // and srcs\n-      for(String aSrc: srcs) {\n-        final INodesInPath srcIip \u003d fsd.getINodesInPath4Write(aSrc);\n-        fsd.checkPathAccess(pc, srcIip, FsAction.READ); // read the file\n-        fsd.checkParentAccess(pc, srcIip, FsAction.WRITE); // for delete\n-      }\n+      pc \u003d fsd.getPermissionChecker();\n+      fsd.checkPathAccess(pc, targetIIP, FsAction.WRITE);\n     }\n \n-    // to make sure no two files are the same\n-    Set\u003cINode\u003e si \u003d new HashSet\u003cINode\u003e();\n-\n-    // we put the following prerequisite for the operation\n-    // replication and blocks sizes should be the same for ALL the blocks\n-\n     // check the target\n-    if (fsd.getEZForPath(trgIip) !\u003d null) {\n-      throw new HadoopIllegalArgumentException(\n-          \"concat can not be called for files in an encryption zone.\");\n-    }\n-    final INodeFile trgInode \u003d INodeFile.valueOf(trgIip.getLastINode(), target);\n-    if(trgInode.isUnderConstruction()) {\n-      throw new HadoopIllegalArgumentException(\"concat: target file \"\n-          + target + \" is under construction\");\n-    }\n-    // per design target shouldn\u0027t be empty and all the blocks same size\n-    if(trgInode.numBlocks() \u003d\u003d 0) {\n-      throw new HadoopIllegalArgumentException(\"concat: target file \"\n-          + target + \" is empty\");\n-    }\n-    if (trgInode.isWithSnapshot()) {\n-      throw new HadoopIllegalArgumentException(\"concat: target file \"\n-          + target + \" is in a snapshot\");\n-    }\n-\n-    long blockSize \u003d trgInode.getPreferredBlockSize();\n-\n-    // check the end block to be full\n-    final BlockInfo last \u003d trgInode.getLastBlock();\n-    if(blockSize !\u003d last.getNumBytes()) {\n-      throw new HadoopIllegalArgumentException(\"The last block in \" + target\n-          + \" is not full; last block size \u003d \" + last.getNumBytes()\n-          + \" but file block size \u003d \" + blockSize);\n-    }\n-\n-    si.add(trgInode);\n-    final short repl \u003d trgInode.getFileReplication();\n-\n-    // now check the srcs\n-    boolean endSrc \u003d false; // final src file doesn\u0027t have to have full end block\n-    for(int i\u003d0; i\u003c srcs.length; i++) {\n-      String src \u003d srcs[i];\n-      if(i\u003d\u003d srcs.length-1)\n-        endSrc\u003dtrue;\n-\n-      final INodeFile srcInode \u003d INodeFile.valueOf(fsd.getINode4Write(src), src);\n-      if(src.isEmpty()\n-          || srcInode.isUnderConstruction()\n-          || srcInode.numBlocks() \u003d\u003d 0) {\n-        throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n-            + \" is invalid or empty or underConstruction\");\n-      }\n-\n-      // check replication and blocks size\n-      if(repl !\u003d srcInode.getBlockReplication()) {\n-        throw new HadoopIllegalArgumentException(\"concat: the source file \"\n-            + src + \" and the target file \" + target\n-            + \" should have the same replication: source replication is \"\n-            + srcInode.getBlockReplication()\n-            + \" but target replication is \" + repl);\n-      }\n-\n-      //boolean endBlock\u003dfalse;\n-      // verify that all the blocks are of the same length as target\n-      // should be enough to check the end blocks\n-      final BlockInfo[] srcBlocks \u003d srcInode.getBlocks();\n-      int idx \u003d srcBlocks.length-1;\n-      if(endSrc)\n-        idx \u003d srcBlocks.length-2; // end block of endSrc is OK not to be full\n-      if(idx \u003e\u003d 0 \u0026\u0026 srcBlocks[idx].getNumBytes() !\u003d blockSize) {\n-        throw new HadoopIllegalArgumentException(\"concat: the source file \"\n-            + src + \" and the target file \" + target\n-            + \" should have the same blocks sizes: target block size is \"\n-            + blockSize + \" but the size of source block \" + idx + \" is \"\n-            + srcBlocks[idx].getNumBytes());\n-      }\n-\n-      si.add(srcInode);\n-    }\n-\n-    // make sure no two files are the same\n-    if(si.size() \u003c srcs.length+1) { // trg + srcs\n-      // it means at least two files are the same\n-      throw new HadoopIllegalArgumentException(\n-          \"concat: at least two of the source files are the same\");\n-    }\n+    verifyTargetFile(fsd, target, targetIIP);\n+    // check the srcs\n+    INodeFile[] srcFiles \u003d verifySrcFiles(fsd, srcs, targetIIP, pc);\n \n     if(NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" +\n           Arrays.toString(srcs) + \" to \" + target);\n     }\n \n     long timestamp \u003d now();\n     fsd.writeLock();\n     try {\n-      unprotectedConcat(fsd, target, srcs, timestamp);\n+      unprotectedConcat(fsd, targetIIP, srcFiles, timestamp);\n     } finally {\n       fsd.writeUnlock();\n     }\n     fsd.getEditLog().logConcat(target, srcs, timestamp, logRetryCache);\n-    return fsd.getAuditFileInfo(trgIip);\n+    return fsd.getAuditFileInfo(targetIIP);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static HdfsFileStatus concat(FSDirectory fsd, String target, String[] srcs,\n    boolean logRetryCache) throws IOException {\n    Preconditions.checkArgument(!target.isEmpty(), \"Target file name is empty\");\n    Preconditions.checkArgument(srcs !\u003d null \u0026\u0026 srcs.length \u003e 0,\n      \"No sources given\");\n    assert srcs !\u003d null;\n    if (FSDirectory.LOG.isDebugEnabled()) {\n      FSDirectory.LOG.debug(\"concat {} to {}\", Arrays.toString(srcs), target);\n    }\n\n    final INodesInPath targetIIP \u003d fsd.getINodesInPath4Write(target);\n    // write permission for the target\n    FSPermissionChecker pc \u003d null;\n    if (fsd.isPermissionEnabled()) {\n      pc \u003d fsd.getPermissionChecker();\n      fsd.checkPathAccess(pc, targetIIP, FsAction.WRITE);\n    }\n\n    // check the target\n    verifyTargetFile(fsd, target, targetIIP);\n    // check the srcs\n    INodeFile[] srcFiles \u003d verifySrcFiles(fsd, srcs, targetIIP, pc);\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" +\n          Arrays.toString(srcs) + \" to \" + target);\n    }\n\n    long timestamp \u003d now();\n    fsd.writeLock();\n    try {\n      unprotectedConcat(fsd, targetIIP, srcFiles, timestamp);\n    } finally {\n      fsd.writeUnlock();\n    }\n    fsd.getEditLog().logConcat(target, srcs, timestamp, logRetryCache);\n    return fsd.getAuditFileInfo(targetIIP);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java",
      "extendedDetails": {}
    },
    "65f2a4ee600dfffa5203450261da3c1989de25a9": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7543. Avoid path resolution when getting FileStatus for audit logs. Contributed by Haohui Mai.\n",
      "commitDate": "18/12/14 11:25 AM",
      "commitName": "65f2a4ee600dfffa5203450261da3c1989de25a9",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "09/12/14 11:37 AM",
      "commitNameOld": "5776a41da08af653206bb94d7c76c9c4dcce059a",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 8.99,
      "commitsBetweenForRepo": 75,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,137 +1,137 @@\n   static HdfsFileStatus concat(\n     FSDirectory fsd, String target, String[] srcs,\n     boolean logRetryCache) throws IOException {\n     Preconditions.checkArgument(!target.isEmpty(), \"Target file name is empty\");\n     Preconditions.checkArgument(srcs !\u003d null \u0026\u0026 srcs.length \u003e 0,\n       \"No sources given\");\n     assert srcs !\u003d null;\n \n     FSDirectory.LOG.debug(\"concat {} to {}\", Arrays.toString(srcs), target);\n     // We require all files be in the same directory\n     String trgParent \u003d\n       target.substring(0, target.lastIndexOf(Path.SEPARATOR_CHAR));\n     for (String s : srcs) {\n       String srcParent \u003d s.substring(0, s.lastIndexOf(Path.SEPARATOR_CHAR));\n       if (!srcParent.equals(trgParent)) {\n         throw new IllegalArgumentException(\n            \"Sources and target are not in the same directory\");\n       }\n     }\n \n     final INodesInPath trgIip \u003d fsd.getINodesInPath4Write(target);\n     // write permission for the target\n     if (fsd.isPermissionEnabled()) {\n       FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n       fsd.checkPathAccess(pc, trgIip, FsAction.WRITE);\n \n       // and srcs\n       for(String aSrc: srcs) {\n         final INodesInPath srcIip \u003d fsd.getINodesInPath4Write(aSrc);\n         fsd.checkPathAccess(pc, srcIip, FsAction.READ); // read the file\n         fsd.checkParentAccess(pc, srcIip, FsAction.WRITE); // for delete\n       }\n     }\n \n     // to make sure no two files are the same\n     Set\u003cINode\u003e si \u003d new HashSet\u003cINode\u003e();\n \n     // we put the following prerequisite for the operation\n     // replication and blocks sizes should be the same for ALL the blocks\n \n     // check the target\n     if (fsd.getEZForPath(trgIip) !\u003d null) {\n       throw new HadoopIllegalArgumentException(\n           \"concat can not be called for files in an encryption zone.\");\n     }\n     final INodeFile trgInode \u003d INodeFile.valueOf(trgIip.getLastINode(), target);\n     if(trgInode.isUnderConstruction()) {\n       throw new HadoopIllegalArgumentException(\"concat: target file \"\n           + target + \" is under construction\");\n     }\n     // per design target shouldn\u0027t be empty and all the blocks same size\n     if(trgInode.numBlocks() \u003d\u003d 0) {\n       throw new HadoopIllegalArgumentException(\"concat: target file \"\n           + target + \" is empty\");\n     }\n     if (trgInode.isWithSnapshot()) {\n       throw new HadoopIllegalArgumentException(\"concat: target file \"\n           + target + \" is in a snapshot\");\n     }\n \n     long blockSize \u003d trgInode.getPreferredBlockSize();\n \n     // check the end block to be full\n     final BlockInfo last \u003d trgInode.getLastBlock();\n     if(blockSize !\u003d last.getNumBytes()) {\n       throw new HadoopIllegalArgumentException(\"The last block in \" + target\n           + \" is not full; last block size \u003d \" + last.getNumBytes()\n           + \" but file block size \u003d \" + blockSize);\n     }\n \n     si.add(trgInode);\n     final short repl \u003d trgInode.getFileReplication();\n \n     // now check the srcs\n     boolean endSrc \u003d false; // final src file doesn\u0027t have to have full end block\n     for(int i\u003d0; i\u003c srcs.length; i++) {\n       String src \u003d srcs[i];\n       if(i\u003d\u003d srcs.length-1)\n         endSrc\u003dtrue;\n \n       final INodeFile srcInode \u003d INodeFile.valueOf(fsd.getINode4Write(src), src);\n       if(src.isEmpty()\n           || srcInode.isUnderConstruction()\n           || srcInode.numBlocks() \u003d\u003d 0) {\n         throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n             + \" is invalid or empty or underConstruction\");\n       }\n \n       // check replication and blocks size\n       if(repl !\u003d srcInode.getBlockReplication()) {\n         throw new HadoopIllegalArgumentException(\"concat: the source file \"\n             + src + \" and the target file \" + target\n             + \" should have the same replication: source replication is \"\n             + srcInode.getBlockReplication()\n             + \" but target replication is \" + repl);\n       }\n \n       //boolean endBlock\u003dfalse;\n       // verify that all the blocks are of the same length as target\n       // should be enough to check the end blocks\n       final BlockInfo[] srcBlocks \u003d srcInode.getBlocks();\n       int idx \u003d srcBlocks.length-1;\n       if(endSrc)\n         idx \u003d srcBlocks.length-2; // end block of endSrc is OK not to be full\n       if(idx \u003e\u003d 0 \u0026\u0026 srcBlocks[idx].getNumBytes() !\u003d blockSize) {\n         throw new HadoopIllegalArgumentException(\"concat: the source file \"\n             + src + \" and the target file \" + target\n             + \" should have the same blocks sizes: target block size is \"\n             + blockSize + \" but the size of source block \" + idx + \" is \"\n             + srcBlocks[idx].getNumBytes());\n       }\n \n       si.add(srcInode);\n     }\n \n     // make sure no two files are the same\n     if(si.size() \u003c srcs.length+1) { // trg + srcs\n       // it means at least two files are the same\n       throw new HadoopIllegalArgumentException(\n           \"concat: at least two of the source files are the same\");\n     }\n \n     if(NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" +\n           Arrays.toString(srcs) + \" to \" + target);\n     }\n \n     long timestamp \u003d now();\n     fsd.writeLock();\n     try {\n       unprotectedConcat(fsd, target, srcs, timestamp);\n     } finally {\n       fsd.writeUnlock();\n     }\n     fsd.getEditLog().logConcat(target, srcs, timestamp, logRetryCache);\n-    return fsd.getAuditFileInfo(target, false);\n+    return fsd.getAuditFileInfo(trgIip);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static HdfsFileStatus concat(\n    FSDirectory fsd, String target, String[] srcs,\n    boolean logRetryCache) throws IOException {\n    Preconditions.checkArgument(!target.isEmpty(), \"Target file name is empty\");\n    Preconditions.checkArgument(srcs !\u003d null \u0026\u0026 srcs.length \u003e 0,\n      \"No sources given\");\n    assert srcs !\u003d null;\n\n    FSDirectory.LOG.debug(\"concat {} to {}\", Arrays.toString(srcs), target);\n    // We require all files be in the same directory\n    String trgParent \u003d\n      target.substring(0, target.lastIndexOf(Path.SEPARATOR_CHAR));\n    for (String s : srcs) {\n      String srcParent \u003d s.substring(0, s.lastIndexOf(Path.SEPARATOR_CHAR));\n      if (!srcParent.equals(trgParent)) {\n        throw new IllegalArgumentException(\n           \"Sources and target are not in the same directory\");\n      }\n    }\n\n    final INodesInPath trgIip \u003d fsd.getINodesInPath4Write(target);\n    // write permission for the target\n    if (fsd.isPermissionEnabled()) {\n      FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n      fsd.checkPathAccess(pc, trgIip, FsAction.WRITE);\n\n      // and srcs\n      for(String aSrc: srcs) {\n        final INodesInPath srcIip \u003d fsd.getINodesInPath4Write(aSrc);\n        fsd.checkPathAccess(pc, srcIip, FsAction.READ); // read the file\n        fsd.checkParentAccess(pc, srcIip, FsAction.WRITE); // for delete\n      }\n    }\n\n    // to make sure no two files are the same\n    Set\u003cINode\u003e si \u003d new HashSet\u003cINode\u003e();\n\n    // we put the following prerequisite for the operation\n    // replication and blocks sizes should be the same for ALL the blocks\n\n    // check the target\n    if (fsd.getEZForPath(trgIip) !\u003d null) {\n      throw new HadoopIllegalArgumentException(\n          \"concat can not be called for files in an encryption zone.\");\n    }\n    final INodeFile trgInode \u003d INodeFile.valueOf(trgIip.getLastINode(), target);\n    if(trgInode.isUnderConstruction()) {\n      throw new HadoopIllegalArgumentException(\"concat: target file \"\n          + target + \" is under construction\");\n    }\n    // per design target shouldn\u0027t be empty and all the blocks same size\n    if(trgInode.numBlocks() \u003d\u003d 0) {\n      throw new HadoopIllegalArgumentException(\"concat: target file \"\n          + target + \" is empty\");\n    }\n    if (trgInode.isWithSnapshot()) {\n      throw new HadoopIllegalArgumentException(\"concat: target file \"\n          + target + \" is in a snapshot\");\n    }\n\n    long blockSize \u003d trgInode.getPreferredBlockSize();\n\n    // check the end block to be full\n    final BlockInfo last \u003d trgInode.getLastBlock();\n    if(blockSize !\u003d last.getNumBytes()) {\n      throw new HadoopIllegalArgumentException(\"The last block in \" + target\n          + \" is not full; last block size \u003d \" + last.getNumBytes()\n          + \" but file block size \u003d \" + blockSize);\n    }\n\n    si.add(trgInode);\n    final short repl \u003d trgInode.getFileReplication();\n\n    // now check the srcs\n    boolean endSrc \u003d false; // final src file doesn\u0027t have to have full end block\n    for(int i\u003d0; i\u003c srcs.length; i++) {\n      String src \u003d srcs[i];\n      if(i\u003d\u003d srcs.length-1)\n        endSrc\u003dtrue;\n\n      final INodeFile srcInode \u003d INodeFile.valueOf(fsd.getINode4Write(src), src);\n      if(src.isEmpty()\n          || srcInode.isUnderConstruction()\n          || srcInode.numBlocks() \u003d\u003d 0) {\n        throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n            + \" is invalid or empty or underConstruction\");\n      }\n\n      // check replication and blocks size\n      if(repl !\u003d srcInode.getBlockReplication()) {\n        throw new HadoopIllegalArgumentException(\"concat: the source file \"\n            + src + \" and the target file \" + target\n            + \" should have the same replication: source replication is \"\n            + srcInode.getBlockReplication()\n            + \" but target replication is \" + repl);\n      }\n\n      //boolean endBlock\u003dfalse;\n      // verify that all the blocks are of the same length as target\n      // should be enough to check the end blocks\n      final BlockInfo[] srcBlocks \u003d srcInode.getBlocks();\n      int idx \u003d srcBlocks.length-1;\n      if(endSrc)\n        idx \u003d srcBlocks.length-2; // end block of endSrc is OK not to be full\n      if(idx \u003e\u003d 0 \u0026\u0026 srcBlocks[idx].getNumBytes() !\u003d blockSize) {\n        throw new HadoopIllegalArgumentException(\"concat: the source file \"\n            + src + \" and the target file \" + target\n            + \" should have the same blocks sizes: target block size is \"\n            + blockSize + \" but the size of source block \" + idx + \" is \"\n            + srcBlocks[idx].getNumBytes());\n      }\n\n      si.add(srcInode);\n    }\n\n    // make sure no two files are the same\n    if(si.size() \u003c srcs.length+1) { // trg + srcs\n      // it means at least two files are the same\n      throw new HadoopIllegalArgumentException(\n          \"concat: at least two of the source files are the same\");\n    }\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" +\n          Arrays.toString(srcs) + \" to \" + target);\n    }\n\n    long timestamp \u003d now();\n    fsd.writeLock();\n    try {\n      unprotectedConcat(fsd, target, srcs, timestamp);\n    } finally {\n      fsd.writeUnlock();\n    }\n    fsd.getEditLog().logConcat(target, srcs, timestamp, logRetryCache);\n    return fsd.getAuditFileInfo(trgIip);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java",
      "extendedDetails": {}
    },
    "475c6b4978045d55d1ebcea69cc9a2f24355aca2": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7474. Avoid resolving path in FSPermissionChecker. Contributed by Jing Zhao.\n",
      "commitDate": "05/12/14 2:17 PM",
      "commitName": "475c6b4978045d55d1ebcea69cc9a2f24355aca2",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "24/11/14 3:42 PM",
      "commitNameOld": "8caf537afabc70b0c74e0a29aea0cc2935ecb162",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 10.94,
      "commitsBetweenForRepo": 65,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,136 +1,137 @@\n   static HdfsFileStatus concat(\n     FSDirectory fsd, String target, String[] srcs,\n     boolean logRetryCache) throws IOException {\n     Preconditions.checkArgument(!target.isEmpty(), \"Target file name is empty\");\n     Preconditions.checkArgument(srcs !\u003d null \u0026\u0026 srcs.length \u003e 0,\n       \"No sources given\");\n     assert srcs !\u003d null;\n \n     FSDirectory.LOG.debug(\"concat {} to {}\", Arrays.toString(srcs), target);\n     // We require all files be in the same directory\n     String trgParent \u003d\n       target.substring(0, target.lastIndexOf(Path.SEPARATOR_CHAR));\n     for (String s : srcs) {\n       String srcParent \u003d s.substring(0, s.lastIndexOf(Path.SEPARATOR_CHAR));\n       if (!srcParent.equals(trgParent)) {\n         throw new IllegalArgumentException(\n            \"Sources and target are not in the same directory\");\n       }\n     }\n \n+    final INodesInPath trgIip \u003d fsd.getINodesInPath4Write(target);\n     // write permission for the target\n     if (fsd.isPermissionEnabled()) {\n       FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n-      fsd.checkPathAccess(pc, target, FsAction.WRITE);\n+      fsd.checkPathAccess(pc, trgIip, FsAction.WRITE);\n \n       // and srcs\n       for(String aSrc: srcs) {\n-        fsd.checkPathAccess(pc, aSrc, FsAction.READ); // read the file\n-        fsd.checkParentAccess(pc, aSrc, FsAction.WRITE); // for delete\n+        final INodesInPath srcIip \u003d fsd.getINodesInPath4Write(aSrc);\n+        fsd.checkPathAccess(pc, srcIip, FsAction.READ); // read the file\n+        fsd.checkParentAccess(pc, srcIip, FsAction.WRITE); // for delete\n       }\n     }\n \n     // to make sure no two files are the same\n     Set\u003cINode\u003e si \u003d new HashSet\u003cINode\u003e();\n \n     // we put the following prerequisite for the operation\n     // replication and blocks sizes should be the same for ALL the blocks\n \n     // check the target\n-    final INodesInPath trgIip \u003d fsd.getINodesInPath4Write(target);\n     if (fsd.getEZForPath(trgIip) !\u003d null) {\n       throw new HadoopIllegalArgumentException(\n           \"concat can not be called for files in an encryption zone.\");\n     }\n     final INodeFile trgInode \u003d INodeFile.valueOf(trgIip.getLastINode(), target);\n     if(trgInode.isUnderConstruction()) {\n       throw new HadoopIllegalArgumentException(\"concat: target file \"\n           + target + \" is under construction\");\n     }\n     // per design target shouldn\u0027t be empty and all the blocks same size\n     if(trgInode.numBlocks() \u003d\u003d 0) {\n       throw new HadoopIllegalArgumentException(\"concat: target file \"\n           + target + \" is empty\");\n     }\n     if (trgInode.isWithSnapshot()) {\n       throw new HadoopIllegalArgumentException(\"concat: target file \"\n           + target + \" is in a snapshot\");\n     }\n \n     long blockSize \u003d trgInode.getPreferredBlockSize();\n \n     // check the end block to be full\n     final BlockInfo last \u003d trgInode.getLastBlock();\n     if(blockSize !\u003d last.getNumBytes()) {\n       throw new HadoopIllegalArgumentException(\"The last block in \" + target\n           + \" is not full; last block size \u003d \" + last.getNumBytes()\n           + \" but file block size \u003d \" + blockSize);\n     }\n \n     si.add(trgInode);\n     final short repl \u003d trgInode.getFileReplication();\n \n     // now check the srcs\n     boolean endSrc \u003d false; // final src file doesn\u0027t have to have full end block\n     for(int i\u003d0; i\u003c srcs.length; i++) {\n       String src \u003d srcs[i];\n       if(i\u003d\u003d srcs.length-1)\n         endSrc\u003dtrue;\n \n       final INodeFile srcInode \u003d INodeFile.valueOf(fsd.getINode4Write(src), src);\n       if(src.isEmpty()\n           || srcInode.isUnderConstruction()\n           || srcInode.numBlocks() \u003d\u003d 0) {\n         throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n             + \" is invalid or empty or underConstruction\");\n       }\n \n       // check replication and blocks size\n       if(repl !\u003d srcInode.getBlockReplication()) {\n         throw new HadoopIllegalArgumentException(\"concat: the source file \"\n             + src + \" and the target file \" + target\n             + \" should have the same replication: source replication is \"\n             + srcInode.getBlockReplication()\n             + \" but target replication is \" + repl);\n       }\n \n       //boolean endBlock\u003dfalse;\n       // verify that all the blocks are of the same length as target\n       // should be enough to check the end blocks\n       final BlockInfo[] srcBlocks \u003d srcInode.getBlocks();\n       int idx \u003d srcBlocks.length-1;\n       if(endSrc)\n         idx \u003d srcBlocks.length-2; // end block of endSrc is OK not to be full\n       if(idx \u003e\u003d 0 \u0026\u0026 srcBlocks[idx].getNumBytes() !\u003d blockSize) {\n         throw new HadoopIllegalArgumentException(\"concat: the source file \"\n             + src + \" and the target file \" + target\n             + \" should have the same blocks sizes: target block size is \"\n             + blockSize + \" but the size of source block \" + idx + \" is \"\n             + srcBlocks[idx].getNumBytes());\n       }\n \n       si.add(srcInode);\n     }\n \n     // make sure no two files are the same\n     if(si.size() \u003c srcs.length+1) { // trg + srcs\n       // it means at least two files are the same\n       throw new HadoopIllegalArgumentException(\n           \"concat: at least two of the source files are the same\");\n     }\n \n     if(NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" +\n           Arrays.toString(srcs) + \" to \" + target);\n     }\n \n     long timestamp \u003d now();\n     fsd.writeLock();\n     try {\n       unprotectedConcat(fsd, target, srcs, timestamp);\n     } finally {\n       fsd.writeUnlock();\n     }\n     fsd.getEditLog().logConcat(target, srcs, timestamp, logRetryCache);\n     return fsd.getAuditFileInfo(target, false);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static HdfsFileStatus concat(\n    FSDirectory fsd, String target, String[] srcs,\n    boolean logRetryCache) throws IOException {\n    Preconditions.checkArgument(!target.isEmpty(), \"Target file name is empty\");\n    Preconditions.checkArgument(srcs !\u003d null \u0026\u0026 srcs.length \u003e 0,\n      \"No sources given\");\n    assert srcs !\u003d null;\n\n    FSDirectory.LOG.debug(\"concat {} to {}\", Arrays.toString(srcs), target);\n    // We require all files be in the same directory\n    String trgParent \u003d\n      target.substring(0, target.lastIndexOf(Path.SEPARATOR_CHAR));\n    for (String s : srcs) {\n      String srcParent \u003d s.substring(0, s.lastIndexOf(Path.SEPARATOR_CHAR));\n      if (!srcParent.equals(trgParent)) {\n        throw new IllegalArgumentException(\n           \"Sources and target are not in the same directory\");\n      }\n    }\n\n    final INodesInPath trgIip \u003d fsd.getINodesInPath4Write(target);\n    // write permission for the target\n    if (fsd.isPermissionEnabled()) {\n      FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n      fsd.checkPathAccess(pc, trgIip, FsAction.WRITE);\n\n      // and srcs\n      for(String aSrc: srcs) {\n        final INodesInPath srcIip \u003d fsd.getINodesInPath4Write(aSrc);\n        fsd.checkPathAccess(pc, srcIip, FsAction.READ); // read the file\n        fsd.checkParentAccess(pc, srcIip, FsAction.WRITE); // for delete\n      }\n    }\n\n    // to make sure no two files are the same\n    Set\u003cINode\u003e si \u003d new HashSet\u003cINode\u003e();\n\n    // we put the following prerequisite for the operation\n    // replication and blocks sizes should be the same for ALL the blocks\n\n    // check the target\n    if (fsd.getEZForPath(trgIip) !\u003d null) {\n      throw new HadoopIllegalArgumentException(\n          \"concat can not be called for files in an encryption zone.\");\n    }\n    final INodeFile trgInode \u003d INodeFile.valueOf(trgIip.getLastINode(), target);\n    if(trgInode.isUnderConstruction()) {\n      throw new HadoopIllegalArgumentException(\"concat: target file \"\n          + target + \" is under construction\");\n    }\n    // per design target shouldn\u0027t be empty and all the blocks same size\n    if(trgInode.numBlocks() \u003d\u003d 0) {\n      throw new HadoopIllegalArgumentException(\"concat: target file \"\n          + target + \" is empty\");\n    }\n    if (trgInode.isWithSnapshot()) {\n      throw new HadoopIllegalArgumentException(\"concat: target file \"\n          + target + \" is in a snapshot\");\n    }\n\n    long blockSize \u003d trgInode.getPreferredBlockSize();\n\n    // check the end block to be full\n    final BlockInfo last \u003d trgInode.getLastBlock();\n    if(blockSize !\u003d last.getNumBytes()) {\n      throw new HadoopIllegalArgumentException(\"The last block in \" + target\n          + \" is not full; last block size \u003d \" + last.getNumBytes()\n          + \" but file block size \u003d \" + blockSize);\n    }\n\n    si.add(trgInode);\n    final short repl \u003d trgInode.getFileReplication();\n\n    // now check the srcs\n    boolean endSrc \u003d false; // final src file doesn\u0027t have to have full end block\n    for(int i\u003d0; i\u003c srcs.length; i++) {\n      String src \u003d srcs[i];\n      if(i\u003d\u003d srcs.length-1)\n        endSrc\u003dtrue;\n\n      final INodeFile srcInode \u003d INodeFile.valueOf(fsd.getINode4Write(src), src);\n      if(src.isEmpty()\n          || srcInode.isUnderConstruction()\n          || srcInode.numBlocks() \u003d\u003d 0) {\n        throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n            + \" is invalid or empty or underConstruction\");\n      }\n\n      // check replication and blocks size\n      if(repl !\u003d srcInode.getBlockReplication()) {\n        throw new HadoopIllegalArgumentException(\"concat: the source file \"\n            + src + \" and the target file \" + target\n            + \" should have the same replication: source replication is \"\n            + srcInode.getBlockReplication()\n            + \" but target replication is \" + repl);\n      }\n\n      //boolean endBlock\u003dfalse;\n      // verify that all the blocks are of the same length as target\n      // should be enough to check the end blocks\n      final BlockInfo[] srcBlocks \u003d srcInode.getBlocks();\n      int idx \u003d srcBlocks.length-1;\n      if(endSrc)\n        idx \u003d srcBlocks.length-2; // end block of endSrc is OK not to be full\n      if(idx \u003e\u003d 0 \u0026\u0026 srcBlocks[idx].getNumBytes() !\u003d blockSize) {\n        throw new HadoopIllegalArgumentException(\"concat: the source file \"\n            + src + \" and the target file \" + target\n            + \" should have the same blocks sizes: target block size is \"\n            + blockSize + \" but the size of source block \" + idx + \" is \"\n            + srcBlocks[idx].getNumBytes());\n      }\n\n      si.add(srcInode);\n    }\n\n    // make sure no two files are the same\n    if(si.size() \u003c srcs.length+1) { // trg + srcs\n      // it means at least two files are the same\n      throw new HadoopIllegalArgumentException(\n          \"concat: at least two of the source files are the same\");\n    }\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" +\n          Arrays.toString(srcs) + \" to \" + target);\n    }\n\n    long timestamp \u003d now();\n    fsd.writeLock();\n    try {\n      unprotectedConcat(fsd, target, srcs, timestamp);\n    } finally {\n      fsd.writeUnlock();\n    }\n    fsd.getEditLog().logConcat(target, srcs, timestamp, logRetryCache);\n    return fsd.getAuditFileInfo(target, false);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java",
      "extendedDetails": {}
    },
    "8caf537afabc70b0c74e0a29aea0cc2935ecb162": {
      "type": "Ymultichange(Ymovefromfile,Yreturntypechange,Ymodifierchange,Yexceptionschange,Ybodychange,Yrename,Yparameterchange)",
      "commitMessage": "HDFS-7436. Consolidate implementation of concat(). Contributed by Haohui Mai.\n",
      "commitDate": "24/11/14 3:42 PM",
      "commitName": "8caf537afabc70b0c74e0a29aea0cc2935ecb162",
      "commitAuthor": "Haohui Mai",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-7436. Consolidate implementation of concat(). Contributed by Haohui Mai.\n",
          "commitDate": "24/11/14 3:42 PM",
          "commitName": "8caf537afabc70b0c74e0a29aea0cc2935ecb162",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "24/11/14 2:58 PM",
          "commitNameOld": "e37a4ff0c1712a1cb80e0412ec53a5d10b8d30f9",
          "commitAuthorOld": "Zhijie Shen",
          "daysBetweenCommits": 0.03,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,115 +1,136 @@\n-  private void concatInternal(FSPermissionChecker pc, String target,\n-      String[] srcs, boolean logRetryCache) throws IOException,\n-      UnresolvedLinkException {\n-    assert hasWriteLock();\n+  static HdfsFileStatus concat(\n+    FSDirectory fsd, String target, String[] srcs,\n+    boolean logRetryCache) throws IOException {\n+    Preconditions.checkArgument(!target.isEmpty(), \"Target file name is empty\");\n+    Preconditions.checkArgument(srcs !\u003d null \u0026\u0026 srcs.length \u003e 0,\n+      \"No sources given\");\n+    assert srcs !\u003d null;\n+\n+    FSDirectory.LOG.debug(\"concat {} to {}\", Arrays.toString(srcs), target);\n+    // We require all files be in the same directory\n+    String trgParent \u003d\n+      target.substring(0, target.lastIndexOf(Path.SEPARATOR_CHAR));\n+    for (String s : srcs) {\n+      String srcParent \u003d s.substring(0, s.lastIndexOf(Path.SEPARATOR_CHAR));\n+      if (!srcParent.equals(trgParent)) {\n+        throw new IllegalArgumentException(\n+           \"Sources and target are not in the same directory\");\n+      }\n+    }\n \n     // write permission for the target\n-    if (isPermissionEnabled) {\n-      checkPathAccess(pc, target, FsAction.WRITE);\n+    if (fsd.isPermissionEnabled()) {\n+      FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n+      fsd.checkPathAccess(pc, target, FsAction.WRITE);\n \n       // and srcs\n       for(String aSrc: srcs) {\n-        checkPathAccess(pc, aSrc, FsAction.READ); // read the file\n-        checkParentAccess(pc, aSrc, FsAction.WRITE); // for delete \n+        fsd.checkPathAccess(pc, aSrc, FsAction.READ); // read the file\n+        fsd.checkParentAccess(pc, aSrc, FsAction.WRITE); // for delete\n       }\n     }\n \n     // to make sure no two files are the same\n     Set\u003cINode\u003e si \u003d new HashSet\u003cINode\u003e();\n \n     // we put the following prerequisite for the operation\n     // replication and blocks sizes should be the same for ALL the blocks\n \n     // check the target\n-    final INodesInPath trgIip \u003d dir.getINodesInPath4Write(target);\n-    if (dir.getEZForPath(trgIip) !\u003d null) {\n+    final INodesInPath trgIip \u003d fsd.getINodesInPath4Write(target);\n+    if (fsd.getEZForPath(trgIip) !\u003d null) {\n       throw new HadoopIllegalArgumentException(\n           \"concat can not be called for files in an encryption zone.\");\n     }\n-    final INodeFile trgInode \u003d INodeFile.valueOf(trgIip.getLastINode(),\n-        target);\n+    final INodeFile trgInode \u003d INodeFile.valueOf(trgIip.getLastINode(), target);\n     if(trgInode.isUnderConstruction()) {\n       throw new HadoopIllegalArgumentException(\"concat: target file \"\n           + target + \" is under construction\");\n     }\n     // per design target shouldn\u0027t be empty and all the blocks same size\n     if(trgInode.numBlocks() \u003d\u003d 0) {\n       throw new HadoopIllegalArgumentException(\"concat: target file \"\n           + target + \" is empty\");\n     }\n     if (trgInode.isWithSnapshot()) {\n       throw new HadoopIllegalArgumentException(\"concat: target file \"\n           + target + \" is in a snapshot\");\n     }\n \n     long blockSize \u003d trgInode.getPreferredBlockSize();\n \n     // check the end block to be full\n     final BlockInfo last \u003d trgInode.getLastBlock();\n     if(blockSize !\u003d last.getNumBytes()) {\n       throw new HadoopIllegalArgumentException(\"The last block in \" + target\n           + \" is not full; last block size \u003d \" + last.getNumBytes()\n           + \" but file block size \u003d \" + blockSize);\n     }\n \n     si.add(trgInode);\n     final short repl \u003d trgInode.getFileReplication();\n \n     // now check the srcs\n     boolean endSrc \u003d false; // final src file doesn\u0027t have to have full end block\n-    for(int i\u003d0; i\u003csrcs.length; i++) {\n+    for(int i\u003d0; i\u003c srcs.length; i++) {\n       String src \u003d srcs[i];\n-      if(i\u003d\u003dsrcs.length-1)\n+      if(i\u003d\u003d srcs.length-1)\n         endSrc\u003dtrue;\n \n-      final INodeFile srcInode \u003d INodeFile.valueOf(dir.getINode4Write(src), src);\n-      if(src.isEmpty() \n+      final INodeFile srcInode \u003d INodeFile.valueOf(fsd.getINode4Write(src), src);\n+      if(src.isEmpty()\n           || srcInode.isUnderConstruction()\n           || srcInode.numBlocks() \u003d\u003d 0) {\n         throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n             + \" is invalid or empty or underConstruction\");\n       }\n \n       // check replication and blocks size\n       if(repl !\u003d srcInode.getBlockReplication()) {\n         throw new HadoopIllegalArgumentException(\"concat: the source file \"\n             + src + \" and the target file \" + target\n             + \" should have the same replication: source replication is \"\n             + srcInode.getBlockReplication()\n             + \" but target replication is \" + repl);\n       }\n \n       //boolean endBlock\u003dfalse;\n       // verify that all the blocks are of the same length as target\n       // should be enough to check the end blocks\n       final BlockInfo[] srcBlocks \u003d srcInode.getBlocks();\n       int idx \u003d srcBlocks.length-1;\n       if(endSrc)\n         idx \u003d srcBlocks.length-2; // end block of endSrc is OK not to be full\n       if(idx \u003e\u003d 0 \u0026\u0026 srcBlocks[idx].getNumBytes() !\u003d blockSize) {\n         throw new HadoopIllegalArgumentException(\"concat: the source file \"\n             + src + \" and the target file \" + target\n             + \" should have the same blocks sizes: target block size is \"\n             + blockSize + \" but the size of source block \" + idx + \" is \"\n             + srcBlocks[idx].getNumBytes());\n       }\n \n       si.add(srcInode);\n     }\n \n     // make sure no two files are the same\n     if(si.size() \u003c srcs.length+1) { // trg + srcs\n       // it means at least two files are the same\n       throw new HadoopIllegalArgumentException(\n           \"concat: at least two of the source files are the same\");\n     }\n \n     if(NameNode.stateChangeLog.isDebugEnabled()) {\n-      NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" + \n+      NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" +\n           Arrays.toString(srcs) + \" to \" + target);\n     }\n \n     long timestamp \u003d now();\n-    dir.concat(target, srcs, timestamp);\n-    getEditLog().logConcat(target, srcs, timestamp, logRetryCache);\n+    fsd.writeLock();\n+    try {\n+      unprotectedConcat(fsd, target, srcs, timestamp);\n+    } finally {\n+      fsd.writeUnlock();\n+    }\n+    fsd.getEditLog().logConcat(target, srcs, timestamp, logRetryCache);\n+    return fsd.getAuditFileInfo(target, false);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static HdfsFileStatus concat(\n    FSDirectory fsd, String target, String[] srcs,\n    boolean logRetryCache) throws IOException {\n    Preconditions.checkArgument(!target.isEmpty(), \"Target file name is empty\");\n    Preconditions.checkArgument(srcs !\u003d null \u0026\u0026 srcs.length \u003e 0,\n      \"No sources given\");\n    assert srcs !\u003d null;\n\n    FSDirectory.LOG.debug(\"concat {} to {}\", Arrays.toString(srcs), target);\n    // We require all files be in the same directory\n    String trgParent \u003d\n      target.substring(0, target.lastIndexOf(Path.SEPARATOR_CHAR));\n    for (String s : srcs) {\n      String srcParent \u003d s.substring(0, s.lastIndexOf(Path.SEPARATOR_CHAR));\n      if (!srcParent.equals(trgParent)) {\n        throw new IllegalArgumentException(\n           \"Sources and target are not in the same directory\");\n      }\n    }\n\n    // write permission for the target\n    if (fsd.isPermissionEnabled()) {\n      FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n      fsd.checkPathAccess(pc, target, FsAction.WRITE);\n\n      // and srcs\n      for(String aSrc: srcs) {\n        fsd.checkPathAccess(pc, aSrc, FsAction.READ); // read the file\n        fsd.checkParentAccess(pc, aSrc, FsAction.WRITE); // for delete\n      }\n    }\n\n    // to make sure no two files are the same\n    Set\u003cINode\u003e si \u003d new HashSet\u003cINode\u003e();\n\n    // we put the following prerequisite for the operation\n    // replication and blocks sizes should be the same for ALL the blocks\n\n    // check the target\n    final INodesInPath trgIip \u003d fsd.getINodesInPath4Write(target);\n    if (fsd.getEZForPath(trgIip) !\u003d null) {\n      throw new HadoopIllegalArgumentException(\n          \"concat can not be called for files in an encryption zone.\");\n    }\n    final INodeFile trgInode \u003d INodeFile.valueOf(trgIip.getLastINode(), target);\n    if(trgInode.isUnderConstruction()) {\n      throw new HadoopIllegalArgumentException(\"concat: target file \"\n          + target + \" is under construction\");\n    }\n    // per design target shouldn\u0027t be empty and all the blocks same size\n    if(trgInode.numBlocks() \u003d\u003d 0) {\n      throw new HadoopIllegalArgumentException(\"concat: target file \"\n          + target + \" is empty\");\n    }\n    if (trgInode.isWithSnapshot()) {\n      throw new HadoopIllegalArgumentException(\"concat: target file \"\n          + target + \" is in a snapshot\");\n    }\n\n    long blockSize \u003d trgInode.getPreferredBlockSize();\n\n    // check the end block to be full\n    final BlockInfo last \u003d trgInode.getLastBlock();\n    if(blockSize !\u003d last.getNumBytes()) {\n      throw new HadoopIllegalArgumentException(\"The last block in \" + target\n          + \" is not full; last block size \u003d \" + last.getNumBytes()\n          + \" but file block size \u003d \" + blockSize);\n    }\n\n    si.add(trgInode);\n    final short repl \u003d trgInode.getFileReplication();\n\n    // now check the srcs\n    boolean endSrc \u003d false; // final src file doesn\u0027t have to have full end block\n    for(int i\u003d0; i\u003c srcs.length; i++) {\n      String src \u003d srcs[i];\n      if(i\u003d\u003d srcs.length-1)\n        endSrc\u003dtrue;\n\n      final INodeFile srcInode \u003d INodeFile.valueOf(fsd.getINode4Write(src), src);\n      if(src.isEmpty()\n          || srcInode.isUnderConstruction()\n          || srcInode.numBlocks() \u003d\u003d 0) {\n        throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n            + \" is invalid or empty or underConstruction\");\n      }\n\n      // check replication and blocks size\n      if(repl !\u003d srcInode.getBlockReplication()) {\n        throw new HadoopIllegalArgumentException(\"concat: the source file \"\n            + src + \" and the target file \" + target\n            + \" should have the same replication: source replication is \"\n            + srcInode.getBlockReplication()\n            + \" but target replication is \" + repl);\n      }\n\n      //boolean endBlock\u003dfalse;\n      // verify that all the blocks are of the same length as target\n      // should be enough to check the end blocks\n      final BlockInfo[] srcBlocks \u003d srcInode.getBlocks();\n      int idx \u003d srcBlocks.length-1;\n      if(endSrc)\n        idx \u003d srcBlocks.length-2; // end block of endSrc is OK not to be full\n      if(idx \u003e\u003d 0 \u0026\u0026 srcBlocks[idx].getNumBytes() !\u003d blockSize) {\n        throw new HadoopIllegalArgumentException(\"concat: the source file \"\n            + src + \" and the target file \" + target\n            + \" should have the same blocks sizes: target block size is \"\n            + blockSize + \" but the size of source block \" + idx + \" is \"\n            + srcBlocks[idx].getNumBytes());\n      }\n\n      si.add(srcInode);\n    }\n\n    // make sure no two files are the same\n    if(si.size() \u003c srcs.length+1) { // trg + srcs\n      // it means at least two files are the same\n      throw new HadoopIllegalArgumentException(\n          \"concat: at least two of the source files are the same\");\n    }\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" +\n          Arrays.toString(srcs) + \" to \" + target);\n    }\n\n    long timestamp \u003d now();\n    fsd.writeLock();\n    try {\n      unprotectedConcat(fsd, target, srcs, timestamp);\n    } finally {\n      fsd.writeUnlock();\n    }\n    fsd.getEditLog().logConcat(target, srcs, timestamp, logRetryCache);\n    return fsd.getAuditFileInfo(target, false);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java",
            "oldMethodName": "concatInternal",
            "newMethodName": "concat"
          }
        },
        {
          "type": "Yreturntypechange",
          "commitMessage": "HDFS-7436. Consolidate implementation of concat(). Contributed by Haohui Mai.\n",
          "commitDate": "24/11/14 3:42 PM",
          "commitName": "8caf537afabc70b0c74e0a29aea0cc2935ecb162",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "24/11/14 2:58 PM",
          "commitNameOld": "e37a4ff0c1712a1cb80e0412ec53a5d10b8d30f9",
          "commitAuthorOld": "Zhijie Shen",
          "daysBetweenCommits": 0.03,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,115 +1,136 @@\n-  private void concatInternal(FSPermissionChecker pc, String target,\n-      String[] srcs, boolean logRetryCache) throws IOException,\n-      UnresolvedLinkException {\n-    assert hasWriteLock();\n+  static HdfsFileStatus concat(\n+    FSDirectory fsd, String target, String[] srcs,\n+    boolean logRetryCache) throws IOException {\n+    Preconditions.checkArgument(!target.isEmpty(), \"Target file name is empty\");\n+    Preconditions.checkArgument(srcs !\u003d null \u0026\u0026 srcs.length \u003e 0,\n+      \"No sources given\");\n+    assert srcs !\u003d null;\n+\n+    FSDirectory.LOG.debug(\"concat {} to {}\", Arrays.toString(srcs), target);\n+    // We require all files be in the same directory\n+    String trgParent \u003d\n+      target.substring(0, target.lastIndexOf(Path.SEPARATOR_CHAR));\n+    for (String s : srcs) {\n+      String srcParent \u003d s.substring(0, s.lastIndexOf(Path.SEPARATOR_CHAR));\n+      if (!srcParent.equals(trgParent)) {\n+        throw new IllegalArgumentException(\n+           \"Sources and target are not in the same directory\");\n+      }\n+    }\n \n     // write permission for the target\n-    if (isPermissionEnabled) {\n-      checkPathAccess(pc, target, FsAction.WRITE);\n+    if (fsd.isPermissionEnabled()) {\n+      FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n+      fsd.checkPathAccess(pc, target, FsAction.WRITE);\n \n       // and srcs\n       for(String aSrc: srcs) {\n-        checkPathAccess(pc, aSrc, FsAction.READ); // read the file\n-        checkParentAccess(pc, aSrc, FsAction.WRITE); // for delete \n+        fsd.checkPathAccess(pc, aSrc, FsAction.READ); // read the file\n+        fsd.checkParentAccess(pc, aSrc, FsAction.WRITE); // for delete\n       }\n     }\n \n     // to make sure no two files are the same\n     Set\u003cINode\u003e si \u003d new HashSet\u003cINode\u003e();\n \n     // we put the following prerequisite for the operation\n     // replication and blocks sizes should be the same for ALL the blocks\n \n     // check the target\n-    final INodesInPath trgIip \u003d dir.getINodesInPath4Write(target);\n-    if (dir.getEZForPath(trgIip) !\u003d null) {\n+    final INodesInPath trgIip \u003d fsd.getINodesInPath4Write(target);\n+    if (fsd.getEZForPath(trgIip) !\u003d null) {\n       throw new HadoopIllegalArgumentException(\n           \"concat can not be called for files in an encryption zone.\");\n     }\n-    final INodeFile trgInode \u003d INodeFile.valueOf(trgIip.getLastINode(),\n-        target);\n+    final INodeFile trgInode \u003d INodeFile.valueOf(trgIip.getLastINode(), target);\n     if(trgInode.isUnderConstruction()) {\n       throw new HadoopIllegalArgumentException(\"concat: target file \"\n           + target + \" is under construction\");\n     }\n     // per design target shouldn\u0027t be empty and all the blocks same size\n     if(trgInode.numBlocks() \u003d\u003d 0) {\n       throw new HadoopIllegalArgumentException(\"concat: target file \"\n           + target + \" is empty\");\n     }\n     if (trgInode.isWithSnapshot()) {\n       throw new HadoopIllegalArgumentException(\"concat: target file \"\n           + target + \" is in a snapshot\");\n     }\n \n     long blockSize \u003d trgInode.getPreferredBlockSize();\n \n     // check the end block to be full\n     final BlockInfo last \u003d trgInode.getLastBlock();\n     if(blockSize !\u003d last.getNumBytes()) {\n       throw new HadoopIllegalArgumentException(\"The last block in \" + target\n           + \" is not full; last block size \u003d \" + last.getNumBytes()\n           + \" but file block size \u003d \" + blockSize);\n     }\n \n     si.add(trgInode);\n     final short repl \u003d trgInode.getFileReplication();\n \n     // now check the srcs\n     boolean endSrc \u003d false; // final src file doesn\u0027t have to have full end block\n-    for(int i\u003d0; i\u003csrcs.length; i++) {\n+    for(int i\u003d0; i\u003c srcs.length; i++) {\n       String src \u003d srcs[i];\n-      if(i\u003d\u003dsrcs.length-1)\n+      if(i\u003d\u003d srcs.length-1)\n         endSrc\u003dtrue;\n \n-      final INodeFile srcInode \u003d INodeFile.valueOf(dir.getINode4Write(src), src);\n-      if(src.isEmpty() \n+      final INodeFile srcInode \u003d INodeFile.valueOf(fsd.getINode4Write(src), src);\n+      if(src.isEmpty()\n           || srcInode.isUnderConstruction()\n           || srcInode.numBlocks() \u003d\u003d 0) {\n         throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n             + \" is invalid or empty or underConstruction\");\n       }\n \n       // check replication and blocks size\n       if(repl !\u003d srcInode.getBlockReplication()) {\n         throw new HadoopIllegalArgumentException(\"concat: the source file \"\n             + src + \" and the target file \" + target\n             + \" should have the same replication: source replication is \"\n             + srcInode.getBlockReplication()\n             + \" but target replication is \" + repl);\n       }\n \n       //boolean endBlock\u003dfalse;\n       // verify that all the blocks are of the same length as target\n       // should be enough to check the end blocks\n       final BlockInfo[] srcBlocks \u003d srcInode.getBlocks();\n       int idx \u003d srcBlocks.length-1;\n       if(endSrc)\n         idx \u003d srcBlocks.length-2; // end block of endSrc is OK not to be full\n       if(idx \u003e\u003d 0 \u0026\u0026 srcBlocks[idx].getNumBytes() !\u003d blockSize) {\n         throw new HadoopIllegalArgumentException(\"concat: the source file \"\n             + src + \" and the target file \" + target\n             + \" should have the same blocks sizes: target block size is \"\n             + blockSize + \" but the size of source block \" + idx + \" is \"\n             + srcBlocks[idx].getNumBytes());\n       }\n \n       si.add(srcInode);\n     }\n \n     // make sure no two files are the same\n     if(si.size() \u003c srcs.length+1) { // trg + srcs\n       // it means at least two files are the same\n       throw new HadoopIllegalArgumentException(\n           \"concat: at least two of the source files are the same\");\n     }\n \n     if(NameNode.stateChangeLog.isDebugEnabled()) {\n-      NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" + \n+      NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" +\n           Arrays.toString(srcs) + \" to \" + target);\n     }\n \n     long timestamp \u003d now();\n-    dir.concat(target, srcs, timestamp);\n-    getEditLog().logConcat(target, srcs, timestamp, logRetryCache);\n+    fsd.writeLock();\n+    try {\n+      unprotectedConcat(fsd, target, srcs, timestamp);\n+    } finally {\n+      fsd.writeUnlock();\n+    }\n+    fsd.getEditLog().logConcat(target, srcs, timestamp, logRetryCache);\n+    return fsd.getAuditFileInfo(target, false);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static HdfsFileStatus concat(\n    FSDirectory fsd, String target, String[] srcs,\n    boolean logRetryCache) throws IOException {\n    Preconditions.checkArgument(!target.isEmpty(), \"Target file name is empty\");\n    Preconditions.checkArgument(srcs !\u003d null \u0026\u0026 srcs.length \u003e 0,\n      \"No sources given\");\n    assert srcs !\u003d null;\n\n    FSDirectory.LOG.debug(\"concat {} to {}\", Arrays.toString(srcs), target);\n    // We require all files be in the same directory\n    String trgParent \u003d\n      target.substring(0, target.lastIndexOf(Path.SEPARATOR_CHAR));\n    for (String s : srcs) {\n      String srcParent \u003d s.substring(0, s.lastIndexOf(Path.SEPARATOR_CHAR));\n      if (!srcParent.equals(trgParent)) {\n        throw new IllegalArgumentException(\n           \"Sources and target are not in the same directory\");\n      }\n    }\n\n    // write permission for the target\n    if (fsd.isPermissionEnabled()) {\n      FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n      fsd.checkPathAccess(pc, target, FsAction.WRITE);\n\n      // and srcs\n      for(String aSrc: srcs) {\n        fsd.checkPathAccess(pc, aSrc, FsAction.READ); // read the file\n        fsd.checkParentAccess(pc, aSrc, FsAction.WRITE); // for delete\n      }\n    }\n\n    // to make sure no two files are the same\n    Set\u003cINode\u003e si \u003d new HashSet\u003cINode\u003e();\n\n    // we put the following prerequisite for the operation\n    // replication and blocks sizes should be the same for ALL the blocks\n\n    // check the target\n    final INodesInPath trgIip \u003d fsd.getINodesInPath4Write(target);\n    if (fsd.getEZForPath(trgIip) !\u003d null) {\n      throw new HadoopIllegalArgumentException(\n          \"concat can not be called for files in an encryption zone.\");\n    }\n    final INodeFile trgInode \u003d INodeFile.valueOf(trgIip.getLastINode(), target);\n    if(trgInode.isUnderConstruction()) {\n      throw new HadoopIllegalArgumentException(\"concat: target file \"\n          + target + \" is under construction\");\n    }\n    // per design target shouldn\u0027t be empty and all the blocks same size\n    if(trgInode.numBlocks() \u003d\u003d 0) {\n      throw new HadoopIllegalArgumentException(\"concat: target file \"\n          + target + \" is empty\");\n    }\n    if (trgInode.isWithSnapshot()) {\n      throw new HadoopIllegalArgumentException(\"concat: target file \"\n          + target + \" is in a snapshot\");\n    }\n\n    long blockSize \u003d trgInode.getPreferredBlockSize();\n\n    // check the end block to be full\n    final BlockInfo last \u003d trgInode.getLastBlock();\n    if(blockSize !\u003d last.getNumBytes()) {\n      throw new HadoopIllegalArgumentException(\"The last block in \" + target\n          + \" is not full; last block size \u003d \" + last.getNumBytes()\n          + \" but file block size \u003d \" + blockSize);\n    }\n\n    si.add(trgInode);\n    final short repl \u003d trgInode.getFileReplication();\n\n    // now check the srcs\n    boolean endSrc \u003d false; // final src file doesn\u0027t have to have full end block\n    for(int i\u003d0; i\u003c srcs.length; i++) {\n      String src \u003d srcs[i];\n      if(i\u003d\u003d srcs.length-1)\n        endSrc\u003dtrue;\n\n      final INodeFile srcInode \u003d INodeFile.valueOf(fsd.getINode4Write(src), src);\n      if(src.isEmpty()\n          || srcInode.isUnderConstruction()\n          || srcInode.numBlocks() \u003d\u003d 0) {\n        throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n            + \" is invalid or empty or underConstruction\");\n      }\n\n      // check replication and blocks size\n      if(repl !\u003d srcInode.getBlockReplication()) {\n        throw new HadoopIllegalArgumentException(\"concat: the source file \"\n            + src + \" and the target file \" + target\n            + \" should have the same replication: source replication is \"\n            + srcInode.getBlockReplication()\n            + \" but target replication is \" + repl);\n      }\n\n      //boolean endBlock\u003dfalse;\n      // verify that all the blocks are of the same length as target\n      // should be enough to check the end blocks\n      final BlockInfo[] srcBlocks \u003d srcInode.getBlocks();\n      int idx \u003d srcBlocks.length-1;\n      if(endSrc)\n        idx \u003d srcBlocks.length-2; // end block of endSrc is OK not to be full\n      if(idx \u003e\u003d 0 \u0026\u0026 srcBlocks[idx].getNumBytes() !\u003d blockSize) {\n        throw new HadoopIllegalArgumentException(\"concat: the source file \"\n            + src + \" and the target file \" + target\n            + \" should have the same blocks sizes: target block size is \"\n            + blockSize + \" but the size of source block \" + idx + \" is \"\n            + srcBlocks[idx].getNumBytes());\n      }\n\n      si.add(srcInode);\n    }\n\n    // make sure no two files are the same\n    if(si.size() \u003c srcs.length+1) { // trg + srcs\n      // it means at least two files are the same\n      throw new HadoopIllegalArgumentException(\n          \"concat: at least two of the source files are the same\");\n    }\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" +\n          Arrays.toString(srcs) + \" to \" + target);\n    }\n\n    long timestamp \u003d now();\n    fsd.writeLock();\n    try {\n      unprotectedConcat(fsd, target, srcs, timestamp);\n    } finally {\n      fsd.writeUnlock();\n    }\n    fsd.getEditLog().logConcat(target, srcs, timestamp, logRetryCache);\n    return fsd.getAuditFileInfo(target, false);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java",
          "extendedDetails": {
            "oldValue": "void",
            "newValue": "HdfsFileStatus"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-7436. Consolidate implementation of concat(). Contributed by Haohui Mai.\n",
          "commitDate": "24/11/14 3:42 PM",
          "commitName": "8caf537afabc70b0c74e0a29aea0cc2935ecb162",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "24/11/14 2:58 PM",
          "commitNameOld": "e37a4ff0c1712a1cb80e0412ec53a5d10b8d30f9",
          "commitAuthorOld": "Zhijie Shen",
          "daysBetweenCommits": 0.03,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,115 +1,136 @@\n-  private void concatInternal(FSPermissionChecker pc, String target,\n-      String[] srcs, boolean logRetryCache) throws IOException,\n-      UnresolvedLinkException {\n-    assert hasWriteLock();\n+  static HdfsFileStatus concat(\n+    FSDirectory fsd, String target, String[] srcs,\n+    boolean logRetryCache) throws IOException {\n+    Preconditions.checkArgument(!target.isEmpty(), \"Target file name is empty\");\n+    Preconditions.checkArgument(srcs !\u003d null \u0026\u0026 srcs.length \u003e 0,\n+      \"No sources given\");\n+    assert srcs !\u003d null;\n+\n+    FSDirectory.LOG.debug(\"concat {} to {}\", Arrays.toString(srcs), target);\n+    // We require all files be in the same directory\n+    String trgParent \u003d\n+      target.substring(0, target.lastIndexOf(Path.SEPARATOR_CHAR));\n+    for (String s : srcs) {\n+      String srcParent \u003d s.substring(0, s.lastIndexOf(Path.SEPARATOR_CHAR));\n+      if (!srcParent.equals(trgParent)) {\n+        throw new IllegalArgumentException(\n+           \"Sources and target are not in the same directory\");\n+      }\n+    }\n \n     // write permission for the target\n-    if (isPermissionEnabled) {\n-      checkPathAccess(pc, target, FsAction.WRITE);\n+    if (fsd.isPermissionEnabled()) {\n+      FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n+      fsd.checkPathAccess(pc, target, FsAction.WRITE);\n \n       // and srcs\n       for(String aSrc: srcs) {\n-        checkPathAccess(pc, aSrc, FsAction.READ); // read the file\n-        checkParentAccess(pc, aSrc, FsAction.WRITE); // for delete \n+        fsd.checkPathAccess(pc, aSrc, FsAction.READ); // read the file\n+        fsd.checkParentAccess(pc, aSrc, FsAction.WRITE); // for delete\n       }\n     }\n \n     // to make sure no two files are the same\n     Set\u003cINode\u003e si \u003d new HashSet\u003cINode\u003e();\n \n     // we put the following prerequisite for the operation\n     // replication and blocks sizes should be the same for ALL the blocks\n \n     // check the target\n-    final INodesInPath trgIip \u003d dir.getINodesInPath4Write(target);\n-    if (dir.getEZForPath(trgIip) !\u003d null) {\n+    final INodesInPath trgIip \u003d fsd.getINodesInPath4Write(target);\n+    if (fsd.getEZForPath(trgIip) !\u003d null) {\n       throw new HadoopIllegalArgumentException(\n           \"concat can not be called for files in an encryption zone.\");\n     }\n-    final INodeFile trgInode \u003d INodeFile.valueOf(trgIip.getLastINode(),\n-        target);\n+    final INodeFile trgInode \u003d INodeFile.valueOf(trgIip.getLastINode(), target);\n     if(trgInode.isUnderConstruction()) {\n       throw new HadoopIllegalArgumentException(\"concat: target file \"\n           + target + \" is under construction\");\n     }\n     // per design target shouldn\u0027t be empty and all the blocks same size\n     if(trgInode.numBlocks() \u003d\u003d 0) {\n       throw new HadoopIllegalArgumentException(\"concat: target file \"\n           + target + \" is empty\");\n     }\n     if (trgInode.isWithSnapshot()) {\n       throw new HadoopIllegalArgumentException(\"concat: target file \"\n           + target + \" is in a snapshot\");\n     }\n \n     long blockSize \u003d trgInode.getPreferredBlockSize();\n \n     // check the end block to be full\n     final BlockInfo last \u003d trgInode.getLastBlock();\n     if(blockSize !\u003d last.getNumBytes()) {\n       throw new HadoopIllegalArgumentException(\"The last block in \" + target\n           + \" is not full; last block size \u003d \" + last.getNumBytes()\n           + \" but file block size \u003d \" + blockSize);\n     }\n \n     si.add(trgInode);\n     final short repl \u003d trgInode.getFileReplication();\n \n     // now check the srcs\n     boolean endSrc \u003d false; // final src file doesn\u0027t have to have full end block\n-    for(int i\u003d0; i\u003csrcs.length; i++) {\n+    for(int i\u003d0; i\u003c srcs.length; i++) {\n       String src \u003d srcs[i];\n-      if(i\u003d\u003dsrcs.length-1)\n+      if(i\u003d\u003d srcs.length-1)\n         endSrc\u003dtrue;\n \n-      final INodeFile srcInode \u003d INodeFile.valueOf(dir.getINode4Write(src), src);\n-      if(src.isEmpty() \n+      final INodeFile srcInode \u003d INodeFile.valueOf(fsd.getINode4Write(src), src);\n+      if(src.isEmpty()\n           || srcInode.isUnderConstruction()\n           || srcInode.numBlocks() \u003d\u003d 0) {\n         throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n             + \" is invalid or empty or underConstruction\");\n       }\n \n       // check replication and blocks size\n       if(repl !\u003d srcInode.getBlockReplication()) {\n         throw new HadoopIllegalArgumentException(\"concat: the source file \"\n             + src + \" and the target file \" + target\n             + \" should have the same replication: source replication is \"\n             + srcInode.getBlockReplication()\n             + \" but target replication is \" + repl);\n       }\n \n       //boolean endBlock\u003dfalse;\n       // verify that all the blocks are of the same length as target\n       // should be enough to check the end blocks\n       final BlockInfo[] srcBlocks \u003d srcInode.getBlocks();\n       int idx \u003d srcBlocks.length-1;\n       if(endSrc)\n         idx \u003d srcBlocks.length-2; // end block of endSrc is OK not to be full\n       if(idx \u003e\u003d 0 \u0026\u0026 srcBlocks[idx].getNumBytes() !\u003d blockSize) {\n         throw new HadoopIllegalArgumentException(\"concat: the source file \"\n             + src + \" and the target file \" + target\n             + \" should have the same blocks sizes: target block size is \"\n             + blockSize + \" but the size of source block \" + idx + \" is \"\n             + srcBlocks[idx].getNumBytes());\n       }\n \n       si.add(srcInode);\n     }\n \n     // make sure no two files are the same\n     if(si.size() \u003c srcs.length+1) { // trg + srcs\n       // it means at least two files are the same\n       throw new HadoopIllegalArgumentException(\n           \"concat: at least two of the source files are the same\");\n     }\n \n     if(NameNode.stateChangeLog.isDebugEnabled()) {\n-      NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" + \n+      NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" +\n           Arrays.toString(srcs) + \" to \" + target);\n     }\n \n     long timestamp \u003d now();\n-    dir.concat(target, srcs, timestamp);\n-    getEditLog().logConcat(target, srcs, timestamp, logRetryCache);\n+    fsd.writeLock();\n+    try {\n+      unprotectedConcat(fsd, target, srcs, timestamp);\n+    } finally {\n+      fsd.writeUnlock();\n+    }\n+    fsd.getEditLog().logConcat(target, srcs, timestamp, logRetryCache);\n+    return fsd.getAuditFileInfo(target, false);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static HdfsFileStatus concat(\n    FSDirectory fsd, String target, String[] srcs,\n    boolean logRetryCache) throws IOException {\n    Preconditions.checkArgument(!target.isEmpty(), \"Target file name is empty\");\n    Preconditions.checkArgument(srcs !\u003d null \u0026\u0026 srcs.length \u003e 0,\n      \"No sources given\");\n    assert srcs !\u003d null;\n\n    FSDirectory.LOG.debug(\"concat {} to {}\", Arrays.toString(srcs), target);\n    // We require all files be in the same directory\n    String trgParent \u003d\n      target.substring(0, target.lastIndexOf(Path.SEPARATOR_CHAR));\n    for (String s : srcs) {\n      String srcParent \u003d s.substring(0, s.lastIndexOf(Path.SEPARATOR_CHAR));\n      if (!srcParent.equals(trgParent)) {\n        throw new IllegalArgumentException(\n           \"Sources and target are not in the same directory\");\n      }\n    }\n\n    // write permission for the target\n    if (fsd.isPermissionEnabled()) {\n      FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n      fsd.checkPathAccess(pc, target, FsAction.WRITE);\n\n      // and srcs\n      for(String aSrc: srcs) {\n        fsd.checkPathAccess(pc, aSrc, FsAction.READ); // read the file\n        fsd.checkParentAccess(pc, aSrc, FsAction.WRITE); // for delete\n      }\n    }\n\n    // to make sure no two files are the same\n    Set\u003cINode\u003e si \u003d new HashSet\u003cINode\u003e();\n\n    // we put the following prerequisite for the operation\n    // replication and blocks sizes should be the same for ALL the blocks\n\n    // check the target\n    final INodesInPath trgIip \u003d fsd.getINodesInPath4Write(target);\n    if (fsd.getEZForPath(trgIip) !\u003d null) {\n      throw new HadoopIllegalArgumentException(\n          \"concat can not be called for files in an encryption zone.\");\n    }\n    final INodeFile trgInode \u003d INodeFile.valueOf(trgIip.getLastINode(), target);\n    if(trgInode.isUnderConstruction()) {\n      throw new HadoopIllegalArgumentException(\"concat: target file \"\n          + target + \" is under construction\");\n    }\n    // per design target shouldn\u0027t be empty and all the blocks same size\n    if(trgInode.numBlocks() \u003d\u003d 0) {\n      throw new HadoopIllegalArgumentException(\"concat: target file \"\n          + target + \" is empty\");\n    }\n    if (trgInode.isWithSnapshot()) {\n      throw new HadoopIllegalArgumentException(\"concat: target file \"\n          + target + \" is in a snapshot\");\n    }\n\n    long blockSize \u003d trgInode.getPreferredBlockSize();\n\n    // check the end block to be full\n    final BlockInfo last \u003d trgInode.getLastBlock();\n    if(blockSize !\u003d last.getNumBytes()) {\n      throw new HadoopIllegalArgumentException(\"The last block in \" + target\n          + \" is not full; last block size \u003d \" + last.getNumBytes()\n          + \" but file block size \u003d \" + blockSize);\n    }\n\n    si.add(trgInode);\n    final short repl \u003d trgInode.getFileReplication();\n\n    // now check the srcs\n    boolean endSrc \u003d false; // final src file doesn\u0027t have to have full end block\n    for(int i\u003d0; i\u003c srcs.length; i++) {\n      String src \u003d srcs[i];\n      if(i\u003d\u003d srcs.length-1)\n        endSrc\u003dtrue;\n\n      final INodeFile srcInode \u003d INodeFile.valueOf(fsd.getINode4Write(src), src);\n      if(src.isEmpty()\n          || srcInode.isUnderConstruction()\n          || srcInode.numBlocks() \u003d\u003d 0) {\n        throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n            + \" is invalid or empty or underConstruction\");\n      }\n\n      // check replication and blocks size\n      if(repl !\u003d srcInode.getBlockReplication()) {\n        throw new HadoopIllegalArgumentException(\"concat: the source file \"\n            + src + \" and the target file \" + target\n            + \" should have the same replication: source replication is \"\n            + srcInode.getBlockReplication()\n            + \" but target replication is \" + repl);\n      }\n\n      //boolean endBlock\u003dfalse;\n      // verify that all the blocks are of the same length as target\n      // should be enough to check the end blocks\n      final BlockInfo[] srcBlocks \u003d srcInode.getBlocks();\n      int idx \u003d srcBlocks.length-1;\n      if(endSrc)\n        idx \u003d srcBlocks.length-2; // end block of endSrc is OK not to be full\n      if(idx \u003e\u003d 0 \u0026\u0026 srcBlocks[idx].getNumBytes() !\u003d blockSize) {\n        throw new HadoopIllegalArgumentException(\"concat: the source file \"\n            + src + \" and the target file \" + target\n            + \" should have the same blocks sizes: target block size is \"\n            + blockSize + \" but the size of source block \" + idx + \" is \"\n            + srcBlocks[idx].getNumBytes());\n      }\n\n      si.add(srcInode);\n    }\n\n    // make sure no two files are the same\n    if(si.size() \u003c srcs.length+1) { // trg + srcs\n      // it means at least two files are the same\n      throw new HadoopIllegalArgumentException(\n          \"concat: at least two of the source files are the same\");\n    }\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" +\n          Arrays.toString(srcs) + \" to \" + target);\n    }\n\n    long timestamp \u003d now();\n    fsd.writeLock();\n    try {\n      unprotectedConcat(fsd, target, srcs, timestamp);\n    } finally {\n      fsd.writeUnlock();\n    }\n    fsd.getEditLog().logConcat(target, srcs, timestamp, logRetryCache);\n    return fsd.getAuditFileInfo(target, false);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java",
          "extendedDetails": {
            "oldValue": "[private]",
            "newValue": "[static]"
          }
        },
        {
          "type": "Yexceptionschange",
          "commitMessage": "HDFS-7436. Consolidate implementation of concat(). Contributed by Haohui Mai.\n",
          "commitDate": "24/11/14 3:42 PM",
          "commitName": "8caf537afabc70b0c74e0a29aea0cc2935ecb162",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "24/11/14 2:58 PM",
          "commitNameOld": "e37a4ff0c1712a1cb80e0412ec53a5d10b8d30f9",
          "commitAuthorOld": "Zhijie Shen",
          "daysBetweenCommits": 0.03,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,115 +1,136 @@\n-  private void concatInternal(FSPermissionChecker pc, String target,\n-      String[] srcs, boolean logRetryCache) throws IOException,\n-      UnresolvedLinkException {\n-    assert hasWriteLock();\n+  static HdfsFileStatus concat(\n+    FSDirectory fsd, String target, String[] srcs,\n+    boolean logRetryCache) throws IOException {\n+    Preconditions.checkArgument(!target.isEmpty(), \"Target file name is empty\");\n+    Preconditions.checkArgument(srcs !\u003d null \u0026\u0026 srcs.length \u003e 0,\n+      \"No sources given\");\n+    assert srcs !\u003d null;\n+\n+    FSDirectory.LOG.debug(\"concat {} to {}\", Arrays.toString(srcs), target);\n+    // We require all files be in the same directory\n+    String trgParent \u003d\n+      target.substring(0, target.lastIndexOf(Path.SEPARATOR_CHAR));\n+    for (String s : srcs) {\n+      String srcParent \u003d s.substring(0, s.lastIndexOf(Path.SEPARATOR_CHAR));\n+      if (!srcParent.equals(trgParent)) {\n+        throw new IllegalArgumentException(\n+           \"Sources and target are not in the same directory\");\n+      }\n+    }\n \n     // write permission for the target\n-    if (isPermissionEnabled) {\n-      checkPathAccess(pc, target, FsAction.WRITE);\n+    if (fsd.isPermissionEnabled()) {\n+      FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n+      fsd.checkPathAccess(pc, target, FsAction.WRITE);\n \n       // and srcs\n       for(String aSrc: srcs) {\n-        checkPathAccess(pc, aSrc, FsAction.READ); // read the file\n-        checkParentAccess(pc, aSrc, FsAction.WRITE); // for delete \n+        fsd.checkPathAccess(pc, aSrc, FsAction.READ); // read the file\n+        fsd.checkParentAccess(pc, aSrc, FsAction.WRITE); // for delete\n       }\n     }\n \n     // to make sure no two files are the same\n     Set\u003cINode\u003e si \u003d new HashSet\u003cINode\u003e();\n \n     // we put the following prerequisite for the operation\n     // replication and blocks sizes should be the same for ALL the blocks\n \n     // check the target\n-    final INodesInPath trgIip \u003d dir.getINodesInPath4Write(target);\n-    if (dir.getEZForPath(trgIip) !\u003d null) {\n+    final INodesInPath trgIip \u003d fsd.getINodesInPath4Write(target);\n+    if (fsd.getEZForPath(trgIip) !\u003d null) {\n       throw new HadoopIllegalArgumentException(\n           \"concat can not be called for files in an encryption zone.\");\n     }\n-    final INodeFile trgInode \u003d INodeFile.valueOf(trgIip.getLastINode(),\n-        target);\n+    final INodeFile trgInode \u003d INodeFile.valueOf(trgIip.getLastINode(), target);\n     if(trgInode.isUnderConstruction()) {\n       throw new HadoopIllegalArgumentException(\"concat: target file \"\n           + target + \" is under construction\");\n     }\n     // per design target shouldn\u0027t be empty and all the blocks same size\n     if(trgInode.numBlocks() \u003d\u003d 0) {\n       throw new HadoopIllegalArgumentException(\"concat: target file \"\n           + target + \" is empty\");\n     }\n     if (trgInode.isWithSnapshot()) {\n       throw new HadoopIllegalArgumentException(\"concat: target file \"\n           + target + \" is in a snapshot\");\n     }\n \n     long blockSize \u003d trgInode.getPreferredBlockSize();\n \n     // check the end block to be full\n     final BlockInfo last \u003d trgInode.getLastBlock();\n     if(blockSize !\u003d last.getNumBytes()) {\n       throw new HadoopIllegalArgumentException(\"The last block in \" + target\n           + \" is not full; last block size \u003d \" + last.getNumBytes()\n           + \" but file block size \u003d \" + blockSize);\n     }\n \n     si.add(trgInode);\n     final short repl \u003d trgInode.getFileReplication();\n \n     // now check the srcs\n     boolean endSrc \u003d false; // final src file doesn\u0027t have to have full end block\n-    for(int i\u003d0; i\u003csrcs.length; i++) {\n+    for(int i\u003d0; i\u003c srcs.length; i++) {\n       String src \u003d srcs[i];\n-      if(i\u003d\u003dsrcs.length-1)\n+      if(i\u003d\u003d srcs.length-1)\n         endSrc\u003dtrue;\n \n-      final INodeFile srcInode \u003d INodeFile.valueOf(dir.getINode4Write(src), src);\n-      if(src.isEmpty() \n+      final INodeFile srcInode \u003d INodeFile.valueOf(fsd.getINode4Write(src), src);\n+      if(src.isEmpty()\n           || srcInode.isUnderConstruction()\n           || srcInode.numBlocks() \u003d\u003d 0) {\n         throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n             + \" is invalid or empty or underConstruction\");\n       }\n \n       // check replication and blocks size\n       if(repl !\u003d srcInode.getBlockReplication()) {\n         throw new HadoopIllegalArgumentException(\"concat: the source file \"\n             + src + \" and the target file \" + target\n             + \" should have the same replication: source replication is \"\n             + srcInode.getBlockReplication()\n             + \" but target replication is \" + repl);\n       }\n \n       //boolean endBlock\u003dfalse;\n       // verify that all the blocks are of the same length as target\n       // should be enough to check the end blocks\n       final BlockInfo[] srcBlocks \u003d srcInode.getBlocks();\n       int idx \u003d srcBlocks.length-1;\n       if(endSrc)\n         idx \u003d srcBlocks.length-2; // end block of endSrc is OK not to be full\n       if(idx \u003e\u003d 0 \u0026\u0026 srcBlocks[idx].getNumBytes() !\u003d blockSize) {\n         throw new HadoopIllegalArgumentException(\"concat: the source file \"\n             + src + \" and the target file \" + target\n             + \" should have the same blocks sizes: target block size is \"\n             + blockSize + \" but the size of source block \" + idx + \" is \"\n             + srcBlocks[idx].getNumBytes());\n       }\n \n       si.add(srcInode);\n     }\n \n     // make sure no two files are the same\n     if(si.size() \u003c srcs.length+1) { // trg + srcs\n       // it means at least two files are the same\n       throw new HadoopIllegalArgumentException(\n           \"concat: at least two of the source files are the same\");\n     }\n \n     if(NameNode.stateChangeLog.isDebugEnabled()) {\n-      NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" + \n+      NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" +\n           Arrays.toString(srcs) + \" to \" + target);\n     }\n \n     long timestamp \u003d now();\n-    dir.concat(target, srcs, timestamp);\n-    getEditLog().logConcat(target, srcs, timestamp, logRetryCache);\n+    fsd.writeLock();\n+    try {\n+      unprotectedConcat(fsd, target, srcs, timestamp);\n+    } finally {\n+      fsd.writeUnlock();\n+    }\n+    fsd.getEditLog().logConcat(target, srcs, timestamp, logRetryCache);\n+    return fsd.getAuditFileInfo(target, false);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static HdfsFileStatus concat(\n    FSDirectory fsd, String target, String[] srcs,\n    boolean logRetryCache) throws IOException {\n    Preconditions.checkArgument(!target.isEmpty(), \"Target file name is empty\");\n    Preconditions.checkArgument(srcs !\u003d null \u0026\u0026 srcs.length \u003e 0,\n      \"No sources given\");\n    assert srcs !\u003d null;\n\n    FSDirectory.LOG.debug(\"concat {} to {}\", Arrays.toString(srcs), target);\n    // We require all files be in the same directory\n    String trgParent \u003d\n      target.substring(0, target.lastIndexOf(Path.SEPARATOR_CHAR));\n    for (String s : srcs) {\n      String srcParent \u003d s.substring(0, s.lastIndexOf(Path.SEPARATOR_CHAR));\n      if (!srcParent.equals(trgParent)) {\n        throw new IllegalArgumentException(\n           \"Sources and target are not in the same directory\");\n      }\n    }\n\n    // write permission for the target\n    if (fsd.isPermissionEnabled()) {\n      FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n      fsd.checkPathAccess(pc, target, FsAction.WRITE);\n\n      // and srcs\n      for(String aSrc: srcs) {\n        fsd.checkPathAccess(pc, aSrc, FsAction.READ); // read the file\n        fsd.checkParentAccess(pc, aSrc, FsAction.WRITE); // for delete\n      }\n    }\n\n    // to make sure no two files are the same\n    Set\u003cINode\u003e si \u003d new HashSet\u003cINode\u003e();\n\n    // we put the following prerequisite for the operation\n    // replication and blocks sizes should be the same for ALL the blocks\n\n    // check the target\n    final INodesInPath trgIip \u003d fsd.getINodesInPath4Write(target);\n    if (fsd.getEZForPath(trgIip) !\u003d null) {\n      throw new HadoopIllegalArgumentException(\n          \"concat can not be called for files in an encryption zone.\");\n    }\n    final INodeFile trgInode \u003d INodeFile.valueOf(trgIip.getLastINode(), target);\n    if(trgInode.isUnderConstruction()) {\n      throw new HadoopIllegalArgumentException(\"concat: target file \"\n          + target + \" is under construction\");\n    }\n    // per design target shouldn\u0027t be empty and all the blocks same size\n    if(trgInode.numBlocks() \u003d\u003d 0) {\n      throw new HadoopIllegalArgumentException(\"concat: target file \"\n          + target + \" is empty\");\n    }\n    if (trgInode.isWithSnapshot()) {\n      throw new HadoopIllegalArgumentException(\"concat: target file \"\n          + target + \" is in a snapshot\");\n    }\n\n    long blockSize \u003d trgInode.getPreferredBlockSize();\n\n    // check the end block to be full\n    final BlockInfo last \u003d trgInode.getLastBlock();\n    if(blockSize !\u003d last.getNumBytes()) {\n      throw new HadoopIllegalArgumentException(\"The last block in \" + target\n          + \" is not full; last block size \u003d \" + last.getNumBytes()\n          + \" but file block size \u003d \" + blockSize);\n    }\n\n    si.add(trgInode);\n    final short repl \u003d trgInode.getFileReplication();\n\n    // now check the srcs\n    boolean endSrc \u003d false; // final src file doesn\u0027t have to have full end block\n    for(int i\u003d0; i\u003c srcs.length; i++) {\n      String src \u003d srcs[i];\n      if(i\u003d\u003d srcs.length-1)\n        endSrc\u003dtrue;\n\n      final INodeFile srcInode \u003d INodeFile.valueOf(fsd.getINode4Write(src), src);\n      if(src.isEmpty()\n          || srcInode.isUnderConstruction()\n          || srcInode.numBlocks() \u003d\u003d 0) {\n        throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n            + \" is invalid or empty or underConstruction\");\n      }\n\n      // check replication and blocks size\n      if(repl !\u003d srcInode.getBlockReplication()) {\n        throw new HadoopIllegalArgumentException(\"concat: the source file \"\n            + src + \" and the target file \" + target\n            + \" should have the same replication: source replication is \"\n            + srcInode.getBlockReplication()\n            + \" but target replication is \" + repl);\n      }\n\n      //boolean endBlock\u003dfalse;\n      // verify that all the blocks are of the same length as target\n      // should be enough to check the end blocks\n      final BlockInfo[] srcBlocks \u003d srcInode.getBlocks();\n      int idx \u003d srcBlocks.length-1;\n      if(endSrc)\n        idx \u003d srcBlocks.length-2; // end block of endSrc is OK not to be full\n      if(idx \u003e\u003d 0 \u0026\u0026 srcBlocks[idx].getNumBytes() !\u003d blockSize) {\n        throw new HadoopIllegalArgumentException(\"concat: the source file \"\n            + src + \" and the target file \" + target\n            + \" should have the same blocks sizes: target block size is \"\n            + blockSize + \" but the size of source block \" + idx + \" is \"\n            + srcBlocks[idx].getNumBytes());\n      }\n\n      si.add(srcInode);\n    }\n\n    // make sure no two files are the same\n    if(si.size() \u003c srcs.length+1) { // trg + srcs\n      // it means at least two files are the same\n      throw new HadoopIllegalArgumentException(\n          \"concat: at least two of the source files are the same\");\n    }\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" +\n          Arrays.toString(srcs) + \" to \" + target);\n    }\n\n    long timestamp \u003d now();\n    fsd.writeLock();\n    try {\n      unprotectedConcat(fsd, target, srcs, timestamp);\n    } finally {\n      fsd.writeUnlock();\n    }\n    fsd.getEditLog().logConcat(target, srcs, timestamp, logRetryCache);\n    return fsd.getAuditFileInfo(target, false);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java",
          "extendedDetails": {
            "oldValue": "[IOException, UnresolvedLinkException]",
            "newValue": "[IOException]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-7436. Consolidate implementation of concat(). Contributed by Haohui Mai.\n",
          "commitDate": "24/11/14 3:42 PM",
          "commitName": "8caf537afabc70b0c74e0a29aea0cc2935ecb162",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "24/11/14 2:58 PM",
          "commitNameOld": "e37a4ff0c1712a1cb80e0412ec53a5d10b8d30f9",
          "commitAuthorOld": "Zhijie Shen",
          "daysBetweenCommits": 0.03,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,115 +1,136 @@\n-  private void concatInternal(FSPermissionChecker pc, String target,\n-      String[] srcs, boolean logRetryCache) throws IOException,\n-      UnresolvedLinkException {\n-    assert hasWriteLock();\n+  static HdfsFileStatus concat(\n+    FSDirectory fsd, String target, String[] srcs,\n+    boolean logRetryCache) throws IOException {\n+    Preconditions.checkArgument(!target.isEmpty(), \"Target file name is empty\");\n+    Preconditions.checkArgument(srcs !\u003d null \u0026\u0026 srcs.length \u003e 0,\n+      \"No sources given\");\n+    assert srcs !\u003d null;\n+\n+    FSDirectory.LOG.debug(\"concat {} to {}\", Arrays.toString(srcs), target);\n+    // We require all files be in the same directory\n+    String trgParent \u003d\n+      target.substring(0, target.lastIndexOf(Path.SEPARATOR_CHAR));\n+    for (String s : srcs) {\n+      String srcParent \u003d s.substring(0, s.lastIndexOf(Path.SEPARATOR_CHAR));\n+      if (!srcParent.equals(trgParent)) {\n+        throw new IllegalArgumentException(\n+           \"Sources and target are not in the same directory\");\n+      }\n+    }\n \n     // write permission for the target\n-    if (isPermissionEnabled) {\n-      checkPathAccess(pc, target, FsAction.WRITE);\n+    if (fsd.isPermissionEnabled()) {\n+      FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n+      fsd.checkPathAccess(pc, target, FsAction.WRITE);\n \n       // and srcs\n       for(String aSrc: srcs) {\n-        checkPathAccess(pc, aSrc, FsAction.READ); // read the file\n-        checkParentAccess(pc, aSrc, FsAction.WRITE); // for delete \n+        fsd.checkPathAccess(pc, aSrc, FsAction.READ); // read the file\n+        fsd.checkParentAccess(pc, aSrc, FsAction.WRITE); // for delete\n       }\n     }\n \n     // to make sure no two files are the same\n     Set\u003cINode\u003e si \u003d new HashSet\u003cINode\u003e();\n \n     // we put the following prerequisite for the operation\n     // replication and blocks sizes should be the same for ALL the blocks\n \n     // check the target\n-    final INodesInPath trgIip \u003d dir.getINodesInPath4Write(target);\n-    if (dir.getEZForPath(trgIip) !\u003d null) {\n+    final INodesInPath trgIip \u003d fsd.getINodesInPath4Write(target);\n+    if (fsd.getEZForPath(trgIip) !\u003d null) {\n       throw new HadoopIllegalArgumentException(\n           \"concat can not be called for files in an encryption zone.\");\n     }\n-    final INodeFile trgInode \u003d INodeFile.valueOf(trgIip.getLastINode(),\n-        target);\n+    final INodeFile trgInode \u003d INodeFile.valueOf(trgIip.getLastINode(), target);\n     if(trgInode.isUnderConstruction()) {\n       throw new HadoopIllegalArgumentException(\"concat: target file \"\n           + target + \" is under construction\");\n     }\n     // per design target shouldn\u0027t be empty and all the blocks same size\n     if(trgInode.numBlocks() \u003d\u003d 0) {\n       throw new HadoopIllegalArgumentException(\"concat: target file \"\n           + target + \" is empty\");\n     }\n     if (trgInode.isWithSnapshot()) {\n       throw new HadoopIllegalArgumentException(\"concat: target file \"\n           + target + \" is in a snapshot\");\n     }\n \n     long blockSize \u003d trgInode.getPreferredBlockSize();\n \n     // check the end block to be full\n     final BlockInfo last \u003d trgInode.getLastBlock();\n     if(blockSize !\u003d last.getNumBytes()) {\n       throw new HadoopIllegalArgumentException(\"The last block in \" + target\n           + \" is not full; last block size \u003d \" + last.getNumBytes()\n           + \" but file block size \u003d \" + blockSize);\n     }\n \n     si.add(trgInode);\n     final short repl \u003d trgInode.getFileReplication();\n \n     // now check the srcs\n     boolean endSrc \u003d false; // final src file doesn\u0027t have to have full end block\n-    for(int i\u003d0; i\u003csrcs.length; i++) {\n+    for(int i\u003d0; i\u003c srcs.length; i++) {\n       String src \u003d srcs[i];\n-      if(i\u003d\u003dsrcs.length-1)\n+      if(i\u003d\u003d srcs.length-1)\n         endSrc\u003dtrue;\n \n-      final INodeFile srcInode \u003d INodeFile.valueOf(dir.getINode4Write(src), src);\n-      if(src.isEmpty() \n+      final INodeFile srcInode \u003d INodeFile.valueOf(fsd.getINode4Write(src), src);\n+      if(src.isEmpty()\n           || srcInode.isUnderConstruction()\n           || srcInode.numBlocks() \u003d\u003d 0) {\n         throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n             + \" is invalid or empty or underConstruction\");\n       }\n \n       // check replication and blocks size\n       if(repl !\u003d srcInode.getBlockReplication()) {\n         throw new HadoopIllegalArgumentException(\"concat: the source file \"\n             + src + \" and the target file \" + target\n             + \" should have the same replication: source replication is \"\n             + srcInode.getBlockReplication()\n             + \" but target replication is \" + repl);\n       }\n \n       //boolean endBlock\u003dfalse;\n       // verify that all the blocks are of the same length as target\n       // should be enough to check the end blocks\n       final BlockInfo[] srcBlocks \u003d srcInode.getBlocks();\n       int idx \u003d srcBlocks.length-1;\n       if(endSrc)\n         idx \u003d srcBlocks.length-2; // end block of endSrc is OK not to be full\n       if(idx \u003e\u003d 0 \u0026\u0026 srcBlocks[idx].getNumBytes() !\u003d blockSize) {\n         throw new HadoopIllegalArgumentException(\"concat: the source file \"\n             + src + \" and the target file \" + target\n             + \" should have the same blocks sizes: target block size is \"\n             + blockSize + \" but the size of source block \" + idx + \" is \"\n             + srcBlocks[idx].getNumBytes());\n       }\n \n       si.add(srcInode);\n     }\n \n     // make sure no two files are the same\n     if(si.size() \u003c srcs.length+1) { // trg + srcs\n       // it means at least two files are the same\n       throw new HadoopIllegalArgumentException(\n           \"concat: at least two of the source files are the same\");\n     }\n \n     if(NameNode.stateChangeLog.isDebugEnabled()) {\n-      NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" + \n+      NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" +\n           Arrays.toString(srcs) + \" to \" + target);\n     }\n \n     long timestamp \u003d now();\n-    dir.concat(target, srcs, timestamp);\n-    getEditLog().logConcat(target, srcs, timestamp, logRetryCache);\n+    fsd.writeLock();\n+    try {\n+      unprotectedConcat(fsd, target, srcs, timestamp);\n+    } finally {\n+      fsd.writeUnlock();\n+    }\n+    fsd.getEditLog().logConcat(target, srcs, timestamp, logRetryCache);\n+    return fsd.getAuditFileInfo(target, false);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static HdfsFileStatus concat(\n    FSDirectory fsd, String target, String[] srcs,\n    boolean logRetryCache) throws IOException {\n    Preconditions.checkArgument(!target.isEmpty(), \"Target file name is empty\");\n    Preconditions.checkArgument(srcs !\u003d null \u0026\u0026 srcs.length \u003e 0,\n      \"No sources given\");\n    assert srcs !\u003d null;\n\n    FSDirectory.LOG.debug(\"concat {} to {}\", Arrays.toString(srcs), target);\n    // We require all files be in the same directory\n    String trgParent \u003d\n      target.substring(0, target.lastIndexOf(Path.SEPARATOR_CHAR));\n    for (String s : srcs) {\n      String srcParent \u003d s.substring(0, s.lastIndexOf(Path.SEPARATOR_CHAR));\n      if (!srcParent.equals(trgParent)) {\n        throw new IllegalArgumentException(\n           \"Sources and target are not in the same directory\");\n      }\n    }\n\n    // write permission for the target\n    if (fsd.isPermissionEnabled()) {\n      FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n      fsd.checkPathAccess(pc, target, FsAction.WRITE);\n\n      // and srcs\n      for(String aSrc: srcs) {\n        fsd.checkPathAccess(pc, aSrc, FsAction.READ); // read the file\n        fsd.checkParentAccess(pc, aSrc, FsAction.WRITE); // for delete\n      }\n    }\n\n    // to make sure no two files are the same\n    Set\u003cINode\u003e si \u003d new HashSet\u003cINode\u003e();\n\n    // we put the following prerequisite for the operation\n    // replication and blocks sizes should be the same for ALL the blocks\n\n    // check the target\n    final INodesInPath trgIip \u003d fsd.getINodesInPath4Write(target);\n    if (fsd.getEZForPath(trgIip) !\u003d null) {\n      throw new HadoopIllegalArgumentException(\n          \"concat can not be called for files in an encryption zone.\");\n    }\n    final INodeFile trgInode \u003d INodeFile.valueOf(trgIip.getLastINode(), target);\n    if(trgInode.isUnderConstruction()) {\n      throw new HadoopIllegalArgumentException(\"concat: target file \"\n          + target + \" is under construction\");\n    }\n    // per design target shouldn\u0027t be empty and all the blocks same size\n    if(trgInode.numBlocks() \u003d\u003d 0) {\n      throw new HadoopIllegalArgumentException(\"concat: target file \"\n          + target + \" is empty\");\n    }\n    if (trgInode.isWithSnapshot()) {\n      throw new HadoopIllegalArgumentException(\"concat: target file \"\n          + target + \" is in a snapshot\");\n    }\n\n    long blockSize \u003d trgInode.getPreferredBlockSize();\n\n    // check the end block to be full\n    final BlockInfo last \u003d trgInode.getLastBlock();\n    if(blockSize !\u003d last.getNumBytes()) {\n      throw new HadoopIllegalArgumentException(\"The last block in \" + target\n          + \" is not full; last block size \u003d \" + last.getNumBytes()\n          + \" but file block size \u003d \" + blockSize);\n    }\n\n    si.add(trgInode);\n    final short repl \u003d trgInode.getFileReplication();\n\n    // now check the srcs\n    boolean endSrc \u003d false; // final src file doesn\u0027t have to have full end block\n    for(int i\u003d0; i\u003c srcs.length; i++) {\n      String src \u003d srcs[i];\n      if(i\u003d\u003d srcs.length-1)\n        endSrc\u003dtrue;\n\n      final INodeFile srcInode \u003d INodeFile.valueOf(fsd.getINode4Write(src), src);\n      if(src.isEmpty()\n          || srcInode.isUnderConstruction()\n          || srcInode.numBlocks() \u003d\u003d 0) {\n        throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n            + \" is invalid or empty or underConstruction\");\n      }\n\n      // check replication and blocks size\n      if(repl !\u003d srcInode.getBlockReplication()) {\n        throw new HadoopIllegalArgumentException(\"concat: the source file \"\n            + src + \" and the target file \" + target\n            + \" should have the same replication: source replication is \"\n            + srcInode.getBlockReplication()\n            + \" but target replication is \" + repl);\n      }\n\n      //boolean endBlock\u003dfalse;\n      // verify that all the blocks are of the same length as target\n      // should be enough to check the end blocks\n      final BlockInfo[] srcBlocks \u003d srcInode.getBlocks();\n      int idx \u003d srcBlocks.length-1;\n      if(endSrc)\n        idx \u003d srcBlocks.length-2; // end block of endSrc is OK not to be full\n      if(idx \u003e\u003d 0 \u0026\u0026 srcBlocks[idx].getNumBytes() !\u003d blockSize) {\n        throw new HadoopIllegalArgumentException(\"concat: the source file \"\n            + src + \" and the target file \" + target\n            + \" should have the same blocks sizes: target block size is \"\n            + blockSize + \" but the size of source block \" + idx + \" is \"\n            + srcBlocks[idx].getNumBytes());\n      }\n\n      si.add(srcInode);\n    }\n\n    // make sure no two files are the same\n    if(si.size() \u003c srcs.length+1) { // trg + srcs\n      // it means at least two files are the same\n      throw new HadoopIllegalArgumentException(\n          \"concat: at least two of the source files are the same\");\n    }\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" +\n          Arrays.toString(srcs) + \" to \" + target);\n    }\n\n    long timestamp \u003d now();\n    fsd.writeLock();\n    try {\n      unprotectedConcat(fsd, target, srcs, timestamp);\n    } finally {\n      fsd.writeUnlock();\n    }\n    fsd.getEditLog().logConcat(target, srcs, timestamp, logRetryCache);\n    return fsd.getAuditFileInfo(target, false);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java",
          "extendedDetails": {}
        },
        {
          "type": "Yrename",
          "commitMessage": "HDFS-7436. Consolidate implementation of concat(). Contributed by Haohui Mai.\n",
          "commitDate": "24/11/14 3:42 PM",
          "commitName": "8caf537afabc70b0c74e0a29aea0cc2935ecb162",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "24/11/14 2:58 PM",
          "commitNameOld": "e37a4ff0c1712a1cb80e0412ec53a5d10b8d30f9",
          "commitAuthorOld": "Zhijie Shen",
          "daysBetweenCommits": 0.03,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,115 +1,136 @@\n-  private void concatInternal(FSPermissionChecker pc, String target,\n-      String[] srcs, boolean logRetryCache) throws IOException,\n-      UnresolvedLinkException {\n-    assert hasWriteLock();\n+  static HdfsFileStatus concat(\n+    FSDirectory fsd, String target, String[] srcs,\n+    boolean logRetryCache) throws IOException {\n+    Preconditions.checkArgument(!target.isEmpty(), \"Target file name is empty\");\n+    Preconditions.checkArgument(srcs !\u003d null \u0026\u0026 srcs.length \u003e 0,\n+      \"No sources given\");\n+    assert srcs !\u003d null;\n+\n+    FSDirectory.LOG.debug(\"concat {} to {}\", Arrays.toString(srcs), target);\n+    // We require all files be in the same directory\n+    String trgParent \u003d\n+      target.substring(0, target.lastIndexOf(Path.SEPARATOR_CHAR));\n+    for (String s : srcs) {\n+      String srcParent \u003d s.substring(0, s.lastIndexOf(Path.SEPARATOR_CHAR));\n+      if (!srcParent.equals(trgParent)) {\n+        throw new IllegalArgumentException(\n+           \"Sources and target are not in the same directory\");\n+      }\n+    }\n \n     // write permission for the target\n-    if (isPermissionEnabled) {\n-      checkPathAccess(pc, target, FsAction.WRITE);\n+    if (fsd.isPermissionEnabled()) {\n+      FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n+      fsd.checkPathAccess(pc, target, FsAction.WRITE);\n \n       // and srcs\n       for(String aSrc: srcs) {\n-        checkPathAccess(pc, aSrc, FsAction.READ); // read the file\n-        checkParentAccess(pc, aSrc, FsAction.WRITE); // for delete \n+        fsd.checkPathAccess(pc, aSrc, FsAction.READ); // read the file\n+        fsd.checkParentAccess(pc, aSrc, FsAction.WRITE); // for delete\n       }\n     }\n \n     // to make sure no two files are the same\n     Set\u003cINode\u003e si \u003d new HashSet\u003cINode\u003e();\n \n     // we put the following prerequisite for the operation\n     // replication and blocks sizes should be the same for ALL the blocks\n \n     // check the target\n-    final INodesInPath trgIip \u003d dir.getINodesInPath4Write(target);\n-    if (dir.getEZForPath(trgIip) !\u003d null) {\n+    final INodesInPath trgIip \u003d fsd.getINodesInPath4Write(target);\n+    if (fsd.getEZForPath(trgIip) !\u003d null) {\n       throw new HadoopIllegalArgumentException(\n           \"concat can not be called for files in an encryption zone.\");\n     }\n-    final INodeFile trgInode \u003d INodeFile.valueOf(trgIip.getLastINode(),\n-        target);\n+    final INodeFile trgInode \u003d INodeFile.valueOf(trgIip.getLastINode(), target);\n     if(trgInode.isUnderConstruction()) {\n       throw new HadoopIllegalArgumentException(\"concat: target file \"\n           + target + \" is under construction\");\n     }\n     // per design target shouldn\u0027t be empty and all the blocks same size\n     if(trgInode.numBlocks() \u003d\u003d 0) {\n       throw new HadoopIllegalArgumentException(\"concat: target file \"\n           + target + \" is empty\");\n     }\n     if (trgInode.isWithSnapshot()) {\n       throw new HadoopIllegalArgumentException(\"concat: target file \"\n           + target + \" is in a snapshot\");\n     }\n \n     long blockSize \u003d trgInode.getPreferredBlockSize();\n \n     // check the end block to be full\n     final BlockInfo last \u003d trgInode.getLastBlock();\n     if(blockSize !\u003d last.getNumBytes()) {\n       throw new HadoopIllegalArgumentException(\"The last block in \" + target\n           + \" is not full; last block size \u003d \" + last.getNumBytes()\n           + \" but file block size \u003d \" + blockSize);\n     }\n \n     si.add(trgInode);\n     final short repl \u003d trgInode.getFileReplication();\n \n     // now check the srcs\n     boolean endSrc \u003d false; // final src file doesn\u0027t have to have full end block\n-    for(int i\u003d0; i\u003csrcs.length; i++) {\n+    for(int i\u003d0; i\u003c srcs.length; i++) {\n       String src \u003d srcs[i];\n-      if(i\u003d\u003dsrcs.length-1)\n+      if(i\u003d\u003d srcs.length-1)\n         endSrc\u003dtrue;\n \n-      final INodeFile srcInode \u003d INodeFile.valueOf(dir.getINode4Write(src), src);\n-      if(src.isEmpty() \n+      final INodeFile srcInode \u003d INodeFile.valueOf(fsd.getINode4Write(src), src);\n+      if(src.isEmpty()\n           || srcInode.isUnderConstruction()\n           || srcInode.numBlocks() \u003d\u003d 0) {\n         throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n             + \" is invalid or empty or underConstruction\");\n       }\n \n       // check replication and blocks size\n       if(repl !\u003d srcInode.getBlockReplication()) {\n         throw new HadoopIllegalArgumentException(\"concat: the source file \"\n             + src + \" and the target file \" + target\n             + \" should have the same replication: source replication is \"\n             + srcInode.getBlockReplication()\n             + \" but target replication is \" + repl);\n       }\n \n       //boolean endBlock\u003dfalse;\n       // verify that all the blocks are of the same length as target\n       // should be enough to check the end blocks\n       final BlockInfo[] srcBlocks \u003d srcInode.getBlocks();\n       int idx \u003d srcBlocks.length-1;\n       if(endSrc)\n         idx \u003d srcBlocks.length-2; // end block of endSrc is OK not to be full\n       if(idx \u003e\u003d 0 \u0026\u0026 srcBlocks[idx].getNumBytes() !\u003d blockSize) {\n         throw new HadoopIllegalArgumentException(\"concat: the source file \"\n             + src + \" and the target file \" + target\n             + \" should have the same blocks sizes: target block size is \"\n             + blockSize + \" but the size of source block \" + idx + \" is \"\n             + srcBlocks[idx].getNumBytes());\n       }\n \n       si.add(srcInode);\n     }\n \n     // make sure no two files are the same\n     if(si.size() \u003c srcs.length+1) { // trg + srcs\n       // it means at least two files are the same\n       throw new HadoopIllegalArgumentException(\n           \"concat: at least two of the source files are the same\");\n     }\n \n     if(NameNode.stateChangeLog.isDebugEnabled()) {\n-      NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" + \n+      NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" +\n           Arrays.toString(srcs) + \" to \" + target);\n     }\n \n     long timestamp \u003d now();\n-    dir.concat(target, srcs, timestamp);\n-    getEditLog().logConcat(target, srcs, timestamp, logRetryCache);\n+    fsd.writeLock();\n+    try {\n+      unprotectedConcat(fsd, target, srcs, timestamp);\n+    } finally {\n+      fsd.writeUnlock();\n+    }\n+    fsd.getEditLog().logConcat(target, srcs, timestamp, logRetryCache);\n+    return fsd.getAuditFileInfo(target, false);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static HdfsFileStatus concat(\n    FSDirectory fsd, String target, String[] srcs,\n    boolean logRetryCache) throws IOException {\n    Preconditions.checkArgument(!target.isEmpty(), \"Target file name is empty\");\n    Preconditions.checkArgument(srcs !\u003d null \u0026\u0026 srcs.length \u003e 0,\n      \"No sources given\");\n    assert srcs !\u003d null;\n\n    FSDirectory.LOG.debug(\"concat {} to {}\", Arrays.toString(srcs), target);\n    // We require all files be in the same directory\n    String trgParent \u003d\n      target.substring(0, target.lastIndexOf(Path.SEPARATOR_CHAR));\n    for (String s : srcs) {\n      String srcParent \u003d s.substring(0, s.lastIndexOf(Path.SEPARATOR_CHAR));\n      if (!srcParent.equals(trgParent)) {\n        throw new IllegalArgumentException(\n           \"Sources and target are not in the same directory\");\n      }\n    }\n\n    // write permission for the target\n    if (fsd.isPermissionEnabled()) {\n      FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n      fsd.checkPathAccess(pc, target, FsAction.WRITE);\n\n      // and srcs\n      for(String aSrc: srcs) {\n        fsd.checkPathAccess(pc, aSrc, FsAction.READ); // read the file\n        fsd.checkParentAccess(pc, aSrc, FsAction.WRITE); // for delete\n      }\n    }\n\n    // to make sure no two files are the same\n    Set\u003cINode\u003e si \u003d new HashSet\u003cINode\u003e();\n\n    // we put the following prerequisite for the operation\n    // replication and blocks sizes should be the same for ALL the blocks\n\n    // check the target\n    final INodesInPath trgIip \u003d fsd.getINodesInPath4Write(target);\n    if (fsd.getEZForPath(trgIip) !\u003d null) {\n      throw new HadoopIllegalArgumentException(\n          \"concat can not be called for files in an encryption zone.\");\n    }\n    final INodeFile trgInode \u003d INodeFile.valueOf(trgIip.getLastINode(), target);\n    if(trgInode.isUnderConstruction()) {\n      throw new HadoopIllegalArgumentException(\"concat: target file \"\n          + target + \" is under construction\");\n    }\n    // per design target shouldn\u0027t be empty and all the blocks same size\n    if(trgInode.numBlocks() \u003d\u003d 0) {\n      throw new HadoopIllegalArgumentException(\"concat: target file \"\n          + target + \" is empty\");\n    }\n    if (trgInode.isWithSnapshot()) {\n      throw new HadoopIllegalArgumentException(\"concat: target file \"\n          + target + \" is in a snapshot\");\n    }\n\n    long blockSize \u003d trgInode.getPreferredBlockSize();\n\n    // check the end block to be full\n    final BlockInfo last \u003d trgInode.getLastBlock();\n    if(blockSize !\u003d last.getNumBytes()) {\n      throw new HadoopIllegalArgumentException(\"The last block in \" + target\n          + \" is not full; last block size \u003d \" + last.getNumBytes()\n          + \" but file block size \u003d \" + blockSize);\n    }\n\n    si.add(trgInode);\n    final short repl \u003d trgInode.getFileReplication();\n\n    // now check the srcs\n    boolean endSrc \u003d false; // final src file doesn\u0027t have to have full end block\n    for(int i\u003d0; i\u003c srcs.length; i++) {\n      String src \u003d srcs[i];\n      if(i\u003d\u003d srcs.length-1)\n        endSrc\u003dtrue;\n\n      final INodeFile srcInode \u003d INodeFile.valueOf(fsd.getINode4Write(src), src);\n      if(src.isEmpty()\n          || srcInode.isUnderConstruction()\n          || srcInode.numBlocks() \u003d\u003d 0) {\n        throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n            + \" is invalid or empty or underConstruction\");\n      }\n\n      // check replication and blocks size\n      if(repl !\u003d srcInode.getBlockReplication()) {\n        throw new HadoopIllegalArgumentException(\"concat: the source file \"\n            + src + \" and the target file \" + target\n            + \" should have the same replication: source replication is \"\n            + srcInode.getBlockReplication()\n            + \" but target replication is \" + repl);\n      }\n\n      //boolean endBlock\u003dfalse;\n      // verify that all the blocks are of the same length as target\n      // should be enough to check the end blocks\n      final BlockInfo[] srcBlocks \u003d srcInode.getBlocks();\n      int idx \u003d srcBlocks.length-1;\n      if(endSrc)\n        idx \u003d srcBlocks.length-2; // end block of endSrc is OK not to be full\n      if(idx \u003e\u003d 0 \u0026\u0026 srcBlocks[idx].getNumBytes() !\u003d blockSize) {\n        throw new HadoopIllegalArgumentException(\"concat: the source file \"\n            + src + \" and the target file \" + target\n            + \" should have the same blocks sizes: target block size is \"\n            + blockSize + \" but the size of source block \" + idx + \" is \"\n            + srcBlocks[idx].getNumBytes());\n      }\n\n      si.add(srcInode);\n    }\n\n    // make sure no two files are the same\n    if(si.size() \u003c srcs.length+1) { // trg + srcs\n      // it means at least two files are the same\n      throw new HadoopIllegalArgumentException(\n          \"concat: at least two of the source files are the same\");\n    }\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" +\n          Arrays.toString(srcs) + \" to \" + target);\n    }\n\n    long timestamp \u003d now();\n    fsd.writeLock();\n    try {\n      unprotectedConcat(fsd, target, srcs, timestamp);\n    } finally {\n      fsd.writeUnlock();\n    }\n    fsd.getEditLog().logConcat(target, srcs, timestamp, logRetryCache);\n    return fsd.getAuditFileInfo(target, false);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java",
          "extendedDetails": {
            "oldValue": "concatInternal",
            "newValue": "concat"
          }
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-7436. Consolidate implementation of concat(). Contributed by Haohui Mai.\n",
          "commitDate": "24/11/14 3:42 PM",
          "commitName": "8caf537afabc70b0c74e0a29aea0cc2935ecb162",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "24/11/14 2:58 PM",
          "commitNameOld": "e37a4ff0c1712a1cb80e0412ec53a5d10b8d30f9",
          "commitAuthorOld": "Zhijie Shen",
          "daysBetweenCommits": 0.03,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,115 +1,136 @@\n-  private void concatInternal(FSPermissionChecker pc, String target,\n-      String[] srcs, boolean logRetryCache) throws IOException,\n-      UnresolvedLinkException {\n-    assert hasWriteLock();\n+  static HdfsFileStatus concat(\n+    FSDirectory fsd, String target, String[] srcs,\n+    boolean logRetryCache) throws IOException {\n+    Preconditions.checkArgument(!target.isEmpty(), \"Target file name is empty\");\n+    Preconditions.checkArgument(srcs !\u003d null \u0026\u0026 srcs.length \u003e 0,\n+      \"No sources given\");\n+    assert srcs !\u003d null;\n+\n+    FSDirectory.LOG.debug(\"concat {} to {}\", Arrays.toString(srcs), target);\n+    // We require all files be in the same directory\n+    String trgParent \u003d\n+      target.substring(0, target.lastIndexOf(Path.SEPARATOR_CHAR));\n+    for (String s : srcs) {\n+      String srcParent \u003d s.substring(0, s.lastIndexOf(Path.SEPARATOR_CHAR));\n+      if (!srcParent.equals(trgParent)) {\n+        throw new IllegalArgumentException(\n+           \"Sources and target are not in the same directory\");\n+      }\n+    }\n \n     // write permission for the target\n-    if (isPermissionEnabled) {\n-      checkPathAccess(pc, target, FsAction.WRITE);\n+    if (fsd.isPermissionEnabled()) {\n+      FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n+      fsd.checkPathAccess(pc, target, FsAction.WRITE);\n \n       // and srcs\n       for(String aSrc: srcs) {\n-        checkPathAccess(pc, aSrc, FsAction.READ); // read the file\n-        checkParentAccess(pc, aSrc, FsAction.WRITE); // for delete \n+        fsd.checkPathAccess(pc, aSrc, FsAction.READ); // read the file\n+        fsd.checkParentAccess(pc, aSrc, FsAction.WRITE); // for delete\n       }\n     }\n \n     // to make sure no two files are the same\n     Set\u003cINode\u003e si \u003d new HashSet\u003cINode\u003e();\n \n     // we put the following prerequisite for the operation\n     // replication and blocks sizes should be the same for ALL the blocks\n \n     // check the target\n-    final INodesInPath trgIip \u003d dir.getINodesInPath4Write(target);\n-    if (dir.getEZForPath(trgIip) !\u003d null) {\n+    final INodesInPath trgIip \u003d fsd.getINodesInPath4Write(target);\n+    if (fsd.getEZForPath(trgIip) !\u003d null) {\n       throw new HadoopIllegalArgumentException(\n           \"concat can not be called for files in an encryption zone.\");\n     }\n-    final INodeFile trgInode \u003d INodeFile.valueOf(trgIip.getLastINode(),\n-        target);\n+    final INodeFile trgInode \u003d INodeFile.valueOf(trgIip.getLastINode(), target);\n     if(trgInode.isUnderConstruction()) {\n       throw new HadoopIllegalArgumentException(\"concat: target file \"\n           + target + \" is under construction\");\n     }\n     // per design target shouldn\u0027t be empty and all the blocks same size\n     if(trgInode.numBlocks() \u003d\u003d 0) {\n       throw new HadoopIllegalArgumentException(\"concat: target file \"\n           + target + \" is empty\");\n     }\n     if (trgInode.isWithSnapshot()) {\n       throw new HadoopIllegalArgumentException(\"concat: target file \"\n           + target + \" is in a snapshot\");\n     }\n \n     long blockSize \u003d trgInode.getPreferredBlockSize();\n \n     // check the end block to be full\n     final BlockInfo last \u003d trgInode.getLastBlock();\n     if(blockSize !\u003d last.getNumBytes()) {\n       throw new HadoopIllegalArgumentException(\"The last block in \" + target\n           + \" is not full; last block size \u003d \" + last.getNumBytes()\n           + \" but file block size \u003d \" + blockSize);\n     }\n \n     si.add(trgInode);\n     final short repl \u003d trgInode.getFileReplication();\n \n     // now check the srcs\n     boolean endSrc \u003d false; // final src file doesn\u0027t have to have full end block\n-    for(int i\u003d0; i\u003csrcs.length; i++) {\n+    for(int i\u003d0; i\u003c srcs.length; i++) {\n       String src \u003d srcs[i];\n-      if(i\u003d\u003dsrcs.length-1)\n+      if(i\u003d\u003d srcs.length-1)\n         endSrc\u003dtrue;\n \n-      final INodeFile srcInode \u003d INodeFile.valueOf(dir.getINode4Write(src), src);\n-      if(src.isEmpty() \n+      final INodeFile srcInode \u003d INodeFile.valueOf(fsd.getINode4Write(src), src);\n+      if(src.isEmpty()\n           || srcInode.isUnderConstruction()\n           || srcInode.numBlocks() \u003d\u003d 0) {\n         throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n             + \" is invalid or empty or underConstruction\");\n       }\n \n       // check replication and blocks size\n       if(repl !\u003d srcInode.getBlockReplication()) {\n         throw new HadoopIllegalArgumentException(\"concat: the source file \"\n             + src + \" and the target file \" + target\n             + \" should have the same replication: source replication is \"\n             + srcInode.getBlockReplication()\n             + \" but target replication is \" + repl);\n       }\n \n       //boolean endBlock\u003dfalse;\n       // verify that all the blocks are of the same length as target\n       // should be enough to check the end blocks\n       final BlockInfo[] srcBlocks \u003d srcInode.getBlocks();\n       int idx \u003d srcBlocks.length-1;\n       if(endSrc)\n         idx \u003d srcBlocks.length-2; // end block of endSrc is OK not to be full\n       if(idx \u003e\u003d 0 \u0026\u0026 srcBlocks[idx].getNumBytes() !\u003d blockSize) {\n         throw new HadoopIllegalArgumentException(\"concat: the source file \"\n             + src + \" and the target file \" + target\n             + \" should have the same blocks sizes: target block size is \"\n             + blockSize + \" but the size of source block \" + idx + \" is \"\n             + srcBlocks[idx].getNumBytes());\n       }\n \n       si.add(srcInode);\n     }\n \n     // make sure no two files are the same\n     if(si.size() \u003c srcs.length+1) { // trg + srcs\n       // it means at least two files are the same\n       throw new HadoopIllegalArgumentException(\n           \"concat: at least two of the source files are the same\");\n     }\n \n     if(NameNode.stateChangeLog.isDebugEnabled()) {\n-      NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" + \n+      NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" +\n           Arrays.toString(srcs) + \" to \" + target);\n     }\n \n     long timestamp \u003d now();\n-    dir.concat(target, srcs, timestamp);\n-    getEditLog().logConcat(target, srcs, timestamp, logRetryCache);\n+    fsd.writeLock();\n+    try {\n+      unprotectedConcat(fsd, target, srcs, timestamp);\n+    } finally {\n+      fsd.writeUnlock();\n+    }\n+    fsd.getEditLog().logConcat(target, srcs, timestamp, logRetryCache);\n+    return fsd.getAuditFileInfo(target, false);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static HdfsFileStatus concat(\n    FSDirectory fsd, String target, String[] srcs,\n    boolean logRetryCache) throws IOException {\n    Preconditions.checkArgument(!target.isEmpty(), \"Target file name is empty\");\n    Preconditions.checkArgument(srcs !\u003d null \u0026\u0026 srcs.length \u003e 0,\n      \"No sources given\");\n    assert srcs !\u003d null;\n\n    FSDirectory.LOG.debug(\"concat {} to {}\", Arrays.toString(srcs), target);\n    // We require all files be in the same directory\n    String trgParent \u003d\n      target.substring(0, target.lastIndexOf(Path.SEPARATOR_CHAR));\n    for (String s : srcs) {\n      String srcParent \u003d s.substring(0, s.lastIndexOf(Path.SEPARATOR_CHAR));\n      if (!srcParent.equals(trgParent)) {\n        throw new IllegalArgumentException(\n           \"Sources and target are not in the same directory\");\n      }\n    }\n\n    // write permission for the target\n    if (fsd.isPermissionEnabled()) {\n      FSPermissionChecker pc \u003d fsd.getPermissionChecker();\n      fsd.checkPathAccess(pc, target, FsAction.WRITE);\n\n      // and srcs\n      for(String aSrc: srcs) {\n        fsd.checkPathAccess(pc, aSrc, FsAction.READ); // read the file\n        fsd.checkParentAccess(pc, aSrc, FsAction.WRITE); // for delete\n      }\n    }\n\n    // to make sure no two files are the same\n    Set\u003cINode\u003e si \u003d new HashSet\u003cINode\u003e();\n\n    // we put the following prerequisite for the operation\n    // replication and blocks sizes should be the same for ALL the blocks\n\n    // check the target\n    final INodesInPath trgIip \u003d fsd.getINodesInPath4Write(target);\n    if (fsd.getEZForPath(trgIip) !\u003d null) {\n      throw new HadoopIllegalArgumentException(\n          \"concat can not be called for files in an encryption zone.\");\n    }\n    final INodeFile trgInode \u003d INodeFile.valueOf(trgIip.getLastINode(), target);\n    if(trgInode.isUnderConstruction()) {\n      throw new HadoopIllegalArgumentException(\"concat: target file \"\n          + target + \" is under construction\");\n    }\n    // per design target shouldn\u0027t be empty and all the blocks same size\n    if(trgInode.numBlocks() \u003d\u003d 0) {\n      throw new HadoopIllegalArgumentException(\"concat: target file \"\n          + target + \" is empty\");\n    }\n    if (trgInode.isWithSnapshot()) {\n      throw new HadoopIllegalArgumentException(\"concat: target file \"\n          + target + \" is in a snapshot\");\n    }\n\n    long blockSize \u003d trgInode.getPreferredBlockSize();\n\n    // check the end block to be full\n    final BlockInfo last \u003d trgInode.getLastBlock();\n    if(blockSize !\u003d last.getNumBytes()) {\n      throw new HadoopIllegalArgumentException(\"The last block in \" + target\n          + \" is not full; last block size \u003d \" + last.getNumBytes()\n          + \" but file block size \u003d \" + blockSize);\n    }\n\n    si.add(trgInode);\n    final short repl \u003d trgInode.getFileReplication();\n\n    // now check the srcs\n    boolean endSrc \u003d false; // final src file doesn\u0027t have to have full end block\n    for(int i\u003d0; i\u003c srcs.length; i++) {\n      String src \u003d srcs[i];\n      if(i\u003d\u003d srcs.length-1)\n        endSrc\u003dtrue;\n\n      final INodeFile srcInode \u003d INodeFile.valueOf(fsd.getINode4Write(src), src);\n      if(src.isEmpty()\n          || srcInode.isUnderConstruction()\n          || srcInode.numBlocks() \u003d\u003d 0) {\n        throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n            + \" is invalid or empty or underConstruction\");\n      }\n\n      // check replication and blocks size\n      if(repl !\u003d srcInode.getBlockReplication()) {\n        throw new HadoopIllegalArgumentException(\"concat: the source file \"\n            + src + \" and the target file \" + target\n            + \" should have the same replication: source replication is \"\n            + srcInode.getBlockReplication()\n            + \" but target replication is \" + repl);\n      }\n\n      //boolean endBlock\u003dfalse;\n      // verify that all the blocks are of the same length as target\n      // should be enough to check the end blocks\n      final BlockInfo[] srcBlocks \u003d srcInode.getBlocks();\n      int idx \u003d srcBlocks.length-1;\n      if(endSrc)\n        idx \u003d srcBlocks.length-2; // end block of endSrc is OK not to be full\n      if(idx \u003e\u003d 0 \u0026\u0026 srcBlocks[idx].getNumBytes() !\u003d blockSize) {\n        throw new HadoopIllegalArgumentException(\"concat: the source file \"\n            + src + \" and the target file \" + target\n            + \" should have the same blocks sizes: target block size is \"\n            + blockSize + \" but the size of source block \" + idx + \" is \"\n            + srcBlocks[idx].getNumBytes());\n      }\n\n      si.add(srcInode);\n    }\n\n    // make sure no two files are the same\n    if(si.size() \u003c srcs.length+1) { // trg + srcs\n      // it means at least two files are the same\n      throw new HadoopIllegalArgumentException(\n          \"concat: at least two of the source files are the same\");\n    }\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" +\n          Arrays.toString(srcs) + \" to \" + target);\n    }\n\n    long timestamp \u003d now();\n    fsd.writeLock();\n    try {\n      unprotectedConcat(fsd, target, srcs, timestamp);\n    } finally {\n      fsd.writeUnlock();\n    }\n    fsd.getEditLog().logConcat(target, srcs, timestamp, logRetryCache);\n    return fsd.getAuditFileInfo(target, false);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirConcatOp.java",
          "extendedDetails": {
            "oldValue": "[pc-FSPermissionChecker, target-String, srcs-String[], logRetryCache-boolean]",
            "newValue": "[fsd-FSDirectory, target-String, srcs-String[], logRetryCache-boolean]"
          }
        }
      ]
    },
    "57dec288070f903931771485d6424317b20551aa": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7243. HDFS concat operation should not be allowed in Encryption Zone. (clamb via yliu)\n",
      "commitDate": "23/10/14 8:12 PM",
      "commitName": "57dec288070f903931771485d6424317b20551aa",
      "commitAuthor": "yliu",
      "commitDateOld": "23/10/14 12:28 PM",
      "commitNameOld": "8c5b23b5473e447384f818d69d907d5c35ed6d6a",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 0.32,
      "commitsBetweenForRepo": 7,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,110 +1,115 @@\n   private void concatInternal(FSPermissionChecker pc, String target,\n       String[] srcs, boolean logRetryCache) throws IOException,\n       UnresolvedLinkException {\n     assert hasWriteLock();\n \n     // write permission for the target\n     if (isPermissionEnabled) {\n       checkPathAccess(pc, target, FsAction.WRITE);\n \n       // and srcs\n       for(String aSrc: srcs) {\n         checkPathAccess(pc, aSrc, FsAction.READ); // read the file\n         checkParentAccess(pc, aSrc, FsAction.WRITE); // for delete \n       }\n     }\n \n     // to make sure no two files are the same\n     Set\u003cINode\u003e si \u003d new HashSet\u003cINode\u003e();\n \n     // we put the following prerequisite for the operation\n     // replication and blocks sizes should be the same for ALL the blocks\n \n     // check the target\n-    final INodeFile trgInode \u003d INodeFile.valueOf(dir.getINode4Write(target),\n+    final INodesInPath trgIip \u003d dir.getINodesInPath4Write(target);\n+    if (dir.getEZForPath(trgIip) !\u003d null) {\n+      throw new HadoopIllegalArgumentException(\n+          \"concat can not be called for files in an encryption zone.\");\n+    }\n+    final INodeFile trgInode \u003d INodeFile.valueOf(trgIip.getLastINode(),\n         target);\n     if(trgInode.isUnderConstruction()) {\n       throw new HadoopIllegalArgumentException(\"concat: target file \"\n           + target + \" is under construction\");\n     }\n     // per design target shouldn\u0027t be empty and all the blocks same size\n     if(trgInode.numBlocks() \u003d\u003d 0) {\n       throw new HadoopIllegalArgumentException(\"concat: target file \"\n           + target + \" is empty\");\n     }\n     if (trgInode.isWithSnapshot()) {\n       throw new HadoopIllegalArgumentException(\"concat: target file \"\n           + target + \" is in a snapshot\");\n     }\n \n     long blockSize \u003d trgInode.getPreferredBlockSize();\n \n     // check the end block to be full\n     final BlockInfo last \u003d trgInode.getLastBlock();\n     if(blockSize !\u003d last.getNumBytes()) {\n       throw new HadoopIllegalArgumentException(\"The last block in \" + target\n           + \" is not full; last block size \u003d \" + last.getNumBytes()\n           + \" but file block size \u003d \" + blockSize);\n     }\n \n     si.add(trgInode);\n     final short repl \u003d trgInode.getFileReplication();\n \n     // now check the srcs\n     boolean endSrc \u003d false; // final src file doesn\u0027t have to have full end block\n     for(int i\u003d0; i\u003csrcs.length; i++) {\n       String src \u003d srcs[i];\n       if(i\u003d\u003dsrcs.length-1)\n         endSrc\u003dtrue;\n \n       final INodeFile srcInode \u003d INodeFile.valueOf(dir.getINode4Write(src), src);\n       if(src.isEmpty() \n           || srcInode.isUnderConstruction()\n           || srcInode.numBlocks() \u003d\u003d 0) {\n         throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n             + \" is invalid or empty or underConstruction\");\n       }\n \n       // check replication and blocks size\n       if(repl !\u003d srcInode.getBlockReplication()) {\n         throw new HadoopIllegalArgumentException(\"concat: the source file \"\n             + src + \" and the target file \" + target\n             + \" should have the same replication: source replication is \"\n             + srcInode.getBlockReplication()\n             + \" but target replication is \" + repl);\n       }\n \n       //boolean endBlock\u003dfalse;\n       // verify that all the blocks are of the same length as target\n       // should be enough to check the end blocks\n       final BlockInfo[] srcBlocks \u003d srcInode.getBlocks();\n       int idx \u003d srcBlocks.length-1;\n       if(endSrc)\n         idx \u003d srcBlocks.length-2; // end block of endSrc is OK not to be full\n       if(idx \u003e\u003d 0 \u0026\u0026 srcBlocks[idx].getNumBytes() !\u003d blockSize) {\n         throw new HadoopIllegalArgumentException(\"concat: the source file \"\n             + src + \" and the target file \" + target\n             + \" should have the same blocks sizes: target block size is \"\n             + blockSize + \" but the size of source block \" + idx + \" is \"\n             + srcBlocks[idx].getNumBytes());\n       }\n \n       si.add(srcInode);\n     }\n \n     // make sure no two files are the same\n     if(si.size() \u003c srcs.length+1) { // trg + srcs\n       // it means at least two files are the same\n       throw new HadoopIllegalArgumentException(\n           \"concat: at least two of the source files are the same\");\n     }\n \n     if(NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" + \n           Arrays.toString(srcs) + \" to \" + target);\n     }\n \n     long timestamp \u003d now();\n     dir.concat(target, srcs, timestamp);\n     getEditLog().logConcat(target, srcs, timestamp, logRetryCache);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void concatInternal(FSPermissionChecker pc, String target,\n      String[] srcs, boolean logRetryCache) throws IOException,\n      UnresolvedLinkException {\n    assert hasWriteLock();\n\n    // write permission for the target\n    if (isPermissionEnabled) {\n      checkPathAccess(pc, target, FsAction.WRITE);\n\n      // and srcs\n      for(String aSrc: srcs) {\n        checkPathAccess(pc, aSrc, FsAction.READ); // read the file\n        checkParentAccess(pc, aSrc, FsAction.WRITE); // for delete \n      }\n    }\n\n    // to make sure no two files are the same\n    Set\u003cINode\u003e si \u003d new HashSet\u003cINode\u003e();\n\n    // we put the following prerequisite for the operation\n    // replication and blocks sizes should be the same for ALL the blocks\n\n    // check the target\n    final INodesInPath trgIip \u003d dir.getINodesInPath4Write(target);\n    if (dir.getEZForPath(trgIip) !\u003d null) {\n      throw new HadoopIllegalArgumentException(\n          \"concat can not be called for files in an encryption zone.\");\n    }\n    final INodeFile trgInode \u003d INodeFile.valueOf(trgIip.getLastINode(),\n        target);\n    if(trgInode.isUnderConstruction()) {\n      throw new HadoopIllegalArgumentException(\"concat: target file \"\n          + target + \" is under construction\");\n    }\n    // per design target shouldn\u0027t be empty and all the blocks same size\n    if(trgInode.numBlocks() \u003d\u003d 0) {\n      throw new HadoopIllegalArgumentException(\"concat: target file \"\n          + target + \" is empty\");\n    }\n    if (trgInode.isWithSnapshot()) {\n      throw new HadoopIllegalArgumentException(\"concat: target file \"\n          + target + \" is in a snapshot\");\n    }\n\n    long blockSize \u003d trgInode.getPreferredBlockSize();\n\n    // check the end block to be full\n    final BlockInfo last \u003d trgInode.getLastBlock();\n    if(blockSize !\u003d last.getNumBytes()) {\n      throw new HadoopIllegalArgumentException(\"The last block in \" + target\n          + \" is not full; last block size \u003d \" + last.getNumBytes()\n          + \" but file block size \u003d \" + blockSize);\n    }\n\n    si.add(trgInode);\n    final short repl \u003d trgInode.getFileReplication();\n\n    // now check the srcs\n    boolean endSrc \u003d false; // final src file doesn\u0027t have to have full end block\n    for(int i\u003d0; i\u003csrcs.length; i++) {\n      String src \u003d srcs[i];\n      if(i\u003d\u003dsrcs.length-1)\n        endSrc\u003dtrue;\n\n      final INodeFile srcInode \u003d INodeFile.valueOf(dir.getINode4Write(src), src);\n      if(src.isEmpty() \n          || srcInode.isUnderConstruction()\n          || srcInode.numBlocks() \u003d\u003d 0) {\n        throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n            + \" is invalid or empty or underConstruction\");\n      }\n\n      // check replication and blocks size\n      if(repl !\u003d srcInode.getBlockReplication()) {\n        throw new HadoopIllegalArgumentException(\"concat: the source file \"\n            + src + \" and the target file \" + target\n            + \" should have the same replication: source replication is \"\n            + srcInode.getBlockReplication()\n            + \" but target replication is \" + repl);\n      }\n\n      //boolean endBlock\u003dfalse;\n      // verify that all the blocks are of the same length as target\n      // should be enough to check the end blocks\n      final BlockInfo[] srcBlocks \u003d srcInode.getBlocks();\n      int idx \u003d srcBlocks.length-1;\n      if(endSrc)\n        idx \u003d srcBlocks.length-2; // end block of endSrc is OK not to be full\n      if(idx \u003e\u003d 0 \u0026\u0026 srcBlocks[idx].getNumBytes() !\u003d blockSize) {\n        throw new HadoopIllegalArgumentException(\"concat: the source file \"\n            + src + \" and the target file \" + target\n            + \" should have the same blocks sizes: target block size is \"\n            + blockSize + \" but the size of source block \" + idx + \" is \"\n            + srcBlocks[idx].getNumBytes());\n      }\n\n      si.add(srcInode);\n    }\n\n    // make sure no two files are the same\n    if(si.size() \u003c srcs.length+1) { // trg + srcs\n      // it means at least two files are the same\n      throw new HadoopIllegalArgumentException(\n          \"concat: at least two of the source files are the same\");\n    }\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" + \n          Arrays.toString(srcs) + \" to \" + target);\n    }\n\n    long timestamp \u003d now();\n    dir.concat(target, srcs, timestamp);\n    getEditLog().logConcat(target, srcs, timestamp, logRetryCache);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "e98529858edeed11c4f900b0db30d7e4eb2ab1ec": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6315. Decouple recording edit logs from FSDirectory. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1601960 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "11/06/14 10:22 AM",
      "commitName": "e98529858edeed11c4f900b0db30d7e4eb2ab1ec",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "03/06/14 11:33 AM",
      "commitNameOld": "02fcb6b6bae7c3fe2a10b00b2a563e4098ff225e",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 7.95,
      "commitsBetweenForRepo": 35,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,108 +1,110 @@\n   private void concatInternal(FSPermissionChecker pc, String target,\n       String[] srcs, boolean logRetryCache) throws IOException,\n       UnresolvedLinkException {\n     assert hasWriteLock();\n \n     // write permission for the target\n     if (isPermissionEnabled) {\n       checkPathAccess(pc, target, FsAction.WRITE);\n \n       // and srcs\n       for(String aSrc: srcs) {\n         checkPathAccess(pc, aSrc, FsAction.READ); // read the file\n         checkParentAccess(pc, aSrc, FsAction.WRITE); // for delete \n       }\n     }\n \n     // to make sure no two files are the same\n     Set\u003cINode\u003e si \u003d new HashSet\u003cINode\u003e();\n \n     // we put the following prerequisite for the operation\n     // replication and blocks sizes should be the same for ALL the blocks\n \n     // check the target\n     final INodeFile trgInode \u003d INodeFile.valueOf(dir.getINode4Write(target),\n         target);\n     if(trgInode.isUnderConstruction()) {\n       throw new HadoopIllegalArgumentException(\"concat: target file \"\n           + target + \" is under construction\");\n     }\n     // per design target shouldn\u0027t be empty and all the blocks same size\n     if(trgInode.numBlocks() \u003d\u003d 0) {\n       throw new HadoopIllegalArgumentException(\"concat: target file \"\n           + target + \" is empty\");\n     }\n     if (trgInode.isWithSnapshot()) {\n       throw new HadoopIllegalArgumentException(\"concat: target file \"\n           + target + \" is in a snapshot\");\n     }\n \n     long blockSize \u003d trgInode.getPreferredBlockSize();\n \n     // check the end block to be full\n     final BlockInfo last \u003d trgInode.getLastBlock();\n     if(blockSize !\u003d last.getNumBytes()) {\n       throw new HadoopIllegalArgumentException(\"The last block in \" + target\n           + \" is not full; last block size \u003d \" + last.getNumBytes()\n           + \" but file block size \u003d \" + blockSize);\n     }\n \n     si.add(trgInode);\n     final short repl \u003d trgInode.getFileReplication();\n \n     // now check the srcs\n     boolean endSrc \u003d false; // final src file doesn\u0027t have to have full end block\n     for(int i\u003d0; i\u003csrcs.length; i++) {\n       String src \u003d srcs[i];\n       if(i\u003d\u003dsrcs.length-1)\n         endSrc\u003dtrue;\n \n       final INodeFile srcInode \u003d INodeFile.valueOf(dir.getINode4Write(src), src);\n       if(src.isEmpty() \n           || srcInode.isUnderConstruction()\n           || srcInode.numBlocks() \u003d\u003d 0) {\n         throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n             + \" is invalid or empty or underConstruction\");\n       }\n \n       // check replication and blocks size\n       if(repl !\u003d srcInode.getBlockReplication()) {\n         throw new HadoopIllegalArgumentException(\"concat: the source file \"\n             + src + \" and the target file \" + target\n             + \" should have the same replication: source replication is \"\n             + srcInode.getBlockReplication()\n             + \" but target replication is \" + repl);\n       }\n \n       //boolean endBlock\u003dfalse;\n       // verify that all the blocks are of the same length as target\n       // should be enough to check the end blocks\n       final BlockInfo[] srcBlocks \u003d srcInode.getBlocks();\n       int idx \u003d srcBlocks.length-1;\n       if(endSrc)\n         idx \u003d srcBlocks.length-2; // end block of endSrc is OK not to be full\n       if(idx \u003e\u003d 0 \u0026\u0026 srcBlocks[idx].getNumBytes() !\u003d blockSize) {\n         throw new HadoopIllegalArgumentException(\"concat: the source file \"\n             + src + \" and the target file \" + target\n             + \" should have the same blocks sizes: target block size is \"\n             + blockSize + \" but the size of source block \" + idx + \" is \"\n             + srcBlocks[idx].getNumBytes());\n       }\n \n       si.add(srcInode);\n     }\n \n     // make sure no two files are the same\n     if(si.size() \u003c srcs.length+1) { // trg + srcs\n       // it means at least two files are the same\n       throw new HadoopIllegalArgumentException(\n           \"concat: at least two of the source files are the same\");\n     }\n \n     if(NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" + \n           Arrays.toString(srcs) + \" to \" + target);\n     }\n \n-    dir.concat(target,srcs, logRetryCache);\n+    long timestamp \u003d now();\n+    dir.concat(target, srcs, timestamp);\n+    getEditLog().logConcat(target, srcs, timestamp, logRetryCache);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void concatInternal(FSPermissionChecker pc, String target,\n      String[] srcs, boolean logRetryCache) throws IOException,\n      UnresolvedLinkException {\n    assert hasWriteLock();\n\n    // write permission for the target\n    if (isPermissionEnabled) {\n      checkPathAccess(pc, target, FsAction.WRITE);\n\n      // and srcs\n      for(String aSrc: srcs) {\n        checkPathAccess(pc, aSrc, FsAction.READ); // read the file\n        checkParentAccess(pc, aSrc, FsAction.WRITE); // for delete \n      }\n    }\n\n    // to make sure no two files are the same\n    Set\u003cINode\u003e si \u003d new HashSet\u003cINode\u003e();\n\n    // we put the following prerequisite for the operation\n    // replication and blocks sizes should be the same for ALL the blocks\n\n    // check the target\n    final INodeFile trgInode \u003d INodeFile.valueOf(dir.getINode4Write(target),\n        target);\n    if(trgInode.isUnderConstruction()) {\n      throw new HadoopIllegalArgumentException(\"concat: target file \"\n          + target + \" is under construction\");\n    }\n    // per design target shouldn\u0027t be empty and all the blocks same size\n    if(trgInode.numBlocks() \u003d\u003d 0) {\n      throw new HadoopIllegalArgumentException(\"concat: target file \"\n          + target + \" is empty\");\n    }\n    if (trgInode.isWithSnapshot()) {\n      throw new HadoopIllegalArgumentException(\"concat: target file \"\n          + target + \" is in a snapshot\");\n    }\n\n    long blockSize \u003d trgInode.getPreferredBlockSize();\n\n    // check the end block to be full\n    final BlockInfo last \u003d trgInode.getLastBlock();\n    if(blockSize !\u003d last.getNumBytes()) {\n      throw new HadoopIllegalArgumentException(\"The last block in \" + target\n          + \" is not full; last block size \u003d \" + last.getNumBytes()\n          + \" but file block size \u003d \" + blockSize);\n    }\n\n    si.add(trgInode);\n    final short repl \u003d trgInode.getFileReplication();\n\n    // now check the srcs\n    boolean endSrc \u003d false; // final src file doesn\u0027t have to have full end block\n    for(int i\u003d0; i\u003csrcs.length; i++) {\n      String src \u003d srcs[i];\n      if(i\u003d\u003dsrcs.length-1)\n        endSrc\u003dtrue;\n\n      final INodeFile srcInode \u003d INodeFile.valueOf(dir.getINode4Write(src), src);\n      if(src.isEmpty() \n          || srcInode.isUnderConstruction()\n          || srcInode.numBlocks() \u003d\u003d 0) {\n        throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n            + \" is invalid or empty or underConstruction\");\n      }\n\n      // check replication and blocks size\n      if(repl !\u003d srcInode.getBlockReplication()) {\n        throw new HadoopIllegalArgumentException(\"concat: the source file \"\n            + src + \" and the target file \" + target\n            + \" should have the same replication: source replication is \"\n            + srcInode.getBlockReplication()\n            + \" but target replication is \" + repl);\n      }\n\n      //boolean endBlock\u003dfalse;\n      // verify that all the blocks are of the same length as target\n      // should be enough to check the end blocks\n      final BlockInfo[] srcBlocks \u003d srcInode.getBlocks();\n      int idx \u003d srcBlocks.length-1;\n      if(endSrc)\n        idx \u003d srcBlocks.length-2; // end block of endSrc is OK not to be full\n      if(idx \u003e\u003d 0 \u0026\u0026 srcBlocks[idx].getNumBytes() !\u003d blockSize) {\n        throw new HadoopIllegalArgumentException(\"concat: the source file \"\n            + src + \" and the target file \" + target\n            + \" should have the same blocks sizes: target block size is \"\n            + blockSize + \" but the size of source block \" + idx + \" is \"\n            + srcBlocks[idx].getNumBytes());\n      }\n\n      si.add(srcInode);\n    }\n\n    // make sure no two files are the same\n    if(si.size() \u003c srcs.length+1) { // trg + srcs\n      // it means at least two files are the same\n      throw new HadoopIllegalArgumentException(\n          \"concat: at least two of the source files are the same\");\n    }\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" + \n          Arrays.toString(srcs) + \" to \" + target);\n    }\n\n    long timestamp \u003d now();\n    dir.concat(target, srcs, timestamp);\n    getEditLog().logConcat(target, srcs, timestamp, logRetryCache);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "78b9321539f973c7a1da5ce14acb49cdab41737a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6119. FSNamesystem code cleanup. Contributed by Suresh Srinivas.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1582073 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "26/03/14 2:32 PM",
      "commitName": "78b9321539f973c7a1da5ce14acb49cdab41737a",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "26/03/14 9:32 AM",
      "commitNameOld": "c00703dd082474fea98a63b871c2183ca01147ed",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 0.21,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,108 +1,108 @@\n   private void concatInternal(FSPermissionChecker pc, String target,\n       String[] srcs, boolean logRetryCache) throws IOException,\n       UnresolvedLinkException {\n     assert hasWriteLock();\n \n     // write permission for the target\n     if (isPermissionEnabled) {\n       checkPathAccess(pc, target, FsAction.WRITE);\n \n       // and srcs\n       for(String aSrc: srcs) {\n         checkPathAccess(pc, aSrc, FsAction.READ); // read the file\n         checkParentAccess(pc, aSrc, FsAction.WRITE); // for delete \n       }\n     }\n \n     // to make sure no two files are the same\n     Set\u003cINode\u003e si \u003d new HashSet\u003cINode\u003e();\n \n     // we put the following prerequisite for the operation\n     // replication and blocks sizes should be the same for ALL the blocks\n \n     // check the target\n     final INodeFile trgInode \u003d INodeFile.valueOf(dir.getINode4Write(target),\n         target);\n     if(trgInode.isUnderConstruction()) {\n       throw new HadoopIllegalArgumentException(\"concat: target file \"\n           + target + \" is under construction\");\n     }\n     // per design target shouldn\u0027t be empty and all the blocks same size\n     if(trgInode.numBlocks() \u003d\u003d 0) {\n       throw new HadoopIllegalArgumentException(\"concat: target file \"\n           + target + \" is empty\");\n     }\n     if (trgInode.isWithSnapshot()) {\n       throw new HadoopIllegalArgumentException(\"concat: target file \"\n           + target + \" is in a snapshot\");\n     }\n \n     long blockSize \u003d trgInode.getPreferredBlockSize();\n \n     // check the end block to be full\n     final BlockInfo last \u003d trgInode.getLastBlock();\n     if(blockSize !\u003d last.getNumBytes()) {\n       throw new HadoopIllegalArgumentException(\"The last block in \" + target\n           + \" is not full; last block size \u003d \" + last.getNumBytes()\n           + \" but file block size \u003d \" + blockSize);\n     }\n \n     si.add(trgInode);\n     final short repl \u003d trgInode.getFileReplication();\n \n     // now check the srcs\n     boolean endSrc \u003d false; // final src file doesn\u0027t have to have full end block\n     for(int i\u003d0; i\u003csrcs.length; i++) {\n       String src \u003d srcs[i];\n       if(i\u003d\u003dsrcs.length-1)\n         endSrc\u003dtrue;\n \n       final INodeFile srcInode \u003d INodeFile.valueOf(dir.getINode4Write(src), src);\n       if(src.isEmpty() \n           || srcInode.isUnderConstruction()\n           || srcInode.numBlocks() \u003d\u003d 0) {\n         throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n             + \" is invalid or empty or underConstruction\");\n       }\n \n       // check replication and blocks size\n       if(repl !\u003d srcInode.getBlockReplication()) {\n-        throw new HadoopIllegalArgumentException(\"concat: the soruce file \"\n+        throw new HadoopIllegalArgumentException(\"concat: the source file \"\n             + src + \" and the target file \" + target\n             + \" should have the same replication: source replication is \"\n             + srcInode.getBlockReplication()\n             + \" but target replication is \" + repl);\n       }\n \n       //boolean endBlock\u003dfalse;\n       // verify that all the blocks are of the same length as target\n       // should be enough to check the end blocks\n       final BlockInfo[] srcBlocks \u003d srcInode.getBlocks();\n       int idx \u003d srcBlocks.length-1;\n       if(endSrc)\n         idx \u003d srcBlocks.length-2; // end block of endSrc is OK not to be full\n       if(idx \u003e\u003d 0 \u0026\u0026 srcBlocks[idx].getNumBytes() !\u003d blockSize) {\n-        throw new HadoopIllegalArgumentException(\"concat: the soruce file \"\n+        throw new HadoopIllegalArgumentException(\"concat: the source file \"\n             + src + \" and the target file \" + target\n             + \" should have the same blocks sizes: target block size is \"\n             + blockSize + \" but the size of source block \" + idx + \" is \"\n             + srcBlocks[idx].getNumBytes());\n       }\n \n       si.add(srcInode);\n     }\n \n     // make sure no two files are the same\n     if(si.size() \u003c srcs.length+1) { // trg + srcs\n       // it means at least two files are the same\n       throw new HadoopIllegalArgumentException(\n           \"concat: at least two of the source files are the same\");\n     }\n \n     if(NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" + \n           Arrays.toString(srcs) + \" to \" + target);\n     }\n \n     dir.concat(target,srcs, logRetryCache);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void concatInternal(FSPermissionChecker pc, String target,\n      String[] srcs, boolean logRetryCache) throws IOException,\n      UnresolvedLinkException {\n    assert hasWriteLock();\n\n    // write permission for the target\n    if (isPermissionEnabled) {\n      checkPathAccess(pc, target, FsAction.WRITE);\n\n      // and srcs\n      for(String aSrc: srcs) {\n        checkPathAccess(pc, aSrc, FsAction.READ); // read the file\n        checkParentAccess(pc, aSrc, FsAction.WRITE); // for delete \n      }\n    }\n\n    // to make sure no two files are the same\n    Set\u003cINode\u003e si \u003d new HashSet\u003cINode\u003e();\n\n    // we put the following prerequisite for the operation\n    // replication and blocks sizes should be the same for ALL the blocks\n\n    // check the target\n    final INodeFile trgInode \u003d INodeFile.valueOf(dir.getINode4Write(target),\n        target);\n    if(trgInode.isUnderConstruction()) {\n      throw new HadoopIllegalArgumentException(\"concat: target file \"\n          + target + \" is under construction\");\n    }\n    // per design target shouldn\u0027t be empty and all the blocks same size\n    if(trgInode.numBlocks() \u003d\u003d 0) {\n      throw new HadoopIllegalArgumentException(\"concat: target file \"\n          + target + \" is empty\");\n    }\n    if (trgInode.isWithSnapshot()) {\n      throw new HadoopIllegalArgumentException(\"concat: target file \"\n          + target + \" is in a snapshot\");\n    }\n\n    long blockSize \u003d trgInode.getPreferredBlockSize();\n\n    // check the end block to be full\n    final BlockInfo last \u003d trgInode.getLastBlock();\n    if(blockSize !\u003d last.getNumBytes()) {\n      throw new HadoopIllegalArgumentException(\"The last block in \" + target\n          + \" is not full; last block size \u003d \" + last.getNumBytes()\n          + \" but file block size \u003d \" + blockSize);\n    }\n\n    si.add(trgInode);\n    final short repl \u003d trgInode.getFileReplication();\n\n    // now check the srcs\n    boolean endSrc \u003d false; // final src file doesn\u0027t have to have full end block\n    for(int i\u003d0; i\u003csrcs.length; i++) {\n      String src \u003d srcs[i];\n      if(i\u003d\u003dsrcs.length-1)\n        endSrc\u003dtrue;\n\n      final INodeFile srcInode \u003d INodeFile.valueOf(dir.getINode4Write(src), src);\n      if(src.isEmpty() \n          || srcInode.isUnderConstruction()\n          || srcInode.numBlocks() \u003d\u003d 0) {\n        throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n            + \" is invalid or empty or underConstruction\");\n      }\n\n      // check replication and blocks size\n      if(repl !\u003d srcInode.getBlockReplication()) {\n        throw new HadoopIllegalArgumentException(\"concat: the source file \"\n            + src + \" and the target file \" + target\n            + \" should have the same replication: source replication is \"\n            + srcInode.getBlockReplication()\n            + \" but target replication is \" + repl);\n      }\n\n      //boolean endBlock\u003dfalse;\n      // verify that all the blocks are of the same length as target\n      // should be enough to check the end blocks\n      final BlockInfo[] srcBlocks \u003d srcInode.getBlocks();\n      int idx \u003d srcBlocks.length-1;\n      if(endSrc)\n        idx \u003d srcBlocks.length-2; // end block of endSrc is OK not to be full\n      if(idx \u003e\u003d 0 \u0026\u0026 srcBlocks[idx].getNumBytes() !\u003d blockSize) {\n        throw new HadoopIllegalArgumentException(\"concat: the source file \"\n            + src + \" and the target file \" + target\n            + \" should have the same blocks sizes: target block size is \"\n            + blockSize + \" but the size of source block \" + idx + \" is \"\n            + srcBlocks[idx].getNumBytes());\n      }\n\n      si.add(srcInode);\n    }\n\n    // make sure no two files are the same\n    if(si.size() \u003c srcs.length+1) { // trg + srcs\n      // it means at least two files are the same\n      throw new HadoopIllegalArgumentException(\n          \"concat: at least two of the source files are the same\");\n    }\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" + \n          Arrays.toString(srcs) + \" to \" + target);\n    }\n\n    dir.concat(target,srcs, logRetryCache);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "4c87a27ad851ffaa3cc3e2074a9ef7073b5a164a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5554. Flatten INodeFile hierarchy: Replace INodeFileWithSnapshot with FileWithSnapshotFeature.  Contributed by jing9\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1548796 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "06/12/13 10:17 PM",
      "commitName": "4c87a27ad851ffaa3cc3e2074a9ef7073b5a164a",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "05/12/13 5:55 PM",
      "commitNameOld": "9df84c35d5a228e3ae42e90487f2d1f1264e5eea",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 1.18,
      "commitsBetweenForRepo": 4,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,108 +1,108 @@\n   private void concatInternal(FSPermissionChecker pc, String target,\n       String[] srcs, boolean logRetryCache) throws IOException,\n       UnresolvedLinkException {\n     assert hasWriteLock();\n \n     // write permission for the target\n     if (isPermissionEnabled) {\n       checkPathAccess(pc, target, FsAction.WRITE);\n \n       // and srcs\n       for(String aSrc: srcs) {\n         checkPathAccess(pc, aSrc, FsAction.READ); // read the file\n         checkParentAccess(pc, aSrc, FsAction.WRITE); // for delete \n       }\n     }\n \n     // to make sure no two files are the same\n     Set\u003cINode\u003e si \u003d new HashSet\u003cINode\u003e();\n \n     // we put the following prerequisite for the operation\n     // replication and blocks sizes should be the same for ALL the blocks\n \n     // check the target\n     final INodeFile trgInode \u003d INodeFile.valueOf(dir.getINode4Write(target),\n         target);\n     if(trgInode.isUnderConstruction()) {\n       throw new HadoopIllegalArgumentException(\"concat: target file \"\n           + target + \" is under construction\");\n     }\n     // per design target shouldn\u0027t be empty and all the blocks same size\n     if(trgInode.numBlocks() \u003d\u003d 0) {\n       throw new HadoopIllegalArgumentException(\"concat: target file \"\n           + target + \" is empty\");\n     }\n-    if (trgInode instanceof INodeFileWithSnapshot) {\n+    if (trgInode.isWithSnapshot()) {\n       throw new HadoopIllegalArgumentException(\"concat: target file \"\n           + target + \" is in a snapshot\");\n     }\n \n     long blockSize \u003d trgInode.getPreferredBlockSize();\n \n     // check the end block to be full\n     final BlockInfo last \u003d trgInode.getLastBlock();\n     if(blockSize !\u003d last.getNumBytes()) {\n       throw new HadoopIllegalArgumentException(\"The last block in \" + target\n           + \" is not full; last block size \u003d \" + last.getNumBytes()\n           + \" but file block size \u003d \" + blockSize);\n     }\n \n     si.add(trgInode);\n     final short repl \u003d trgInode.getFileReplication();\n \n     // now check the srcs\n     boolean endSrc \u003d false; // final src file doesn\u0027t have to have full end block\n     for(int i\u003d0; i\u003csrcs.length; i++) {\n       String src \u003d srcs[i];\n       if(i\u003d\u003dsrcs.length-1)\n         endSrc\u003dtrue;\n \n       final INodeFile srcInode \u003d INodeFile.valueOf(dir.getINode4Write(src), src);\n       if(src.isEmpty() \n           || srcInode.isUnderConstruction()\n           || srcInode.numBlocks() \u003d\u003d 0) {\n         throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n             + \" is invalid or empty or underConstruction\");\n       }\n \n       // check replication and blocks size\n       if(repl !\u003d srcInode.getBlockReplication()) {\n         throw new HadoopIllegalArgumentException(\"concat: the soruce file \"\n             + src + \" and the target file \" + target\n             + \" should have the same replication: source replication is \"\n             + srcInode.getBlockReplication()\n             + \" but target replication is \" + repl);\n       }\n \n       //boolean endBlock\u003dfalse;\n       // verify that all the blocks are of the same length as target\n       // should be enough to check the end blocks\n       final BlockInfo[] srcBlocks \u003d srcInode.getBlocks();\n       int idx \u003d srcBlocks.length-1;\n       if(endSrc)\n         idx \u003d srcBlocks.length-2; // end block of endSrc is OK not to be full\n       if(idx \u003e\u003d 0 \u0026\u0026 srcBlocks[idx].getNumBytes() !\u003d blockSize) {\n         throw new HadoopIllegalArgumentException(\"concat: the soruce file \"\n             + src + \" and the target file \" + target\n             + \" should have the same blocks sizes: target block size is \"\n             + blockSize + \" but the size of source block \" + idx + \" is \"\n             + srcBlocks[idx].getNumBytes());\n       }\n \n       si.add(srcInode);\n     }\n \n     // make sure no two files are the same\n     if(si.size() \u003c srcs.length+1) { // trg + srcs\n       // it means at least two files are the same\n       throw new HadoopIllegalArgumentException(\n           \"concat: at least two of the source files are the same\");\n     }\n \n     if(NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" + \n           Arrays.toString(srcs) + \" to \" + target);\n     }\n \n     dir.concat(target,srcs, logRetryCache);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void concatInternal(FSPermissionChecker pc, String target,\n      String[] srcs, boolean logRetryCache) throws IOException,\n      UnresolvedLinkException {\n    assert hasWriteLock();\n\n    // write permission for the target\n    if (isPermissionEnabled) {\n      checkPathAccess(pc, target, FsAction.WRITE);\n\n      // and srcs\n      for(String aSrc: srcs) {\n        checkPathAccess(pc, aSrc, FsAction.READ); // read the file\n        checkParentAccess(pc, aSrc, FsAction.WRITE); // for delete \n      }\n    }\n\n    // to make sure no two files are the same\n    Set\u003cINode\u003e si \u003d new HashSet\u003cINode\u003e();\n\n    // we put the following prerequisite for the operation\n    // replication and blocks sizes should be the same for ALL the blocks\n\n    // check the target\n    final INodeFile trgInode \u003d INodeFile.valueOf(dir.getINode4Write(target),\n        target);\n    if(trgInode.isUnderConstruction()) {\n      throw new HadoopIllegalArgumentException(\"concat: target file \"\n          + target + \" is under construction\");\n    }\n    // per design target shouldn\u0027t be empty and all the blocks same size\n    if(trgInode.numBlocks() \u003d\u003d 0) {\n      throw new HadoopIllegalArgumentException(\"concat: target file \"\n          + target + \" is empty\");\n    }\n    if (trgInode.isWithSnapshot()) {\n      throw new HadoopIllegalArgumentException(\"concat: target file \"\n          + target + \" is in a snapshot\");\n    }\n\n    long blockSize \u003d trgInode.getPreferredBlockSize();\n\n    // check the end block to be full\n    final BlockInfo last \u003d trgInode.getLastBlock();\n    if(blockSize !\u003d last.getNumBytes()) {\n      throw new HadoopIllegalArgumentException(\"The last block in \" + target\n          + \" is not full; last block size \u003d \" + last.getNumBytes()\n          + \" but file block size \u003d \" + blockSize);\n    }\n\n    si.add(trgInode);\n    final short repl \u003d trgInode.getFileReplication();\n\n    // now check the srcs\n    boolean endSrc \u003d false; // final src file doesn\u0027t have to have full end block\n    for(int i\u003d0; i\u003csrcs.length; i++) {\n      String src \u003d srcs[i];\n      if(i\u003d\u003dsrcs.length-1)\n        endSrc\u003dtrue;\n\n      final INodeFile srcInode \u003d INodeFile.valueOf(dir.getINode4Write(src), src);\n      if(src.isEmpty() \n          || srcInode.isUnderConstruction()\n          || srcInode.numBlocks() \u003d\u003d 0) {\n        throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n            + \" is invalid or empty or underConstruction\");\n      }\n\n      // check replication and blocks size\n      if(repl !\u003d srcInode.getBlockReplication()) {\n        throw new HadoopIllegalArgumentException(\"concat: the soruce file \"\n            + src + \" and the target file \" + target\n            + \" should have the same replication: source replication is \"\n            + srcInode.getBlockReplication()\n            + \" but target replication is \" + repl);\n      }\n\n      //boolean endBlock\u003dfalse;\n      // verify that all the blocks are of the same length as target\n      // should be enough to check the end blocks\n      final BlockInfo[] srcBlocks \u003d srcInode.getBlocks();\n      int idx \u003d srcBlocks.length-1;\n      if(endSrc)\n        idx \u003d srcBlocks.length-2; // end block of endSrc is OK not to be full\n      if(idx \u003e\u003d 0 \u0026\u0026 srcBlocks[idx].getNumBytes() !\u003d blockSize) {\n        throw new HadoopIllegalArgumentException(\"concat: the soruce file \"\n            + src + \" and the target file \" + target\n            + \" should have the same blocks sizes: target block size is \"\n            + blockSize + \" but the size of source block \" + idx + \" is \"\n            + srcBlocks[idx].getNumBytes());\n      }\n\n      si.add(srcInode);\n    }\n\n    // make sure no two files are the same\n    if(si.size() \u003c srcs.length+1) { // trg + srcs\n      // it means at least two files are the same\n      throw new HadoopIllegalArgumentException(\n          \"concat: at least two of the source files are the same\");\n    }\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" + \n          Arrays.toString(srcs) + \" to \" + target);\n    }\n\n    dir.concat(target,srcs, logRetryCache);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "8c7a7e619699386f9e6991842558d78aa0c8053d": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-5025. Record ClientId and CallId in EditLog to enable rebuilding retry cache in case of HA failover. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1508332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "30/07/13 12:51 AM",
      "commitName": "8c7a7e619699386f9e6991842558d78aa0c8053d",
      "commitAuthor": "Suresh Srinivas",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-5025. Record ClientId and CallId in EditLog to enable rebuilding retry cache in case of HA failover. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1508332 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "30/07/13 12:51 AM",
          "commitName": "8c7a7e619699386f9e6991842558d78aa0c8053d",
          "commitAuthor": "Suresh Srinivas",
          "commitDateOld": "26/07/13 4:59 PM",
          "commitNameOld": "dc17bda4b677e30c02c2a9a053895a43e41f7a12",
          "commitAuthorOld": "Konstantin Boudnik",
          "daysBetweenCommits": 3.33,
          "commitsBetweenForRepo": 18,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,107 +1,108 @@\n-  private void concatInternal(FSPermissionChecker pc, String target, String [] srcs) \n-      throws IOException, UnresolvedLinkException {\n+  private void concatInternal(FSPermissionChecker pc, String target,\n+      String[] srcs, boolean logRetryCache) throws IOException,\n+      UnresolvedLinkException {\n     assert hasWriteLock();\n \n     // write permission for the target\n     if (isPermissionEnabled) {\n       checkPathAccess(pc, target, FsAction.WRITE);\n \n       // and srcs\n       for(String aSrc: srcs) {\n         checkPathAccess(pc, aSrc, FsAction.READ); // read the file\n         checkParentAccess(pc, aSrc, FsAction.WRITE); // for delete \n       }\n     }\n \n     // to make sure no two files are the same\n     Set\u003cINode\u003e si \u003d new HashSet\u003cINode\u003e();\n \n     // we put the following prerequisite for the operation\n     // replication and blocks sizes should be the same for ALL the blocks\n \n     // check the target\n     final INodeFile trgInode \u003d INodeFile.valueOf(dir.getINode4Write(target),\n         target);\n     if(trgInode.isUnderConstruction()) {\n       throw new HadoopIllegalArgumentException(\"concat: target file \"\n           + target + \" is under construction\");\n     }\n     // per design target shouldn\u0027t be empty and all the blocks same size\n     if(trgInode.numBlocks() \u003d\u003d 0) {\n       throw new HadoopIllegalArgumentException(\"concat: target file \"\n           + target + \" is empty\");\n     }\n     if (trgInode instanceof INodeFileWithSnapshot) {\n       throw new HadoopIllegalArgumentException(\"concat: target file \"\n           + target + \" is in a snapshot\");\n     }\n \n     long blockSize \u003d trgInode.getPreferredBlockSize();\n \n     // check the end block to be full\n     final BlockInfo last \u003d trgInode.getLastBlock();\n     if(blockSize !\u003d last.getNumBytes()) {\n       throw new HadoopIllegalArgumentException(\"The last block in \" + target\n           + \" is not full; last block size \u003d \" + last.getNumBytes()\n           + \" but file block size \u003d \" + blockSize);\n     }\n \n     si.add(trgInode);\n     final short repl \u003d trgInode.getFileReplication();\n \n     // now check the srcs\n     boolean endSrc \u003d false; // final src file doesn\u0027t have to have full end block\n     for(int i\u003d0; i\u003csrcs.length; i++) {\n       String src \u003d srcs[i];\n       if(i\u003d\u003dsrcs.length-1)\n         endSrc\u003dtrue;\n \n       final INodeFile srcInode \u003d INodeFile.valueOf(dir.getINode4Write(src), src);\n       if(src.isEmpty() \n           || srcInode.isUnderConstruction()\n           || srcInode.numBlocks() \u003d\u003d 0) {\n         throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n             + \" is invalid or empty or underConstruction\");\n       }\n \n       // check replication and blocks size\n       if(repl !\u003d srcInode.getBlockReplication()) {\n         throw new HadoopIllegalArgumentException(\"concat: the soruce file \"\n             + src + \" and the target file \" + target\n             + \" should have the same replication: source replication is \"\n             + srcInode.getBlockReplication()\n             + \" but target replication is \" + repl);\n       }\n \n       //boolean endBlock\u003dfalse;\n       // verify that all the blocks are of the same length as target\n       // should be enough to check the end blocks\n       final BlockInfo[] srcBlocks \u003d srcInode.getBlocks();\n       int idx \u003d srcBlocks.length-1;\n       if(endSrc)\n         idx \u003d srcBlocks.length-2; // end block of endSrc is OK not to be full\n       if(idx \u003e\u003d 0 \u0026\u0026 srcBlocks[idx].getNumBytes() !\u003d blockSize) {\n         throw new HadoopIllegalArgumentException(\"concat: the soruce file \"\n             + src + \" and the target file \" + target\n             + \" should have the same blocks sizes: target block size is \"\n             + blockSize + \" but the size of source block \" + idx + \" is \"\n             + srcBlocks[idx].getNumBytes());\n       }\n \n       si.add(srcInode);\n     }\n \n     // make sure no two files are the same\n     if(si.size() \u003c srcs.length+1) { // trg + srcs\n       // it means at least two files are the same\n       throw new HadoopIllegalArgumentException(\n           \"concat: at least two of the source files are the same\");\n     }\n \n     if(NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" + \n           Arrays.toString(srcs) + \" to \" + target);\n     }\n \n-    dir.concat(target,srcs);\n+    dir.concat(target,srcs, logRetryCache);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void concatInternal(FSPermissionChecker pc, String target,\n      String[] srcs, boolean logRetryCache) throws IOException,\n      UnresolvedLinkException {\n    assert hasWriteLock();\n\n    // write permission for the target\n    if (isPermissionEnabled) {\n      checkPathAccess(pc, target, FsAction.WRITE);\n\n      // and srcs\n      for(String aSrc: srcs) {\n        checkPathAccess(pc, aSrc, FsAction.READ); // read the file\n        checkParentAccess(pc, aSrc, FsAction.WRITE); // for delete \n      }\n    }\n\n    // to make sure no two files are the same\n    Set\u003cINode\u003e si \u003d new HashSet\u003cINode\u003e();\n\n    // we put the following prerequisite for the operation\n    // replication and blocks sizes should be the same for ALL the blocks\n\n    // check the target\n    final INodeFile trgInode \u003d INodeFile.valueOf(dir.getINode4Write(target),\n        target);\n    if(trgInode.isUnderConstruction()) {\n      throw new HadoopIllegalArgumentException(\"concat: target file \"\n          + target + \" is under construction\");\n    }\n    // per design target shouldn\u0027t be empty and all the blocks same size\n    if(trgInode.numBlocks() \u003d\u003d 0) {\n      throw new HadoopIllegalArgumentException(\"concat: target file \"\n          + target + \" is empty\");\n    }\n    if (trgInode instanceof INodeFileWithSnapshot) {\n      throw new HadoopIllegalArgumentException(\"concat: target file \"\n          + target + \" is in a snapshot\");\n    }\n\n    long blockSize \u003d trgInode.getPreferredBlockSize();\n\n    // check the end block to be full\n    final BlockInfo last \u003d trgInode.getLastBlock();\n    if(blockSize !\u003d last.getNumBytes()) {\n      throw new HadoopIllegalArgumentException(\"The last block in \" + target\n          + \" is not full; last block size \u003d \" + last.getNumBytes()\n          + \" but file block size \u003d \" + blockSize);\n    }\n\n    si.add(trgInode);\n    final short repl \u003d trgInode.getFileReplication();\n\n    // now check the srcs\n    boolean endSrc \u003d false; // final src file doesn\u0027t have to have full end block\n    for(int i\u003d0; i\u003csrcs.length; i++) {\n      String src \u003d srcs[i];\n      if(i\u003d\u003dsrcs.length-1)\n        endSrc\u003dtrue;\n\n      final INodeFile srcInode \u003d INodeFile.valueOf(dir.getINode4Write(src), src);\n      if(src.isEmpty() \n          || srcInode.isUnderConstruction()\n          || srcInode.numBlocks() \u003d\u003d 0) {\n        throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n            + \" is invalid or empty or underConstruction\");\n      }\n\n      // check replication and blocks size\n      if(repl !\u003d srcInode.getBlockReplication()) {\n        throw new HadoopIllegalArgumentException(\"concat: the soruce file \"\n            + src + \" and the target file \" + target\n            + \" should have the same replication: source replication is \"\n            + srcInode.getBlockReplication()\n            + \" but target replication is \" + repl);\n      }\n\n      //boolean endBlock\u003dfalse;\n      // verify that all the blocks are of the same length as target\n      // should be enough to check the end blocks\n      final BlockInfo[] srcBlocks \u003d srcInode.getBlocks();\n      int idx \u003d srcBlocks.length-1;\n      if(endSrc)\n        idx \u003d srcBlocks.length-2; // end block of endSrc is OK not to be full\n      if(idx \u003e\u003d 0 \u0026\u0026 srcBlocks[idx].getNumBytes() !\u003d blockSize) {\n        throw new HadoopIllegalArgumentException(\"concat: the soruce file \"\n            + src + \" and the target file \" + target\n            + \" should have the same blocks sizes: target block size is \"\n            + blockSize + \" but the size of source block \" + idx + \" is \"\n            + srcBlocks[idx].getNumBytes());\n      }\n\n      si.add(srcInode);\n    }\n\n    // make sure no two files are the same\n    if(si.size() \u003c srcs.length+1) { // trg + srcs\n      // it means at least two files are the same\n      throw new HadoopIllegalArgumentException(\n          \"concat: at least two of the source files are the same\");\n    }\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" + \n          Arrays.toString(srcs) + \" to \" + target);\n    }\n\n    dir.concat(target,srcs, logRetryCache);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
          "extendedDetails": {
            "oldValue": "[pc-FSPermissionChecker, target-String, srcs-String[]]",
            "newValue": "[pc-FSPermissionChecker, target-String, srcs-String[], logRetryCache-boolean]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-5025. Record ClientId and CallId in EditLog to enable rebuilding retry cache in case of HA failover. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1508332 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "30/07/13 12:51 AM",
          "commitName": "8c7a7e619699386f9e6991842558d78aa0c8053d",
          "commitAuthor": "Suresh Srinivas",
          "commitDateOld": "26/07/13 4:59 PM",
          "commitNameOld": "dc17bda4b677e30c02c2a9a053895a43e41f7a12",
          "commitAuthorOld": "Konstantin Boudnik",
          "daysBetweenCommits": 3.33,
          "commitsBetweenForRepo": 18,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,107 +1,108 @@\n-  private void concatInternal(FSPermissionChecker pc, String target, String [] srcs) \n-      throws IOException, UnresolvedLinkException {\n+  private void concatInternal(FSPermissionChecker pc, String target,\n+      String[] srcs, boolean logRetryCache) throws IOException,\n+      UnresolvedLinkException {\n     assert hasWriteLock();\n \n     // write permission for the target\n     if (isPermissionEnabled) {\n       checkPathAccess(pc, target, FsAction.WRITE);\n \n       // and srcs\n       for(String aSrc: srcs) {\n         checkPathAccess(pc, aSrc, FsAction.READ); // read the file\n         checkParentAccess(pc, aSrc, FsAction.WRITE); // for delete \n       }\n     }\n \n     // to make sure no two files are the same\n     Set\u003cINode\u003e si \u003d new HashSet\u003cINode\u003e();\n \n     // we put the following prerequisite for the operation\n     // replication and blocks sizes should be the same for ALL the blocks\n \n     // check the target\n     final INodeFile trgInode \u003d INodeFile.valueOf(dir.getINode4Write(target),\n         target);\n     if(trgInode.isUnderConstruction()) {\n       throw new HadoopIllegalArgumentException(\"concat: target file \"\n           + target + \" is under construction\");\n     }\n     // per design target shouldn\u0027t be empty and all the blocks same size\n     if(trgInode.numBlocks() \u003d\u003d 0) {\n       throw new HadoopIllegalArgumentException(\"concat: target file \"\n           + target + \" is empty\");\n     }\n     if (trgInode instanceof INodeFileWithSnapshot) {\n       throw new HadoopIllegalArgumentException(\"concat: target file \"\n           + target + \" is in a snapshot\");\n     }\n \n     long blockSize \u003d trgInode.getPreferredBlockSize();\n \n     // check the end block to be full\n     final BlockInfo last \u003d trgInode.getLastBlock();\n     if(blockSize !\u003d last.getNumBytes()) {\n       throw new HadoopIllegalArgumentException(\"The last block in \" + target\n           + \" is not full; last block size \u003d \" + last.getNumBytes()\n           + \" but file block size \u003d \" + blockSize);\n     }\n \n     si.add(trgInode);\n     final short repl \u003d trgInode.getFileReplication();\n \n     // now check the srcs\n     boolean endSrc \u003d false; // final src file doesn\u0027t have to have full end block\n     for(int i\u003d0; i\u003csrcs.length; i++) {\n       String src \u003d srcs[i];\n       if(i\u003d\u003dsrcs.length-1)\n         endSrc\u003dtrue;\n \n       final INodeFile srcInode \u003d INodeFile.valueOf(dir.getINode4Write(src), src);\n       if(src.isEmpty() \n           || srcInode.isUnderConstruction()\n           || srcInode.numBlocks() \u003d\u003d 0) {\n         throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n             + \" is invalid or empty or underConstruction\");\n       }\n \n       // check replication and blocks size\n       if(repl !\u003d srcInode.getBlockReplication()) {\n         throw new HadoopIllegalArgumentException(\"concat: the soruce file \"\n             + src + \" and the target file \" + target\n             + \" should have the same replication: source replication is \"\n             + srcInode.getBlockReplication()\n             + \" but target replication is \" + repl);\n       }\n \n       //boolean endBlock\u003dfalse;\n       // verify that all the blocks are of the same length as target\n       // should be enough to check the end blocks\n       final BlockInfo[] srcBlocks \u003d srcInode.getBlocks();\n       int idx \u003d srcBlocks.length-1;\n       if(endSrc)\n         idx \u003d srcBlocks.length-2; // end block of endSrc is OK not to be full\n       if(idx \u003e\u003d 0 \u0026\u0026 srcBlocks[idx].getNumBytes() !\u003d blockSize) {\n         throw new HadoopIllegalArgumentException(\"concat: the soruce file \"\n             + src + \" and the target file \" + target\n             + \" should have the same blocks sizes: target block size is \"\n             + blockSize + \" but the size of source block \" + idx + \" is \"\n             + srcBlocks[idx].getNumBytes());\n       }\n \n       si.add(srcInode);\n     }\n \n     // make sure no two files are the same\n     if(si.size() \u003c srcs.length+1) { // trg + srcs\n       // it means at least two files are the same\n       throw new HadoopIllegalArgumentException(\n           \"concat: at least two of the source files are the same\");\n     }\n \n     if(NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" + \n           Arrays.toString(srcs) + \" to \" + target);\n     }\n \n-    dir.concat(target,srcs);\n+    dir.concat(target,srcs, logRetryCache);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void concatInternal(FSPermissionChecker pc, String target,\n      String[] srcs, boolean logRetryCache) throws IOException,\n      UnresolvedLinkException {\n    assert hasWriteLock();\n\n    // write permission for the target\n    if (isPermissionEnabled) {\n      checkPathAccess(pc, target, FsAction.WRITE);\n\n      // and srcs\n      for(String aSrc: srcs) {\n        checkPathAccess(pc, aSrc, FsAction.READ); // read the file\n        checkParentAccess(pc, aSrc, FsAction.WRITE); // for delete \n      }\n    }\n\n    // to make sure no two files are the same\n    Set\u003cINode\u003e si \u003d new HashSet\u003cINode\u003e();\n\n    // we put the following prerequisite for the operation\n    // replication and blocks sizes should be the same for ALL the blocks\n\n    // check the target\n    final INodeFile trgInode \u003d INodeFile.valueOf(dir.getINode4Write(target),\n        target);\n    if(trgInode.isUnderConstruction()) {\n      throw new HadoopIllegalArgumentException(\"concat: target file \"\n          + target + \" is under construction\");\n    }\n    // per design target shouldn\u0027t be empty and all the blocks same size\n    if(trgInode.numBlocks() \u003d\u003d 0) {\n      throw new HadoopIllegalArgumentException(\"concat: target file \"\n          + target + \" is empty\");\n    }\n    if (trgInode instanceof INodeFileWithSnapshot) {\n      throw new HadoopIllegalArgumentException(\"concat: target file \"\n          + target + \" is in a snapshot\");\n    }\n\n    long blockSize \u003d trgInode.getPreferredBlockSize();\n\n    // check the end block to be full\n    final BlockInfo last \u003d trgInode.getLastBlock();\n    if(blockSize !\u003d last.getNumBytes()) {\n      throw new HadoopIllegalArgumentException(\"The last block in \" + target\n          + \" is not full; last block size \u003d \" + last.getNumBytes()\n          + \" but file block size \u003d \" + blockSize);\n    }\n\n    si.add(trgInode);\n    final short repl \u003d trgInode.getFileReplication();\n\n    // now check the srcs\n    boolean endSrc \u003d false; // final src file doesn\u0027t have to have full end block\n    for(int i\u003d0; i\u003csrcs.length; i++) {\n      String src \u003d srcs[i];\n      if(i\u003d\u003dsrcs.length-1)\n        endSrc\u003dtrue;\n\n      final INodeFile srcInode \u003d INodeFile.valueOf(dir.getINode4Write(src), src);\n      if(src.isEmpty() \n          || srcInode.isUnderConstruction()\n          || srcInode.numBlocks() \u003d\u003d 0) {\n        throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n            + \" is invalid or empty or underConstruction\");\n      }\n\n      // check replication and blocks size\n      if(repl !\u003d srcInode.getBlockReplication()) {\n        throw new HadoopIllegalArgumentException(\"concat: the soruce file \"\n            + src + \" and the target file \" + target\n            + \" should have the same replication: source replication is \"\n            + srcInode.getBlockReplication()\n            + \" but target replication is \" + repl);\n      }\n\n      //boolean endBlock\u003dfalse;\n      // verify that all the blocks are of the same length as target\n      // should be enough to check the end blocks\n      final BlockInfo[] srcBlocks \u003d srcInode.getBlocks();\n      int idx \u003d srcBlocks.length-1;\n      if(endSrc)\n        idx \u003d srcBlocks.length-2; // end block of endSrc is OK not to be full\n      if(idx \u003e\u003d 0 \u0026\u0026 srcBlocks[idx].getNumBytes() !\u003d blockSize) {\n        throw new HadoopIllegalArgumentException(\"concat: the soruce file \"\n            + src + \" and the target file \" + target\n            + \" should have the same blocks sizes: target block size is \"\n            + blockSize + \" but the size of source block \" + idx + \" is \"\n            + srcBlocks[idx].getNumBytes());\n      }\n\n      si.add(srcInode);\n    }\n\n    // make sure no two files are the same\n    if(si.size() \u003c srcs.length+1) { // trg + srcs\n      // it means at least two files are the same\n      throw new HadoopIllegalArgumentException(\n          \"concat: at least two of the source files are the same\");\n    }\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" + \n          Arrays.toString(srcs) + \" to \" + target);\n    }\n\n    dir.concat(target,srcs, logRetryCache);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
          "extendedDetails": {}
        }
      ]
    },
    "e2a618e1cc3fb99115547af6540932860dc6766e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4523. Fix INodeFile replacement, TestQuota and javac errors from trunk merge.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1450477 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "26/02/13 2:04 PM",
      "commitName": "e2a618e1cc3fb99115547af6540932860dc6766e",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "25/02/13 4:10 PM",
      "commitNameOld": "aa82b03823d809fb70cc3d420570ef20e3368bdf",
      "commitAuthorOld": "",
      "daysBetweenCommits": 0.91,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,107 +1,107 @@\n   private void concatInternal(FSPermissionChecker pc, String target, String [] srcs) \n       throws IOException, UnresolvedLinkException {\n     assert hasWriteLock();\n \n     // write permission for the target\n     if (isPermissionEnabled) {\n       checkPathAccess(pc, target, FsAction.WRITE);\n \n       // and srcs\n       for(String aSrc: srcs) {\n         checkPathAccess(pc, aSrc, FsAction.READ); // read the file\n         checkParentAccess(pc, aSrc, FsAction.WRITE); // for delete \n       }\n     }\n \n     // to make sure no two files are the same\n     Set\u003cINode\u003e si \u003d new HashSet\u003cINode\u003e();\n \n     // we put the following prerequisite for the operation\n     // replication and blocks sizes should be the same for ALL the blocks\n \n     // check the target\n     final INodeFile trgInode \u003d INodeFile.valueOf(dir.getINode4Write(target),\n         target);\n     if(trgInode.isUnderConstruction()) {\n       throw new HadoopIllegalArgumentException(\"concat: target file \"\n           + target + \" is under construction\");\n     }\n     // per design target shouldn\u0027t be empty and all the blocks same size\n     if(trgInode.numBlocks() \u003d\u003d 0) {\n       throw new HadoopIllegalArgumentException(\"concat: target file \"\n           + target + \" is empty\");\n     }\n     if (trgInode instanceof INodeFileWithSnapshot) {\n       throw new HadoopIllegalArgumentException(\"concat: target file \"\n           + target + \" is in a snapshot\");\n     }\n \n     long blockSize \u003d trgInode.getPreferredBlockSize();\n \n     // check the end block to be full\n     final BlockInfo last \u003d trgInode.getLastBlock();\n     if(blockSize !\u003d last.getNumBytes()) {\n       throw new HadoopIllegalArgumentException(\"The last block in \" + target\n           + \" is not full; last block size \u003d \" + last.getNumBytes()\n           + \" but file block size \u003d \" + blockSize);\n     }\n \n     si.add(trgInode);\n     final short repl \u003d trgInode.getFileReplication();\n \n     // now check the srcs\n     boolean endSrc \u003d false; // final src file doesn\u0027t have to have full end block\n     for(int i\u003d0; i\u003csrcs.length; i++) {\n       String src \u003d srcs[i];\n       if(i\u003d\u003dsrcs.length-1)\n         endSrc\u003dtrue;\n \n       final INodeFile srcInode \u003d INodeFile.valueOf(dir.getINode4Write(src), src);\n       if(src.isEmpty() \n           || srcInode.isUnderConstruction()\n           || srcInode.numBlocks() \u003d\u003d 0) {\n         throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n             + \" is invalid or empty or underConstruction\");\n       }\n \n       // check replication and blocks size\n-      if(repl !\u003d srcInode.getFileReplication()) {\n+      if(repl !\u003d srcInode.getBlockReplication()) {\n         throw new HadoopIllegalArgumentException(\"concat: the soruce file \"\n             + src + \" and the target file \" + target\n             + \" should have the same replication: source replication is \"\n             + srcInode.getBlockReplication()\n             + \" but target replication is \" + repl);\n       }\n \n       //boolean endBlock\u003dfalse;\n       // verify that all the blocks are of the same length as target\n       // should be enough to check the end blocks\n       final BlockInfo[] srcBlocks \u003d srcInode.getBlocks();\n       int idx \u003d srcBlocks.length-1;\n       if(endSrc)\n         idx \u003d srcBlocks.length-2; // end block of endSrc is OK not to be full\n       if(idx \u003e\u003d 0 \u0026\u0026 srcBlocks[idx].getNumBytes() !\u003d blockSize) {\n         throw new HadoopIllegalArgumentException(\"concat: the soruce file \"\n             + src + \" and the target file \" + target\n             + \" should have the same blocks sizes: target block size is \"\n             + blockSize + \" but the size of source block \" + idx + \" is \"\n             + srcBlocks[idx].getNumBytes());\n       }\n \n       si.add(srcInode);\n     }\n \n     // make sure no two files are the same\n     if(si.size() \u003c srcs.length+1) { // trg + srcs\n       // it means at least two files are the same\n       throw new HadoopIllegalArgumentException(\n           \"concat: at least two of the source files are the same\");\n     }\n \n     if(NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" + \n           Arrays.toString(srcs) + \" to \" + target);\n     }\n \n     dir.concat(target,srcs);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void concatInternal(FSPermissionChecker pc, String target, String [] srcs) \n      throws IOException, UnresolvedLinkException {\n    assert hasWriteLock();\n\n    // write permission for the target\n    if (isPermissionEnabled) {\n      checkPathAccess(pc, target, FsAction.WRITE);\n\n      // and srcs\n      for(String aSrc: srcs) {\n        checkPathAccess(pc, aSrc, FsAction.READ); // read the file\n        checkParentAccess(pc, aSrc, FsAction.WRITE); // for delete \n      }\n    }\n\n    // to make sure no two files are the same\n    Set\u003cINode\u003e si \u003d new HashSet\u003cINode\u003e();\n\n    // we put the following prerequisite for the operation\n    // replication and blocks sizes should be the same for ALL the blocks\n\n    // check the target\n    final INodeFile trgInode \u003d INodeFile.valueOf(dir.getINode4Write(target),\n        target);\n    if(trgInode.isUnderConstruction()) {\n      throw new HadoopIllegalArgumentException(\"concat: target file \"\n          + target + \" is under construction\");\n    }\n    // per design target shouldn\u0027t be empty and all the blocks same size\n    if(trgInode.numBlocks() \u003d\u003d 0) {\n      throw new HadoopIllegalArgumentException(\"concat: target file \"\n          + target + \" is empty\");\n    }\n    if (trgInode instanceof INodeFileWithSnapshot) {\n      throw new HadoopIllegalArgumentException(\"concat: target file \"\n          + target + \" is in a snapshot\");\n    }\n\n    long blockSize \u003d trgInode.getPreferredBlockSize();\n\n    // check the end block to be full\n    final BlockInfo last \u003d trgInode.getLastBlock();\n    if(blockSize !\u003d last.getNumBytes()) {\n      throw new HadoopIllegalArgumentException(\"The last block in \" + target\n          + \" is not full; last block size \u003d \" + last.getNumBytes()\n          + \" but file block size \u003d \" + blockSize);\n    }\n\n    si.add(trgInode);\n    final short repl \u003d trgInode.getFileReplication();\n\n    // now check the srcs\n    boolean endSrc \u003d false; // final src file doesn\u0027t have to have full end block\n    for(int i\u003d0; i\u003csrcs.length; i++) {\n      String src \u003d srcs[i];\n      if(i\u003d\u003dsrcs.length-1)\n        endSrc\u003dtrue;\n\n      final INodeFile srcInode \u003d INodeFile.valueOf(dir.getINode4Write(src), src);\n      if(src.isEmpty() \n          || srcInode.isUnderConstruction()\n          || srcInode.numBlocks() \u003d\u003d 0) {\n        throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n            + \" is invalid or empty or underConstruction\");\n      }\n\n      // check replication and blocks size\n      if(repl !\u003d srcInode.getBlockReplication()) {\n        throw new HadoopIllegalArgumentException(\"concat: the soruce file \"\n            + src + \" and the target file \" + target\n            + \" should have the same replication: source replication is \"\n            + srcInode.getBlockReplication()\n            + \" but target replication is \" + repl);\n      }\n\n      //boolean endBlock\u003dfalse;\n      // verify that all the blocks are of the same length as target\n      // should be enough to check the end blocks\n      final BlockInfo[] srcBlocks \u003d srcInode.getBlocks();\n      int idx \u003d srcBlocks.length-1;\n      if(endSrc)\n        idx \u003d srcBlocks.length-2; // end block of endSrc is OK not to be full\n      if(idx \u003e\u003d 0 \u0026\u0026 srcBlocks[idx].getNumBytes() !\u003d blockSize) {\n        throw new HadoopIllegalArgumentException(\"concat: the soruce file \"\n            + src + \" and the target file \" + target\n            + \" should have the same blocks sizes: target block size is \"\n            + blockSize + \" but the size of source block \" + idx + \" is \"\n            + srcBlocks[idx].getNumBytes());\n      }\n\n      si.add(srcInode);\n    }\n\n    // make sure no two files are the same\n    if(si.size() \u003c srcs.length+1) { // trg + srcs\n      // it means at least two files are the same\n      throw new HadoopIllegalArgumentException(\n          \"concat: at least two of the source files are the same\");\n    }\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" + \n          Arrays.toString(srcs) + \" to \" + target);\n    }\n\n    dir.concat(target,srcs);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "cdb292f44caff9763631d9e9bcd69c375a7cddea": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-4222. NN is unresponsive and loses heartbeats from DNs when configured to use LDAP and LDAP has issues. Contributed by Xiaobo Peng and Suresh Srinivas.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1448801 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "21/02/13 1:02 PM",
      "commitName": "cdb292f44caff9763631d9e9bcd69c375a7cddea",
      "commitAuthor": "Suresh Srinivas",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-4222. NN is unresponsive and loses heartbeats from DNs when configured to use LDAP and LDAP has issues. Contributed by Xiaobo Peng and Suresh Srinivas.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1448801 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "21/02/13 1:02 PM",
          "commitName": "cdb292f44caff9763631d9e9bcd69c375a7cddea",
          "commitAuthor": "Suresh Srinivas",
          "commitDateOld": "11/02/13 4:50 PM",
          "commitNameOld": "969e84decbc976bd98f1050aead695d15a024ab6",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 9.84,
          "commitsBetweenForRepo": 29,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,102 +1,102 @@\n-  private void concatInternal(String target, String [] srcs) \n+  private void concatInternal(FSPermissionChecker pc, String target, String [] srcs) \n       throws IOException, UnresolvedLinkException {\n     assert hasWriteLock();\n \n     // write permission for the target\n     if (isPermissionEnabled) {\n-      checkPathAccess(target, FsAction.WRITE);\n+      checkPathAccess(pc, target, FsAction.WRITE);\n \n       // and srcs\n       for(String aSrc: srcs) {\n-        checkPathAccess(aSrc, FsAction.READ); // read the file\n-        checkParentAccess(aSrc, FsAction.WRITE); // for delete \n+        checkPathAccess(pc, aSrc, FsAction.READ); // read the file\n+        checkParentAccess(pc, aSrc, FsAction.WRITE); // for delete \n       }\n     }\n \n     // to make sure no two files are the same\n     Set\u003cINode\u003e si \u003d new HashSet\u003cINode\u003e();\n \n     // we put the following prerequisite for the operation\n     // replication and blocks sizes should be the same for ALL the blocks\n \n     // check the target\n     final INodeFile trgInode \u003d INodeFile.valueOf(dir.getINode(target), target);\n     if(trgInode.isUnderConstruction()) {\n       throw new HadoopIllegalArgumentException(\"concat: target file \"\n           + target + \" is under construction\");\n     }\n     // per design target shouldn\u0027t be empty and all the blocks same size\n     if(trgInode.numBlocks() \u003d\u003d 0) {\n       throw new HadoopIllegalArgumentException(\"concat: target file \"\n           + target + \" is empty\");\n     }\n \n     long blockSize \u003d trgInode.getPreferredBlockSize();\n \n     // check the end block to be full\n     final BlockInfo last \u003d trgInode.getLastBlock();\n     if(blockSize !\u003d last.getNumBytes()) {\n       throw new HadoopIllegalArgumentException(\"The last block in \" + target\n           + \" is not full; last block size \u003d \" + last.getNumBytes()\n           + \" but file block size \u003d \" + blockSize);\n     }\n \n     si.add(trgInode);\n     short repl \u003d trgInode.getBlockReplication();\n \n     // now check the srcs\n     boolean endSrc \u003d false; // final src file doesn\u0027t have to have full end block\n     for(int i\u003d0; i\u003csrcs.length; i++) {\n       String src \u003d srcs[i];\n       if(i\u003d\u003dsrcs.length-1)\n         endSrc\u003dtrue;\n \n       final INodeFile srcInode \u003d INodeFile.valueOf(dir.getINode(src), src);\n       if(src.isEmpty() \n           || srcInode.isUnderConstruction()\n           || srcInode.numBlocks() \u003d\u003d 0) {\n         throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n             + \" is invalid or empty or underConstruction\");\n       }\n \n       // check replication and blocks size\n       if(repl !\u003d srcInode.getBlockReplication()) {\n         throw new HadoopIllegalArgumentException(\"concat: the soruce file \"\n             + src + \" and the target file \" + target\n             + \" should have the same replication: source replication is \"\n             + srcInode.getBlockReplication()\n             + \" but target replication is \" + repl);\n       }\n \n       //boolean endBlock\u003dfalse;\n       // verify that all the blocks are of the same length as target\n       // should be enough to check the end blocks\n       final BlockInfo[] srcBlocks \u003d srcInode.getBlocks();\n       int idx \u003d srcBlocks.length-1;\n       if(endSrc)\n         idx \u003d srcBlocks.length-2; // end block of endSrc is OK not to be full\n       if(idx \u003e\u003d 0 \u0026\u0026 srcBlocks[idx].getNumBytes() !\u003d blockSize) {\n         throw new HadoopIllegalArgumentException(\"concat: the soruce file \"\n             + src + \" and the target file \" + target\n             + \" should have the same blocks sizes: target block size is \"\n             + blockSize + \" but the size of source block \" + idx + \" is \"\n             + srcBlocks[idx].getNumBytes());\n       }\n \n       si.add(srcInode);\n     }\n \n     // make sure no two files are the same\n     if(si.size() \u003c srcs.length+1) { // trg + srcs\n       // it means at least two files are the same\n       throw new HadoopIllegalArgumentException(\n           \"concat: at least two of the source files are the same\");\n     }\n \n     if(NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" + \n           Arrays.toString(srcs) + \" to \" + target);\n     }\n \n     dir.concat(target,srcs);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void concatInternal(FSPermissionChecker pc, String target, String [] srcs) \n      throws IOException, UnresolvedLinkException {\n    assert hasWriteLock();\n\n    // write permission for the target\n    if (isPermissionEnabled) {\n      checkPathAccess(pc, target, FsAction.WRITE);\n\n      // and srcs\n      for(String aSrc: srcs) {\n        checkPathAccess(pc, aSrc, FsAction.READ); // read the file\n        checkParentAccess(pc, aSrc, FsAction.WRITE); // for delete \n      }\n    }\n\n    // to make sure no two files are the same\n    Set\u003cINode\u003e si \u003d new HashSet\u003cINode\u003e();\n\n    // we put the following prerequisite for the operation\n    // replication and blocks sizes should be the same for ALL the blocks\n\n    // check the target\n    final INodeFile trgInode \u003d INodeFile.valueOf(dir.getINode(target), target);\n    if(trgInode.isUnderConstruction()) {\n      throw new HadoopIllegalArgumentException(\"concat: target file \"\n          + target + \" is under construction\");\n    }\n    // per design target shouldn\u0027t be empty and all the blocks same size\n    if(trgInode.numBlocks() \u003d\u003d 0) {\n      throw new HadoopIllegalArgumentException(\"concat: target file \"\n          + target + \" is empty\");\n    }\n\n    long blockSize \u003d trgInode.getPreferredBlockSize();\n\n    // check the end block to be full\n    final BlockInfo last \u003d trgInode.getLastBlock();\n    if(blockSize !\u003d last.getNumBytes()) {\n      throw new HadoopIllegalArgumentException(\"The last block in \" + target\n          + \" is not full; last block size \u003d \" + last.getNumBytes()\n          + \" but file block size \u003d \" + blockSize);\n    }\n\n    si.add(trgInode);\n    short repl \u003d trgInode.getBlockReplication();\n\n    // now check the srcs\n    boolean endSrc \u003d false; // final src file doesn\u0027t have to have full end block\n    for(int i\u003d0; i\u003csrcs.length; i++) {\n      String src \u003d srcs[i];\n      if(i\u003d\u003dsrcs.length-1)\n        endSrc\u003dtrue;\n\n      final INodeFile srcInode \u003d INodeFile.valueOf(dir.getINode(src), src);\n      if(src.isEmpty() \n          || srcInode.isUnderConstruction()\n          || srcInode.numBlocks() \u003d\u003d 0) {\n        throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n            + \" is invalid or empty or underConstruction\");\n      }\n\n      // check replication and blocks size\n      if(repl !\u003d srcInode.getBlockReplication()) {\n        throw new HadoopIllegalArgumentException(\"concat: the soruce file \"\n            + src + \" and the target file \" + target\n            + \" should have the same replication: source replication is \"\n            + srcInode.getBlockReplication()\n            + \" but target replication is \" + repl);\n      }\n\n      //boolean endBlock\u003dfalse;\n      // verify that all the blocks are of the same length as target\n      // should be enough to check the end blocks\n      final BlockInfo[] srcBlocks \u003d srcInode.getBlocks();\n      int idx \u003d srcBlocks.length-1;\n      if(endSrc)\n        idx \u003d srcBlocks.length-2; // end block of endSrc is OK not to be full\n      if(idx \u003e\u003d 0 \u0026\u0026 srcBlocks[idx].getNumBytes() !\u003d blockSize) {\n        throw new HadoopIllegalArgumentException(\"concat: the soruce file \"\n            + src + \" and the target file \" + target\n            + \" should have the same blocks sizes: target block size is \"\n            + blockSize + \" but the size of source block \" + idx + \" is \"\n            + srcBlocks[idx].getNumBytes());\n      }\n\n      si.add(srcInode);\n    }\n\n    // make sure no two files are the same\n    if(si.size() \u003c srcs.length+1) { // trg + srcs\n      // it means at least two files are the same\n      throw new HadoopIllegalArgumentException(\n          \"concat: at least two of the source files are the same\");\n    }\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" + \n          Arrays.toString(srcs) + \" to \" + target);\n    }\n\n    dir.concat(target,srcs);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
          "extendedDetails": {
            "oldValue": "[target-String, srcs-String[]]",
            "newValue": "[pc-FSPermissionChecker, target-String, srcs-String[]]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-4222. NN is unresponsive and loses heartbeats from DNs when configured to use LDAP and LDAP has issues. Contributed by Xiaobo Peng and Suresh Srinivas.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1448801 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "21/02/13 1:02 PM",
          "commitName": "cdb292f44caff9763631d9e9bcd69c375a7cddea",
          "commitAuthor": "Suresh Srinivas",
          "commitDateOld": "11/02/13 4:50 PM",
          "commitNameOld": "969e84decbc976bd98f1050aead695d15a024ab6",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 9.84,
          "commitsBetweenForRepo": 29,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,102 +1,102 @@\n-  private void concatInternal(String target, String [] srcs) \n+  private void concatInternal(FSPermissionChecker pc, String target, String [] srcs) \n       throws IOException, UnresolvedLinkException {\n     assert hasWriteLock();\n \n     // write permission for the target\n     if (isPermissionEnabled) {\n-      checkPathAccess(target, FsAction.WRITE);\n+      checkPathAccess(pc, target, FsAction.WRITE);\n \n       // and srcs\n       for(String aSrc: srcs) {\n-        checkPathAccess(aSrc, FsAction.READ); // read the file\n-        checkParentAccess(aSrc, FsAction.WRITE); // for delete \n+        checkPathAccess(pc, aSrc, FsAction.READ); // read the file\n+        checkParentAccess(pc, aSrc, FsAction.WRITE); // for delete \n       }\n     }\n \n     // to make sure no two files are the same\n     Set\u003cINode\u003e si \u003d new HashSet\u003cINode\u003e();\n \n     // we put the following prerequisite for the operation\n     // replication and blocks sizes should be the same for ALL the blocks\n \n     // check the target\n     final INodeFile trgInode \u003d INodeFile.valueOf(dir.getINode(target), target);\n     if(trgInode.isUnderConstruction()) {\n       throw new HadoopIllegalArgumentException(\"concat: target file \"\n           + target + \" is under construction\");\n     }\n     // per design target shouldn\u0027t be empty and all the blocks same size\n     if(trgInode.numBlocks() \u003d\u003d 0) {\n       throw new HadoopIllegalArgumentException(\"concat: target file \"\n           + target + \" is empty\");\n     }\n \n     long blockSize \u003d trgInode.getPreferredBlockSize();\n \n     // check the end block to be full\n     final BlockInfo last \u003d trgInode.getLastBlock();\n     if(blockSize !\u003d last.getNumBytes()) {\n       throw new HadoopIllegalArgumentException(\"The last block in \" + target\n           + \" is not full; last block size \u003d \" + last.getNumBytes()\n           + \" but file block size \u003d \" + blockSize);\n     }\n \n     si.add(trgInode);\n     short repl \u003d trgInode.getBlockReplication();\n \n     // now check the srcs\n     boolean endSrc \u003d false; // final src file doesn\u0027t have to have full end block\n     for(int i\u003d0; i\u003csrcs.length; i++) {\n       String src \u003d srcs[i];\n       if(i\u003d\u003dsrcs.length-1)\n         endSrc\u003dtrue;\n \n       final INodeFile srcInode \u003d INodeFile.valueOf(dir.getINode(src), src);\n       if(src.isEmpty() \n           || srcInode.isUnderConstruction()\n           || srcInode.numBlocks() \u003d\u003d 0) {\n         throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n             + \" is invalid or empty or underConstruction\");\n       }\n \n       // check replication and blocks size\n       if(repl !\u003d srcInode.getBlockReplication()) {\n         throw new HadoopIllegalArgumentException(\"concat: the soruce file \"\n             + src + \" and the target file \" + target\n             + \" should have the same replication: source replication is \"\n             + srcInode.getBlockReplication()\n             + \" but target replication is \" + repl);\n       }\n \n       //boolean endBlock\u003dfalse;\n       // verify that all the blocks are of the same length as target\n       // should be enough to check the end blocks\n       final BlockInfo[] srcBlocks \u003d srcInode.getBlocks();\n       int idx \u003d srcBlocks.length-1;\n       if(endSrc)\n         idx \u003d srcBlocks.length-2; // end block of endSrc is OK not to be full\n       if(idx \u003e\u003d 0 \u0026\u0026 srcBlocks[idx].getNumBytes() !\u003d blockSize) {\n         throw new HadoopIllegalArgumentException(\"concat: the soruce file \"\n             + src + \" and the target file \" + target\n             + \" should have the same blocks sizes: target block size is \"\n             + blockSize + \" but the size of source block \" + idx + \" is \"\n             + srcBlocks[idx].getNumBytes());\n       }\n \n       si.add(srcInode);\n     }\n \n     // make sure no two files are the same\n     if(si.size() \u003c srcs.length+1) { // trg + srcs\n       // it means at least two files are the same\n       throw new HadoopIllegalArgumentException(\n           \"concat: at least two of the source files are the same\");\n     }\n \n     if(NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" + \n           Arrays.toString(srcs) + \" to \" + target);\n     }\n \n     dir.concat(target,srcs);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void concatInternal(FSPermissionChecker pc, String target, String [] srcs) \n      throws IOException, UnresolvedLinkException {\n    assert hasWriteLock();\n\n    // write permission for the target\n    if (isPermissionEnabled) {\n      checkPathAccess(pc, target, FsAction.WRITE);\n\n      // and srcs\n      for(String aSrc: srcs) {\n        checkPathAccess(pc, aSrc, FsAction.READ); // read the file\n        checkParentAccess(pc, aSrc, FsAction.WRITE); // for delete \n      }\n    }\n\n    // to make sure no two files are the same\n    Set\u003cINode\u003e si \u003d new HashSet\u003cINode\u003e();\n\n    // we put the following prerequisite for the operation\n    // replication and blocks sizes should be the same for ALL the blocks\n\n    // check the target\n    final INodeFile trgInode \u003d INodeFile.valueOf(dir.getINode(target), target);\n    if(trgInode.isUnderConstruction()) {\n      throw new HadoopIllegalArgumentException(\"concat: target file \"\n          + target + \" is under construction\");\n    }\n    // per design target shouldn\u0027t be empty and all the blocks same size\n    if(trgInode.numBlocks() \u003d\u003d 0) {\n      throw new HadoopIllegalArgumentException(\"concat: target file \"\n          + target + \" is empty\");\n    }\n\n    long blockSize \u003d trgInode.getPreferredBlockSize();\n\n    // check the end block to be full\n    final BlockInfo last \u003d trgInode.getLastBlock();\n    if(blockSize !\u003d last.getNumBytes()) {\n      throw new HadoopIllegalArgumentException(\"The last block in \" + target\n          + \" is not full; last block size \u003d \" + last.getNumBytes()\n          + \" but file block size \u003d \" + blockSize);\n    }\n\n    si.add(trgInode);\n    short repl \u003d trgInode.getBlockReplication();\n\n    // now check the srcs\n    boolean endSrc \u003d false; // final src file doesn\u0027t have to have full end block\n    for(int i\u003d0; i\u003csrcs.length; i++) {\n      String src \u003d srcs[i];\n      if(i\u003d\u003dsrcs.length-1)\n        endSrc\u003dtrue;\n\n      final INodeFile srcInode \u003d INodeFile.valueOf(dir.getINode(src), src);\n      if(src.isEmpty() \n          || srcInode.isUnderConstruction()\n          || srcInode.numBlocks() \u003d\u003d 0) {\n        throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n            + \" is invalid or empty or underConstruction\");\n      }\n\n      // check replication and blocks size\n      if(repl !\u003d srcInode.getBlockReplication()) {\n        throw new HadoopIllegalArgumentException(\"concat: the soruce file \"\n            + src + \" and the target file \" + target\n            + \" should have the same replication: source replication is \"\n            + srcInode.getBlockReplication()\n            + \" but target replication is \" + repl);\n      }\n\n      //boolean endBlock\u003dfalse;\n      // verify that all the blocks are of the same length as target\n      // should be enough to check the end blocks\n      final BlockInfo[] srcBlocks \u003d srcInode.getBlocks();\n      int idx \u003d srcBlocks.length-1;\n      if(endSrc)\n        idx \u003d srcBlocks.length-2; // end block of endSrc is OK not to be full\n      if(idx \u003e\u003d 0 \u0026\u0026 srcBlocks[idx].getNumBytes() !\u003d blockSize) {\n        throw new HadoopIllegalArgumentException(\"concat: the soruce file \"\n            + src + \" and the target file \" + target\n            + \" should have the same blocks sizes: target block size is \"\n            + blockSize + \" but the size of source block \" + idx + \" is \"\n            + srcBlocks[idx].getNumBytes());\n      }\n\n      si.add(srcInode);\n    }\n\n    // make sure no two files are the same\n    if(si.size() \u003c srcs.length+1) { // trg + srcs\n      // it means at least two files are the same\n      throw new HadoopIllegalArgumentException(\n          \"concat: at least two of the source files are the same\");\n    }\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" + \n          Arrays.toString(srcs) + \" to \" + target);\n    }\n\n    dir.concat(target,srcs);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
          "extendedDetails": {}
        }
      ]
    },
    "d174f574bafcfefc635c64a47f258b1ce5d5c84e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4143. Change blocks to private in INodeFile and renames isLink() to isSymlink() in INode.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1405237 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "02/11/12 5:20 PM",
      "commitName": "d174f574bafcfefc635c64a47f258b1ce5d5c84e",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "30/10/12 7:33 PM",
      "commitNameOld": "07e0d7730d32be0ed2cb16f8e988115c14d75075",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 2.91,
      "commitsBetweenForRepo": 12,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,101 +1,102 @@\n   private void concatInternal(String target, String [] srcs) \n       throws IOException, UnresolvedLinkException {\n     assert hasWriteLock();\n \n     // write permission for the target\n     if (isPermissionEnabled) {\n       checkPathAccess(target, FsAction.WRITE);\n \n       // and srcs\n       for(String aSrc: srcs) {\n         checkPathAccess(aSrc, FsAction.READ); // read the file\n         checkParentAccess(aSrc, FsAction.WRITE); // for delete \n       }\n     }\n \n     // to make sure no two files are the same\n     Set\u003cINode\u003e si \u003d new HashSet\u003cINode\u003e();\n \n     // we put the following prerequisite for the operation\n     // replication and blocks sizes should be the same for ALL the blocks\n \n     // check the target\n     final INodeFile trgInode \u003d INodeFile.valueOf(dir.getINode(target), target);\n     if(trgInode.isUnderConstruction()) {\n       throw new HadoopIllegalArgumentException(\"concat: target file \"\n           + target + \" is under construction\");\n     }\n     // per design target shouldn\u0027t be empty and all the blocks same size\n-    if(trgInode.blocks.length \u003d\u003d 0) {\n+    if(trgInode.numBlocks() \u003d\u003d 0) {\n       throw new HadoopIllegalArgumentException(\"concat: target file \"\n           + target + \" is empty\");\n     }\n \n     long blockSize \u003d trgInode.getPreferredBlockSize();\n \n     // check the end block to be full\n-    if(blockSize !\u003d trgInode.blocks[trgInode.blocks.length-1].getNumBytes()) {\n+    final BlockInfo last \u003d trgInode.getLastBlock();\n+    if(blockSize !\u003d last.getNumBytes()) {\n       throw new HadoopIllegalArgumentException(\"The last block in \" + target\n-          + \" is not full; last block size \u003d \"\n-          + trgInode.blocks[trgInode.blocks.length-1].getNumBytes()\n+          + \" is not full; last block size \u003d \" + last.getNumBytes()\n           + \" but file block size \u003d \" + blockSize);\n     }\n \n     si.add(trgInode);\n     short repl \u003d trgInode.getBlockReplication();\n \n     // now check the srcs\n     boolean endSrc \u003d false; // final src file doesn\u0027t have to have full end block\n     for(int i\u003d0; i\u003csrcs.length; i++) {\n       String src \u003d srcs[i];\n       if(i\u003d\u003dsrcs.length-1)\n         endSrc\u003dtrue;\n \n       final INodeFile srcInode \u003d INodeFile.valueOf(dir.getINode(src), src);\n       if(src.isEmpty() \n           || srcInode.isUnderConstruction()\n-          || srcInode.blocks.length \u003d\u003d 0) {\n+          || srcInode.numBlocks() \u003d\u003d 0) {\n         throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n             + \" is invalid or empty or underConstruction\");\n       }\n \n       // check replication and blocks size\n       if(repl !\u003d srcInode.getBlockReplication()) {\n         throw new HadoopIllegalArgumentException(\"concat: the soruce file \"\n             + src + \" and the target file \" + target\n             + \" should have the same replication: source replication is \"\n             + srcInode.getBlockReplication()\n             + \" but target replication is \" + repl);\n       }\n \n       //boolean endBlock\u003dfalse;\n       // verify that all the blocks are of the same length as target\n       // should be enough to check the end blocks\n-      int idx \u003d srcInode.blocks.length-1;\n+      final BlockInfo[] srcBlocks \u003d srcInode.getBlocks();\n+      int idx \u003d srcBlocks.length-1;\n       if(endSrc)\n-        idx \u003d srcInode.blocks.length-2; // end block of endSrc is OK not to be full\n-      if(idx \u003e\u003d 0 \u0026\u0026 srcInode.blocks[idx].getNumBytes() !\u003d blockSize) {\n+        idx \u003d srcBlocks.length-2; // end block of endSrc is OK not to be full\n+      if(idx \u003e\u003d 0 \u0026\u0026 srcBlocks[idx].getNumBytes() !\u003d blockSize) {\n         throw new HadoopIllegalArgumentException(\"concat: the soruce file \"\n             + src + \" and the target file \" + target\n             + \" should have the same blocks sizes: target block size is \"\n             + blockSize + \" but the size of source block \" + idx + \" is \"\n-            + srcInode.blocks[idx].getNumBytes());\n+            + srcBlocks[idx].getNumBytes());\n       }\n \n       si.add(srcInode);\n     }\n \n     // make sure no two files are the same\n     if(si.size() \u003c srcs.length+1) { // trg + srcs\n       // it means at least two files are the same\n       throw new HadoopIllegalArgumentException(\n           \"concat: at least two of the source files are the same\");\n     }\n \n     if(NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" + \n           Arrays.toString(srcs) + \" to \" + target);\n     }\n \n     dir.concat(target,srcs);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void concatInternal(String target, String [] srcs) \n      throws IOException, UnresolvedLinkException {\n    assert hasWriteLock();\n\n    // write permission for the target\n    if (isPermissionEnabled) {\n      checkPathAccess(target, FsAction.WRITE);\n\n      // and srcs\n      for(String aSrc: srcs) {\n        checkPathAccess(aSrc, FsAction.READ); // read the file\n        checkParentAccess(aSrc, FsAction.WRITE); // for delete \n      }\n    }\n\n    // to make sure no two files are the same\n    Set\u003cINode\u003e si \u003d new HashSet\u003cINode\u003e();\n\n    // we put the following prerequisite for the operation\n    // replication and blocks sizes should be the same for ALL the blocks\n\n    // check the target\n    final INodeFile trgInode \u003d INodeFile.valueOf(dir.getINode(target), target);\n    if(trgInode.isUnderConstruction()) {\n      throw new HadoopIllegalArgumentException(\"concat: target file \"\n          + target + \" is under construction\");\n    }\n    // per design target shouldn\u0027t be empty and all the blocks same size\n    if(trgInode.numBlocks() \u003d\u003d 0) {\n      throw new HadoopIllegalArgumentException(\"concat: target file \"\n          + target + \" is empty\");\n    }\n\n    long blockSize \u003d trgInode.getPreferredBlockSize();\n\n    // check the end block to be full\n    final BlockInfo last \u003d trgInode.getLastBlock();\n    if(blockSize !\u003d last.getNumBytes()) {\n      throw new HadoopIllegalArgumentException(\"The last block in \" + target\n          + \" is not full; last block size \u003d \" + last.getNumBytes()\n          + \" but file block size \u003d \" + blockSize);\n    }\n\n    si.add(trgInode);\n    short repl \u003d trgInode.getBlockReplication();\n\n    // now check the srcs\n    boolean endSrc \u003d false; // final src file doesn\u0027t have to have full end block\n    for(int i\u003d0; i\u003csrcs.length; i++) {\n      String src \u003d srcs[i];\n      if(i\u003d\u003dsrcs.length-1)\n        endSrc\u003dtrue;\n\n      final INodeFile srcInode \u003d INodeFile.valueOf(dir.getINode(src), src);\n      if(src.isEmpty() \n          || srcInode.isUnderConstruction()\n          || srcInode.numBlocks() \u003d\u003d 0) {\n        throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n            + \" is invalid or empty or underConstruction\");\n      }\n\n      // check replication and blocks size\n      if(repl !\u003d srcInode.getBlockReplication()) {\n        throw new HadoopIllegalArgumentException(\"concat: the soruce file \"\n            + src + \" and the target file \" + target\n            + \" should have the same replication: source replication is \"\n            + srcInode.getBlockReplication()\n            + \" but target replication is \" + repl);\n      }\n\n      //boolean endBlock\u003dfalse;\n      // verify that all the blocks are of the same length as target\n      // should be enough to check the end blocks\n      final BlockInfo[] srcBlocks \u003d srcInode.getBlocks();\n      int idx \u003d srcBlocks.length-1;\n      if(endSrc)\n        idx \u003d srcBlocks.length-2; // end block of endSrc is OK not to be full\n      if(idx \u003e\u003d 0 \u0026\u0026 srcBlocks[idx].getNumBytes() !\u003d blockSize) {\n        throw new HadoopIllegalArgumentException(\"concat: the soruce file \"\n            + src + \" and the target file \" + target\n            + \" should have the same blocks sizes: target block size is \"\n            + blockSize + \" but the size of source block \" + idx + \" is \"\n            + srcBlocks[idx].getNumBytes());\n      }\n\n      si.add(srcInode);\n    }\n\n    // make sure no two files are the same\n    if(si.size() \u003c srcs.length+1) { // trg + srcs\n      // it means at least two files are the same\n      throw new HadoopIllegalArgumentException(\n          \"concat: at least two of the source files are the same\");\n    }\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" + \n          Arrays.toString(srcs) + \" to \" + target);\n    }\n\n    dir.concat(target,srcs);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "ba2ee1d7fb91462c861169224d250d2d90bec3a6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4107. Add utility methods for casting INode to INodeFile and INodeFileUnderConstruction.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1402265 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "25/10/12 11:44 AM",
      "commitName": "ba2ee1d7fb91462c861169224d250d2d90bec3a6",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "22/10/12 11:30 AM",
      "commitNameOld": "75cdb5bb4965161021df47376cccf058bf413f3b",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 3.01,
      "commitsBetweenForRepo": 23,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,98 +1,101 @@\n   private void concatInternal(String target, String [] srcs) \n       throws IOException, UnresolvedLinkException {\n     assert hasWriteLock();\n \n     // write permission for the target\n     if (isPermissionEnabled) {\n       checkPathAccess(target, FsAction.WRITE);\n \n       // and srcs\n       for(String aSrc: srcs) {\n         checkPathAccess(aSrc, FsAction.READ); // read the file\n         checkParentAccess(aSrc, FsAction.WRITE); // for delete \n       }\n     }\n \n     // to make sure no two files are the same\n     Set\u003cINode\u003e si \u003d new HashSet\u003cINode\u003e();\n \n     // we put the following prerequisite for the operation\n     // replication and blocks sizes should be the same for ALL the blocks\n+\n     // check the target\n-    INode inode \u003d dir.getFileINode(target);\n-\n-    if(inode \u003d\u003d null) {\n-      throw new IllegalArgumentException(\"concat: trg file doesn\u0027t exist\");\n+    final INodeFile trgInode \u003d INodeFile.valueOf(dir.getINode(target), target);\n+    if(trgInode.isUnderConstruction()) {\n+      throw new HadoopIllegalArgumentException(\"concat: target file \"\n+          + target + \" is under construction\");\n     }\n-    if(inode.isUnderConstruction()) {\n-      throw new IllegalArgumentException(\"concat: trg file is uner construction\");\n-    }\n-\n-    INodeFile trgInode \u003d (INodeFile) inode;\n-\n-    // per design trg shouldn\u0027t be empty and all the blocks same size\n+    // per design target shouldn\u0027t be empty and all the blocks same size\n     if(trgInode.blocks.length \u003d\u003d 0) {\n-      throw new IllegalArgumentException(\"concat: \"+ target + \" file is empty\");\n+      throw new HadoopIllegalArgumentException(\"concat: target file \"\n+          + target + \" is empty\");\n     }\n \n     long blockSize \u003d trgInode.getPreferredBlockSize();\n \n     // check the end block to be full\n     if(blockSize !\u003d trgInode.blocks[trgInode.blocks.length-1].getNumBytes()) {\n-      throw new IllegalArgumentException(target + \" blocks size should be the same\");\n+      throw new HadoopIllegalArgumentException(\"The last block in \" + target\n+          + \" is not full; last block size \u003d \"\n+          + trgInode.blocks[trgInode.blocks.length-1].getNumBytes()\n+          + \" but file block size \u003d \" + blockSize);\n     }\n \n     si.add(trgInode);\n     short repl \u003d trgInode.getBlockReplication();\n \n     // now check the srcs\n     boolean endSrc \u003d false; // final src file doesn\u0027t have to have full end block\n     for(int i\u003d0; i\u003csrcs.length; i++) {\n       String src \u003d srcs[i];\n       if(i\u003d\u003dsrcs.length-1)\n         endSrc\u003dtrue;\n \n-      INodeFile srcInode \u003d dir.getFileINode(src);\n-\n+      final INodeFile srcInode \u003d INodeFile.valueOf(dir.getINode(src), src);\n       if(src.isEmpty() \n-          || srcInode \u003d\u003d null\n           || srcInode.isUnderConstruction()\n           || srcInode.blocks.length \u003d\u003d 0) {\n-        throw new IllegalArgumentException(\"concat: file \" + src + \n-        \" is invalid or empty or underConstruction\");\n+        throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n+            + \" is invalid or empty or underConstruction\");\n       }\n \n       // check replication and blocks size\n       if(repl !\u003d srcInode.getBlockReplication()) {\n-        throw new IllegalArgumentException(src + \" and \" + target + \" \" +\n-            \"should have same replication: \"\n-            + repl + \" vs. \" + srcInode.getBlockReplication());\n+        throw new HadoopIllegalArgumentException(\"concat: the soruce file \"\n+            + src + \" and the target file \" + target\n+            + \" should have the same replication: source replication is \"\n+            + srcInode.getBlockReplication()\n+            + \" but target replication is \" + repl);\n       }\n \n       //boolean endBlock\u003dfalse;\n       // verify that all the blocks are of the same length as target\n       // should be enough to check the end blocks\n       int idx \u003d srcInode.blocks.length-1;\n       if(endSrc)\n         idx \u003d srcInode.blocks.length-2; // end block of endSrc is OK not to be full\n       if(idx \u003e\u003d 0 \u0026\u0026 srcInode.blocks[idx].getNumBytes() !\u003d blockSize) {\n-        throw new IllegalArgumentException(\"concat: blocks sizes of \" + \n-            src + \" and \" + target + \" should all be the same\");\n+        throw new HadoopIllegalArgumentException(\"concat: the soruce file \"\n+            + src + \" and the target file \" + target\n+            + \" should have the same blocks sizes: target block size is \"\n+            + blockSize + \" but the size of source block \" + idx + \" is \"\n+            + srcInode.blocks[idx].getNumBytes());\n       }\n \n       si.add(srcInode);\n     }\n \n     // make sure no two files are the same\n     if(si.size() \u003c srcs.length+1) { // trg + srcs\n       // it means at least two files are the same\n-      throw new IllegalArgumentException(\"at least two files are the same\");\n+      throw new HadoopIllegalArgumentException(\n+          \"concat: at least two of the source files are the same\");\n     }\n \n     if(NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" + \n           Arrays.toString(srcs) + \" to \" + target);\n     }\n \n     dir.concat(target,srcs);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void concatInternal(String target, String [] srcs) \n      throws IOException, UnresolvedLinkException {\n    assert hasWriteLock();\n\n    // write permission for the target\n    if (isPermissionEnabled) {\n      checkPathAccess(target, FsAction.WRITE);\n\n      // and srcs\n      for(String aSrc: srcs) {\n        checkPathAccess(aSrc, FsAction.READ); // read the file\n        checkParentAccess(aSrc, FsAction.WRITE); // for delete \n      }\n    }\n\n    // to make sure no two files are the same\n    Set\u003cINode\u003e si \u003d new HashSet\u003cINode\u003e();\n\n    // we put the following prerequisite for the operation\n    // replication and blocks sizes should be the same for ALL the blocks\n\n    // check the target\n    final INodeFile trgInode \u003d INodeFile.valueOf(dir.getINode(target), target);\n    if(trgInode.isUnderConstruction()) {\n      throw new HadoopIllegalArgumentException(\"concat: target file \"\n          + target + \" is under construction\");\n    }\n    // per design target shouldn\u0027t be empty and all the blocks same size\n    if(trgInode.blocks.length \u003d\u003d 0) {\n      throw new HadoopIllegalArgumentException(\"concat: target file \"\n          + target + \" is empty\");\n    }\n\n    long blockSize \u003d trgInode.getPreferredBlockSize();\n\n    // check the end block to be full\n    if(blockSize !\u003d trgInode.blocks[trgInode.blocks.length-1].getNumBytes()) {\n      throw new HadoopIllegalArgumentException(\"The last block in \" + target\n          + \" is not full; last block size \u003d \"\n          + trgInode.blocks[trgInode.blocks.length-1].getNumBytes()\n          + \" but file block size \u003d \" + blockSize);\n    }\n\n    si.add(trgInode);\n    short repl \u003d trgInode.getBlockReplication();\n\n    // now check the srcs\n    boolean endSrc \u003d false; // final src file doesn\u0027t have to have full end block\n    for(int i\u003d0; i\u003csrcs.length; i++) {\n      String src \u003d srcs[i];\n      if(i\u003d\u003dsrcs.length-1)\n        endSrc\u003dtrue;\n\n      final INodeFile srcInode \u003d INodeFile.valueOf(dir.getINode(src), src);\n      if(src.isEmpty() \n          || srcInode.isUnderConstruction()\n          || srcInode.blocks.length \u003d\u003d 0) {\n        throw new HadoopIllegalArgumentException(\"concat: source file \" + src\n            + \" is invalid or empty or underConstruction\");\n      }\n\n      // check replication and blocks size\n      if(repl !\u003d srcInode.getBlockReplication()) {\n        throw new HadoopIllegalArgumentException(\"concat: the soruce file \"\n            + src + \" and the target file \" + target\n            + \" should have the same replication: source replication is \"\n            + srcInode.getBlockReplication()\n            + \" but target replication is \" + repl);\n      }\n\n      //boolean endBlock\u003dfalse;\n      // verify that all the blocks are of the same length as target\n      // should be enough to check the end blocks\n      int idx \u003d srcInode.blocks.length-1;\n      if(endSrc)\n        idx \u003d srcInode.blocks.length-2; // end block of endSrc is OK not to be full\n      if(idx \u003e\u003d 0 \u0026\u0026 srcInode.blocks[idx].getNumBytes() !\u003d blockSize) {\n        throw new HadoopIllegalArgumentException(\"concat: the soruce file \"\n            + src + \" and the target file \" + target\n            + \" should have the same blocks sizes: target block size is \"\n            + blockSize + \" but the size of source block \" + idx + \" is \"\n            + srcInode.blocks[idx].getNumBytes());\n      }\n\n      si.add(srcInode);\n    }\n\n    // make sure no two files are the same\n    if(si.size() \u003c srcs.length+1) { // trg + srcs\n      // it means at least two files are the same\n      throw new HadoopIllegalArgumentException(\n          \"concat: at least two of the source files are the same\");\n    }\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" + \n          Arrays.toString(srcs) + \" to \" + target);\n    }\n\n    dir.concat(target,srcs);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "ad06a087131d69d173d8e03dce5c97650a530f2e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4037. Rename the getReplication() method in BlockCollection to getBlockReplication(). \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1398288 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "15/10/12 6:48 AM",
      "commitName": "ad06a087131d69d173d8e03dce5c97650a530f2e",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "11/10/12 11:08 AM",
      "commitNameOld": "2887bbb33cefaac0c548eb2450a1f8e3e60f5ea7",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 3.82,
      "commitsBetweenForRepo": 12,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,98 +1,98 @@\n   private void concatInternal(String target, String [] srcs) \n       throws IOException, UnresolvedLinkException {\n     assert hasWriteLock();\n \n     // write permission for the target\n     if (isPermissionEnabled) {\n       checkPathAccess(target, FsAction.WRITE);\n \n       // and srcs\n       for(String aSrc: srcs) {\n         checkPathAccess(aSrc, FsAction.READ); // read the file\n         checkParentAccess(aSrc, FsAction.WRITE); // for delete \n       }\n     }\n \n     // to make sure no two files are the same\n     Set\u003cINode\u003e si \u003d new HashSet\u003cINode\u003e();\n \n     // we put the following prerequisite for the operation\n     // replication and blocks sizes should be the same for ALL the blocks\n     // check the target\n     INode inode \u003d dir.getFileINode(target);\n \n     if(inode \u003d\u003d null) {\n       throw new IllegalArgumentException(\"concat: trg file doesn\u0027t exist\");\n     }\n     if(inode.isUnderConstruction()) {\n       throw new IllegalArgumentException(\"concat: trg file is uner construction\");\n     }\n \n     INodeFile trgInode \u003d (INodeFile) inode;\n \n     // per design trg shouldn\u0027t be empty and all the blocks same size\n     if(trgInode.blocks.length \u003d\u003d 0) {\n       throw new IllegalArgumentException(\"concat: \"+ target + \" file is empty\");\n     }\n \n     long blockSize \u003d trgInode.getPreferredBlockSize();\n \n     // check the end block to be full\n     if(blockSize !\u003d trgInode.blocks[trgInode.blocks.length-1].getNumBytes()) {\n       throw new IllegalArgumentException(target + \" blocks size should be the same\");\n     }\n \n     si.add(trgInode);\n-    short repl \u003d trgInode.getReplication();\n+    short repl \u003d trgInode.getBlockReplication();\n \n     // now check the srcs\n     boolean endSrc \u003d false; // final src file doesn\u0027t have to have full end block\n     for(int i\u003d0; i\u003csrcs.length; i++) {\n       String src \u003d srcs[i];\n       if(i\u003d\u003dsrcs.length-1)\n         endSrc\u003dtrue;\n \n       INodeFile srcInode \u003d dir.getFileINode(src);\n \n       if(src.isEmpty() \n           || srcInode \u003d\u003d null\n           || srcInode.isUnderConstruction()\n           || srcInode.blocks.length \u003d\u003d 0) {\n         throw new IllegalArgumentException(\"concat: file \" + src + \n         \" is invalid or empty or underConstruction\");\n       }\n \n       // check replication and blocks size\n-      if(repl !\u003d srcInode.getReplication()) {\n+      if(repl !\u003d srcInode.getBlockReplication()) {\n         throw new IllegalArgumentException(src + \" and \" + target + \" \" +\n             \"should have same replication: \"\n-            + repl + \" vs. \" + srcInode.getReplication());\n+            + repl + \" vs. \" + srcInode.getBlockReplication());\n       }\n \n       //boolean endBlock\u003dfalse;\n       // verify that all the blocks are of the same length as target\n       // should be enough to check the end blocks\n       int idx \u003d srcInode.blocks.length-1;\n       if(endSrc)\n         idx \u003d srcInode.blocks.length-2; // end block of endSrc is OK not to be full\n       if(idx \u003e\u003d 0 \u0026\u0026 srcInode.blocks[idx].getNumBytes() !\u003d blockSize) {\n         throw new IllegalArgumentException(\"concat: blocks sizes of \" + \n             src + \" and \" + target + \" should all be the same\");\n       }\n \n       si.add(srcInode);\n     }\n \n     // make sure no two files are the same\n     if(si.size() \u003c srcs.length+1) { // trg + srcs\n       // it means at least two files are the same\n       throw new IllegalArgumentException(\"at least two files are the same\");\n     }\n \n     if(NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" + \n           Arrays.toString(srcs) + \" to \" + target);\n     }\n \n     dir.concat(target,srcs);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void concatInternal(String target, String [] srcs) \n      throws IOException, UnresolvedLinkException {\n    assert hasWriteLock();\n\n    // write permission for the target\n    if (isPermissionEnabled) {\n      checkPathAccess(target, FsAction.WRITE);\n\n      // and srcs\n      for(String aSrc: srcs) {\n        checkPathAccess(aSrc, FsAction.READ); // read the file\n        checkParentAccess(aSrc, FsAction.WRITE); // for delete \n      }\n    }\n\n    // to make sure no two files are the same\n    Set\u003cINode\u003e si \u003d new HashSet\u003cINode\u003e();\n\n    // we put the following prerequisite for the operation\n    // replication and blocks sizes should be the same for ALL the blocks\n    // check the target\n    INode inode \u003d dir.getFileINode(target);\n\n    if(inode \u003d\u003d null) {\n      throw new IllegalArgumentException(\"concat: trg file doesn\u0027t exist\");\n    }\n    if(inode.isUnderConstruction()) {\n      throw new IllegalArgumentException(\"concat: trg file is uner construction\");\n    }\n\n    INodeFile trgInode \u003d (INodeFile) inode;\n\n    // per design trg shouldn\u0027t be empty and all the blocks same size\n    if(trgInode.blocks.length \u003d\u003d 0) {\n      throw new IllegalArgumentException(\"concat: \"+ target + \" file is empty\");\n    }\n\n    long blockSize \u003d trgInode.getPreferredBlockSize();\n\n    // check the end block to be full\n    if(blockSize !\u003d trgInode.blocks[trgInode.blocks.length-1].getNumBytes()) {\n      throw new IllegalArgumentException(target + \" blocks size should be the same\");\n    }\n\n    si.add(trgInode);\n    short repl \u003d trgInode.getBlockReplication();\n\n    // now check the srcs\n    boolean endSrc \u003d false; // final src file doesn\u0027t have to have full end block\n    for(int i\u003d0; i\u003csrcs.length; i++) {\n      String src \u003d srcs[i];\n      if(i\u003d\u003dsrcs.length-1)\n        endSrc\u003dtrue;\n\n      INodeFile srcInode \u003d dir.getFileINode(src);\n\n      if(src.isEmpty() \n          || srcInode \u003d\u003d null\n          || srcInode.isUnderConstruction()\n          || srcInode.blocks.length \u003d\u003d 0) {\n        throw new IllegalArgumentException(\"concat: file \" + src + \n        \" is invalid or empty or underConstruction\");\n      }\n\n      // check replication and blocks size\n      if(repl !\u003d srcInode.getBlockReplication()) {\n        throw new IllegalArgumentException(src + \" and \" + target + \" \" +\n            \"should have same replication: \"\n            + repl + \" vs. \" + srcInode.getBlockReplication());\n      }\n\n      //boolean endBlock\u003dfalse;\n      // verify that all the blocks are of the same length as target\n      // should be enough to check the end blocks\n      int idx \u003d srcInode.blocks.length-1;\n      if(endSrc)\n        idx \u003d srcInode.blocks.length-2; // end block of endSrc is OK not to be full\n      if(idx \u003e\u003d 0 \u0026\u0026 srcInode.blocks[idx].getNumBytes() !\u003d blockSize) {\n        throw new IllegalArgumentException(\"concat: blocks sizes of \" + \n            src + \" and \" + target + \" should all be the same\");\n      }\n\n      si.add(srcInode);\n    }\n\n    // make sure no two files are the same\n    if(si.size() \u003c srcs.length+1) { // trg + srcs\n      // it means at least two files are the same\n      throw new IllegalArgumentException(\"at least two files are the same\");\n    }\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" + \n          Arrays.toString(srcs) + \" to \" + target);\n    }\n\n    dir.concat(target,srcs);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private void concatInternal(String target, String [] srcs) \n      throws IOException, UnresolvedLinkException {\n    assert hasWriteLock();\n\n    // write permission for the target\n    if (isPermissionEnabled) {\n      checkPathAccess(target, FsAction.WRITE);\n\n      // and srcs\n      for(String aSrc: srcs) {\n        checkPathAccess(aSrc, FsAction.READ); // read the file\n        checkParentAccess(aSrc, FsAction.WRITE); // for delete \n      }\n    }\n\n    // to make sure no two files are the same\n    Set\u003cINode\u003e si \u003d new HashSet\u003cINode\u003e();\n\n    // we put the following prerequisite for the operation\n    // replication and blocks sizes should be the same for ALL the blocks\n    // check the target\n    INode inode \u003d dir.getFileINode(target);\n\n    if(inode \u003d\u003d null) {\n      throw new IllegalArgumentException(\"concat: trg file doesn\u0027t exist\");\n    }\n    if(inode.isUnderConstruction()) {\n      throw new IllegalArgumentException(\"concat: trg file is uner construction\");\n    }\n\n    INodeFile trgInode \u003d (INodeFile) inode;\n\n    // per design trg shouldn\u0027t be empty and all the blocks same size\n    if(trgInode.blocks.length \u003d\u003d 0) {\n      throw new IllegalArgumentException(\"concat: \"+ target + \" file is empty\");\n    }\n\n    long blockSize \u003d trgInode.getPreferredBlockSize();\n\n    // check the end block to be full\n    if(blockSize !\u003d trgInode.blocks[trgInode.blocks.length-1].getNumBytes()) {\n      throw new IllegalArgumentException(target + \" blocks size should be the same\");\n    }\n\n    si.add(trgInode);\n    short repl \u003d trgInode.getReplication();\n\n    // now check the srcs\n    boolean endSrc \u003d false; // final src file doesn\u0027t have to have full end block\n    for(int i\u003d0; i\u003csrcs.length; i++) {\n      String src \u003d srcs[i];\n      if(i\u003d\u003dsrcs.length-1)\n        endSrc\u003dtrue;\n\n      INodeFile srcInode \u003d dir.getFileINode(src);\n\n      if(src.isEmpty() \n          || srcInode \u003d\u003d null\n          || srcInode.isUnderConstruction()\n          || srcInode.blocks.length \u003d\u003d 0) {\n        throw new IllegalArgumentException(\"concat: file \" + src + \n        \" is invalid or empty or underConstruction\");\n      }\n\n      // check replication and blocks size\n      if(repl !\u003d srcInode.getReplication()) {\n        throw new IllegalArgumentException(src + \" and \" + target + \" \" +\n            \"should have same replication: \"\n            + repl + \" vs. \" + srcInode.getReplication());\n      }\n\n      //boolean endBlock\u003dfalse;\n      // verify that all the blocks are of the same length as target\n      // should be enough to check the end blocks\n      int idx \u003d srcInode.blocks.length-1;\n      if(endSrc)\n        idx \u003d srcInode.blocks.length-2; // end block of endSrc is OK not to be full\n      if(idx \u003e\u003d 0 \u0026\u0026 srcInode.blocks[idx].getNumBytes() !\u003d blockSize) {\n        throw new IllegalArgumentException(\"concat: blocks sizes of \" + \n            src + \" and \" + target + \" should all be the same\");\n      }\n\n      si.add(srcInode);\n    }\n\n    // make sure no two files are the same\n    if(si.size() \u003c srcs.length+1) { // trg + srcs\n      // it means at least two files are the same\n      throw new IllegalArgumentException(\"at least two files are the same\");\n    }\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" + \n          Arrays.toString(srcs) + \" to \" + target);\n    }\n\n    dir.concat(target,srcs);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private void concatInternal(String target, String [] srcs) \n      throws IOException, UnresolvedLinkException {\n    assert hasWriteLock();\n\n    // write permission for the target\n    if (isPermissionEnabled) {\n      checkPathAccess(target, FsAction.WRITE);\n\n      // and srcs\n      for(String aSrc: srcs) {\n        checkPathAccess(aSrc, FsAction.READ); // read the file\n        checkParentAccess(aSrc, FsAction.WRITE); // for delete \n      }\n    }\n\n    // to make sure no two files are the same\n    Set\u003cINode\u003e si \u003d new HashSet\u003cINode\u003e();\n\n    // we put the following prerequisite for the operation\n    // replication and blocks sizes should be the same for ALL the blocks\n    // check the target\n    INode inode \u003d dir.getFileINode(target);\n\n    if(inode \u003d\u003d null) {\n      throw new IllegalArgumentException(\"concat: trg file doesn\u0027t exist\");\n    }\n    if(inode.isUnderConstruction()) {\n      throw new IllegalArgumentException(\"concat: trg file is uner construction\");\n    }\n\n    INodeFile trgInode \u003d (INodeFile) inode;\n\n    // per design trg shouldn\u0027t be empty and all the blocks same size\n    if(trgInode.blocks.length \u003d\u003d 0) {\n      throw new IllegalArgumentException(\"concat: \"+ target + \" file is empty\");\n    }\n\n    long blockSize \u003d trgInode.getPreferredBlockSize();\n\n    // check the end block to be full\n    if(blockSize !\u003d trgInode.blocks[trgInode.blocks.length-1].getNumBytes()) {\n      throw new IllegalArgumentException(target + \" blocks size should be the same\");\n    }\n\n    si.add(trgInode);\n    short repl \u003d trgInode.getReplication();\n\n    // now check the srcs\n    boolean endSrc \u003d false; // final src file doesn\u0027t have to have full end block\n    for(int i\u003d0; i\u003csrcs.length; i++) {\n      String src \u003d srcs[i];\n      if(i\u003d\u003dsrcs.length-1)\n        endSrc\u003dtrue;\n\n      INodeFile srcInode \u003d dir.getFileINode(src);\n\n      if(src.isEmpty() \n          || srcInode \u003d\u003d null\n          || srcInode.isUnderConstruction()\n          || srcInode.blocks.length \u003d\u003d 0) {\n        throw new IllegalArgumentException(\"concat: file \" + src + \n        \" is invalid or empty or underConstruction\");\n      }\n\n      // check replication and blocks size\n      if(repl !\u003d srcInode.getReplication()) {\n        throw new IllegalArgumentException(src + \" and \" + target + \" \" +\n            \"should have same replication: \"\n            + repl + \" vs. \" + srcInode.getReplication());\n      }\n\n      //boolean endBlock\u003dfalse;\n      // verify that all the blocks are of the same length as target\n      // should be enough to check the end blocks\n      int idx \u003d srcInode.blocks.length-1;\n      if(endSrc)\n        idx \u003d srcInode.blocks.length-2; // end block of endSrc is OK not to be full\n      if(idx \u003e\u003d 0 \u0026\u0026 srcInode.blocks[idx].getNumBytes() !\u003d blockSize) {\n        throw new IllegalArgumentException(\"concat: blocks sizes of \" + \n            src + \" and \" + target + \" should all be the same\");\n      }\n\n      si.add(srcInode);\n    }\n\n    // make sure no two files are the same\n    if(si.size() \u003c srcs.length+1) { // trg + srcs\n      // it means at least two files are the same\n      throw new IllegalArgumentException(\"at least two files are the same\");\n    }\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" + \n          Arrays.toString(srcs) + \" to \" + target);\n    }\n\n    dir.concat(target,srcs);\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java"
      }
    },
    "5d5b1c6c10c66c6a17b483a3e1a98d59d3d0bdee": {
      "type": "Ymodifierchange",
      "commitMessage": "HDFS-2239. Reduce access levels of the fields and methods in FSNamesystem.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1155998 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "09/08/11 6:50 PM",
      "commitName": "5d5b1c6c10c66c6a17b483a3e1a98d59d3d0bdee",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "08/08/11 3:06 AM",
      "commitNameOld": "371f4a59059322000a40eb4bdf5386b96b626ece",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 1.66,
      "commitsBetweenForRepo": 6,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,98 +1,98 @@\n-  public void concatInternal(String target, String [] srcs) \n+  private void concatInternal(String target, String [] srcs) \n       throws IOException, UnresolvedLinkException {\n     assert hasWriteLock();\n \n     // write permission for the target\n     if (isPermissionEnabled) {\n       checkPathAccess(target, FsAction.WRITE);\n \n       // and srcs\n       for(String aSrc: srcs) {\n         checkPathAccess(aSrc, FsAction.READ); // read the file\n         checkParentAccess(aSrc, FsAction.WRITE); // for delete \n       }\n     }\n \n     // to make sure no two files are the same\n     Set\u003cINode\u003e si \u003d new HashSet\u003cINode\u003e();\n \n     // we put the following prerequisite for the operation\n     // replication and blocks sizes should be the same for ALL the blocks\n     // check the target\n     INode inode \u003d dir.getFileINode(target);\n \n     if(inode \u003d\u003d null) {\n       throw new IllegalArgumentException(\"concat: trg file doesn\u0027t exist\");\n     }\n     if(inode.isUnderConstruction()) {\n       throw new IllegalArgumentException(\"concat: trg file is uner construction\");\n     }\n \n     INodeFile trgInode \u003d (INodeFile) inode;\n \n     // per design trg shouldn\u0027t be empty and all the blocks same size\n     if(trgInode.blocks.length \u003d\u003d 0) {\n       throw new IllegalArgumentException(\"concat: \"+ target + \" file is empty\");\n     }\n \n     long blockSize \u003d trgInode.getPreferredBlockSize();\n \n     // check the end block to be full\n     if(blockSize !\u003d trgInode.blocks[trgInode.blocks.length-1].getNumBytes()) {\n       throw new IllegalArgumentException(target + \" blocks size should be the same\");\n     }\n \n     si.add(trgInode);\n     short repl \u003d trgInode.getReplication();\n \n     // now check the srcs\n     boolean endSrc \u003d false; // final src file doesn\u0027t have to have full end block\n     for(int i\u003d0; i\u003csrcs.length; i++) {\n       String src \u003d srcs[i];\n       if(i\u003d\u003dsrcs.length-1)\n         endSrc\u003dtrue;\n \n       INodeFile srcInode \u003d dir.getFileINode(src);\n \n       if(src.isEmpty() \n           || srcInode \u003d\u003d null\n           || srcInode.isUnderConstruction()\n           || srcInode.blocks.length \u003d\u003d 0) {\n         throw new IllegalArgumentException(\"concat: file \" + src + \n         \" is invalid or empty or underConstruction\");\n       }\n \n       // check replication and blocks size\n       if(repl !\u003d srcInode.getReplication()) {\n         throw new IllegalArgumentException(src + \" and \" + target + \" \" +\n             \"should have same replication: \"\n             + repl + \" vs. \" + srcInode.getReplication());\n       }\n \n       //boolean endBlock\u003dfalse;\n       // verify that all the blocks are of the same length as target\n       // should be enough to check the end blocks\n       int idx \u003d srcInode.blocks.length-1;\n       if(endSrc)\n         idx \u003d srcInode.blocks.length-2; // end block of endSrc is OK not to be full\n       if(idx \u003e\u003d 0 \u0026\u0026 srcInode.blocks[idx].getNumBytes() !\u003d blockSize) {\n         throw new IllegalArgumentException(\"concat: blocks sizes of \" + \n             src + \" and \" + target + \" should all be the same\");\n       }\n \n       si.add(srcInode);\n     }\n \n     // make sure no two files are the same\n     if(si.size() \u003c srcs.length+1) { // trg + srcs\n       // it means at least two files are the same\n       throw new IllegalArgumentException(\"at least two files are the same\");\n     }\n \n     if(NameNode.stateChangeLog.isDebugEnabled()) {\n       NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" + \n           Arrays.toString(srcs) + \" to \" + target);\n     }\n \n     dir.concat(target,srcs);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void concatInternal(String target, String [] srcs) \n      throws IOException, UnresolvedLinkException {\n    assert hasWriteLock();\n\n    // write permission for the target\n    if (isPermissionEnabled) {\n      checkPathAccess(target, FsAction.WRITE);\n\n      // and srcs\n      for(String aSrc: srcs) {\n        checkPathAccess(aSrc, FsAction.READ); // read the file\n        checkParentAccess(aSrc, FsAction.WRITE); // for delete \n      }\n    }\n\n    // to make sure no two files are the same\n    Set\u003cINode\u003e si \u003d new HashSet\u003cINode\u003e();\n\n    // we put the following prerequisite for the operation\n    // replication and blocks sizes should be the same for ALL the blocks\n    // check the target\n    INode inode \u003d dir.getFileINode(target);\n\n    if(inode \u003d\u003d null) {\n      throw new IllegalArgumentException(\"concat: trg file doesn\u0027t exist\");\n    }\n    if(inode.isUnderConstruction()) {\n      throw new IllegalArgumentException(\"concat: trg file is uner construction\");\n    }\n\n    INodeFile trgInode \u003d (INodeFile) inode;\n\n    // per design trg shouldn\u0027t be empty and all the blocks same size\n    if(trgInode.blocks.length \u003d\u003d 0) {\n      throw new IllegalArgumentException(\"concat: \"+ target + \" file is empty\");\n    }\n\n    long blockSize \u003d trgInode.getPreferredBlockSize();\n\n    // check the end block to be full\n    if(blockSize !\u003d trgInode.blocks[trgInode.blocks.length-1].getNumBytes()) {\n      throw new IllegalArgumentException(target + \" blocks size should be the same\");\n    }\n\n    si.add(trgInode);\n    short repl \u003d trgInode.getReplication();\n\n    // now check the srcs\n    boolean endSrc \u003d false; // final src file doesn\u0027t have to have full end block\n    for(int i\u003d0; i\u003csrcs.length; i++) {\n      String src \u003d srcs[i];\n      if(i\u003d\u003dsrcs.length-1)\n        endSrc\u003dtrue;\n\n      INodeFile srcInode \u003d dir.getFileINode(src);\n\n      if(src.isEmpty() \n          || srcInode \u003d\u003d null\n          || srcInode.isUnderConstruction()\n          || srcInode.blocks.length \u003d\u003d 0) {\n        throw new IllegalArgumentException(\"concat: file \" + src + \n        \" is invalid or empty or underConstruction\");\n      }\n\n      // check replication and blocks size\n      if(repl !\u003d srcInode.getReplication()) {\n        throw new IllegalArgumentException(src + \" and \" + target + \" \" +\n            \"should have same replication: \"\n            + repl + \" vs. \" + srcInode.getReplication());\n      }\n\n      //boolean endBlock\u003dfalse;\n      // verify that all the blocks are of the same length as target\n      // should be enough to check the end blocks\n      int idx \u003d srcInode.blocks.length-1;\n      if(endSrc)\n        idx \u003d srcInode.blocks.length-2; // end block of endSrc is OK not to be full\n      if(idx \u003e\u003d 0 \u0026\u0026 srcInode.blocks[idx].getNumBytes() !\u003d blockSize) {\n        throw new IllegalArgumentException(\"concat: blocks sizes of \" + \n            src + \" and \" + target + \" should all be the same\");\n      }\n\n      si.add(srcInode);\n    }\n\n    // make sure no two files are the same\n    if(si.size() \u003c srcs.length+1) { // trg + srcs\n      // it means at least two files are the same\n      throw new IllegalArgumentException(\"at least two files are the same\");\n    }\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" + \n          Arrays.toString(srcs) + \" to \" + target);\n    }\n\n    dir.concat(target,srcs);\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
      "extendedDetails": {
        "oldValue": "[public]",
        "newValue": "[private]"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,98 @@\n+  public void concatInternal(String target, String [] srcs) \n+      throws IOException, UnresolvedLinkException {\n+    assert hasWriteLock();\n+\n+    // write permission for the target\n+    if (isPermissionEnabled) {\n+      checkPathAccess(target, FsAction.WRITE);\n+\n+      // and srcs\n+      for(String aSrc: srcs) {\n+        checkPathAccess(aSrc, FsAction.READ); // read the file\n+        checkParentAccess(aSrc, FsAction.WRITE); // for delete \n+      }\n+    }\n+\n+    // to make sure no two files are the same\n+    Set\u003cINode\u003e si \u003d new HashSet\u003cINode\u003e();\n+\n+    // we put the following prerequisite for the operation\n+    // replication and blocks sizes should be the same for ALL the blocks\n+    // check the target\n+    INode inode \u003d dir.getFileINode(target);\n+\n+    if(inode \u003d\u003d null) {\n+      throw new IllegalArgumentException(\"concat: trg file doesn\u0027t exist\");\n+    }\n+    if(inode.isUnderConstruction()) {\n+      throw new IllegalArgumentException(\"concat: trg file is uner construction\");\n+    }\n+\n+    INodeFile trgInode \u003d (INodeFile) inode;\n+\n+    // per design trg shouldn\u0027t be empty and all the blocks same size\n+    if(trgInode.blocks.length \u003d\u003d 0) {\n+      throw new IllegalArgumentException(\"concat: \"+ target + \" file is empty\");\n+    }\n+\n+    long blockSize \u003d trgInode.getPreferredBlockSize();\n+\n+    // check the end block to be full\n+    if(blockSize !\u003d trgInode.blocks[trgInode.blocks.length-1].getNumBytes()) {\n+      throw new IllegalArgumentException(target + \" blocks size should be the same\");\n+    }\n+\n+    si.add(trgInode);\n+    short repl \u003d trgInode.getReplication();\n+\n+    // now check the srcs\n+    boolean endSrc \u003d false; // final src file doesn\u0027t have to have full end block\n+    for(int i\u003d0; i\u003csrcs.length; i++) {\n+      String src \u003d srcs[i];\n+      if(i\u003d\u003dsrcs.length-1)\n+        endSrc\u003dtrue;\n+\n+      INodeFile srcInode \u003d dir.getFileINode(src);\n+\n+      if(src.isEmpty() \n+          || srcInode \u003d\u003d null\n+          || srcInode.isUnderConstruction()\n+          || srcInode.blocks.length \u003d\u003d 0) {\n+        throw new IllegalArgumentException(\"concat: file \" + src + \n+        \" is invalid or empty or underConstruction\");\n+      }\n+\n+      // check replication and blocks size\n+      if(repl !\u003d srcInode.getReplication()) {\n+        throw new IllegalArgumentException(src + \" and \" + target + \" \" +\n+            \"should have same replication: \"\n+            + repl + \" vs. \" + srcInode.getReplication());\n+      }\n+\n+      //boolean endBlock\u003dfalse;\n+      // verify that all the blocks are of the same length as target\n+      // should be enough to check the end blocks\n+      int idx \u003d srcInode.blocks.length-1;\n+      if(endSrc)\n+        idx \u003d srcInode.blocks.length-2; // end block of endSrc is OK not to be full\n+      if(idx \u003e\u003d 0 \u0026\u0026 srcInode.blocks[idx].getNumBytes() !\u003d blockSize) {\n+        throw new IllegalArgumentException(\"concat: blocks sizes of \" + \n+            src + \" and \" + target + \" should all be the same\");\n+      }\n+\n+      si.add(srcInode);\n+    }\n+\n+    // make sure no two files are the same\n+    if(si.size() \u003c srcs.length+1) { // trg + srcs\n+      // it means at least two files are the same\n+      throw new IllegalArgumentException(\"at least two files are the same\");\n+    }\n+\n+    if(NameNode.stateChangeLog.isDebugEnabled()) {\n+      NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" + \n+          Arrays.toString(srcs) + \" to \" + target);\n+    }\n+\n+    dir.concat(target,srcs);\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public void concatInternal(String target, String [] srcs) \n      throws IOException, UnresolvedLinkException {\n    assert hasWriteLock();\n\n    // write permission for the target\n    if (isPermissionEnabled) {\n      checkPathAccess(target, FsAction.WRITE);\n\n      // and srcs\n      for(String aSrc: srcs) {\n        checkPathAccess(aSrc, FsAction.READ); // read the file\n        checkParentAccess(aSrc, FsAction.WRITE); // for delete \n      }\n    }\n\n    // to make sure no two files are the same\n    Set\u003cINode\u003e si \u003d new HashSet\u003cINode\u003e();\n\n    // we put the following prerequisite for the operation\n    // replication and blocks sizes should be the same for ALL the blocks\n    // check the target\n    INode inode \u003d dir.getFileINode(target);\n\n    if(inode \u003d\u003d null) {\n      throw new IllegalArgumentException(\"concat: trg file doesn\u0027t exist\");\n    }\n    if(inode.isUnderConstruction()) {\n      throw new IllegalArgumentException(\"concat: trg file is uner construction\");\n    }\n\n    INodeFile trgInode \u003d (INodeFile) inode;\n\n    // per design trg shouldn\u0027t be empty and all the blocks same size\n    if(trgInode.blocks.length \u003d\u003d 0) {\n      throw new IllegalArgumentException(\"concat: \"+ target + \" file is empty\");\n    }\n\n    long blockSize \u003d trgInode.getPreferredBlockSize();\n\n    // check the end block to be full\n    if(blockSize !\u003d trgInode.blocks[trgInode.blocks.length-1].getNumBytes()) {\n      throw new IllegalArgumentException(target + \" blocks size should be the same\");\n    }\n\n    si.add(trgInode);\n    short repl \u003d trgInode.getReplication();\n\n    // now check the srcs\n    boolean endSrc \u003d false; // final src file doesn\u0027t have to have full end block\n    for(int i\u003d0; i\u003csrcs.length; i++) {\n      String src \u003d srcs[i];\n      if(i\u003d\u003dsrcs.length-1)\n        endSrc\u003dtrue;\n\n      INodeFile srcInode \u003d dir.getFileINode(src);\n\n      if(src.isEmpty() \n          || srcInode \u003d\u003d null\n          || srcInode.isUnderConstruction()\n          || srcInode.blocks.length \u003d\u003d 0) {\n        throw new IllegalArgumentException(\"concat: file \" + src + \n        \" is invalid or empty or underConstruction\");\n      }\n\n      // check replication and blocks size\n      if(repl !\u003d srcInode.getReplication()) {\n        throw new IllegalArgumentException(src + \" and \" + target + \" \" +\n            \"should have same replication: \"\n            + repl + \" vs. \" + srcInode.getReplication());\n      }\n\n      //boolean endBlock\u003dfalse;\n      // verify that all the blocks are of the same length as target\n      // should be enough to check the end blocks\n      int idx \u003d srcInode.blocks.length-1;\n      if(endSrc)\n        idx \u003d srcInode.blocks.length-2; // end block of endSrc is OK not to be full\n      if(idx \u003e\u003d 0 \u0026\u0026 srcInode.blocks[idx].getNumBytes() !\u003d blockSize) {\n        throw new IllegalArgumentException(\"concat: blocks sizes of \" + \n            src + \" and \" + target + \" should all be the same\");\n      }\n\n      si.add(srcInode);\n    }\n\n    // make sure no two files are the same\n    if(si.size() \u003c srcs.length+1) { // trg + srcs\n      // it means at least two files are the same\n      throw new IllegalArgumentException(\"at least two files are the same\");\n    }\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* NameSystem.concat: \" + \n          Arrays.toString(srcs) + \" to \" + target);\n    }\n\n    dir.concat(target,srcs);\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java"
    }
  }
}