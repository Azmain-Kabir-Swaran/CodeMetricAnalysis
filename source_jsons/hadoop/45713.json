{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "LogAggregationIndexedFileController.java",
  "functionName": "parseCheckSumFiles",
  "functionId": "parseCheckSumFiles___fileList-List__FileStatus__",
  "sourceFilePath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/logaggregation/filecontroller/ifile/LogAggregationIndexedFileController.java",
  "functionStartLine": 706,
  "functionEndLine": 749,
  "numCommitsSeen": 18,
  "timeTaken": 1652,
  "changeHistory": [
    "d4c98579e36df7eeb788352d7b76cd2c7448c511",
    "280080fad01304c85a9ede4d4f7b707eb36c0155"
  ],
  "changeHistoryShort": {
    "d4c98579e36df7eeb788352d7b76cd2c7448c511": "Ybodychange",
    "280080fad01304c85a9ede4d4f7b707eb36c0155": "Yintroduced"
  },
  "changeHistoryDetails": {
    "d4c98579e36df7eeb788352d7b76cd2c7448c511": {
      "type": "Ybodychange",
      "commitMessage": "YARN-7697. NM goes down with OOM due to leak in log-aggregation. (Xuan Gong via wangda)\n\nChange-Id: Ie4fc7979d834e25f37a033c314f3efceeb8f4a9e\n",
      "commitDate": "11/02/18 6:28 PM",
      "commitName": "d4c98579e36df7eeb788352d7b76cd2c7448c511",
      "commitAuthor": "Wangda Tan",
      "commitDateOld": "02/10/17 3:30 PM",
      "commitNameOld": "280080fad01304c85a9ede4d4f7b707eb36c0155",
      "commitAuthorOld": "Wangda Tan",
      "daysBetweenCommits": 132.17,
      "commitsBetweenForRepo": 906,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,43 +1,44 @@\n   public Map\u003cString, Long\u003e parseCheckSumFiles(\n       List\u003cFileStatus\u003e fileList) throws IOException {\n     Map\u003cString, Long\u003e checkSumFiles \u003d new HashMap\u003c\u003e();\n     Set\u003cFileStatus\u003e status \u003d new HashSet\u003cFileStatus\u003e(fileList);\n     Iterable\u003cFileStatus\u003e mask \u003d\n         Iterables.filter(status, new Predicate\u003cFileStatus\u003e() {\n           @Override\n           public boolean apply(FileStatus next) {\n             return next.getPath().getName().endsWith(\n                 CHECK_SUM_FILE_SUFFIX);\n           }\n         });\n     status \u003d Sets.newHashSet(mask);\n     FileContext fc \u003d null;\n     for (FileStatus file : status) {\n       FSDataInputStream checksumFileInputStream \u003d null;\n       try {\n         if (fc \u003d\u003d null) {\n           fc \u003d FileContext.getFileContext(file.getPath().toUri(), conf);\n         }\n         String nodeName \u003d null;\n         long index \u003d 0L;\n         checksumFileInputStream \u003d fc.open(file.getPath());\n         int nameLength \u003d checksumFileInputStream.readInt();\n         byte[] b \u003d new byte[nameLength];\n         int actualLength \u003d checksumFileInputStream.read(b);\n         if (actualLength \u003d\u003d nameLength) {\n           nodeName \u003d new String(b, Charset.forName(\"UTF-8\"));\n           index \u003d checksumFileInputStream.readLong();\n         } else {\n           continue;\n         }\n         if (nodeName !\u003d null \u0026\u0026 !nodeName.isEmpty()) {\n           checkSumFiles.put(nodeName, Long.valueOf(index));\n         }\n       } catch (IOException ex) {\n+        LOG.warn(ex.getMessage());\n         continue;\n       } finally {\n         IOUtils.cleanupWithLogger(LOG, checksumFileInputStream);\n       }\n     }\n     return checkSumFiles;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public Map\u003cString, Long\u003e parseCheckSumFiles(\n      List\u003cFileStatus\u003e fileList) throws IOException {\n    Map\u003cString, Long\u003e checkSumFiles \u003d new HashMap\u003c\u003e();\n    Set\u003cFileStatus\u003e status \u003d new HashSet\u003cFileStatus\u003e(fileList);\n    Iterable\u003cFileStatus\u003e mask \u003d\n        Iterables.filter(status, new Predicate\u003cFileStatus\u003e() {\n          @Override\n          public boolean apply(FileStatus next) {\n            return next.getPath().getName().endsWith(\n                CHECK_SUM_FILE_SUFFIX);\n          }\n        });\n    status \u003d Sets.newHashSet(mask);\n    FileContext fc \u003d null;\n    for (FileStatus file : status) {\n      FSDataInputStream checksumFileInputStream \u003d null;\n      try {\n        if (fc \u003d\u003d null) {\n          fc \u003d FileContext.getFileContext(file.getPath().toUri(), conf);\n        }\n        String nodeName \u003d null;\n        long index \u003d 0L;\n        checksumFileInputStream \u003d fc.open(file.getPath());\n        int nameLength \u003d checksumFileInputStream.readInt();\n        byte[] b \u003d new byte[nameLength];\n        int actualLength \u003d checksumFileInputStream.read(b);\n        if (actualLength \u003d\u003d nameLength) {\n          nodeName \u003d new String(b, Charset.forName(\"UTF-8\"));\n          index \u003d checksumFileInputStream.readLong();\n        } else {\n          continue;\n        }\n        if (nodeName !\u003d null \u0026\u0026 !nodeName.isEmpty()) {\n          checkSumFiles.put(nodeName, Long.valueOf(index));\n        }\n      } catch (IOException ex) {\n        LOG.warn(ex.getMessage());\n        continue;\n      } finally {\n        IOUtils.cleanupWithLogger(LOG, checksumFileInputStream);\n      }\n    }\n    return checkSumFiles;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/logaggregation/filecontroller/ifile/LogAggregationIndexedFileController.java",
      "extendedDetails": {}
    },
    "280080fad01304c85a9ede4d4f7b707eb36c0155": {
      "type": "Yintroduced",
      "commitMessage": "YARN-7259. Add size-based rolling policy to LogAggregationIndexedFileController. (xgong via wangda)\n\nChange-Id: Ifaf82c0aee6b73b9b6ebf103aa72e131e3942f31\n",
      "commitDate": "02/10/17 3:30 PM",
      "commitName": "280080fad01304c85a9ede4d4f7b707eb36c0155",
      "commitAuthor": "Wangda Tan",
      "diff": "@@ -0,0 +1,43 @@\n+  public Map\u003cString, Long\u003e parseCheckSumFiles(\n+      List\u003cFileStatus\u003e fileList) throws IOException {\n+    Map\u003cString, Long\u003e checkSumFiles \u003d new HashMap\u003c\u003e();\n+    Set\u003cFileStatus\u003e status \u003d new HashSet\u003cFileStatus\u003e(fileList);\n+    Iterable\u003cFileStatus\u003e mask \u003d\n+        Iterables.filter(status, new Predicate\u003cFileStatus\u003e() {\n+          @Override\n+          public boolean apply(FileStatus next) {\n+            return next.getPath().getName().endsWith(\n+                CHECK_SUM_FILE_SUFFIX);\n+          }\n+        });\n+    status \u003d Sets.newHashSet(mask);\n+    FileContext fc \u003d null;\n+    for (FileStatus file : status) {\n+      FSDataInputStream checksumFileInputStream \u003d null;\n+      try {\n+        if (fc \u003d\u003d null) {\n+          fc \u003d FileContext.getFileContext(file.getPath().toUri(), conf);\n+        }\n+        String nodeName \u003d null;\n+        long index \u003d 0L;\n+        checksumFileInputStream \u003d fc.open(file.getPath());\n+        int nameLength \u003d checksumFileInputStream.readInt();\n+        byte[] b \u003d new byte[nameLength];\n+        int actualLength \u003d checksumFileInputStream.read(b);\n+        if (actualLength \u003d\u003d nameLength) {\n+          nodeName \u003d new String(b, Charset.forName(\"UTF-8\"));\n+          index \u003d checksumFileInputStream.readLong();\n+        } else {\n+          continue;\n+        }\n+        if (nodeName !\u003d null \u0026\u0026 !nodeName.isEmpty()) {\n+          checkSumFiles.put(nodeName, Long.valueOf(index));\n+        }\n+      } catch (IOException ex) {\n+        continue;\n+      } finally {\n+        IOUtils.cleanupWithLogger(LOG, checksumFileInputStream);\n+      }\n+    }\n+    return checkSumFiles;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public Map\u003cString, Long\u003e parseCheckSumFiles(\n      List\u003cFileStatus\u003e fileList) throws IOException {\n    Map\u003cString, Long\u003e checkSumFiles \u003d new HashMap\u003c\u003e();\n    Set\u003cFileStatus\u003e status \u003d new HashSet\u003cFileStatus\u003e(fileList);\n    Iterable\u003cFileStatus\u003e mask \u003d\n        Iterables.filter(status, new Predicate\u003cFileStatus\u003e() {\n          @Override\n          public boolean apply(FileStatus next) {\n            return next.getPath().getName().endsWith(\n                CHECK_SUM_FILE_SUFFIX);\n          }\n        });\n    status \u003d Sets.newHashSet(mask);\n    FileContext fc \u003d null;\n    for (FileStatus file : status) {\n      FSDataInputStream checksumFileInputStream \u003d null;\n      try {\n        if (fc \u003d\u003d null) {\n          fc \u003d FileContext.getFileContext(file.getPath().toUri(), conf);\n        }\n        String nodeName \u003d null;\n        long index \u003d 0L;\n        checksumFileInputStream \u003d fc.open(file.getPath());\n        int nameLength \u003d checksumFileInputStream.readInt();\n        byte[] b \u003d new byte[nameLength];\n        int actualLength \u003d checksumFileInputStream.read(b);\n        if (actualLength \u003d\u003d nameLength) {\n          nodeName \u003d new String(b, Charset.forName(\"UTF-8\"));\n          index \u003d checksumFileInputStream.readLong();\n        } else {\n          continue;\n        }\n        if (nodeName !\u003d null \u0026\u0026 !nodeName.isEmpty()) {\n          checkSumFiles.put(nodeName, Long.valueOf(index));\n        }\n      } catch (IOException ex) {\n        continue;\n      } finally {\n        IOUtils.cleanupWithLogger(LOG, checksumFileInputStream);\n      }\n    }\n    return checkSumFiles;\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/logaggregation/filecontroller/ifile/LogAggregationIndexedFileController.java"
    }
  }
}