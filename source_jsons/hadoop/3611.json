{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DFSInputStream.java",
  "functionName": "hedgedFetchBlockByteRange",
  "functionId": "hedgedFetchBlockByteRange___block-LocatedBlock__start-long__end-long__buf-ByteBuffer__corruptedBlocks-CorruptedBlocks",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
  "functionStartLine": 1284,
  "functionEndLine": 1384,
  "numCommitsSeen": 325,
  "timeTaken": 9729,
  "changeHistory": [
    "b6bfb2fcb2391d51b8de97c01c1290880779132e",
    "8b242f09a61a7536d2422546bfa6c2aaf1d57ed6",
    "70fc6746b326b9a913e8bebca5f5afaf01ab9e11",
    "401db4fc65140979fe7665983e36905e886df971",
    "be34e85e682880f46eee0310bf00ecc7d39cd5bd",
    "d8383c687c95dbb37effa307ab2d41497da1cfc2",
    "8808779db351fe444388d4acb3094766b5980718",
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
    "39285e6a1978ea5e53bdc1b0aef62421382124a8",
    "6ee0539ede78b640f01c5eac18ded161182a7835",
    "d5a9a3daa0224249221ffa7b8bd5751ab2feca56",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
    "bff5999d07e9416a22846c849487e509ede55040",
    "439614b0c8a3df3d8b7967451c5331a0e034e13a",
    "75bbcc8bf3fa1daf54f56868dae737f6da12ab1f",
    "a42bb1cd915abe5dc33eda3c01e8c74c64f35748",
    "caa42adf208bfb5625d1b3ef665fbf334ffcccd9",
    "463aec11718e47d4aabb86a7a539cb973460aae6",
    "7f6ed7fe365166e8075359f1d0ad035fa876c70f",
    "0ca41a8f35e4f05bb04805a2e0a617850707b4db",
    "f8904ad299bbcd109e3460f9b8ab9fbb9cebdad4",
    "c0a903da22c65294b232c7530a6a684ee93daba4",
    "17db74a1c1972392a5aba48a3e0334dcd6c76487"
  ],
  "changeHistoryShort": {
    "b6bfb2fcb2391d51b8de97c01c1290880779132e": "Ybodychange",
    "8b242f09a61a7536d2422546bfa6c2aaf1d57ed6": "Ybodychange",
    "70fc6746b326b9a913e8bebca5f5afaf01ab9e11": "Ybodychange",
    "401db4fc65140979fe7665983e36905e886df971": "Ymultichange(Yparameterchange,Ybodychange)",
    "be34e85e682880f46eee0310bf00ecc7d39cd5bd": "Ybodychange",
    "d8383c687c95dbb37effa307ab2d41497da1cfc2": "Ybodychange",
    "8808779db351fe444388d4acb3094766b5980718": "Ymultichange(Yparameterchange,Ybodychange)",
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93": "Ybodychange",
    "39285e6a1978ea5e53bdc1b0aef62421382124a8": "Ybodychange",
    "6ee0539ede78b640f01c5eac18ded161182a7835": "Ybodychange",
    "d5a9a3daa0224249221ffa7b8bd5751ab2feca56": "Ybodychange",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": "Yfilerename",
    "bff5999d07e9416a22846c849487e509ede55040": "Ymultichange(Yparameterchange,Ybodychange)",
    "439614b0c8a3df3d8b7967451c5331a0e034e13a": "Ybodychange",
    "75bbcc8bf3fa1daf54f56868dae737f6da12ab1f": "Ybodychange",
    "a42bb1cd915abe5dc33eda3c01e8c74c64f35748": "Ymultichange(Yparameterchange,Ybodychange)",
    "caa42adf208bfb5625d1b3ef665fbf334ffcccd9": "Ybodychange",
    "463aec11718e47d4aabb86a7a539cb973460aae6": "Ybodychange",
    "7f6ed7fe365166e8075359f1d0ad035fa876c70f": "Ybodychange",
    "0ca41a8f35e4f05bb04805a2e0a617850707b4db": "Ybodychange",
    "f8904ad299bbcd109e3460f9b8ab9fbb9cebdad4": "Ybodychange",
    "c0a903da22c65294b232c7530a6a684ee93daba4": "Ybodychange",
    "17db74a1c1972392a5aba48a3e0334dcd6c76487": "Yintroduced"
  },
  "changeHistoryDetails": {
    "b6bfb2fcb2391d51b8de97c01c1290880779132e": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11738. Hedged pread takes more time when block moved from initial locations. Contributed by Vinayakumar B.\n",
      "commitDate": "21/08/17 1:45 PM",
      "commitName": "b6bfb2fcb2391d51b8de97c01c1290880779132e",
      "commitAuthor": "John Zhuge",
      "commitDateOld": "11/08/17 7:42 PM",
      "commitNameOld": "8b242f09a61a7536d2422546bfa6c2aaf1d57ed6",
      "commitAuthorOld": "John Zhuge",
      "daysBetweenCommits": 9.75,
      "commitsBetweenForRepo": 46,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,96 +1,101 @@\n   private void hedgedFetchBlockByteRange(LocatedBlock block, long start,\n       long end, ByteBuffer buf, CorruptedBlocks corruptedBlocks)\n       throws IOException {\n     final DfsClientConf conf \u003d dfsClient.getConf();\n     ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e futures \u003d new ArrayList\u003c\u003e();\n     CompletionService\u003cByteBuffer\u003e hedgedService \u003d\n         new ExecutorCompletionService\u003c\u003e(dfsClient.getHedgedReadsThreadPool());\n     ArrayList\u003cDatanodeInfo\u003e ignored \u003d new ArrayList\u003c\u003e();\n     ByteBuffer bb;\n     int len \u003d (int) (end - start + 1);\n     int hedgedReadId \u003d 0;\n     while (true) {\n       // see HDFS-6591, this metric is used to verify/catch unnecessary loops\n       hedgedReadOpsLoopNumForTesting++;\n       DNAddrPair chosenNode \u003d null;\n       // there is no request already executing.\n       if (futures.isEmpty()) {\n         // chooseDataNode is a commitment. If no node, we go to\n         // the NN to reget block locations. Only go here on first read.\n         chosenNode \u003d chooseDataNode(block, ignored);\n         // Latest block, if refreshed internally\n         block \u003d chosenNode.block;\n         bb \u003d ByteBuffer.allocate(len);\n         Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n             chosenNode, block, start, end, bb,\n             corruptedBlocks, hedgedReadId++);\n         Future\u003cByteBuffer\u003e firstRequest \u003d hedgedService\n             .submit(getFromDataNodeCallable);\n         futures.add(firstRequest);\n         Future\u003cByteBuffer\u003e future \u003d null;\n         try {\n           future \u003d hedgedService.poll(\n               conf.getHedgedReadThresholdMillis(), TimeUnit.MILLISECONDS);\n           if (future !\u003d null) {\n             ByteBuffer result \u003d future.get();\n             result.flip();\n             buf.put(result);\n             return;\n           }\n           DFSClient.LOG.debug(\"Waited {}ms to read from {}; spawning hedged \"\n               + \"read\", conf.getHedgedReadThresholdMillis(), chosenNode.info);\n           dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n           // continue; no need to refresh block locations\n         } catch (ExecutionException e) {\n           futures.remove(future);\n         } catch (InterruptedException e) {\n           throw new InterruptedIOException(\n               \"Interrupted while waiting for reading task\");\n         }\n         // Ignore this node on next go around.\n         // If poll timeout and the request still ongoing, don\u0027t consider it\n         // again. If read data failed, don\u0027t consider it either.\n         ignored.add(chosenNode.info);\n       } else {\n         // We are starting up a \u0027hedged\u0027 read. We have a read already\n         // ongoing. Call getBestNodeDNAddrPair instead of chooseDataNode.\n         // If no nodes to do hedged reads against, pass.\n+        boolean refetch \u003d false;\n         try {\n-          chosenNode \u003d getBestNodeDNAddrPair(block, ignored);\n-          if (chosenNode \u003d\u003d null) {\n-            chosenNode \u003d chooseDataNode(block, ignored);\n+          chosenNode \u003d chooseDataNode(block, ignored, false);\n+          if (chosenNode !\u003d null) {\n+            // Latest block, if refreshed internally\n+            block \u003d chosenNode.block;\n+            bb \u003d ByteBuffer.allocate(len);\n+            Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d\n+                getFromOneDataNode(chosenNode, block, start, end, bb,\n+                    corruptedBlocks, hedgedReadId++);\n+            Future\u003cByteBuffer\u003e oneMoreRequest \u003d\n+                hedgedService.submit(getFromDataNodeCallable);\n+            futures.add(oneMoreRequest);\n+          } else {\n+            refetch \u003d true;\n           }\n-          // Latest block, if refreshed internally\n-          block \u003d chosenNode.block;\n-          bb \u003d ByteBuffer.allocate(len);\n-          Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n-              chosenNode, block, start, end, bb,\n-              corruptedBlocks, hedgedReadId++);\n-          Future\u003cByteBuffer\u003e oneMoreRequest \u003d hedgedService\n-              .submit(getFromDataNodeCallable);\n-          futures.add(oneMoreRequest);\n         } catch (IOException ioe) {\n           DFSClient.LOG.debug(\"Failed getting node for hedged read: {}\",\n               ioe.getMessage());\n         }\n         // if not succeeded. Submit callables for each datanode in a loop, wait\n         // for a fixed interval and get the result from the fastest one.\n         try {\n           ByteBuffer result \u003d getFirstToComplete(hedgedService, futures);\n           // cancel the rest.\n           cancelAll(futures);\n           dfsClient.getHedgedReadMetrics().incHedgedReadWins();\n           result.flip();\n           buf.put(result);\n           return;\n         } catch (InterruptedException ie) {\n           // Ignore and retry\n         }\n+        if (refetch) {\n+          refetchLocations(block, ignored);\n+        }\n         // We got here if exception. Ignore this node on next go around IFF\n         // we found a chosenNode to hedge read against.\n         if (chosenNode !\u003d null \u0026\u0026 chosenNode.info !\u003d null) {\n           ignored.add(chosenNode.info);\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void hedgedFetchBlockByteRange(LocatedBlock block, long start,\n      long end, ByteBuffer buf, CorruptedBlocks corruptedBlocks)\n      throws IOException {\n    final DfsClientConf conf \u003d dfsClient.getConf();\n    ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e futures \u003d new ArrayList\u003c\u003e();\n    CompletionService\u003cByteBuffer\u003e hedgedService \u003d\n        new ExecutorCompletionService\u003c\u003e(dfsClient.getHedgedReadsThreadPool());\n    ArrayList\u003cDatanodeInfo\u003e ignored \u003d new ArrayList\u003c\u003e();\n    ByteBuffer bb;\n    int len \u003d (int) (end - start + 1);\n    int hedgedReadId \u003d 0;\n    while (true) {\n      // see HDFS-6591, this metric is used to verify/catch unnecessary loops\n      hedgedReadOpsLoopNumForTesting++;\n      DNAddrPair chosenNode \u003d null;\n      // there is no request already executing.\n      if (futures.isEmpty()) {\n        // chooseDataNode is a commitment. If no node, we go to\n        // the NN to reget block locations. Only go here on first read.\n        chosenNode \u003d chooseDataNode(block, ignored);\n        // Latest block, if refreshed internally\n        block \u003d chosenNode.block;\n        bb \u003d ByteBuffer.allocate(len);\n        Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n            chosenNode, block, start, end, bb,\n            corruptedBlocks, hedgedReadId++);\n        Future\u003cByteBuffer\u003e firstRequest \u003d hedgedService\n            .submit(getFromDataNodeCallable);\n        futures.add(firstRequest);\n        Future\u003cByteBuffer\u003e future \u003d null;\n        try {\n          future \u003d hedgedService.poll(\n              conf.getHedgedReadThresholdMillis(), TimeUnit.MILLISECONDS);\n          if (future !\u003d null) {\n            ByteBuffer result \u003d future.get();\n            result.flip();\n            buf.put(result);\n            return;\n          }\n          DFSClient.LOG.debug(\"Waited {}ms to read from {}; spawning hedged \"\n              + \"read\", conf.getHedgedReadThresholdMillis(), chosenNode.info);\n          dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n          // continue; no need to refresh block locations\n        } catch (ExecutionException e) {\n          futures.remove(future);\n        } catch (InterruptedException e) {\n          throw new InterruptedIOException(\n              \"Interrupted while waiting for reading task\");\n        }\n        // Ignore this node on next go around.\n        // If poll timeout and the request still ongoing, don\u0027t consider it\n        // again. If read data failed, don\u0027t consider it either.\n        ignored.add(chosenNode.info);\n      } else {\n        // We are starting up a \u0027hedged\u0027 read. We have a read already\n        // ongoing. Call getBestNodeDNAddrPair instead of chooseDataNode.\n        // If no nodes to do hedged reads against, pass.\n        boolean refetch \u003d false;\n        try {\n          chosenNode \u003d chooseDataNode(block, ignored, false);\n          if (chosenNode !\u003d null) {\n            // Latest block, if refreshed internally\n            block \u003d chosenNode.block;\n            bb \u003d ByteBuffer.allocate(len);\n            Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d\n                getFromOneDataNode(chosenNode, block, start, end, bb,\n                    corruptedBlocks, hedgedReadId++);\n            Future\u003cByteBuffer\u003e oneMoreRequest \u003d\n                hedgedService.submit(getFromDataNodeCallable);\n            futures.add(oneMoreRequest);\n          } else {\n            refetch \u003d true;\n          }\n        } catch (IOException ioe) {\n          DFSClient.LOG.debug(\"Failed getting node for hedged read: {}\",\n              ioe.getMessage());\n        }\n        // if not succeeded. Submit callables for each datanode in a loop, wait\n        // for a fixed interval and get the result from the fastest one.\n        try {\n          ByteBuffer result \u003d getFirstToComplete(hedgedService, futures);\n          // cancel the rest.\n          cancelAll(futures);\n          dfsClient.getHedgedReadMetrics().incHedgedReadWins();\n          result.flip();\n          buf.put(result);\n          return;\n        } catch (InterruptedException ie) {\n          // Ignore and retry\n        }\n        if (refetch) {\n          refetchLocations(block, ignored);\n        }\n        // We got here if exception. Ignore this node on next go around IFF\n        // we found a chosenNode to hedge read against.\n        if (chosenNode !\u003d null \u0026\u0026 chosenNode.info !\u003d null) {\n          ignored.add(chosenNode.info);\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "8b242f09a61a7536d2422546bfa6c2aaf1d57ed6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11303. Hedged read might hang infinitely if read data from all DN failed . Contributed by Chen Zhang, Wei-chiu Chuang, and John Zhuge.\n",
      "commitDate": "11/08/17 7:42 PM",
      "commitName": "8b242f09a61a7536d2422546bfa6c2aaf1d57ed6",
      "commitAuthor": "John Zhuge",
      "commitDateOld": "26/06/17 1:24 PM",
      "commitNameOld": "a9d3412b4ce40f5ab5a18756ede7e0606b653171",
      "commitAuthorOld": "Ravi Prakash",
      "daysBetweenCommits": 46.26,
      "commitsBetweenForRepo": 313,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,93 +1,96 @@\n   private void hedgedFetchBlockByteRange(LocatedBlock block, long start,\n       long end, ByteBuffer buf, CorruptedBlocks corruptedBlocks)\n       throws IOException {\n     final DfsClientConf conf \u003d dfsClient.getConf();\n     ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e futures \u003d new ArrayList\u003c\u003e();\n     CompletionService\u003cByteBuffer\u003e hedgedService \u003d\n         new ExecutorCompletionService\u003c\u003e(dfsClient.getHedgedReadsThreadPool());\n     ArrayList\u003cDatanodeInfo\u003e ignored \u003d new ArrayList\u003c\u003e();\n     ByteBuffer bb;\n     int len \u003d (int) (end - start + 1);\n     int hedgedReadId \u003d 0;\n     while (true) {\n       // see HDFS-6591, this metric is used to verify/catch unnecessary loops\n       hedgedReadOpsLoopNumForTesting++;\n       DNAddrPair chosenNode \u003d null;\n       // there is no request already executing.\n       if (futures.isEmpty()) {\n         // chooseDataNode is a commitment. If no node, we go to\n         // the NN to reget block locations. Only go here on first read.\n         chosenNode \u003d chooseDataNode(block, ignored);\n         // Latest block, if refreshed internally\n         block \u003d chosenNode.block;\n         bb \u003d ByteBuffer.allocate(len);\n         Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n             chosenNode, block, start, end, bb,\n             corruptedBlocks, hedgedReadId++);\n         Future\u003cByteBuffer\u003e firstRequest \u003d hedgedService\n             .submit(getFromDataNodeCallable);\n         futures.add(firstRequest);\n+        Future\u003cByteBuffer\u003e future \u003d null;\n         try {\n-          Future\u003cByteBuffer\u003e future \u003d hedgedService.poll(\n+          future \u003d hedgedService.poll(\n               conf.getHedgedReadThresholdMillis(), TimeUnit.MILLISECONDS);\n           if (future !\u003d null) {\n             ByteBuffer result \u003d future.get();\n             result.flip();\n             buf.put(result);\n             return;\n           }\n           DFSClient.LOG.debug(\"Waited {}ms to read from {}; spawning hedged \"\n               + \"read\", conf.getHedgedReadThresholdMillis(), chosenNode.info);\n-          // Ignore this node on next go around.\n-          ignored.add(chosenNode.info);\n           dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n           // continue; no need to refresh block locations\n         } catch (ExecutionException e) {\n-          // Ignore\n+          futures.remove(future);\n         } catch (InterruptedException e) {\n           throw new InterruptedIOException(\n               \"Interrupted while waiting for reading task\");\n         }\n+        // Ignore this node on next go around.\n+        // If poll timeout and the request still ongoing, don\u0027t consider it\n+        // again. If read data failed, don\u0027t consider it either.\n+        ignored.add(chosenNode.info);\n       } else {\n         // We are starting up a \u0027hedged\u0027 read. We have a read already\n         // ongoing. Call getBestNodeDNAddrPair instead of chooseDataNode.\n         // If no nodes to do hedged reads against, pass.\n         try {\n           chosenNode \u003d getBestNodeDNAddrPair(block, ignored);\n           if (chosenNode \u003d\u003d null) {\n             chosenNode \u003d chooseDataNode(block, ignored);\n           }\n           // Latest block, if refreshed internally\n           block \u003d chosenNode.block;\n           bb \u003d ByteBuffer.allocate(len);\n           Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n               chosenNode, block, start, end, bb,\n               corruptedBlocks, hedgedReadId++);\n           Future\u003cByteBuffer\u003e oneMoreRequest \u003d hedgedService\n               .submit(getFromDataNodeCallable);\n           futures.add(oneMoreRequest);\n         } catch (IOException ioe) {\n           DFSClient.LOG.debug(\"Failed getting node for hedged read: {}\",\n               ioe.getMessage());\n         }\n         // if not succeeded. Submit callables for each datanode in a loop, wait\n         // for a fixed interval and get the result from the fastest one.\n         try {\n           ByteBuffer result \u003d getFirstToComplete(hedgedService, futures);\n           // cancel the rest.\n           cancelAll(futures);\n           dfsClient.getHedgedReadMetrics().incHedgedReadWins();\n           result.flip();\n           buf.put(result);\n           return;\n         } catch (InterruptedException ie) {\n           // Ignore and retry\n         }\n         // We got here if exception. Ignore this node on next go around IFF\n         // we found a chosenNode to hedge read against.\n         if (chosenNode !\u003d null \u0026\u0026 chosenNode.info !\u003d null) {\n           ignored.add(chosenNode.info);\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void hedgedFetchBlockByteRange(LocatedBlock block, long start,\n      long end, ByteBuffer buf, CorruptedBlocks corruptedBlocks)\n      throws IOException {\n    final DfsClientConf conf \u003d dfsClient.getConf();\n    ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e futures \u003d new ArrayList\u003c\u003e();\n    CompletionService\u003cByteBuffer\u003e hedgedService \u003d\n        new ExecutorCompletionService\u003c\u003e(dfsClient.getHedgedReadsThreadPool());\n    ArrayList\u003cDatanodeInfo\u003e ignored \u003d new ArrayList\u003c\u003e();\n    ByteBuffer bb;\n    int len \u003d (int) (end - start + 1);\n    int hedgedReadId \u003d 0;\n    while (true) {\n      // see HDFS-6591, this metric is used to verify/catch unnecessary loops\n      hedgedReadOpsLoopNumForTesting++;\n      DNAddrPair chosenNode \u003d null;\n      // there is no request already executing.\n      if (futures.isEmpty()) {\n        // chooseDataNode is a commitment. If no node, we go to\n        // the NN to reget block locations. Only go here on first read.\n        chosenNode \u003d chooseDataNode(block, ignored);\n        // Latest block, if refreshed internally\n        block \u003d chosenNode.block;\n        bb \u003d ByteBuffer.allocate(len);\n        Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n            chosenNode, block, start, end, bb,\n            corruptedBlocks, hedgedReadId++);\n        Future\u003cByteBuffer\u003e firstRequest \u003d hedgedService\n            .submit(getFromDataNodeCallable);\n        futures.add(firstRequest);\n        Future\u003cByteBuffer\u003e future \u003d null;\n        try {\n          future \u003d hedgedService.poll(\n              conf.getHedgedReadThresholdMillis(), TimeUnit.MILLISECONDS);\n          if (future !\u003d null) {\n            ByteBuffer result \u003d future.get();\n            result.flip();\n            buf.put(result);\n            return;\n          }\n          DFSClient.LOG.debug(\"Waited {}ms to read from {}; spawning hedged \"\n              + \"read\", conf.getHedgedReadThresholdMillis(), chosenNode.info);\n          dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n          // continue; no need to refresh block locations\n        } catch (ExecutionException e) {\n          futures.remove(future);\n        } catch (InterruptedException e) {\n          throw new InterruptedIOException(\n              \"Interrupted while waiting for reading task\");\n        }\n        // Ignore this node on next go around.\n        // If poll timeout and the request still ongoing, don\u0027t consider it\n        // again. If read data failed, don\u0027t consider it either.\n        ignored.add(chosenNode.info);\n      } else {\n        // We are starting up a \u0027hedged\u0027 read. We have a read already\n        // ongoing. Call getBestNodeDNAddrPair instead of chooseDataNode.\n        // If no nodes to do hedged reads against, pass.\n        try {\n          chosenNode \u003d getBestNodeDNAddrPair(block, ignored);\n          if (chosenNode \u003d\u003d null) {\n            chosenNode \u003d chooseDataNode(block, ignored);\n          }\n          // Latest block, if refreshed internally\n          block \u003d chosenNode.block;\n          bb \u003d ByteBuffer.allocate(len);\n          Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n              chosenNode, block, start, end, bb,\n              corruptedBlocks, hedgedReadId++);\n          Future\u003cByteBuffer\u003e oneMoreRequest \u003d hedgedService\n              .submit(getFromDataNodeCallable);\n          futures.add(oneMoreRequest);\n        } catch (IOException ioe) {\n          DFSClient.LOG.debug(\"Failed getting node for hedged read: {}\",\n              ioe.getMessage());\n        }\n        // if not succeeded. Submit callables for each datanode in a loop, wait\n        // for a fixed interval and get the result from the fastest one.\n        try {\n          ByteBuffer result \u003d getFirstToComplete(hedgedService, futures);\n          // cancel the rest.\n          cancelAll(futures);\n          dfsClient.getHedgedReadMetrics().incHedgedReadWins();\n          result.flip();\n          buf.put(result);\n          return;\n        } catch (InterruptedException ie) {\n          // Ignore and retry\n        }\n        // We got here if exception. Ignore this node on next go around IFF\n        // we found a chosenNode to hedge read against.\n        if (chosenNode !\u003d null \u0026\u0026 chosenNode.info !\u003d null) {\n          ignored.add(chosenNode.info);\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "70fc6746b326b9a913e8bebca5f5afaf01ab9e11": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11708. Positional read will fail if replicas moved to different DNs after stream is opened. Contributed by Vinayakumar B.\n",
      "commitDate": "06/06/17 10:25 PM",
      "commitName": "70fc6746b326b9a913e8bebca5f5afaf01ab9e11",
      "commitAuthor": "Vinayakumar B",
      "commitDateOld": "03/04/17 8:13 PM",
      "commitNameOld": "6eba79232f36b36e0196163adc8fe4219a6b6bf9",
      "commitAuthorOld": "Chris Douglas",
      "daysBetweenCommits": 64.09,
      "commitsBetweenForRepo": 349,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,90 +1,93 @@\n   private void hedgedFetchBlockByteRange(LocatedBlock block, long start,\n       long end, ByteBuffer buf, CorruptedBlocks corruptedBlocks)\n       throws IOException {\n     final DfsClientConf conf \u003d dfsClient.getConf();\n     ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e futures \u003d new ArrayList\u003c\u003e();\n     CompletionService\u003cByteBuffer\u003e hedgedService \u003d\n         new ExecutorCompletionService\u003c\u003e(dfsClient.getHedgedReadsThreadPool());\n     ArrayList\u003cDatanodeInfo\u003e ignored \u003d new ArrayList\u003c\u003e();\n     ByteBuffer bb;\n     int len \u003d (int) (end - start + 1);\n     int hedgedReadId \u003d 0;\n-    block \u003d refreshLocatedBlock(block);\n     while (true) {\n       // see HDFS-6591, this metric is used to verify/catch unnecessary loops\n       hedgedReadOpsLoopNumForTesting++;\n       DNAddrPair chosenNode \u003d null;\n       // there is no request already executing.\n       if (futures.isEmpty()) {\n         // chooseDataNode is a commitment. If no node, we go to\n         // the NN to reget block locations. Only go here on first read.\n         chosenNode \u003d chooseDataNode(block, ignored);\n+        // Latest block, if refreshed internally\n+        block \u003d chosenNode.block;\n         bb \u003d ByteBuffer.allocate(len);\n         Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n             chosenNode, block, start, end, bb,\n             corruptedBlocks, hedgedReadId++);\n         Future\u003cByteBuffer\u003e firstRequest \u003d hedgedService\n             .submit(getFromDataNodeCallable);\n         futures.add(firstRequest);\n         try {\n           Future\u003cByteBuffer\u003e future \u003d hedgedService.poll(\n               conf.getHedgedReadThresholdMillis(), TimeUnit.MILLISECONDS);\n           if (future !\u003d null) {\n             ByteBuffer result \u003d future.get();\n             result.flip();\n             buf.put(result);\n             return;\n           }\n           DFSClient.LOG.debug(\"Waited {}ms to read from {}; spawning hedged \"\n               + \"read\", conf.getHedgedReadThresholdMillis(), chosenNode.info);\n           // Ignore this node on next go around.\n           ignored.add(chosenNode.info);\n           dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n           // continue; no need to refresh block locations\n         } catch (ExecutionException e) {\n           // Ignore\n         } catch (InterruptedException e) {\n           throw new InterruptedIOException(\n               \"Interrupted while waiting for reading task\");\n         }\n       } else {\n         // We are starting up a \u0027hedged\u0027 read. We have a read already\n         // ongoing. Call getBestNodeDNAddrPair instead of chooseDataNode.\n         // If no nodes to do hedged reads against, pass.\n         try {\n           chosenNode \u003d getBestNodeDNAddrPair(block, ignored);\n           if (chosenNode \u003d\u003d null) {\n             chosenNode \u003d chooseDataNode(block, ignored);\n           }\n+          // Latest block, if refreshed internally\n+          block \u003d chosenNode.block;\n           bb \u003d ByteBuffer.allocate(len);\n           Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n               chosenNode, block, start, end, bb,\n               corruptedBlocks, hedgedReadId++);\n           Future\u003cByteBuffer\u003e oneMoreRequest \u003d hedgedService\n               .submit(getFromDataNodeCallable);\n           futures.add(oneMoreRequest);\n         } catch (IOException ioe) {\n           DFSClient.LOG.debug(\"Failed getting node for hedged read: {}\",\n               ioe.getMessage());\n         }\n         // if not succeeded. Submit callables for each datanode in a loop, wait\n         // for a fixed interval and get the result from the fastest one.\n         try {\n           ByteBuffer result \u003d getFirstToComplete(hedgedService, futures);\n           // cancel the rest.\n           cancelAll(futures);\n           dfsClient.getHedgedReadMetrics().incHedgedReadWins();\n           result.flip();\n           buf.put(result);\n           return;\n         } catch (InterruptedException ie) {\n           // Ignore and retry\n         }\n         // We got here if exception. Ignore this node on next go around IFF\n         // we found a chosenNode to hedge read against.\n         if (chosenNode !\u003d null \u0026\u0026 chosenNode.info !\u003d null) {\n           ignored.add(chosenNode.info);\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void hedgedFetchBlockByteRange(LocatedBlock block, long start,\n      long end, ByteBuffer buf, CorruptedBlocks corruptedBlocks)\n      throws IOException {\n    final DfsClientConf conf \u003d dfsClient.getConf();\n    ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e futures \u003d new ArrayList\u003c\u003e();\n    CompletionService\u003cByteBuffer\u003e hedgedService \u003d\n        new ExecutorCompletionService\u003c\u003e(dfsClient.getHedgedReadsThreadPool());\n    ArrayList\u003cDatanodeInfo\u003e ignored \u003d new ArrayList\u003c\u003e();\n    ByteBuffer bb;\n    int len \u003d (int) (end - start + 1);\n    int hedgedReadId \u003d 0;\n    while (true) {\n      // see HDFS-6591, this metric is used to verify/catch unnecessary loops\n      hedgedReadOpsLoopNumForTesting++;\n      DNAddrPair chosenNode \u003d null;\n      // there is no request already executing.\n      if (futures.isEmpty()) {\n        // chooseDataNode is a commitment. If no node, we go to\n        // the NN to reget block locations. Only go here on first read.\n        chosenNode \u003d chooseDataNode(block, ignored);\n        // Latest block, if refreshed internally\n        block \u003d chosenNode.block;\n        bb \u003d ByteBuffer.allocate(len);\n        Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n            chosenNode, block, start, end, bb,\n            corruptedBlocks, hedgedReadId++);\n        Future\u003cByteBuffer\u003e firstRequest \u003d hedgedService\n            .submit(getFromDataNodeCallable);\n        futures.add(firstRequest);\n        try {\n          Future\u003cByteBuffer\u003e future \u003d hedgedService.poll(\n              conf.getHedgedReadThresholdMillis(), TimeUnit.MILLISECONDS);\n          if (future !\u003d null) {\n            ByteBuffer result \u003d future.get();\n            result.flip();\n            buf.put(result);\n            return;\n          }\n          DFSClient.LOG.debug(\"Waited {}ms to read from {}; spawning hedged \"\n              + \"read\", conf.getHedgedReadThresholdMillis(), chosenNode.info);\n          // Ignore this node on next go around.\n          ignored.add(chosenNode.info);\n          dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n          // continue; no need to refresh block locations\n        } catch (ExecutionException e) {\n          // Ignore\n        } catch (InterruptedException e) {\n          throw new InterruptedIOException(\n              \"Interrupted while waiting for reading task\");\n        }\n      } else {\n        // We are starting up a \u0027hedged\u0027 read. We have a read already\n        // ongoing. Call getBestNodeDNAddrPair instead of chooseDataNode.\n        // If no nodes to do hedged reads against, pass.\n        try {\n          chosenNode \u003d getBestNodeDNAddrPair(block, ignored);\n          if (chosenNode \u003d\u003d null) {\n            chosenNode \u003d chooseDataNode(block, ignored);\n          }\n          // Latest block, if refreshed internally\n          block \u003d chosenNode.block;\n          bb \u003d ByteBuffer.allocate(len);\n          Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n              chosenNode, block, start, end, bb,\n              corruptedBlocks, hedgedReadId++);\n          Future\u003cByteBuffer\u003e oneMoreRequest \u003d hedgedService\n              .submit(getFromDataNodeCallable);\n          futures.add(oneMoreRequest);\n        } catch (IOException ioe) {\n          DFSClient.LOG.debug(\"Failed getting node for hedged read: {}\",\n              ioe.getMessage());\n        }\n        // if not succeeded. Submit callables for each datanode in a loop, wait\n        // for a fixed interval and get the result from the fastest one.\n        try {\n          ByteBuffer result \u003d getFirstToComplete(hedgedService, futures);\n          // cancel the rest.\n          cancelAll(futures);\n          dfsClient.getHedgedReadMetrics().incHedgedReadWins();\n          result.flip();\n          buf.put(result);\n          return;\n        } catch (InterruptedException ie) {\n          // Ignore and retry\n        }\n        // We got here if exception. Ignore this node on next go around IFF\n        // we found a chosenNode to hedge read against.\n        if (chosenNode !\u003d null \u0026\u0026 chosenNode.info !\u003d null) {\n          ignored.add(chosenNode.info);\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "401db4fc65140979fe7665983e36905e886df971": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-8901. Use ByteBuffer in striping positional read. Contributed by Sammi Chen and Kai Zheng.\n",
      "commitDate": "08/09/16 11:54 AM",
      "commitName": "401db4fc65140979fe7665983e36905e886df971",
      "commitAuthor": "Zhe Zhang",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-8901. Use ByteBuffer in striping positional read. Contributed by Sammi Chen and Kai Zheng.\n",
          "commitDate": "08/09/16 11:54 AM",
          "commitName": "401db4fc65140979fe7665983e36905e886df971",
          "commitAuthor": "Zhe Zhang",
          "commitDateOld": "24/08/16 6:57 AM",
          "commitNameOld": "793447f79924c97c2b562d5e41fa85adf19673fe",
          "commitAuthorOld": "Kai Zheng",
          "daysBetweenCommits": 15.21,
          "commitsBetweenForRepo": 83,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,90 +1,90 @@\n   private void hedgedFetchBlockByteRange(LocatedBlock block, long start,\n-      long end, byte[] buf, int offset, CorruptedBlocks corruptedBlocks)\n+      long end, ByteBuffer buf, CorruptedBlocks corruptedBlocks)\n       throws IOException {\n     final DfsClientConf conf \u003d dfsClient.getConf();\n     ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e futures \u003d new ArrayList\u003c\u003e();\n     CompletionService\u003cByteBuffer\u003e hedgedService \u003d\n         new ExecutorCompletionService\u003c\u003e(dfsClient.getHedgedReadsThreadPool());\n     ArrayList\u003cDatanodeInfo\u003e ignored \u003d new ArrayList\u003c\u003e();\n     ByteBuffer bb;\n     int len \u003d (int) (end - start + 1);\n     int hedgedReadId \u003d 0;\n     block \u003d refreshLocatedBlock(block);\n     while (true) {\n       // see HDFS-6591, this metric is used to verify/catch unnecessary loops\n       hedgedReadOpsLoopNumForTesting++;\n       DNAddrPair chosenNode \u003d null;\n       // there is no request already executing.\n       if (futures.isEmpty()) {\n         // chooseDataNode is a commitment. If no node, we go to\n         // the NN to reget block locations. Only go here on first read.\n         chosenNode \u003d chooseDataNode(block, ignored);\n         bb \u003d ByteBuffer.allocate(len);\n         Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n             chosenNode, block, start, end, bb,\n             corruptedBlocks, hedgedReadId++);\n         Future\u003cByteBuffer\u003e firstRequest \u003d hedgedService\n             .submit(getFromDataNodeCallable);\n         futures.add(firstRequest);\n         try {\n           Future\u003cByteBuffer\u003e future \u003d hedgedService.poll(\n               conf.getHedgedReadThresholdMillis(), TimeUnit.MILLISECONDS);\n           if (future !\u003d null) {\n             ByteBuffer result \u003d future.get();\n-            System.arraycopy(result.array(), result.position(), buf, offset,\n-                len);\n+            result.flip();\n+            buf.put(result);\n             return;\n           }\n           DFSClient.LOG.debug(\"Waited {}ms to read from {}; spawning hedged \"\n               + \"read\", conf.getHedgedReadThresholdMillis(), chosenNode.info);\n           // Ignore this node on next go around.\n           ignored.add(chosenNode.info);\n           dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n           // continue; no need to refresh block locations\n         } catch (ExecutionException e) {\n           // Ignore\n         } catch (InterruptedException e) {\n           throw new InterruptedIOException(\n               \"Interrupted while waiting for reading task\");\n         }\n       } else {\n         // We are starting up a \u0027hedged\u0027 read. We have a read already\n         // ongoing. Call getBestNodeDNAddrPair instead of chooseDataNode.\n         // If no nodes to do hedged reads against, pass.\n         try {\n           chosenNode \u003d getBestNodeDNAddrPair(block, ignored);\n           if (chosenNode \u003d\u003d null) {\n             chosenNode \u003d chooseDataNode(block, ignored);\n           }\n           bb \u003d ByteBuffer.allocate(len);\n           Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n               chosenNode, block, start, end, bb,\n               corruptedBlocks, hedgedReadId++);\n           Future\u003cByteBuffer\u003e oneMoreRequest \u003d hedgedService\n               .submit(getFromDataNodeCallable);\n           futures.add(oneMoreRequest);\n         } catch (IOException ioe) {\n           DFSClient.LOG.debug(\"Failed getting node for hedged read: {}\",\n               ioe.getMessage());\n         }\n         // if not succeeded. Submit callables for each datanode in a loop, wait\n         // for a fixed interval and get the result from the fastest one.\n         try {\n           ByteBuffer result \u003d getFirstToComplete(hedgedService, futures);\n           // cancel the rest.\n           cancelAll(futures);\n           dfsClient.getHedgedReadMetrics().incHedgedReadWins();\n-          System.arraycopy(result.array(), result.position(), buf, offset,\n-              len);\n+          result.flip();\n+          buf.put(result);\n           return;\n         } catch (InterruptedException ie) {\n           // Ignore and retry\n         }\n         // We got here if exception. Ignore this node on next go around IFF\n         // we found a chosenNode to hedge read against.\n         if (chosenNode !\u003d null \u0026\u0026 chosenNode.info !\u003d null) {\n           ignored.add(chosenNode.info);\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void hedgedFetchBlockByteRange(LocatedBlock block, long start,\n      long end, ByteBuffer buf, CorruptedBlocks corruptedBlocks)\n      throws IOException {\n    final DfsClientConf conf \u003d dfsClient.getConf();\n    ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e futures \u003d new ArrayList\u003c\u003e();\n    CompletionService\u003cByteBuffer\u003e hedgedService \u003d\n        new ExecutorCompletionService\u003c\u003e(dfsClient.getHedgedReadsThreadPool());\n    ArrayList\u003cDatanodeInfo\u003e ignored \u003d new ArrayList\u003c\u003e();\n    ByteBuffer bb;\n    int len \u003d (int) (end - start + 1);\n    int hedgedReadId \u003d 0;\n    block \u003d refreshLocatedBlock(block);\n    while (true) {\n      // see HDFS-6591, this metric is used to verify/catch unnecessary loops\n      hedgedReadOpsLoopNumForTesting++;\n      DNAddrPair chosenNode \u003d null;\n      // there is no request already executing.\n      if (futures.isEmpty()) {\n        // chooseDataNode is a commitment. If no node, we go to\n        // the NN to reget block locations. Only go here on first read.\n        chosenNode \u003d chooseDataNode(block, ignored);\n        bb \u003d ByteBuffer.allocate(len);\n        Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n            chosenNode, block, start, end, bb,\n            corruptedBlocks, hedgedReadId++);\n        Future\u003cByteBuffer\u003e firstRequest \u003d hedgedService\n            .submit(getFromDataNodeCallable);\n        futures.add(firstRequest);\n        try {\n          Future\u003cByteBuffer\u003e future \u003d hedgedService.poll(\n              conf.getHedgedReadThresholdMillis(), TimeUnit.MILLISECONDS);\n          if (future !\u003d null) {\n            ByteBuffer result \u003d future.get();\n            result.flip();\n            buf.put(result);\n            return;\n          }\n          DFSClient.LOG.debug(\"Waited {}ms to read from {}; spawning hedged \"\n              + \"read\", conf.getHedgedReadThresholdMillis(), chosenNode.info);\n          // Ignore this node on next go around.\n          ignored.add(chosenNode.info);\n          dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n          // continue; no need to refresh block locations\n        } catch (ExecutionException e) {\n          // Ignore\n        } catch (InterruptedException e) {\n          throw new InterruptedIOException(\n              \"Interrupted while waiting for reading task\");\n        }\n      } else {\n        // We are starting up a \u0027hedged\u0027 read. We have a read already\n        // ongoing. Call getBestNodeDNAddrPair instead of chooseDataNode.\n        // If no nodes to do hedged reads against, pass.\n        try {\n          chosenNode \u003d getBestNodeDNAddrPair(block, ignored);\n          if (chosenNode \u003d\u003d null) {\n            chosenNode \u003d chooseDataNode(block, ignored);\n          }\n          bb \u003d ByteBuffer.allocate(len);\n          Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n              chosenNode, block, start, end, bb,\n              corruptedBlocks, hedgedReadId++);\n          Future\u003cByteBuffer\u003e oneMoreRequest \u003d hedgedService\n              .submit(getFromDataNodeCallable);\n          futures.add(oneMoreRequest);\n        } catch (IOException ioe) {\n          DFSClient.LOG.debug(\"Failed getting node for hedged read: {}\",\n              ioe.getMessage());\n        }\n        // if not succeeded. Submit callables for each datanode in a loop, wait\n        // for a fixed interval and get the result from the fastest one.\n        try {\n          ByteBuffer result \u003d getFirstToComplete(hedgedService, futures);\n          // cancel the rest.\n          cancelAll(futures);\n          dfsClient.getHedgedReadMetrics().incHedgedReadWins();\n          result.flip();\n          buf.put(result);\n          return;\n        } catch (InterruptedException ie) {\n          // Ignore and retry\n        }\n        // We got here if exception. Ignore this node on next go around IFF\n        // we found a chosenNode to hedge read against.\n        if (chosenNode !\u003d null \u0026\u0026 chosenNode.info !\u003d null) {\n          ignored.add(chosenNode.info);\n        }\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
          "extendedDetails": {
            "oldValue": "[block-LocatedBlock, start-long, end-long, buf-byte[], offset-int, corruptedBlocks-CorruptedBlocks]",
            "newValue": "[block-LocatedBlock, start-long, end-long, buf-ByteBuffer, corruptedBlocks-CorruptedBlocks]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-8901. Use ByteBuffer in striping positional read. Contributed by Sammi Chen and Kai Zheng.\n",
          "commitDate": "08/09/16 11:54 AM",
          "commitName": "401db4fc65140979fe7665983e36905e886df971",
          "commitAuthor": "Zhe Zhang",
          "commitDateOld": "24/08/16 6:57 AM",
          "commitNameOld": "793447f79924c97c2b562d5e41fa85adf19673fe",
          "commitAuthorOld": "Kai Zheng",
          "daysBetweenCommits": 15.21,
          "commitsBetweenForRepo": 83,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,90 +1,90 @@\n   private void hedgedFetchBlockByteRange(LocatedBlock block, long start,\n-      long end, byte[] buf, int offset, CorruptedBlocks corruptedBlocks)\n+      long end, ByteBuffer buf, CorruptedBlocks corruptedBlocks)\n       throws IOException {\n     final DfsClientConf conf \u003d dfsClient.getConf();\n     ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e futures \u003d new ArrayList\u003c\u003e();\n     CompletionService\u003cByteBuffer\u003e hedgedService \u003d\n         new ExecutorCompletionService\u003c\u003e(dfsClient.getHedgedReadsThreadPool());\n     ArrayList\u003cDatanodeInfo\u003e ignored \u003d new ArrayList\u003c\u003e();\n     ByteBuffer bb;\n     int len \u003d (int) (end - start + 1);\n     int hedgedReadId \u003d 0;\n     block \u003d refreshLocatedBlock(block);\n     while (true) {\n       // see HDFS-6591, this metric is used to verify/catch unnecessary loops\n       hedgedReadOpsLoopNumForTesting++;\n       DNAddrPair chosenNode \u003d null;\n       // there is no request already executing.\n       if (futures.isEmpty()) {\n         // chooseDataNode is a commitment. If no node, we go to\n         // the NN to reget block locations. Only go here on first read.\n         chosenNode \u003d chooseDataNode(block, ignored);\n         bb \u003d ByteBuffer.allocate(len);\n         Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n             chosenNode, block, start, end, bb,\n             corruptedBlocks, hedgedReadId++);\n         Future\u003cByteBuffer\u003e firstRequest \u003d hedgedService\n             .submit(getFromDataNodeCallable);\n         futures.add(firstRequest);\n         try {\n           Future\u003cByteBuffer\u003e future \u003d hedgedService.poll(\n               conf.getHedgedReadThresholdMillis(), TimeUnit.MILLISECONDS);\n           if (future !\u003d null) {\n             ByteBuffer result \u003d future.get();\n-            System.arraycopy(result.array(), result.position(), buf, offset,\n-                len);\n+            result.flip();\n+            buf.put(result);\n             return;\n           }\n           DFSClient.LOG.debug(\"Waited {}ms to read from {}; spawning hedged \"\n               + \"read\", conf.getHedgedReadThresholdMillis(), chosenNode.info);\n           // Ignore this node on next go around.\n           ignored.add(chosenNode.info);\n           dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n           // continue; no need to refresh block locations\n         } catch (ExecutionException e) {\n           // Ignore\n         } catch (InterruptedException e) {\n           throw new InterruptedIOException(\n               \"Interrupted while waiting for reading task\");\n         }\n       } else {\n         // We are starting up a \u0027hedged\u0027 read. We have a read already\n         // ongoing. Call getBestNodeDNAddrPair instead of chooseDataNode.\n         // If no nodes to do hedged reads against, pass.\n         try {\n           chosenNode \u003d getBestNodeDNAddrPair(block, ignored);\n           if (chosenNode \u003d\u003d null) {\n             chosenNode \u003d chooseDataNode(block, ignored);\n           }\n           bb \u003d ByteBuffer.allocate(len);\n           Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n               chosenNode, block, start, end, bb,\n               corruptedBlocks, hedgedReadId++);\n           Future\u003cByteBuffer\u003e oneMoreRequest \u003d hedgedService\n               .submit(getFromDataNodeCallable);\n           futures.add(oneMoreRequest);\n         } catch (IOException ioe) {\n           DFSClient.LOG.debug(\"Failed getting node for hedged read: {}\",\n               ioe.getMessage());\n         }\n         // if not succeeded. Submit callables for each datanode in a loop, wait\n         // for a fixed interval and get the result from the fastest one.\n         try {\n           ByteBuffer result \u003d getFirstToComplete(hedgedService, futures);\n           // cancel the rest.\n           cancelAll(futures);\n           dfsClient.getHedgedReadMetrics().incHedgedReadWins();\n-          System.arraycopy(result.array(), result.position(), buf, offset,\n-              len);\n+          result.flip();\n+          buf.put(result);\n           return;\n         } catch (InterruptedException ie) {\n           // Ignore and retry\n         }\n         // We got here if exception. Ignore this node on next go around IFF\n         // we found a chosenNode to hedge read against.\n         if (chosenNode !\u003d null \u0026\u0026 chosenNode.info !\u003d null) {\n           ignored.add(chosenNode.info);\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void hedgedFetchBlockByteRange(LocatedBlock block, long start,\n      long end, ByteBuffer buf, CorruptedBlocks corruptedBlocks)\n      throws IOException {\n    final DfsClientConf conf \u003d dfsClient.getConf();\n    ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e futures \u003d new ArrayList\u003c\u003e();\n    CompletionService\u003cByteBuffer\u003e hedgedService \u003d\n        new ExecutorCompletionService\u003c\u003e(dfsClient.getHedgedReadsThreadPool());\n    ArrayList\u003cDatanodeInfo\u003e ignored \u003d new ArrayList\u003c\u003e();\n    ByteBuffer bb;\n    int len \u003d (int) (end - start + 1);\n    int hedgedReadId \u003d 0;\n    block \u003d refreshLocatedBlock(block);\n    while (true) {\n      // see HDFS-6591, this metric is used to verify/catch unnecessary loops\n      hedgedReadOpsLoopNumForTesting++;\n      DNAddrPair chosenNode \u003d null;\n      // there is no request already executing.\n      if (futures.isEmpty()) {\n        // chooseDataNode is a commitment. If no node, we go to\n        // the NN to reget block locations. Only go here on first read.\n        chosenNode \u003d chooseDataNode(block, ignored);\n        bb \u003d ByteBuffer.allocate(len);\n        Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n            chosenNode, block, start, end, bb,\n            corruptedBlocks, hedgedReadId++);\n        Future\u003cByteBuffer\u003e firstRequest \u003d hedgedService\n            .submit(getFromDataNodeCallable);\n        futures.add(firstRequest);\n        try {\n          Future\u003cByteBuffer\u003e future \u003d hedgedService.poll(\n              conf.getHedgedReadThresholdMillis(), TimeUnit.MILLISECONDS);\n          if (future !\u003d null) {\n            ByteBuffer result \u003d future.get();\n            result.flip();\n            buf.put(result);\n            return;\n          }\n          DFSClient.LOG.debug(\"Waited {}ms to read from {}; spawning hedged \"\n              + \"read\", conf.getHedgedReadThresholdMillis(), chosenNode.info);\n          // Ignore this node on next go around.\n          ignored.add(chosenNode.info);\n          dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n          // continue; no need to refresh block locations\n        } catch (ExecutionException e) {\n          // Ignore\n        } catch (InterruptedException e) {\n          throw new InterruptedIOException(\n              \"Interrupted while waiting for reading task\");\n        }\n      } else {\n        // We are starting up a \u0027hedged\u0027 read. We have a read already\n        // ongoing. Call getBestNodeDNAddrPair instead of chooseDataNode.\n        // If no nodes to do hedged reads against, pass.\n        try {\n          chosenNode \u003d getBestNodeDNAddrPair(block, ignored);\n          if (chosenNode \u003d\u003d null) {\n            chosenNode \u003d chooseDataNode(block, ignored);\n          }\n          bb \u003d ByteBuffer.allocate(len);\n          Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n              chosenNode, block, start, end, bb,\n              corruptedBlocks, hedgedReadId++);\n          Future\u003cByteBuffer\u003e oneMoreRequest \u003d hedgedService\n              .submit(getFromDataNodeCallable);\n          futures.add(oneMoreRequest);\n        } catch (IOException ioe) {\n          DFSClient.LOG.debug(\"Failed getting node for hedged read: {}\",\n              ioe.getMessage());\n        }\n        // if not succeeded. Submit callables for each datanode in a loop, wait\n        // for a fixed interval and get the result from the fastest one.\n        try {\n          ByteBuffer result \u003d getFirstToComplete(hedgedService, futures);\n          // cancel the rest.\n          cancelAll(futures);\n          dfsClient.getHedgedReadMetrics().incHedgedReadWins();\n          result.flip();\n          buf.put(result);\n          return;\n        } catch (InterruptedException ie) {\n          // Ignore and retry\n        }\n        // We got here if exception. Ignore this node on next go around IFF\n        // we found a chosenNode to hedge read against.\n        if (chosenNode !\u003d null \u0026\u0026 chosenNode.info !\u003d null) {\n          ignored.add(chosenNode.info);\n        }\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
          "extendedDetails": {}
        }
      ]
    },
    "be34e85e682880f46eee0310bf00ecc7d39cd5bd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10468. HDFS read ends up ignoring an interrupt. Contributed by Jing Zhao\n",
      "commitDate": "07/06/16 10:48 AM",
      "commitName": "be34e85e682880f46eee0310bf00ecc7d39cd5bd",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "25/04/16 12:01 PM",
      "commitNameOld": "f308561f1d885491b88db73ac63003202056d661",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 42.95,
      "commitsBetweenForRepo": 291,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,87 +1,90 @@\n   private void hedgedFetchBlockByteRange(LocatedBlock block, long start,\n       long end, byte[] buf, int offset, CorruptedBlocks corruptedBlocks)\n       throws IOException {\n     final DfsClientConf conf \u003d dfsClient.getConf();\n     ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e futures \u003d new ArrayList\u003c\u003e();\n     CompletionService\u003cByteBuffer\u003e hedgedService \u003d\n         new ExecutorCompletionService\u003c\u003e(dfsClient.getHedgedReadsThreadPool());\n     ArrayList\u003cDatanodeInfo\u003e ignored \u003d new ArrayList\u003c\u003e();\n     ByteBuffer bb;\n     int len \u003d (int) (end - start + 1);\n     int hedgedReadId \u003d 0;\n     block \u003d refreshLocatedBlock(block);\n     while (true) {\n       // see HDFS-6591, this metric is used to verify/catch unnecessary loops\n       hedgedReadOpsLoopNumForTesting++;\n       DNAddrPair chosenNode \u003d null;\n       // there is no request already executing.\n       if (futures.isEmpty()) {\n         // chooseDataNode is a commitment. If no node, we go to\n         // the NN to reget block locations. Only go here on first read.\n         chosenNode \u003d chooseDataNode(block, ignored);\n         bb \u003d ByteBuffer.allocate(len);\n         Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n             chosenNode, block, start, end, bb,\n             corruptedBlocks, hedgedReadId++);\n         Future\u003cByteBuffer\u003e firstRequest \u003d hedgedService\n             .submit(getFromDataNodeCallable);\n         futures.add(firstRequest);\n         try {\n           Future\u003cByteBuffer\u003e future \u003d hedgedService.poll(\n               conf.getHedgedReadThresholdMillis(), TimeUnit.MILLISECONDS);\n           if (future !\u003d null) {\n             ByteBuffer result \u003d future.get();\n             System.arraycopy(result.array(), result.position(), buf, offset,\n                 len);\n             return;\n           }\n           DFSClient.LOG.debug(\"Waited {}ms to read from {}; spawning hedged \"\n               + \"read\", conf.getHedgedReadThresholdMillis(), chosenNode.info);\n           // Ignore this node on next go around.\n           ignored.add(chosenNode.info);\n           dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n           // continue; no need to refresh block locations\n-        } catch (InterruptedException | ExecutionException e) {\n+        } catch (ExecutionException e) {\n           // Ignore\n+        } catch (InterruptedException e) {\n+          throw new InterruptedIOException(\n+              \"Interrupted while waiting for reading task\");\n         }\n       } else {\n         // We are starting up a \u0027hedged\u0027 read. We have a read already\n         // ongoing. Call getBestNodeDNAddrPair instead of chooseDataNode.\n         // If no nodes to do hedged reads against, pass.\n         try {\n           chosenNode \u003d getBestNodeDNAddrPair(block, ignored);\n           if (chosenNode \u003d\u003d null) {\n             chosenNode \u003d chooseDataNode(block, ignored);\n           }\n           bb \u003d ByteBuffer.allocate(len);\n           Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n               chosenNode, block, start, end, bb,\n               corruptedBlocks, hedgedReadId++);\n           Future\u003cByteBuffer\u003e oneMoreRequest \u003d hedgedService\n               .submit(getFromDataNodeCallable);\n           futures.add(oneMoreRequest);\n         } catch (IOException ioe) {\n           DFSClient.LOG.debug(\"Failed getting node for hedged read: {}\",\n               ioe.getMessage());\n         }\n         // if not succeeded. Submit callables for each datanode in a loop, wait\n         // for a fixed interval and get the result from the fastest one.\n         try {\n           ByteBuffer result \u003d getFirstToComplete(hedgedService, futures);\n           // cancel the rest.\n           cancelAll(futures);\n           dfsClient.getHedgedReadMetrics().incHedgedReadWins();\n           System.arraycopy(result.array(), result.position(), buf, offset,\n               len);\n           return;\n         } catch (InterruptedException ie) {\n           // Ignore and retry\n         }\n         // We got here if exception. Ignore this node on next go around IFF\n         // we found a chosenNode to hedge read against.\n         if (chosenNode !\u003d null \u0026\u0026 chosenNode.info !\u003d null) {\n           ignored.add(chosenNode.info);\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void hedgedFetchBlockByteRange(LocatedBlock block, long start,\n      long end, byte[] buf, int offset, CorruptedBlocks corruptedBlocks)\n      throws IOException {\n    final DfsClientConf conf \u003d dfsClient.getConf();\n    ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e futures \u003d new ArrayList\u003c\u003e();\n    CompletionService\u003cByteBuffer\u003e hedgedService \u003d\n        new ExecutorCompletionService\u003c\u003e(dfsClient.getHedgedReadsThreadPool());\n    ArrayList\u003cDatanodeInfo\u003e ignored \u003d new ArrayList\u003c\u003e();\n    ByteBuffer bb;\n    int len \u003d (int) (end - start + 1);\n    int hedgedReadId \u003d 0;\n    block \u003d refreshLocatedBlock(block);\n    while (true) {\n      // see HDFS-6591, this metric is used to verify/catch unnecessary loops\n      hedgedReadOpsLoopNumForTesting++;\n      DNAddrPair chosenNode \u003d null;\n      // there is no request already executing.\n      if (futures.isEmpty()) {\n        // chooseDataNode is a commitment. If no node, we go to\n        // the NN to reget block locations. Only go here on first read.\n        chosenNode \u003d chooseDataNode(block, ignored);\n        bb \u003d ByteBuffer.allocate(len);\n        Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n            chosenNode, block, start, end, bb,\n            corruptedBlocks, hedgedReadId++);\n        Future\u003cByteBuffer\u003e firstRequest \u003d hedgedService\n            .submit(getFromDataNodeCallable);\n        futures.add(firstRequest);\n        try {\n          Future\u003cByteBuffer\u003e future \u003d hedgedService.poll(\n              conf.getHedgedReadThresholdMillis(), TimeUnit.MILLISECONDS);\n          if (future !\u003d null) {\n            ByteBuffer result \u003d future.get();\n            System.arraycopy(result.array(), result.position(), buf, offset,\n                len);\n            return;\n          }\n          DFSClient.LOG.debug(\"Waited {}ms to read from {}; spawning hedged \"\n              + \"read\", conf.getHedgedReadThresholdMillis(), chosenNode.info);\n          // Ignore this node on next go around.\n          ignored.add(chosenNode.info);\n          dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n          // continue; no need to refresh block locations\n        } catch (ExecutionException e) {\n          // Ignore\n        } catch (InterruptedException e) {\n          throw new InterruptedIOException(\n              \"Interrupted while waiting for reading task\");\n        }\n      } else {\n        // We are starting up a \u0027hedged\u0027 read. We have a read already\n        // ongoing. Call getBestNodeDNAddrPair instead of chooseDataNode.\n        // If no nodes to do hedged reads against, pass.\n        try {\n          chosenNode \u003d getBestNodeDNAddrPair(block, ignored);\n          if (chosenNode \u003d\u003d null) {\n            chosenNode \u003d chooseDataNode(block, ignored);\n          }\n          bb \u003d ByteBuffer.allocate(len);\n          Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n              chosenNode, block, start, end, bb,\n              corruptedBlocks, hedgedReadId++);\n          Future\u003cByteBuffer\u003e oneMoreRequest \u003d hedgedService\n              .submit(getFromDataNodeCallable);\n          futures.add(oneMoreRequest);\n        } catch (IOException ioe) {\n          DFSClient.LOG.debug(\"Failed getting node for hedged read: {}\",\n              ioe.getMessage());\n        }\n        // if not succeeded. Submit callables for each datanode in a loop, wait\n        // for a fixed interval and get the result from the fastest one.\n        try {\n          ByteBuffer result \u003d getFirstToComplete(hedgedService, futures);\n          // cancel the rest.\n          cancelAll(futures);\n          dfsClient.getHedgedReadMetrics().incHedgedReadWins();\n          System.arraycopy(result.array(), result.position(), buf, offset,\n              len);\n          return;\n        } catch (InterruptedException ie) {\n          // Ignore and retry\n        }\n        // We got here if exception. Ignore this node on next go around IFF\n        // we found a chosenNode to hedge read against.\n        if (chosenNode !\u003d null \u0026\u0026 chosenNode.info !\u003d null) {\n          ignored.add(chosenNode.info);\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "d8383c687c95dbb37effa307ab2d41497da1cfc2": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10182. Hedged read might overwrite user\u0027s buf. Contributed by zhouyingchao.\n",
      "commitDate": "28/03/16 12:44 AM",
      "commitName": "d8383c687c95dbb37effa307ab2d41497da1cfc2",
      "commitAuthor": "Walter Su",
      "commitDateOld": "19/03/16 2:02 PM",
      "commitNameOld": "cd8b6889a74a949e37f4b2eb664cdf3b59bfb93b",
      "commitAuthorOld": "Sangjin Lee",
      "daysBetweenCommits": 8.45,
      "commitsBetweenForRepo": 38,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,89 +1,87 @@\n   private void hedgedFetchBlockByteRange(LocatedBlock block, long start,\n       long end, byte[] buf, int offset, CorruptedBlocks corruptedBlocks)\n       throws IOException {\n     final DfsClientConf conf \u003d dfsClient.getConf();\n     ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e futures \u003d new ArrayList\u003c\u003e();\n     CompletionService\u003cByteBuffer\u003e hedgedService \u003d\n         new ExecutorCompletionService\u003c\u003e(dfsClient.getHedgedReadsThreadPool());\n     ArrayList\u003cDatanodeInfo\u003e ignored \u003d new ArrayList\u003c\u003e();\n     ByteBuffer bb;\n     int len \u003d (int) (end - start + 1);\n     int hedgedReadId \u003d 0;\n     block \u003d refreshLocatedBlock(block);\n     while (true) {\n       // see HDFS-6591, this metric is used to verify/catch unnecessary loops\n       hedgedReadOpsLoopNumForTesting++;\n       DNAddrPair chosenNode \u003d null;\n       // there is no request already executing.\n       if (futures.isEmpty()) {\n         // chooseDataNode is a commitment. If no node, we go to\n         // the NN to reget block locations. Only go here on first read.\n         chosenNode \u003d chooseDataNode(block, ignored);\n-        bb \u003d ByteBuffer.wrap(buf, offset, len);\n+        bb \u003d ByteBuffer.allocate(len);\n         Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n             chosenNode, block, start, end, bb,\n             corruptedBlocks, hedgedReadId++);\n         Future\u003cByteBuffer\u003e firstRequest \u003d hedgedService\n             .submit(getFromDataNodeCallable);\n         futures.add(firstRequest);\n         try {\n           Future\u003cByteBuffer\u003e future \u003d hedgedService.poll(\n               conf.getHedgedReadThresholdMillis(), TimeUnit.MILLISECONDS);\n           if (future !\u003d null) {\n-            future.get();\n+            ByteBuffer result \u003d future.get();\n+            System.arraycopy(result.array(), result.position(), buf, offset,\n+                len);\n             return;\n           }\n           DFSClient.LOG.debug(\"Waited {}ms to read from {}; spawning hedged \"\n               + \"read\", conf.getHedgedReadThresholdMillis(), chosenNode.info);\n           // Ignore this node on next go around.\n           ignored.add(chosenNode.info);\n           dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n           // continue; no need to refresh block locations\n         } catch (InterruptedException | ExecutionException e) {\n           // Ignore\n         }\n       } else {\n         // We are starting up a \u0027hedged\u0027 read. We have a read already\n         // ongoing. Call getBestNodeDNAddrPair instead of chooseDataNode.\n         // If no nodes to do hedged reads against, pass.\n         try {\n           chosenNode \u003d getBestNodeDNAddrPair(block, ignored);\n           if (chosenNode \u003d\u003d null) {\n             chosenNode \u003d chooseDataNode(block, ignored);\n           }\n           bb \u003d ByteBuffer.allocate(len);\n           Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n               chosenNode, block, start, end, bb,\n               corruptedBlocks, hedgedReadId++);\n           Future\u003cByteBuffer\u003e oneMoreRequest \u003d hedgedService\n               .submit(getFromDataNodeCallable);\n           futures.add(oneMoreRequest);\n         } catch (IOException ioe) {\n           DFSClient.LOG.debug(\"Failed getting node for hedged read: {}\",\n               ioe.getMessage());\n         }\n         // if not succeeded. Submit callables for each datanode in a loop, wait\n         // for a fixed interval and get the result from the fastest one.\n         try {\n           ByteBuffer result \u003d getFirstToComplete(hedgedService, futures);\n           // cancel the rest.\n           cancelAll(futures);\n-          if (result.array() !\u003d buf) { // compare the array pointers\n-            dfsClient.getHedgedReadMetrics().incHedgedReadWins();\n-            System.arraycopy(result.array(), result.position(), buf, offset,\n-                len);\n-          } else {\n-            dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n-          }\n+          dfsClient.getHedgedReadMetrics().incHedgedReadWins();\n+          System.arraycopy(result.array(), result.position(), buf, offset,\n+              len);\n           return;\n         } catch (InterruptedException ie) {\n           // Ignore and retry\n         }\n         // We got here if exception. Ignore this node on next go around IFF\n         // we found a chosenNode to hedge read against.\n         if (chosenNode !\u003d null \u0026\u0026 chosenNode.info !\u003d null) {\n           ignored.add(chosenNode.info);\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void hedgedFetchBlockByteRange(LocatedBlock block, long start,\n      long end, byte[] buf, int offset, CorruptedBlocks corruptedBlocks)\n      throws IOException {\n    final DfsClientConf conf \u003d dfsClient.getConf();\n    ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e futures \u003d new ArrayList\u003c\u003e();\n    CompletionService\u003cByteBuffer\u003e hedgedService \u003d\n        new ExecutorCompletionService\u003c\u003e(dfsClient.getHedgedReadsThreadPool());\n    ArrayList\u003cDatanodeInfo\u003e ignored \u003d new ArrayList\u003c\u003e();\n    ByteBuffer bb;\n    int len \u003d (int) (end - start + 1);\n    int hedgedReadId \u003d 0;\n    block \u003d refreshLocatedBlock(block);\n    while (true) {\n      // see HDFS-6591, this metric is used to verify/catch unnecessary loops\n      hedgedReadOpsLoopNumForTesting++;\n      DNAddrPair chosenNode \u003d null;\n      // there is no request already executing.\n      if (futures.isEmpty()) {\n        // chooseDataNode is a commitment. If no node, we go to\n        // the NN to reget block locations. Only go here on first read.\n        chosenNode \u003d chooseDataNode(block, ignored);\n        bb \u003d ByteBuffer.allocate(len);\n        Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n            chosenNode, block, start, end, bb,\n            corruptedBlocks, hedgedReadId++);\n        Future\u003cByteBuffer\u003e firstRequest \u003d hedgedService\n            .submit(getFromDataNodeCallable);\n        futures.add(firstRequest);\n        try {\n          Future\u003cByteBuffer\u003e future \u003d hedgedService.poll(\n              conf.getHedgedReadThresholdMillis(), TimeUnit.MILLISECONDS);\n          if (future !\u003d null) {\n            ByteBuffer result \u003d future.get();\n            System.arraycopy(result.array(), result.position(), buf, offset,\n                len);\n            return;\n          }\n          DFSClient.LOG.debug(\"Waited {}ms to read from {}; spawning hedged \"\n              + \"read\", conf.getHedgedReadThresholdMillis(), chosenNode.info);\n          // Ignore this node on next go around.\n          ignored.add(chosenNode.info);\n          dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n          // continue; no need to refresh block locations\n        } catch (InterruptedException | ExecutionException e) {\n          // Ignore\n        }\n      } else {\n        // We are starting up a \u0027hedged\u0027 read. We have a read already\n        // ongoing. Call getBestNodeDNAddrPair instead of chooseDataNode.\n        // If no nodes to do hedged reads against, pass.\n        try {\n          chosenNode \u003d getBestNodeDNAddrPair(block, ignored);\n          if (chosenNode \u003d\u003d null) {\n            chosenNode \u003d chooseDataNode(block, ignored);\n          }\n          bb \u003d ByteBuffer.allocate(len);\n          Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n              chosenNode, block, start, end, bb,\n              corruptedBlocks, hedgedReadId++);\n          Future\u003cByteBuffer\u003e oneMoreRequest \u003d hedgedService\n              .submit(getFromDataNodeCallable);\n          futures.add(oneMoreRequest);\n        } catch (IOException ioe) {\n          DFSClient.LOG.debug(\"Failed getting node for hedged read: {}\",\n              ioe.getMessage());\n        }\n        // if not succeeded. Submit callables for each datanode in a loop, wait\n        // for a fixed interval and get the result from the fastest one.\n        try {\n          ByteBuffer result \u003d getFirstToComplete(hedgedService, futures);\n          // cancel the rest.\n          cancelAll(futures);\n          dfsClient.getHedgedReadMetrics().incHedgedReadWins();\n          System.arraycopy(result.array(), result.position(), buf, offset,\n              len);\n          return;\n        } catch (InterruptedException ie) {\n          // Ignore and retry\n        }\n        // We got here if exception. Ignore this node on next go around IFF\n        // we found a chosenNode to hedge read against.\n        if (chosenNode !\u003d null \u0026\u0026 chosenNode.info !\u003d null) {\n          ignored.add(chosenNode.info);\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "8808779db351fe444388d4acb3094766b5980718": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-9734. Refactoring of checksum failure report related codes. Contributed by Kai Zheng.\n\nChange-Id: Ie69a77e3498a360959f8e213c51fb2b17c28b64a\n",
      "commitDate": "25/02/16 9:55 AM",
      "commitName": "8808779db351fe444388d4acb3094766b5980718",
      "commitAuthor": "Zhe Zhang",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-9734. Refactoring of checksum failure report related codes. Contributed by Kai Zheng.\n\nChange-Id: Ie69a77e3498a360959f8e213c51fb2b17c28b64a\n",
          "commitDate": "25/02/16 9:55 AM",
          "commitName": "8808779db351fe444388d4acb3094766b5980718",
          "commitAuthor": "Zhe Zhang",
          "commitDateOld": "22/01/16 9:46 AM",
          "commitNameOld": "95363bcc7dae28ba9ae2cd7ee9a258fcb58cd932",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 34.01,
          "commitsBetweenForRepo": 234,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,90 +1,89 @@\n   private void hedgedFetchBlockByteRange(LocatedBlock block, long start,\n-      long end, byte[] buf, int offset,\n-      Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n+      long end, byte[] buf, int offset, CorruptedBlocks corruptedBlocks)\n       throws IOException {\n     final DfsClientConf conf \u003d dfsClient.getConf();\n     ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e futures \u003d new ArrayList\u003c\u003e();\n     CompletionService\u003cByteBuffer\u003e hedgedService \u003d\n         new ExecutorCompletionService\u003c\u003e(dfsClient.getHedgedReadsThreadPool());\n     ArrayList\u003cDatanodeInfo\u003e ignored \u003d new ArrayList\u003c\u003e();\n     ByteBuffer bb;\n     int len \u003d (int) (end - start + 1);\n     int hedgedReadId \u003d 0;\n     block \u003d refreshLocatedBlock(block);\n     while (true) {\n       // see HDFS-6591, this metric is used to verify/catch unnecessary loops\n       hedgedReadOpsLoopNumForTesting++;\n       DNAddrPair chosenNode \u003d null;\n       // there is no request already executing.\n       if (futures.isEmpty()) {\n         // chooseDataNode is a commitment. If no node, we go to\n         // the NN to reget block locations. Only go here on first read.\n         chosenNode \u003d chooseDataNode(block, ignored);\n         bb \u003d ByteBuffer.wrap(buf, offset, len);\n         Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n             chosenNode, block, start, end, bb,\n-            corruptedBlockMap, hedgedReadId++);\n+            corruptedBlocks, hedgedReadId++);\n         Future\u003cByteBuffer\u003e firstRequest \u003d hedgedService\n             .submit(getFromDataNodeCallable);\n         futures.add(firstRequest);\n         try {\n           Future\u003cByteBuffer\u003e future \u003d hedgedService.poll(\n               conf.getHedgedReadThresholdMillis(), TimeUnit.MILLISECONDS);\n           if (future !\u003d null) {\n             future.get();\n             return;\n           }\n           DFSClient.LOG.debug(\"Waited {}ms to read from {}; spawning hedged \"\n               + \"read\", conf.getHedgedReadThresholdMillis(), chosenNode.info);\n           // Ignore this node on next go around.\n           ignored.add(chosenNode.info);\n           dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n           // continue; no need to refresh block locations\n         } catch (InterruptedException | ExecutionException e) {\n           // Ignore\n         }\n       } else {\n         // We are starting up a \u0027hedged\u0027 read. We have a read already\n         // ongoing. Call getBestNodeDNAddrPair instead of chooseDataNode.\n         // If no nodes to do hedged reads against, pass.\n         try {\n           chosenNode \u003d getBestNodeDNAddrPair(block, ignored);\n           if (chosenNode \u003d\u003d null) {\n             chosenNode \u003d chooseDataNode(block, ignored);\n           }\n           bb \u003d ByteBuffer.allocate(len);\n           Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n               chosenNode, block, start, end, bb,\n-              corruptedBlockMap, hedgedReadId++);\n+              corruptedBlocks, hedgedReadId++);\n           Future\u003cByteBuffer\u003e oneMoreRequest \u003d hedgedService\n               .submit(getFromDataNodeCallable);\n           futures.add(oneMoreRequest);\n         } catch (IOException ioe) {\n           DFSClient.LOG.debug(\"Failed getting node for hedged read: {}\",\n               ioe.getMessage());\n         }\n         // if not succeeded. Submit callables for each datanode in a loop, wait\n         // for a fixed interval and get the result from the fastest one.\n         try {\n           ByteBuffer result \u003d getFirstToComplete(hedgedService, futures);\n           // cancel the rest.\n           cancelAll(futures);\n           if (result.array() !\u003d buf) { // compare the array pointers\n             dfsClient.getHedgedReadMetrics().incHedgedReadWins();\n             System.arraycopy(result.array(), result.position(), buf, offset,\n                 len);\n           } else {\n             dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n           }\n           return;\n         } catch (InterruptedException ie) {\n           // Ignore and retry\n         }\n         // We got here if exception. Ignore this node on next go around IFF\n         // we found a chosenNode to hedge read against.\n         if (chosenNode !\u003d null \u0026\u0026 chosenNode.info !\u003d null) {\n           ignored.add(chosenNode.info);\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void hedgedFetchBlockByteRange(LocatedBlock block, long start,\n      long end, byte[] buf, int offset, CorruptedBlocks corruptedBlocks)\n      throws IOException {\n    final DfsClientConf conf \u003d dfsClient.getConf();\n    ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e futures \u003d new ArrayList\u003c\u003e();\n    CompletionService\u003cByteBuffer\u003e hedgedService \u003d\n        new ExecutorCompletionService\u003c\u003e(dfsClient.getHedgedReadsThreadPool());\n    ArrayList\u003cDatanodeInfo\u003e ignored \u003d new ArrayList\u003c\u003e();\n    ByteBuffer bb;\n    int len \u003d (int) (end - start + 1);\n    int hedgedReadId \u003d 0;\n    block \u003d refreshLocatedBlock(block);\n    while (true) {\n      // see HDFS-6591, this metric is used to verify/catch unnecessary loops\n      hedgedReadOpsLoopNumForTesting++;\n      DNAddrPair chosenNode \u003d null;\n      // there is no request already executing.\n      if (futures.isEmpty()) {\n        // chooseDataNode is a commitment. If no node, we go to\n        // the NN to reget block locations. Only go here on first read.\n        chosenNode \u003d chooseDataNode(block, ignored);\n        bb \u003d ByteBuffer.wrap(buf, offset, len);\n        Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n            chosenNode, block, start, end, bb,\n            corruptedBlocks, hedgedReadId++);\n        Future\u003cByteBuffer\u003e firstRequest \u003d hedgedService\n            .submit(getFromDataNodeCallable);\n        futures.add(firstRequest);\n        try {\n          Future\u003cByteBuffer\u003e future \u003d hedgedService.poll(\n              conf.getHedgedReadThresholdMillis(), TimeUnit.MILLISECONDS);\n          if (future !\u003d null) {\n            future.get();\n            return;\n          }\n          DFSClient.LOG.debug(\"Waited {}ms to read from {}; spawning hedged \"\n              + \"read\", conf.getHedgedReadThresholdMillis(), chosenNode.info);\n          // Ignore this node on next go around.\n          ignored.add(chosenNode.info);\n          dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n          // continue; no need to refresh block locations\n        } catch (InterruptedException | ExecutionException e) {\n          // Ignore\n        }\n      } else {\n        // We are starting up a \u0027hedged\u0027 read. We have a read already\n        // ongoing. Call getBestNodeDNAddrPair instead of chooseDataNode.\n        // If no nodes to do hedged reads against, pass.\n        try {\n          chosenNode \u003d getBestNodeDNAddrPair(block, ignored);\n          if (chosenNode \u003d\u003d null) {\n            chosenNode \u003d chooseDataNode(block, ignored);\n          }\n          bb \u003d ByteBuffer.allocate(len);\n          Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n              chosenNode, block, start, end, bb,\n              corruptedBlocks, hedgedReadId++);\n          Future\u003cByteBuffer\u003e oneMoreRequest \u003d hedgedService\n              .submit(getFromDataNodeCallable);\n          futures.add(oneMoreRequest);\n        } catch (IOException ioe) {\n          DFSClient.LOG.debug(\"Failed getting node for hedged read: {}\",\n              ioe.getMessage());\n        }\n        // if not succeeded. Submit callables for each datanode in a loop, wait\n        // for a fixed interval and get the result from the fastest one.\n        try {\n          ByteBuffer result \u003d getFirstToComplete(hedgedService, futures);\n          // cancel the rest.\n          cancelAll(futures);\n          if (result.array() !\u003d buf) { // compare the array pointers\n            dfsClient.getHedgedReadMetrics().incHedgedReadWins();\n            System.arraycopy(result.array(), result.position(), buf, offset,\n                len);\n          } else {\n            dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n          }\n          return;\n        } catch (InterruptedException ie) {\n          // Ignore and retry\n        }\n        // We got here if exception. Ignore this node on next go around IFF\n        // we found a chosenNode to hedge read against.\n        if (chosenNode !\u003d null \u0026\u0026 chosenNode.info !\u003d null) {\n          ignored.add(chosenNode.info);\n        }\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
          "extendedDetails": {
            "oldValue": "[block-LocatedBlock, start-long, end-long, buf-byte[], offset-int, corruptedBlockMap-Map\u003cExtendedBlock,Set\u003cDatanodeInfo\u003e\u003e]",
            "newValue": "[block-LocatedBlock, start-long, end-long, buf-byte[], offset-int, corruptedBlocks-CorruptedBlocks]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-9734. Refactoring of checksum failure report related codes. Contributed by Kai Zheng.\n\nChange-Id: Ie69a77e3498a360959f8e213c51fb2b17c28b64a\n",
          "commitDate": "25/02/16 9:55 AM",
          "commitName": "8808779db351fe444388d4acb3094766b5980718",
          "commitAuthor": "Zhe Zhang",
          "commitDateOld": "22/01/16 9:46 AM",
          "commitNameOld": "95363bcc7dae28ba9ae2cd7ee9a258fcb58cd932",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 34.01,
          "commitsBetweenForRepo": 234,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,90 +1,89 @@\n   private void hedgedFetchBlockByteRange(LocatedBlock block, long start,\n-      long end, byte[] buf, int offset,\n-      Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n+      long end, byte[] buf, int offset, CorruptedBlocks corruptedBlocks)\n       throws IOException {\n     final DfsClientConf conf \u003d dfsClient.getConf();\n     ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e futures \u003d new ArrayList\u003c\u003e();\n     CompletionService\u003cByteBuffer\u003e hedgedService \u003d\n         new ExecutorCompletionService\u003c\u003e(dfsClient.getHedgedReadsThreadPool());\n     ArrayList\u003cDatanodeInfo\u003e ignored \u003d new ArrayList\u003c\u003e();\n     ByteBuffer bb;\n     int len \u003d (int) (end - start + 1);\n     int hedgedReadId \u003d 0;\n     block \u003d refreshLocatedBlock(block);\n     while (true) {\n       // see HDFS-6591, this metric is used to verify/catch unnecessary loops\n       hedgedReadOpsLoopNumForTesting++;\n       DNAddrPair chosenNode \u003d null;\n       // there is no request already executing.\n       if (futures.isEmpty()) {\n         // chooseDataNode is a commitment. If no node, we go to\n         // the NN to reget block locations. Only go here on first read.\n         chosenNode \u003d chooseDataNode(block, ignored);\n         bb \u003d ByteBuffer.wrap(buf, offset, len);\n         Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n             chosenNode, block, start, end, bb,\n-            corruptedBlockMap, hedgedReadId++);\n+            corruptedBlocks, hedgedReadId++);\n         Future\u003cByteBuffer\u003e firstRequest \u003d hedgedService\n             .submit(getFromDataNodeCallable);\n         futures.add(firstRequest);\n         try {\n           Future\u003cByteBuffer\u003e future \u003d hedgedService.poll(\n               conf.getHedgedReadThresholdMillis(), TimeUnit.MILLISECONDS);\n           if (future !\u003d null) {\n             future.get();\n             return;\n           }\n           DFSClient.LOG.debug(\"Waited {}ms to read from {}; spawning hedged \"\n               + \"read\", conf.getHedgedReadThresholdMillis(), chosenNode.info);\n           // Ignore this node on next go around.\n           ignored.add(chosenNode.info);\n           dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n           // continue; no need to refresh block locations\n         } catch (InterruptedException | ExecutionException e) {\n           // Ignore\n         }\n       } else {\n         // We are starting up a \u0027hedged\u0027 read. We have a read already\n         // ongoing. Call getBestNodeDNAddrPair instead of chooseDataNode.\n         // If no nodes to do hedged reads against, pass.\n         try {\n           chosenNode \u003d getBestNodeDNAddrPair(block, ignored);\n           if (chosenNode \u003d\u003d null) {\n             chosenNode \u003d chooseDataNode(block, ignored);\n           }\n           bb \u003d ByteBuffer.allocate(len);\n           Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n               chosenNode, block, start, end, bb,\n-              corruptedBlockMap, hedgedReadId++);\n+              corruptedBlocks, hedgedReadId++);\n           Future\u003cByteBuffer\u003e oneMoreRequest \u003d hedgedService\n               .submit(getFromDataNodeCallable);\n           futures.add(oneMoreRequest);\n         } catch (IOException ioe) {\n           DFSClient.LOG.debug(\"Failed getting node for hedged read: {}\",\n               ioe.getMessage());\n         }\n         // if not succeeded. Submit callables for each datanode in a loop, wait\n         // for a fixed interval and get the result from the fastest one.\n         try {\n           ByteBuffer result \u003d getFirstToComplete(hedgedService, futures);\n           // cancel the rest.\n           cancelAll(futures);\n           if (result.array() !\u003d buf) { // compare the array pointers\n             dfsClient.getHedgedReadMetrics().incHedgedReadWins();\n             System.arraycopy(result.array(), result.position(), buf, offset,\n                 len);\n           } else {\n             dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n           }\n           return;\n         } catch (InterruptedException ie) {\n           // Ignore and retry\n         }\n         // We got here if exception. Ignore this node on next go around IFF\n         // we found a chosenNode to hedge read against.\n         if (chosenNode !\u003d null \u0026\u0026 chosenNode.info !\u003d null) {\n           ignored.add(chosenNode.info);\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void hedgedFetchBlockByteRange(LocatedBlock block, long start,\n      long end, byte[] buf, int offset, CorruptedBlocks corruptedBlocks)\n      throws IOException {\n    final DfsClientConf conf \u003d dfsClient.getConf();\n    ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e futures \u003d new ArrayList\u003c\u003e();\n    CompletionService\u003cByteBuffer\u003e hedgedService \u003d\n        new ExecutorCompletionService\u003c\u003e(dfsClient.getHedgedReadsThreadPool());\n    ArrayList\u003cDatanodeInfo\u003e ignored \u003d new ArrayList\u003c\u003e();\n    ByteBuffer bb;\n    int len \u003d (int) (end - start + 1);\n    int hedgedReadId \u003d 0;\n    block \u003d refreshLocatedBlock(block);\n    while (true) {\n      // see HDFS-6591, this metric is used to verify/catch unnecessary loops\n      hedgedReadOpsLoopNumForTesting++;\n      DNAddrPair chosenNode \u003d null;\n      // there is no request already executing.\n      if (futures.isEmpty()) {\n        // chooseDataNode is a commitment. If no node, we go to\n        // the NN to reget block locations. Only go here on first read.\n        chosenNode \u003d chooseDataNode(block, ignored);\n        bb \u003d ByteBuffer.wrap(buf, offset, len);\n        Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n            chosenNode, block, start, end, bb,\n            corruptedBlocks, hedgedReadId++);\n        Future\u003cByteBuffer\u003e firstRequest \u003d hedgedService\n            .submit(getFromDataNodeCallable);\n        futures.add(firstRequest);\n        try {\n          Future\u003cByteBuffer\u003e future \u003d hedgedService.poll(\n              conf.getHedgedReadThresholdMillis(), TimeUnit.MILLISECONDS);\n          if (future !\u003d null) {\n            future.get();\n            return;\n          }\n          DFSClient.LOG.debug(\"Waited {}ms to read from {}; spawning hedged \"\n              + \"read\", conf.getHedgedReadThresholdMillis(), chosenNode.info);\n          // Ignore this node on next go around.\n          ignored.add(chosenNode.info);\n          dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n          // continue; no need to refresh block locations\n        } catch (InterruptedException | ExecutionException e) {\n          // Ignore\n        }\n      } else {\n        // We are starting up a \u0027hedged\u0027 read. We have a read already\n        // ongoing. Call getBestNodeDNAddrPair instead of chooseDataNode.\n        // If no nodes to do hedged reads against, pass.\n        try {\n          chosenNode \u003d getBestNodeDNAddrPair(block, ignored);\n          if (chosenNode \u003d\u003d null) {\n            chosenNode \u003d chooseDataNode(block, ignored);\n          }\n          bb \u003d ByteBuffer.allocate(len);\n          Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n              chosenNode, block, start, end, bb,\n              corruptedBlocks, hedgedReadId++);\n          Future\u003cByteBuffer\u003e oneMoreRequest \u003d hedgedService\n              .submit(getFromDataNodeCallable);\n          futures.add(oneMoreRequest);\n        } catch (IOException ioe) {\n          DFSClient.LOG.debug(\"Failed getting node for hedged read: {}\",\n              ioe.getMessage());\n        }\n        // if not succeeded. Submit callables for each datanode in a loop, wait\n        // for a fixed interval and get the result from the fastest one.\n        try {\n          ByteBuffer result \u003d getFirstToComplete(hedgedService, futures);\n          // cancel the rest.\n          cancelAll(futures);\n          if (result.array() !\u003d buf) { // compare the array pointers\n            dfsClient.getHedgedReadMetrics().incHedgedReadWins();\n            System.arraycopy(result.array(), result.position(), buf, offset,\n                len);\n          } else {\n            dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n          }\n          return;\n        } catch (InterruptedException ie) {\n          // Ignore and retry\n        }\n        // We got here if exception. Ignore this node on next go around IFF\n        // we found a chosenNode to hedge read against.\n        if (chosenNode !\u003d null \u0026\u0026 chosenNode.info !\u003d null) {\n          ignored.add(chosenNode.info);\n        }\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
          "extendedDetails": {}
        }
      ]
    },
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8979. Clean up checkstyle warnings in hadoop-hdfs-client module. Contributed by Mingliang Liu.\n",
      "commitDate": "03/10/15 11:38 AM",
      "commitName": "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "30/09/15 8:39 AM",
      "commitNameOld": "6c17d315287020368689fa078a40a1eaedf89d5b",
      "commitAuthorOld": "",
      "daysBetweenCommits": 3.12,
      "commitsBetweenForRepo": 16,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,93 +1,90 @@\n   private void hedgedFetchBlockByteRange(LocatedBlock block, long start,\n       long end, byte[] buf, int offset,\n       Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n       throws IOException {\n     final DfsClientConf conf \u003d dfsClient.getConf();\n-    ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e futures \u003d new ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e();\n+    ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e futures \u003d new ArrayList\u003c\u003e();\n     CompletionService\u003cByteBuffer\u003e hedgedService \u003d\n-        new ExecutorCompletionService\u003cByteBuffer\u003e(\n-        dfsClient.getHedgedReadsThreadPool());\n-    ArrayList\u003cDatanodeInfo\u003e ignored \u003d new ArrayList\u003cDatanodeInfo\u003e();\n-    ByteBuffer bb \u003d null;\n+        new ExecutorCompletionService\u003c\u003e(dfsClient.getHedgedReadsThreadPool());\n+    ArrayList\u003cDatanodeInfo\u003e ignored \u003d new ArrayList\u003c\u003e();\n+    ByteBuffer bb;\n     int len \u003d (int) (end - start + 1);\n     int hedgedReadId \u003d 0;\n     block \u003d refreshLocatedBlock(block);\n     while (true) {\n       // see HDFS-6591, this metric is used to verify/catch unnecessary loops\n       hedgedReadOpsLoopNumForTesting++;\n       DNAddrPair chosenNode \u003d null;\n       // there is no request already executing.\n       if (futures.isEmpty()) {\n         // chooseDataNode is a commitment. If no node, we go to\n         // the NN to reget block locations. Only go here on first read.\n         chosenNode \u003d chooseDataNode(block, ignored);\n         bb \u003d ByteBuffer.wrap(buf, offset, len);\n         Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n             chosenNode, block, start, end, bb,\n             corruptedBlockMap, hedgedReadId++);\n         Future\u003cByteBuffer\u003e firstRequest \u003d hedgedService\n             .submit(getFromDataNodeCallable);\n         futures.add(firstRequest);\n         try {\n           Future\u003cByteBuffer\u003e future \u003d hedgedService.poll(\n               conf.getHedgedReadThresholdMillis(), TimeUnit.MILLISECONDS);\n           if (future !\u003d null) {\n             future.get();\n             return;\n           }\n           DFSClient.LOG.debug(\"Waited {}ms to read from {}; spawning hedged \"\n               + \"read\", conf.getHedgedReadThresholdMillis(), chosenNode.info);\n           // Ignore this node on next go around.\n           ignored.add(chosenNode.info);\n           dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n-          continue; // no need to refresh block locations\n-        } catch (InterruptedException e) {\n+          // continue; no need to refresh block locations\n+        } catch (InterruptedException | ExecutionException e) {\n           // Ignore\n-        } catch (ExecutionException e) {\n-          // Ignore already logged in the call.\n         }\n       } else {\n         // We are starting up a \u0027hedged\u0027 read. We have a read already\n         // ongoing. Call getBestNodeDNAddrPair instead of chooseDataNode.\n         // If no nodes to do hedged reads against, pass.\n         try {\n           chosenNode \u003d getBestNodeDNAddrPair(block, ignored);\n           if (chosenNode \u003d\u003d null) {\n             chosenNode \u003d chooseDataNode(block, ignored);\n           }\n           bb \u003d ByteBuffer.allocate(len);\n           Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n               chosenNode, block, start, end, bb,\n               corruptedBlockMap, hedgedReadId++);\n           Future\u003cByteBuffer\u003e oneMoreRequest \u003d hedgedService\n               .submit(getFromDataNodeCallable);\n           futures.add(oneMoreRequest);\n         } catch (IOException ioe) {\n           DFSClient.LOG.debug(\"Failed getting node for hedged read: {}\",\n               ioe.getMessage());\n         }\n         // if not succeeded. Submit callables for each datanode in a loop, wait\n         // for a fixed interval and get the result from the fastest one.\n         try {\n           ByteBuffer result \u003d getFirstToComplete(hedgedService, futures);\n           // cancel the rest.\n           cancelAll(futures);\n           if (result.array() !\u003d buf) { // compare the array pointers\n             dfsClient.getHedgedReadMetrics().incHedgedReadWins();\n             System.arraycopy(result.array(), result.position(), buf, offset,\n                 len);\n           } else {\n             dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n           }\n           return;\n         } catch (InterruptedException ie) {\n           // Ignore and retry\n         }\n         // We got here if exception. Ignore this node on next go around IFF\n         // we found a chosenNode to hedge read against.\n         if (chosenNode !\u003d null \u0026\u0026 chosenNode.info !\u003d null) {\n           ignored.add(chosenNode.info);\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void hedgedFetchBlockByteRange(LocatedBlock block, long start,\n      long end, byte[] buf, int offset,\n      Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n      throws IOException {\n    final DfsClientConf conf \u003d dfsClient.getConf();\n    ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e futures \u003d new ArrayList\u003c\u003e();\n    CompletionService\u003cByteBuffer\u003e hedgedService \u003d\n        new ExecutorCompletionService\u003c\u003e(dfsClient.getHedgedReadsThreadPool());\n    ArrayList\u003cDatanodeInfo\u003e ignored \u003d new ArrayList\u003c\u003e();\n    ByteBuffer bb;\n    int len \u003d (int) (end - start + 1);\n    int hedgedReadId \u003d 0;\n    block \u003d refreshLocatedBlock(block);\n    while (true) {\n      // see HDFS-6591, this metric is used to verify/catch unnecessary loops\n      hedgedReadOpsLoopNumForTesting++;\n      DNAddrPair chosenNode \u003d null;\n      // there is no request already executing.\n      if (futures.isEmpty()) {\n        // chooseDataNode is a commitment. If no node, we go to\n        // the NN to reget block locations. Only go here on first read.\n        chosenNode \u003d chooseDataNode(block, ignored);\n        bb \u003d ByteBuffer.wrap(buf, offset, len);\n        Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n            chosenNode, block, start, end, bb,\n            corruptedBlockMap, hedgedReadId++);\n        Future\u003cByteBuffer\u003e firstRequest \u003d hedgedService\n            .submit(getFromDataNodeCallable);\n        futures.add(firstRequest);\n        try {\n          Future\u003cByteBuffer\u003e future \u003d hedgedService.poll(\n              conf.getHedgedReadThresholdMillis(), TimeUnit.MILLISECONDS);\n          if (future !\u003d null) {\n            future.get();\n            return;\n          }\n          DFSClient.LOG.debug(\"Waited {}ms to read from {}; spawning hedged \"\n              + \"read\", conf.getHedgedReadThresholdMillis(), chosenNode.info);\n          // Ignore this node on next go around.\n          ignored.add(chosenNode.info);\n          dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n          // continue; no need to refresh block locations\n        } catch (InterruptedException | ExecutionException e) {\n          // Ignore\n        }\n      } else {\n        // We are starting up a \u0027hedged\u0027 read. We have a read already\n        // ongoing. Call getBestNodeDNAddrPair instead of chooseDataNode.\n        // If no nodes to do hedged reads against, pass.\n        try {\n          chosenNode \u003d getBestNodeDNAddrPair(block, ignored);\n          if (chosenNode \u003d\u003d null) {\n            chosenNode \u003d chooseDataNode(block, ignored);\n          }\n          bb \u003d ByteBuffer.allocate(len);\n          Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n              chosenNode, block, start, end, bb,\n              corruptedBlockMap, hedgedReadId++);\n          Future\u003cByteBuffer\u003e oneMoreRequest \u003d hedgedService\n              .submit(getFromDataNodeCallable);\n          futures.add(oneMoreRequest);\n        } catch (IOException ioe) {\n          DFSClient.LOG.debug(\"Failed getting node for hedged read: {}\",\n              ioe.getMessage());\n        }\n        // if not succeeded. Submit callables for each datanode in a loop, wait\n        // for a fixed interval and get the result from the fastest one.\n        try {\n          ByteBuffer result \u003d getFirstToComplete(hedgedService, futures);\n          // cancel the rest.\n          cancelAll(futures);\n          if (result.array() !\u003d buf) { // compare the array pointers\n            dfsClient.getHedgedReadMetrics().incHedgedReadWins();\n            System.arraycopy(result.array(), result.position(), buf, offset,\n                len);\n          } else {\n            dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n          }\n          return;\n        } catch (InterruptedException ie) {\n          // Ignore and retry\n        }\n        // We got here if exception. Ignore this node on next go around IFF\n        // we found a chosenNode to hedge read against.\n        if (chosenNode !\u003d null \u0026\u0026 chosenNode.info !\u003d null) {\n          ignored.add(chosenNode.info);\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "39285e6a1978ea5e53bdc1b0aef62421382124a8": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8971. Remove guards when calling LOG.debug() and LOG.trace() in client package. Contributed by Mingliang Liu.\n",
      "commitDate": "29/09/15 5:52 PM",
      "commitName": "39285e6a1978ea5e53bdc1b0aef62421382124a8",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "29/09/15 5:51 PM",
      "commitNameOld": "6ee0539ede78b640f01c5eac18ded161182a7835",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,98 +1,93 @@\n   private void hedgedFetchBlockByteRange(LocatedBlock block, long start,\n       long end, byte[] buf, int offset,\n       Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n       throws IOException {\n     final DfsClientConf conf \u003d dfsClient.getConf();\n     ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e futures \u003d new ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e();\n     CompletionService\u003cByteBuffer\u003e hedgedService \u003d\n         new ExecutorCompletionService\u003cByteBuffer\u003e(\n         dfsClient.getHedgedReadsThreadPool());\n     ArrayList\u003cDatanodeInfo\u003e ignored \u003d new ArrayList\u003cDatanodeInfo\u003e();\n     ByteBuffer bb \u003d null;\n     int len \u003d (int) (end - start + 1);\n     int hedgedReadId \u003d 0;\n     block \u003d refreshLocatedBlock(block);\n     while (true) {\n       // see HDFS-6591, this metric is used to verify/catch unnecessary loops\n       hedgedReadOpsLoopNumForTesting++;\n       DNAddrPair chosenNode \u003d null;\n       // there is no request already executing.\n       if (futures.isEmpty()) {\n         // chooseDataNode is a commitment. If no node, we go to\n         // the NN to reget block locations. Only go here on first read.\n         chosenNode \u003d chooseDataNode(block, ignored);\n         bb \u003d ByteBuffer.wrap(buf, offset, len);\n         Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n             chosenNode, block, start, end, bb,\n             corruptedBlockMap, hedgedReadId++);\n         Future\u003cByteBuffer\u003e firstRequest \u003d hedgedService\n             .submit(getFromDataNodeCallable);\n         futures.add(firstRequest);\n         try {\n           Future\u003cByteBuffer\u003e future \u003d hedgedService.poll(\n               conf.getHedgedReadThresholdMillis(), TimeUnit.MILLISECONDS);\n           if (future !\u003d null) {\n             future.get();\n             return;\n           }\n-          if (DFSClient.LOG.isDebugEnabled()) {\n-            DFSClient.LOG.debug(\"Waited \" + conf.getHedgedReadThresholdMillis()\n-                + \"ms to read from \" + chosenNode.info\n-                + \"; spawning hedged read\");\n-          }\n+          DFSClient.LOG.debug(\"Waited {}ms to read from {}; spawning hedged \"\n+              + \"read\", conf.getHedgedReadThresholdMillis(), chosenNode.info);\n           // Ignore this node on next go around.\n           ignored.add(chosenNode.info);\n           dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n           continue; // no need to refresh block locations\n         } catch (InterruptedException e) {\n           // Ignore\n         } catch (ExecutionException e) {\n           // Ignore already logged in the call.\n         }\n       } else {\n         // We are starting up a \u0027hedged\u0027 read. We have a read already\n         // ongoing. Call getBestNodeDNAddrPair instead of chooseDataNode.\n         // If no nodes to do hedged reads against, pass.\n         try {\n           chosenNode \u003d getBestNodeDNAddrPair(block, ignored);\n           if (chosenNode \u003d\u003d null) {\n             chosenNode \u003d chooseDataNode(block, ignored);\n           }\n           bb \u003d ByteBuffer.allocate(len);\n           Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n               chosenNode, block, start, end, bb,\n               corruptedBlockMap, hedgedReadId++);\n           Future\u003cByteBuffer\u003e oneMoreRequest \u003d hedgedService\n               .submit(getFromDataNodeCallable);\n           futures.add(oneMoreRequest);\n         } catch (IOException ioe) {\n-          if (DFSClient.LOG.isDebugEnabled()) {\n-            DFSClient.LOG.debug(\"Failed getting node for hedged read: \"\n-                + ioe.getMessage());\n-          }\n+          DFSClient.LOG.debug(\"Failed getting node for hedged read: {}\",\n+              ioe.getMessage());\n         }\n         // if not succeeded. Submit callables for each datanode in a loop, wait\n         // for a fixed interval and get the result from the fastest one.\n         try {\n           ByteBuffer result \u003d getFirstToComplete(hedgedService, futures);\n           // cancel the rest.\n           cancelAll(futures);\n           if (result.array() !\u003d buf) { // compare the array pointers\n             dfsClient.getHedgedReadMetrics().incHedgedReadWins();\n             System.arraycopy(result.array(), result.position(), buf, offset,\n                 len);\n           } else {\n             dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n           }\n           return;\n         } catch (InterruptedException ie) {\n           // Ignore and retry\n         }\n         // We got here if exception. Ignore this node on next go around IFF\n         // we found a chosenNode to hedge read against.\n         if (chosenNode !\u003d null \u0026\u0026 chosenNode.info !\u003d null) {\n           ignored.add(chosenNode.info);\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void hedgedFetchBlockByteRange(LocatedBlock block, long start,\n      long end, byte[] buf, int offset,\n      Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n      throws IOException {\n    final DfsClientConf conf \u003d dfsClient.getConf();\n    ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e futures \u003d new ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e();\n    CompletionService\u003cByteBuffer\u003e hedgedService \u003d\n        new ExecutorCompletionService\u003cByteBuffer\u003e(\n        dfsClient.getHedgedReadsThreadPool());\n    ArrayList\u003cDatanodeInfo\u003e ignored \u003d new ArrayList\u003cDatanodeInfo\u003e();\n    ByteBuffer bb \u003d null;\n    int len \u003d (int) (end - start + 1);\n    int hedgedReadId \u003d 0;\n    block \u003d refreshLocatedBlock(block);\n    while (true) {\n      // see HDFS-6591, this metric is used to verify/catch unnecessary loops\n      hedgedReadOpsLoopNumForTesting++;\n      DNAddrPair chosenNode \u003d null;\n      // there is no request already executing.\n      if (futures.isEmpty()) {\n        // chooseDataNode is a commitment. If no node, we go to\n        // the NN to reget block locations. Only go here on first read.\n        chosenNode \u003d chooseDataNode(block, ignored);\n        bb \u003d ByteBuffer.wrap(buf, offset, len);\n        Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n            chosenNode, block, start, end, bb,\n            corruptedBlockMap, hedgedReadId++);\n        Future\u003cByteBuffer\u003e firstRequest \u003d hedgedService\n            .submit(getFromDataNodeCallable);\n        futures.add(firstRequest);\n        try {\n          Future\u003cByteBuffer\u003e future \u003d hedgedService.poll(\n              conf.getHedgedReadThresholdMillis(), TimeUnit.MILLISECONDS);\n          if (future !\u003d null) {\n            future.get();\n            return;\n          }\n          DFSClient.LOG.debug(\"Waited {}ms to read from {}; spawning hedged \"\n              + \"read\", conf.getHedgedReadThresholdMillis(), chosenNode.info);\n          // Ignore this node on next go around.\n          ignored.add(chosenNode.info);\n          dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n          continue; // no need to refresh block locations\n        } catch (InterruptedException e) {\n          // Ignore\n        } catch (ExecutionException e) {\n          // Ignore already logged in the call.\n        }\n      } else {\n        // We are starting up a \u0027hedged\u0027 read. We have a read already\n        // ongoing. Call getBestNodeDNAddrPair instead of chooseDataNode.\n        // If no nodes to do hedged reads against, pass.\n        try {\n          chosenNode \u003d getBestNodeDNAddrPair(block, ignored);\n          if (chosenNode \u003d\u003d null) {\n            chosenNode \u003d chooseDataNode(block, ignored);\n          }\n          bb \u003d ByteBuffer.allocate(len);\n          Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n              chosenNode, block, start, end, bb,\n              corruptedBlockMap, hedgedReadId++);\n          Future\u003cByteBuffer\u003e oneMoreRequest \u003d hedgedService\n              .submit(getFromDataNodeCallable);\n          futures.add(oneMoreRequest);\n        } catch (IOException ioe) {\n          DFSClient.LOG.debug(\"Failed getting node for hedged read: {}\",\n              ioe.getMessage());\n        }\n        // if not succeeded. Submit callables for each datanode in a loop, wait\n        // for a fixed interval and get the result from the fastest one.\n        try {\n          ByteBuffer result \u003d getFirstToComplete(hedgedService, futures);\n          // cancel the rest.\n          cancelAll(futures);\n          if (result.array() !\u003d buf) { // compare the array pointers\n            dfsClient.getHedgedReadMetrics().incHedgedReadWins();\n            System.arraycopy(result.array(), result.position(), buf, offset,\n                len);\n          } else {\n            dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n          }\n          return;\n        } catch (InterruptedException ie) {\n          // Ignore and retry\n        }\n        // We got here if exception. Ignore this node on next go around IFF\n        // we found a chosenNode to hedge read against.\n        if (chosenNode !\u003d null \u0026\u0026 chosenNode.info !\u003d null) {\n          ignored.add(chosenNode.info);\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "6ee0539ede78b640f01c5eac18ded161182a7835": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"HDFS-9170. Move libhdfs / fuse-dfs / libwebhdfs to hdfs-client. Contributed by Haohui Mai.\"\n\nThis reverts commit d5a9a3daa0224249221ffa7b8bd5751ab2feca56.\n",
      "commitDate": "29/09/15 5:51 PM",
      "commitName": "6ee0539ede78b640f01c5eac18ded161182a7835",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "29/09/15 5:48 PM",
      "commitNameOld": "d5a9a3daa0224249221ffa7b8bd5751ab2feca56",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,93 +1,98 @@\n   private void hedgedFetchBlockByteRange(LocatedBlock block, long start,\n       long end, byte[] buf, int offset,\n       Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n       throws IOException {\n     final DfsClientConf conf \u003d dfsClient.getConf();\n     ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e futures \u003d new ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e();\n     CompletionService\u003cByteBuffer\u003e hedgedService \u003d\n         new ExecutorCompletionService\u003cByteBuffer\u003e(\n         dfsClient.getHedgedReadsThreadPool());\n     ArrayList\u003cDatanodeInfo\u003e ignored \u003d new ArrayList\u003cDatanodeInfo\u003e();\n     ByteBuffer bb \u003d null;\n     int len \u003d (int) (end - start + 1);\n     int hedgedReadId \u003d 0;\n     block \u003d refreshLocatedBlock(block);\n     while (true) {\n       // see HDFS-6591, this metric is used to verify/catch unnecessary loops\n       hedgedReadOpsLoopNumForTesting++;\n       DNAddrPair chosenNode \u003d null;\n       // there is no request already executing.\n       if (futures.isEmpty()) {\n         // chooseDataNode is a commitment. If no node, we go to\n         // the NN to reget block locations. Only go here on first read.\n         chosenNode \u003d chooseDataNode(block, ignored);\n         bb \u003d ByteBuffer.wrap(buf, offset, len);\n         Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n             chosenNode, block, start, end, bb,\n             corruptedBlockMap, hedgedReadId++);\n         Future\u003cByteBuffer\u003e firstRequest \u003d hedgedService\n             .submit(getFromDataNodeCallable);\n         futures.add(firstRequest);\n         try {\n           Future\u003cByteBuffer\u003e future \u003d hedgedService.poll(\n               conf.getHedgedReadThresholdMillis(), TimeUnit.MILLISECONDS);\n           if (future !\u003d null) {\n             future.get();\n             return;\n           }\n-          DFSClient.LOG.debug(\"Waited {}ms to read from {}; spawning hedged \"\n-              + \"read\", conf.getHedgedReadThresholdMillis(), chosenNode.info);\n+          if (DFSClient.LOG.isDebugEnabled()) {\n+            DFSClient.LOG.debug(\"Waited \" + conf.getHedgedReadThresholdMillis()\n+                + \"ms to read from \" + chosenNode.info\n+                + \"; spawning hedged read\");\n+          }\n           // Ignore this node on next go around.\n           ignored.add(chosenNode.info);\n           dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n           continue; // no need to refresh block locations\n         } catch (InterruptedException e) {\n           // Ignore\n         } catch (ExecutionException e) {\n           // Ignore already logged in the call.\n         }\n       } else {\n         // We are starting up a \u0027hedged\u0027 read. We have a read already\n         // ongoing. Call getBestNodeDNAddrPair instead of chooseDataNode.\n         // If no nodes to do hedged reads against, pass.\n         try {\n           chosenNode \u003d getBestNodeDNAddrPair(block, ignored);\n           if (chosenNode \u003d\u003d null) {\n             chosenNode \u003d chooseDataNode(block, ignored);\n           }\n           bb \u003d ByteBuffer.allocate(len);\n           Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n               chosenNode, block, start, end, bb,\n               corruptedBlockMap, hedgedReadId++);\n           Future\u003cByteBuffer\u003e oneMoreRequest \u003d hedgedService\n               .submit(getFromDataNodeCallable);\n           futures.add(oneMoreRequest);\n         } catch (IOException ioe) {\n-          DFSClient.LOG.debug(\"Failed getting node for hedged read: {}\",\n-              ioe.getMessage());\n+          if (DFSClient.LOG.isDebugEnabled()) {\n+            DFSClient.LOG.debug(\"Failed getting node for hedged read: \"\n+                + ioe.getMessage());\n+          }\n         }\n         // if not succeeded. Submit callables for each datanode in a loop, wait\n         // for a fixed interval and get the result from the fastest one.\n         try {\n           ByteBuffer result \u003d getFirstToComplete(hedgedService, futures);\n           // cancel the rest.\n           cancelAll(futures);\n           if (result.array() !\u003d buf) { // compare the array pointers\n             dfsClient.getHedgedReadMetrics().incHedgedReadWins();\n             System.arraycopy(result.array(), result.position(), buf, offset,\n                 len);\n           } else {\n             dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n           }\n           return;\n         } catch (InterruptedException ie) {\n           // Ignore and retry\n         }\n         // We got here if exception. Ignore this node on next go around IFF\n         // we found a chosenNode to hedge read against.\n         if (chosenNode !\u003d null \u0026\u0026 chosenNode.info !\u003d null) {\n           ignored.add(chosenNode.info);\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void hedgedFetchBlockByteRange(LocatedBlock block, long start,\n      long end, byte[] buf, int offset,\n      Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n      throws IOException {\n    final DfsClientConf conf \u003d dfsClient.getConf();\n    ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e futures \u003d new ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e();\n    CompletionService\u003cByteBuffer\u003e hedgedService \u003d\n        new ExecutorCompletionService\u003cByteBuffer\u003e(\n        dfsClient.getHedgedReadsThreadPool());\n    ArrayList\u003cDatanodeInfo\u003e ignored \u003d new ArrayList\u003cDatanodeInfo\u003e();\n    ByteBuffer bb \u003d null;\n    int len \u003d (int) (end - start + 1);\n    int hedgedReadId \u003d 0;\n    block \u003d refreshLocatedBlock(block);\n    while (true) {\n      // see HDFS-6591, this metric is used to verify/catch unnecessary loops\n      hedgedReadOpsLoopNumForTesting++;\n      DNAddrPair chosenNode \u003d null;\n      // there is no request already executing.\n      if (futures.isEmpty()) {\n        // chooseDataNode is a commitment. If no node, we go to\n        // the NN to reget block locations. Only go here on first read.\n        chosenNode \u003d chooseDataNode(block, ignored);\n        bb \u003d ByteBuffer.wrap(buf, offset, len);\n        Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n            chosenNode, block, start, end, bb,\n            corruptedBlockMap, hedgedReadId++);\n        Future\u003cByteBuffer\u003e firstRequest \u003d hedgedService\n            .submit(getFromDataNodeCallable);\n        futures.add(firstRequest);\n        try {\n          Future\u003cByteBuffer\u003e future \u003d hedgedService.poll(\n              conf.getHedgedReadThresholdMillis(), TimeUnit.MILLISECONDS);\n          if (future !\u003d null) {\n            future.get();\n            return;\n          }\n          if (DFSClient.LOG.isDebugEnabled()) {\n            DFSClient.LOG.debug(\"Waited \" + conf.getHedgedReadThresholdMillis()\n                + \"ms to read from \" + chosenNode.info\n                + \"; spawning hedged read\");\n          }\n          // Ignore this node on next go around.\n          ignored.add(chosenNode.info);\n          dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n          continue; // no need to refresh block locations\n        } catch (InterruptedException e) {\n          // Ignore\n        } catch (ExecutionException e) {\n          // Ignore already logged in the call.\n        }\n      } else {\n        // We are starting up a \u0027hedged\u0027 read. We have a read already\n        // ongoing. Call getBestNodeDNAddrPair instead of chooseDataNode.\n        // If no nodes to do hedged reads against, pass.\n        try {\n          chosenNode \u003d getBestNodeDNAddrPair(block, ignored);\n          if (chosenNode \u003d\u003d null) {\n            chosenNode \u003d chooseDataNode(block, ignored);\n          }\n          bb \u003d ByteBuffer.allocate(len);\n          Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n              chosenNode, block, start, end, bb,\n              corruptedBlockMap, hedgedReadId++);\n          Future\u003cByteBuffer\u003e oneMoreRequest \u003d hedgedService\n              .submit(getFromDataNodeCallable);\n          futures.add(oneMoreRequest);\n        } catch (IOException ioe) {\n          if (DFSClient.LOG.isDebugEnabled()) {\n            DFSClient.LOG.debug(\"Failed getting node for hedged read: \"\n                + ioe.getMessage());\n          }\n        }\n        // if not succeeded. Submit callables for each datanode in a loop, wait\n        // for a fixed interval and get the result from the fastest one.\n        try {\n          ByteBuffer result \u003d getFirstToComplete(hedgedService, futures);\n          // cancel the rest.\n          cancelAll(futures);\n          if (result.array() !\u003d buf) { // compare the array pointers\n            dfsClient.getHedgedReadMetrics().incHedgedReadWins();\n            System.arraycopy(result.array(), result.position(), buf, offset,\n                len);\n          } else {\n            dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n          }\n          return;\n        } catch (InterruptedException ie) {\n          // Ignore and retry\n        }\n        // We got here if exception. Ignore this node on next go around IFF\n        // we found a chosenNode to hedge read against.\n        if (chosenNode !\u003d null \u0026\u0026 chosenNode.info !\u003d null) {\n          ignored.add(chosenNode.info);\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "d5a9a3daa0224249221ffa7b8bd5751ab2feca56": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9170. Move libhdfs / fuse-dfs / libwebhdfs to hdfs-client. Contributed by Haohui Mai.\n",
      "commitDate": "29/09/15 5:48 PM",
      "commitName": "d5a9a3daa0224249221ffa7b8bd5751ab2feca56",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "28/09/15 7:42 AM",
      "commitNameOld": "892ade689f9bcce76daae8f66fc00a49bee8548e",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 1.42,
      "commitsBetweenForRepo": 19,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,98 +1,93 @@\n   private void hedgedFetchBlockByteRange(LocatedBlock block, long start,\n       long end, byte[] buf, int offset,\n       Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n       throws IOException {\n     final DfsClientConf conf \u003d dfsClient.getConf();\n     ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e futures \u003d new ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e();\n     CompletionService\u003cByteBuffer\u003e hedgedService \u003d\n         new ExecutorCompletionService\u003cByteBuffer\u003e(\n         dfsClient.getHedgedReadsThreadPool());\n     ArrayList\u003cDatanodeInfo\u003e ignored \u003d new ArrayList\u003cDatanodeInfo\u003e();\n     ByteBuffer bb \u003d null;\n     int len \u003d (int) (end - start + 1);\n     int hedgedReadId \u003d 0;\n     block \u003d refreshLocatedBlock(block);\n     while (true) {\n       // see HDFS-6591, this metric is used to verify/catch unnecessary loops\n       hedgedReadOpsLoopNumForTesting++;\n       DNAddrPair chosenNode \u003d null;\n       // there is no request already executing.\n       if (futures.isEmpty()) {\n         // chooseDataNode is a commitment. If no node, we go to\n         // the NN to reget block locations. Only go here on first read.\n         chosenNode \u003d chooseDataNode(block, ignored);\n         bb \u003d ByteBuffer.wrap(buf, offset, len);\n         Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n             chosenNode, block, start, end, bb,\n             corruptedBlockMap, hedgedReadId++);\n         Future\u003cByteBuffer\u003e firstRequest \u003d hedgedService\n             .submit(getFromDataNodeCallable);\n         futures.add(firstRequest);\n         try {\n           Future\u003cByteBuffer\u003e future \u003d hedgedService.poll(\n               conf.getHedgedReadThresholdMillis(), TimeUnit.MILLISECONDS);\n           if (future !\u003d null) {\n             future.get();\n             return;\n           }\n-          if (DFSClient.LOG.isDebugEnabled()) {\n-            DFSClient.LOG.debug(\"Waited \" + conf.getHedgedReadThresholdMillis()\n-                + \"ms to read from \" + chosenNode.info\n-                + \"; spawning hedged read\");\n-          }\n+          DFSClient.LOG.debug(\"Waited {}ms to read from {}; spawning hedged \"\n+              + \"read\", conf.getHedgedReadThresholdMillis(), chosenNode.info);\n           // Ignore this node on next go around.\n           ignored.add(chosenNode.info);\n           dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n           continue; // no need to refresh block locations\n         } catch (InterruptedException e) {\n           // Ignore\n         } catch (ExecutionException e) {\n           // Ignore already logged in the call.\n         }\n       } else {\n         // We are starting up a \u0027hedged\u0027 read. We have a read already\n         // ongoing. Call getBestNodeDNAddrPair instead of chooseDataNode.\n         // If no nodes to do hedged reads against, pass.\n         try {\n           chosenNode \u003d getBestNodeDNAddrPair(block, ignored);\n           if (chosenNode \u003d\u003d null) {\n             chosenNode \u003d chooseDataNode(block, ignored);\n           }\n           bb \u003d ByteBuffer.allocate(len);\n           Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n               chosenNode, block, start, end, bb,\n               corruptedBlockMap, hedgedReadId++);\n           Future\u003cByteBuffer\u003e oneMoreRequest \u003d hedgedService\n               .submit(getFromDataNodeCallable);\n           futures.add(oneMoreRequest);\n         } catch (IOException ioe) {\n-          if (DFSClient.LOG.isDebugEnabled()) {\n-            DFSClient.LOG.debug(\"Failed getting node for hedged read: \"\n-                + ioe.getMessage());\n-          }\n+          DFSClient.LOG.debug(\"Failed getting node for hedged read: {}\",\n+              ioe.getMessage());\n         }\n         // if not succeeded. Submit callables for each datanode in a loop, wait\n         // for a fixed interval and get the result from the fastest one.\n         try {\n           ByteBuffer result \u003d getFirstToComplete(hedgedService, futures);\n           // cancel the rest.\n           cancelAll(futures);\n           if (result.array() !\u003d buf) { // compare the array pointers\n             dfsClient.getHedgedReadMetrics().incHedgedReadWins();\n             System.arraycopy(result.array(), result.position(), buf, offset,\n                 len);\n           } else {\n             dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n           }\n           return;\n         } catch (InterruptedException ie) {\n           // Ignore and retry\n         }\n         // We got here if exception. Ignore this node on next go around IFF\n         // we found a chosenNode to hedge read against.\n         if (chosenNode !\u003d null \u0026\u0026 chosenNode.info !\u003d null) {\n           ignored.add(chosenNode.info);\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void hedgedFetchBlockByteRange(LocatedBlock block, long start,\n      long end, byte[] buf, int offset,\n      Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n      throws IOException {\n    final DfsClientConf conf \u003d dfsClient.getConf();\n    ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e futures \u003d new ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e();\n    CompletionService\u003cByteBuffer\u003e hedgedService \u003d\n        new ExecutorCompletionService\u003cByteBuffer\u003e(\n        dfsClient.getHedgedReadsThreadPool());\n    ArrayList\u003cDatanodeInfo\u003e ignored \u003d new ArrayList\u003cDatanodeInfo\u003e();\n    ByteBuffer bb \u003d null;\n    int len \u003d (int) (end - start + 1);\n    int hedgedReadId \u003d 0;\n    block \u003d refreshLocatedBlock(block);\n    while (true) {\n      // see HDFS-6591, this metric is used to verify/catch unnecessary loops\n      hedgedReadOpsLoopNumForTesting++;\n      DNAddrPair chosenNode \u003d null;\n      // there is no request already executing.\n      if (futures.isEmpty()) {\n        // chooseDataNode is a commitment. If no node, we go to\n        // the NN to reget block locations. Only go here on first read.\n        chosenNode \u003d chooseDataNode(block, ignored);\n        bb \u003d ByteBuffer.wrap(buf, offset, len);\n        Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n            chosenNode, block, start, end, bb,\n            corruptedBlockMap, hedgedReadId++);\n        Future\u003cByteBuffer\u003e firstRequest \u003d hedgedService\n            .submit(getFromDataNodeCallable);\n        futures.add(firstRequest);\n        try {\n          Future\u003cByteBuffer\u003e future \u003d hedgedService.poll(\n              conf.getHedgedReadThresholdMillis(), TimeUnit.MILLISECONDS);\n          if (future !\u003d null) {\n            future.get();\n            return;\n          }\n          DFSClient.LOG.debug(\"Waited {}ms to read from {}; spawning hedged \"\n              + \"read\", conf.getHedgedReadThresholdMillis(), chosenNode.info);\n          // Ignore this node on next go around.\n          ignored.add(chosenNode.info);\n          dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n          continue; // no need to refresh block locations\n        } catch (InterruptedException e) {\n          // Ignore\n        } catch (ExecutionException e) {\n          // Ignore already logged in the call.\n        }\n      } else {\n        // We are starting up a \u0027hedged\u0027 read. We have a read already\n        // ongoing. Call getBestNodeDNAddrPair instead of chooseDataNode.\n        // If no nodes to do hedged reads against, pass.\n        try {\n          chosenNode \u003d getBestNodeDNAddrPair(block, ignored);\n          if (chosenNode \u003d\u003d null) {\n            chosenNode \u003d chooseDataNode(block, ignored);\n          }\n          bb \u003d ByteBuffer.allocate(len);\n          Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n              chosenNode, block, start, end, bb,\n              corruptedBlockMap, hedgedReadId++);\n          Future\u003cByteBuffer\u003e oneMoreRequest \u003d hedgedService\n              .submit(getFromDataNodeCallable);\n          futures.add(oneMoreRequest);\n        } catch (IOException ioe) {\n          DFSClient.LOG.debug(\"Failed getting node for hedged read: {}\",\n              ioe.getMessage());\n        }\n        // if not succeeded. Submit callables for each datanode in a loop, wait\n        // for a fixed interval and get the result from the fastest one.\n        try {\n          ByteBuffer result \u003d getFirstToComplete(hedgedService, futures);\n          // cancel the rest.\n          cancelAll(futures);\n          if (result.array() !\u003d buf) { // compare the array pointers\n            dfsClient.getHedgedReadMetrics().incHedgedReadWins();\n            System.arraycopy(result.array(), result.position(), buf, offset,\n                len);\n          } else {\n            dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n          }\n          return;\n        } catch (InterruptedException ie) {\n          // Ignore and retry\n        }\n        // We got here if exception. Ignore this node on next go around IFF\n        // we found a chosenNode to hedge read against.\n        if (chosenNode !\u003d null \u0026\u0026 chosenNode.info !\u003d null) {\n          ignored.add(chosenNode.info);\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8053. Move DFSIn/OutputStream and related classes to hadoop-hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "26/09/15 11:08 AM",
      "commitName": "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "26/09/15 9:06 AM",
      "commitNameOld": "861b52db242f238d7e36ad75c158025be959a696",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 0.08,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private void hedgedFetchBlockByteRange(LocatedBlock block, long start,\n      long end, byte[] buf, int offset,\n      Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n      throws IOException {\n    final DfsClientConf conf \u003d dfsClient.getConf();\n    ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e futures \u003d new ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e();\n    CompletionService\u003cByteBuffer\u003e hedgedService \u003d\n        new ExecutorCompletionService\u003cByteBuffer\u003e(\n        dfsClient.getHedgedReadsThreadPool());\n    ArrayList\u003cDatanodeInfo\u003e ignored \u003d new ArrayList\u003cDatanodeInfo\u003e();\n    ByteBuffer bb \u003d null;\n    int len \u003d (int) (end - start + 1);\n    int hedgedReadId \u003d 0;\n    block \u003d refreshLocatedBlock(block);\n    while (true) {\n      // see HDFS-6591, this metric is used to verify/catch unnecessary loops\n      hedgedReadOpsLoopNumForTesting++;\n      DNAddrPair chosenNode \u003d null;\n      // there is no request already executing.\n      if (futures.isEmpty()) {\n        // chooseDataNode is a commitment. If no node, we go to\n        // the NN to reget block locations. Only go here on first read.\n        chosenNode \u003d chooseDataNode(block, ignored);\n        bb \u003d ByteBuffer.wrap(buf, offset, len);\n        Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n            chosenNode, block, start, end, bb,\n            corruptedBlockMap, hedgedReadId++);\n        Future\u003cByteBuffer\u003e firstRequest \u003d hedgedService\n            .submit(getFromDataNodeCallable);\n        futures.add(firstRequest);\n        try {\n          Future\u003cByteBuffer\u003e future \u003d hedgedService.poll(\n              conf.getHedgedReadThresholdMillis(), TimeUnit.MILLISECONDS);\n          if (future !\u003d null) {\n            future.get();\n            return;\n          }\n          if (DFSClient.LOG.isDebugEnabled()) {\n            DFSClient.LOG.debug(\"Waited \" + conf.getHedgedReadThresholdMillis()\n                + \"ms to read from \" + chosenNode.info\n                + \"; spawning hedged read\");\n          }\n          // Ignore this node on next go around.\n          ignored.add(chosenNode.info);\n          dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n          continue; // no need to refresh block locations\n        } catch (InterruptedException e) {\n          // Ignore\n        } catch (ExecutionException e) {\n          // Ignore already logged in the call.\n        }\n      } else {\n        // We are starting up a \u0027hedged\u0027 read. We have a read already\n        // ongoing. Call getBestNodeDNAddrPair instead of chooseDataNode.\n        // If no nodes to do hedged reads against, pass.\n        try {\n          chosenNode \u003d getBestNodeDNAddrPair(block, ignored);\n          if (chosenNode \u003d\u003d null) {\n            chosenNode \u003d chooseDataNode(block, ignored);\n          }\n          bb \u003d ByteBuffer.allocate(len);\n          Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n              chosenNode, block, start, end, bb,\n              corruptedBlockMap, hedgedReadId++);\n          Future\u003cByteBuffer\u003e oneMoreRequest \u003d hedgedService\n              .submit(getFromDataNodeCallable);\n          futures.add(oneMoreRequest);\n        } catch (IOException ioe) {\n          if (DFSClient.LOG.isDebugEnabled()) {\n            DFSClient.LOG.debug(\"Failed getting node for hedged read: \"\n                + ioe.getMessage());\n          }\n        }\n        // if not succeeded. Submit callables for each datanode in a loop, wait\n        // for a fixed interval and get the result from the fastest one.\n        try {\n          ByteBuffer result \u003d getFirstToComplete(hedgedService, futures);\n          // cancel the rest.\n          cancelAll(futures);\n          if (result.array() !\u003d buf) { // compare the array pointers\n            dfsClient.getHedgedReadMetrics().incHedgedReadWins();\n            System.arraycopy(result.array(), result.position(), buf, offset,\n                len);\n          } else {\n            dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n          }\n          return;\n        } catch (InterruptedException ie) {\n          // Ignore and retry\n        }\n        // We got here if exception. Ignore this node on next go around IFF\n        // we found a chosenNode to hedge read against.\n        if (chosenNode !\u003d null \u0026\u0026 chosenNode.info !\u003d null) {\n          ignored.add(chosenNode.info);\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java"
      }
    },
    "bff5999d07e9416a22846c849487e509ede55040": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-8703. Merge refactor of DFSInputStream from ErasureCoding branch (Contributed by Vinayakumar B)\n",
      "commitDate": "02/07/15 3:41 AM",
      "commitName": "bff5999d07e9416a22846c849487e509ede55040",
      "commitAuthor": "Vinayakumar B",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-8703. Merge refactor of DFSInputStream from ErasureCoding branch (Contributed by Vinayakumar B)\n",
          "commitDate": "02/07/15 3:41 AM",
          "commitName": "bff5999d07e9416a22846c849487e509ede55040",
          "commitAuthor": "Vinayakumar B",
          "commitDateOld": "04/06/15 10:51 AM",
          "commitNameOld": "ade6d9a61eb2e57a975f0efcdf8828d51ffec5fd",
          "commitAuthorOld": "Kihwal Lee",
          "daysBetweenCommits": 27.7,
          "commitsBetweenForRepo": 196,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,98 +1,98 @@\n-  private void hedgedFetchBlockByteRange(long blockStartOffset, long start,\n+  private void hedgedFetchBlockByteRange(LocatedBlock block, long start,\n       long end, byte[] buf, int offset,\n       Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n       throws IOException {\n     final DfsClientConf conf \u003d dfsClient.getConf();\n     ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e futures \u003d new ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e();\n     CompletionService\u003cByteBuffer\u003e hedgedService \u003d\n         new ExecutorCompletionService\u003cByteBuffer\u003e(\n         dfsClient.getHedgedReadsThreadPool());\n     ArrayList\u003cDatanodeInfo\u003e ignored \u003d new ArrayList\u003cDatanodeInfo\u003e();\n     ByteBuffer bb \u003d null;\n     int len \u003d (int) (end - start + 1);\n     int hedgedReadId \u003d 0;\n-    LocatedBlock block \u003d getBlockAt(blockStartOffset);\n+    block \u003d refreshLocatedBlock(block);\n     while (true) {\n       // see HDFS-6591, this metric is used to verify/catch unnecessary loops\n       hedgedReadOpsLoopNumForTesting++;\n       DNAddrPair chosenNode \u003d null;\n       // there is no request already executing.\n       if (futures.isEmpty()) {\n         // chooseDataNode is a commitment. If no node, we go to\n         // the NN to reget block locations. Only go here on first read.\n         chosenNode \u003d chooseDataNode(block, ignored);\n         bb \u003d ByteBuffer.wrap(buf, offset, len);\n         Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n-            chosenNode, block.getStartOffset(), start, end, bb,\n+            chosenNode, block, start, end, bb,\n             corruptedBlockMap, hedgedReadId++);\n         Future\u003cByteBuffer\u003e firstRequest \u003d hedgedService\n             .submit(getFromDataNodeCallable);\n         futures.add(firstRequest);\n         try {\n           Future\u003cByteBuffer\u003e future \u003d hedgedService.poll(\n               conf.getHedgedReadThresholdMillis(), TimeUnit.MILLISECONDS);\n           if (future !\u003d null) {\n             future.get();\n             return;\n           }\n           if (DFSClient.LOG.isDebugEnabled()) {\n             DFSClient.LOG.debug(\"Waited \" + conf.getHedgedReadThresholdMillis()\n                 + \"ms to read from \" + chosenNode.info\n                 + \"; spawning hedged read\");\n           }\n           // Ignore this node on next go around.\n           ignored.add(chosenNode.info);\n           dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n           continue; // no need to refresh block locations\n         } catch (InterruptedException e) {\n           // Ignore\n         } catch (ExecutionException e) {\n           // Ignore already logged in the call.\n         }\n       } else {\n         // We are starting up a \u0027hedged\u0027 read. We have a read already\n         // ongoing. Call getBestNodeDNAddrPair instead of chooseDataNode.\n         // If no nodes to do hedged reads against, pass.\n         try {\n           chosenNode \u003d getBestNodeDNAddrPair(block, ignored);\n           if (chosenNode \u003d\u003d null) {\n             chosenNode \u003d chooseDataNode(block, ignored);\n           }\n           bb \u003d ByteBuffer.allocate(len);\n           Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n-              chosenNode, block.getStartOffset(), start, end, bb,\n+              chosenNode, block, start, end, bb,\n               corruptedBlockMap, hedgedReadId++);\n           Future\u003cByteBuffer\u003e oneMoreRequest \u003d hedgedService\n               .submit(getFromDataNodeCallable);\n           futures.add(oneMoreRequest);\n         } catch (IOException ioe) {\n           if (DFSClient.LOG.isDebugEnabled()) {\n             DFSClient.LOG.debug(\"Failed getting node for hedged read: \"\n                 + ioe.getMessage());\n           }\n         }\n         // if not succeeded. Submit callables for each datanode in a loop, wait\n         // for a fixed interval and get the result from the fastest one.\n         try {\n           ByteBuffer result \u003d getFirstToComplete(hedgedService, futures);\n           // cancel the rest.\n           cancelAll(futures);\n           if (result.array() !\u003d buf) { // compare the array pointers\n             dfsClient.getHedgedReadMetrics().incHedgedReadWins();\n             System.arraycopy(result.array(), result.position(), buf, offset,\n                 len);\n           } else {\n             dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n           }\n           return;\n         } catch (InterruptedException ie) {\n           // Ignore and retry\n         }\n         // We got here if exception. Ignore this node on next go around IFF\n         // we found a chosenNode to hedge read against.\n         if (chosenNode !\u003d null \u0026\u0026 chosenNode.info !\u003d null) {\n           ignored.add(chosenNode.info);\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void hedgedFetchBlockByteRange(LocatedBlock block, long start,\n      long end, byte[] buf, int offset,\n      Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n      throws IOException {\n    final DfsClientConf conf \u003d dfsClient.getConf();\n    ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e futures \u003d new ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e();\n    CompletionService\u003cByteBuffer\u003e hedgedService \u003d\n        new ExecutorCompletionService\u003cByteBuffer\u003e(\n        dfsClient.getHedgedReadsThreadPool());\n    ArrayList\u003cDatanodeInfo\u003e ignored \u003d new ArrayList\u003cDatanodeInfo\u003e();\n    ByteBuffer bb \u003d null;\n    int len \u003d (int) (end - start + 1);\n    int hedgedReadId \u003d 0;\n    block \u003d refreshLocatedBlock(block);\n    while (true) {\n      // see HDFS-6591, this metric is used to verify/catch unnecessary loops\n      hedgedReadOpsLoopNumForTesting++;\n      DNAddrPair chosenNode \u003d null;\n      // there is no request already executing.\n      if (futures.isEmpty()) {\n        // chooseDataNode is a commitment. If no node, we go to\n        // the NN to reget block locations. Only go here on first read.\n        chosenNode \u003d chooseDataNode(block, ignored);\n        bb \u003d ByteBuffer.wrap(buf, offset, len);\n        Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n            chosenNode, block, start, end, bb,\n            corruptedBlockMap, hedgedReadId++);\n        Future\u003cByteBuffer\u003e firstRequest \u003d hedgedService\n            .submit(getFromDataNodeCallable);\n        futures.add(firstRequest);\n        try {\n          Future\u003cByteBuffer\u003e future \u003d hedgedService.poll(\n              conf.getHedgedReadThresholdMillis(), TimeUnit.MILLISECONDS);\n          if (future !\u003d null) {\n            future.get();\n            return;\n          }\n          if (DFSClient.LOG.isDebugEnabled()) {\n            DFSClient.LOG.debug(\"Waited \" + conf.getHedgedReadThresholdMillis()\n                + \"ms to read from \" + chosenNode.info\n                + \"; spawning hedged read\");\n          }\n          // Ignore this node on next go around.\n          ignored.add(chosenNode.info);\n          dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n          continue; // no need to refresh block locations\n        } catch (InterruptedException e) {\n          // Ignore\n        } catch (ExecutionException e) {\n          // Ignore already logged in the call.\n        }\n      } else {\n        // We are starting up a \u0027hedged\u0027 read. We have a read already\n        // ongoing. Call getBestNodeDNAddrPair instead of chooseDataNode.\n        // If no nodes to do hedged reads against, pass.\n        try {\n          chosenNode \u003d getBestNodeDNAddrPair(block, ignored);\n          if (chosenNode \u003d\u003d null) {\n            chosenNode \u003d chooseDataNode(block, ignored);\n          }\n          bb \u003d ByteBuffer.allocate(len);\n          Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n              chosenNode, block, start, end, bb,\n              corruptedBlockMap, hedgedReadId++);\n          Future\u003cByteBuffer\u003e oneMoreRequest \u003d hedgedService\n              .submit(getFromDataNodeCallable);\n          futures.add(oneMoreRequest);\n        } catch (IOException ioe) {\n          if (DFSClient.LOG.isDebugEnabled()) {\n            DFSClient.LOG.debug(\"Failed getting node for hedged read: \"\n                + ioe.getMessage());\n          }\n        }\n        // if not succeeded. Submit callables for each datanode in a loop, wait\n        // for a fixed interval and get the result from the fastest one.\n        try {\n          ByteBuffer result \u003d getFirstToComplete(hedgedService, futures);\n          // cancel the rest.\n          cancelAll(futures);\n          if (result.array() !\u003d buf) { // compare the array pointers\n            dfsClient.getHedgedReadMetrics().incHedgedReadWins();\n            System.arraycopy(result.array(), result.position(), buf, offset,\n                len);\n          } else {\n            dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n          }\n          return;\n        } catch (InterruptedException ie) {\n          // Ignore and retry\n        }\n        // We got here if exception. Ignore this node on next go around IFF\n        // we found a chosenNode to hedge read against.\n        if (chosenNode !\u003d null \u0026\u0026 chosenNode.info !\u003d null) {\n          ignored.add(chosenNode.info);\n        }\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
          "extendedDetails": {
            "oldValue": "[blockStartOffset-long, start-long, end-long, buf-byte[], offset-int, corruptedBlockMap-Map\u003cExtendedBlock,Set\u003cDatanodeInfo\u003e\u003e]",
            "newValue": "[block-LocatedBlock, start-long, end-long, buf-byte[], offset-int, corruptedBlockMap-Map\u003cExtendedBlock,Set\u003cDatanodeInfo\u003e\u003e]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-8703. Merge refactor of DFSInputStream from ErasureCoding branch (Contributed by Vinayakumar B)\n",
          "commitDate": "02/07/15 3:41 AM",
          "commitName": "bff5999d07e9416a22846c849487e509ede55040",
          "commitAuthor": "Vinayakumar B",
          "commitDateOld": "04/06/15 10:51 AM",
          "commitNameOld": "ade6d9a61eb2e57a975f0efcdf8828d51ffec5fd",
          "commitAuthorOld": "Kihwal Lee",
          "daysBetweenCommits": 27.7,
          "commitsBetweenForRepo": 196,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,98 +1,98 @@\n-  private void hedgedFetchBlockByteRange(long blockStartOffset, long start,\n+  private void hedgedFetchBlockByteRange(LocatedBlock block, long start,\n       long end, byte[] buf, int offset,\n       Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n       throws IOException {\n     final DfsClientConf conf \u003d dfsClient.getConf();\n     ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e futures \u003d new ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e();\n     CompletionService\u003cByteBuffer\u003e hedgedService \u003d\n         new ExecutorCompletionService\u003cByteBuffer\u003e(\n         dfsClient.getHedgedReadsThreadPool());\n     ArrayList\u003cDatanodeInfo\u003e ignored \u003d new ArrayList\u003cDatanodeInfo\u003e();\n     ByteBuffer bb \u003d null;\n     int len \u003d (int) (end - start + 1);\n     int hedgedReadId \u003d 0;\n-    LocatedBlock block \u003d getBlockAt(blockStartOffset);\n+    block \u003d refreshLocatedBlock(block);\n     while (true) {\n       // see HDFS-6591, this metric is used to verify/catch unnecessary loops\n       hedgedReadOpsLoopNumForTesting++;\n       DNAddrPair chosenNode \u003d null;\n       // there is no request already executing.\n       if (futures.isEmpty()) {\n         // chooseDataNode is a commitment. If no node, we go to\n         // the NN to reget block locations. Only go here on first read.\n         chosenNode \u003d chooseDataNode(block, ignored);\n         bb \u003d ByteBuffer.wrap(buf, offset, len);\n         Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n-            chosenNode, block.getStartOffset(), start, end, bb,\n+            chosenNode, block, start, end, bb,\n             corruptedBlockMap, hedgedReadId++);\n         Future\u003cByteBuffer\u003e firstRequest \u003d hedgedService\n             .submit(getFromDataNodeCallable);\n         futures.add(firstRequest);\n         try {\n           Future\u003cByteBuffer\u003e future \u003d hedgedService.poll(\n               conf.getHedgedReadThresholdMillis(), TimeUnit.MILLISECONDS);\n           if (future !\u003d null) {\n             future.get();\n             return;\n           }\n           if (DFSClient.LOG.isDebugEnabled()) {\n             DFSClient.LOG.debug(\"Waited \" + conf.getHedgedReadThresholdMillis()\n                 + \"ms to read from \" + chosenNode.info\n                 + \"; spawning hedged read\");\n           }\n           // Ignore this node on next go around.\n           ignored.add(chosenNode.info);\n           dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n           continue; // no need to refresh block locations\n         } catch (InterruptedException e) {\n           // Ignore\n         } catch (ExecutionException e) {\n           // Ignore already logged in the call.\n         }\n       } else {\n         // We are starting up a \u0027hedged\u0027 read. We have a read already\n         // ongoing. Call getBestNodeDNAddrPair instead of chooseDataNode.\n         // If no nodes to do hedged reads against, pass.\n         try {\n           chosenNode \u003d getBestNodeDNAddrPair(block, ignored);\n           if (chosenNode \u003d\u003d null) {\n             chosenNode \u003d chooseDataNode(block, ignored);\n           }\n           bb \u003d ByteBuffer.allocate(len);\n           Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n-              chosenNode, block.getStartOffset(), start, end, bb,\n+              chosenNode, block, start, end, bb,\n               corruptedBlockMap, hedgedReadId++);\n           Future\u003cByteBuffer\u003e oneMoreRequest \u003d hedgedService\n               .submit(getFromDataNodeCallable);\n           futures.add(oneMoreRequest);\n         } catch (IOException ioe) {\n           if (DFSClient.LOG.isDebugEnabled()) {\n             DFSClient.LOG.debug(\"Failed getting node for hedged read: \"\n                 + ioe.getMessage());\n           }\n         }\n         // if not succeeded. Submit callables for each datanode in a loop, wait\n         // for a fixed interval and get the result from the fastest one.\n         try {\n           ByteBuffer result \u003d getFirstToComplete(hedgedService, futures);\n           // cancel the rest.\n           cancelAll(futures);\n           if (result.array() !\u003d buf) { // compare the array pointers\n             dfsClient.getHedgedReadMetrics().incHedgedReadWins();\n             System.arraycopy(result.array(), result.position(), buf, offset,\n                 len);\n           } else {\n             dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n           }\n           return;\n         } catch (InterruptedException ie) {\n           // Ignore and retry\n         }\n         // We got here if exception. Ignore this node on next go around IFF\n         // we found a chosenNode to hedge read against.\n         if (chosenNode !\u003d null \u0026\u0026 chosenNode.info !\u003d null) {\n           ignored.add(chosenNode.info);\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void hedgedFetchBlockByteRange(LocatedBlock block, long start,\n      long end, byte[] buf, int offset,\n      Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n      throws IOException {\n    final DfsClientConf conf \u003d dfsClient.getConf();\n    ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e futures \u003d new ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e();\n    CompletionService\u003cByteBuffer\u003e hedgedService \u003d\n        new ExecutorCompletionService\u003cByteBuffer\u003e(\n        dfsClient.getHedgedReadsThreadPool());\n    ArrayList\u003cDatanodeInfo\u003e ignored \u003d new ArrayList\u003cDatanodeInfo\u003e();\n    ByteBuffer bb \u003d null;\n    int len \u003d (int) (end - start + 1);\n    int hedgedReadId \u003d 0;\n    block \u003d refreshLocatedBlock(block);\n    while (true) {\n      // see HDFS-6591, this metric is used to verify/catch unnecessary loops\n      hedgedReadOpsLoopNumForTesting++;\n      DNAddrPair chosenNode \u003d null;\n      // there is no request already executing.\n      if (futures.isEmpty()) {\n        // chooseDataNode is a commitment. If no node, we go to\n        // the NN to reget block locations. Only go here on first read.\n        chosenNode \u003d chooseDataNode(block, ignored);\n        bb \u003d ByteBuffer.wrap(buf, offset, len);\n        Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n            chosenNode, block, start, end, bb,\n            corruptedBlockMap, hedgedReadId++);\n        Future\u003cByteBuffer\u003e firstRequest \u003d hedgedService\n            .submit(getFromDataNodeCallable);\n        futures.add(firstRequest);\n        try {\n          Future\u003cByteBuffer\u003e future \u003d hedgedService.poll(\n              conf.getHedgedReadThresholdMillis(), TimeUnit.MILLISECONDS);\n          if (future !\u003d null) {\n            future.get();\n            return;\n          }\n          if (DFSClient.LOG.isDebugEnabled()) {\n            DFSClient.LOG.debug(\"Waited \" + conf.getHedgedReadThresholdMillis()\n                + \"ms to read from \" + chosenNode.info\n                + \"; spawning hedged read\");\n          }\n          // Ignore this node on next go around.\n          ignored.add(chosenNode.info);\n          dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n          continue; // no need to refresh block locations\n        } catch (InterruptedException e) {\n          // Ignore\n        } catch (ExecutionException e) {\n          // Ignore already logged in the call.\n        }\n      } else {\n        // We are starting up a \u0027hedged\u0027 read. We have a read already\n        // ongoing. Call getBestNodeDNAddrPair instead of chooseDataNode.\n        // If no nodes to do hedged reads against, pass.\n        try {\n          chosenNode \u003d getBestNodeDNAddrPair(block, ignored);\n          if (chosenNode \u003d\u003d null) {\n            chosenNode \u003d chooseDataNode(block, ignored);\n          }\n          bb \u003d ByteBuffer.allocate(len);\n          Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n              chosenNode, block, start, end, bb,\n              corruptedBlockMap, hedgedReadId++);\n          Future\u003cByteBuffer\u003e oneMoreRequest \u003d hedgedService\n              .submit(getFromDataNodeCallable);\n          futures.add(oneMoreRequest);\n        } catch (IOException ioe) {\n          if (DFSClient.LOG.isDebugEnabled()) {\n            DFSClient.LOG.debug(\"Failed getting node for hedged read: \"\n                + ioe.getMessage());\n          }\n        }\n        // if not succeeded. Submit callables for each datanode in a loop, wait\n        // for a fixed interval and get the result from the fastest one.\n        try {\n          ByteBuffer result \u003d getFirstToComplete(hedgedService, futures);\n          // cancel the rest.\n          cancelAll(futures);\n          if (result.array() !\u003d buf) { // compare the array pointers\n            dfsClient.getHedgedReadMetrics().incHedgedReadWins();\n            System.arraycopy(result.array(), result.position(), buf, offset,\n                len);\n          } else {\n            dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n          }\n          return;\n        } catch (InterruptedException ie) {\n          // Ignore and retry\n        }\n        // We got here if exception. Ignore this node on next go around IFF\n        // we found a chosenNode to hedge read against.\n        if (chosenNode !\u003d null \u0026\u0026 chosenNode.info !\u003d null) {\n          ignored.add(chosenNode.info);\n        }\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
          "extendedDetails": {}
        }
      ]
    },
    "439614b0c8a3df3d8b7967451c5331a0e034e13a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8280. Code Cleanup in DFSInputStream. Contributed by Jing Zhao.\n",
      "commitDate": "28/04/15 6:11 PM",
      "commitName": "439614b0c8a3df3d8b7967451c5331a0e034e13a",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "16/04/15 1:22 PM",
      "commitNameOld": "75bbcc8bf3fa1daf54f56868dae737f6da12ab1f",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 12.2,
      "commitsBetweenForRepo": 109,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,99 +1,98 @@\n   private void hedgedFetchBlockByteRange(long blockStartOffset, long start,\n       long end, byte[] buf, int offset,\n       Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n       throws IOException {\n     final DfsClientConf conf \u003d dfsClient.getConf();\n     ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e futures \u003d new ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e();\n     CompletionService\u003cByteBuffer\u003e hedgedService \u003d\n         new ExecutorCompletionService\u003cByteBuffer\u003e(\n         dfsClient.getHedgedReadsThreadPool());\n     ArrayList\u003cDatanodeInfo\u003e ignored \u003d new ArrayList\u003cDatanodeInfo\u003e();\n     ByteBuffer bb \u003d null;\n     int len \u003d (int) (end - start + 1);\n     int hedgedReadId \u003d 0;\n     LocatedBlock block \u003d getBlockAt(blockStartOffset);\n     while (true) {\n       // see HDFS-6591, this metric is used to verify/catch unnecessary loops\n       hedgedReadOpsLoopNumForTesting++;\n       DNAddrPair chosenNode \u003d null;\n       // there is no request already executing.\n       if (futures.isEmpty()) {\n         // chooseDataNode is a commitment. If no node, we go to\n         // the NN to reget block locations. Only go here on first read.\n         chosenNode \u003d chooseDataNode(block, ignored);\n         bb \u003d ByteBuffer.wrap(buf, offset, len);\n         Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n             chosenNode, block.getStartOffset(), start, end, bb,\n             corruptedBlockMap, hedgedReadId++);\n         Future\u003cByteBuffer\u003e firstRequest \u003d hedgedService\n             .submit(getFromDataNodeCallable);\n         futures.add(firstRequest);\n         try {\n           Future\u003cByteBuffer\u003e future \u003d hedgedService.poll(\n               conf.getHedgedReadThresholdMillis(), TimeUnit.MILLISECONDS);\n           if (future !\u003d null) {\n             future.get();\n             return;\n           }\n           if (DFSClient.LOG.isDebugEnabled()) {\n             DFSClient.LOG.debug(\"Waited \" + conf.getHedgedReadThresholdMillis()\n                 + \"ms to read from \" + chosenNode.info\n                 + \"; spawning hedged read\");\n           }\n           // Ignore this node on next go around.\n           ignored.add(chosenNode.info);\n           dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n           continue; // no need to refresh block locations\n         } catch (InterruptedException e) {\n           // Ignore\n         } catch (ExecutionException e) {\n           // Ignore already logged in the call.\n         }\n       } else {\n         // We are starting up a \u0027hedged\u0027 read. We have a read already\n         // ongoing. Call getBestNodeDNAddrPair instead of chooseDataNode.\n         // If no nodes to do hedged reads against, pass.\n         try {\n-          try {\n-            chosenNode \u003d getBestNodeDNAddrPair(block, ignored);\n-          } catch (IOException ioe) {\n+          chosenNode \u003d getBestNodeDNAddrPair(block, ignored);\n+          if (chosenNode \u003d\u003d null) {\n             chosenNode \u003d chooseDataNode(block, ignored);\n           }\n           bb \u003d ByteBuffer.allocate(len);\n           Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n               chosenNode, block.getStartOffset(), start, end, bb,\n               corruptedBlockMap, hedgedReadId++);\n           Future\u003cByteBuffer\u003e oneMoreRequest \u003d hedgedService\n               .submit(getFromDataNodeCallable);\n           futures.add(oneMoreRequest);\n         } catch (IOException ioe) {\n           if (DFSClient.LOG.isDebugEnabled()) {\n             DFSClient.LOG.debug(\"Failed getting node for hedged read: \"\n                 + ioe.getMessage());\n           }\n         }\n         // if not succeeded. Submit callables for each datanode in a loop, wait\n         // for a fixed interval and get the result from the fastest one.\n         try {\n           ByteBuffer result \u003d getFirstToComplete(hedgedService, futures);\n           // cancel the rest.\n           cancelAll(futures);\n           if (result.array() !\u003d buf) { // compare the array pointers\n             dfsClient.getHedgedReadMetrics().incHedgedReadWins();\n             System.arraycopy(result.array(), result.position(), buf, offset,\n                 len);\n           } else {\n             dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n           }\n           return;\n         } catch (InterruptedException ie) {\n           // Ignore and retry\n         }\n         // We got here if exception. Ignore this node on next go around IFF\n         // we found a chosenNode to hedge read against.\n         if (chosenNode !\u003d null \u0026\u0026 chosenNode.info !\u003d null) {\n           ignored.add(chosenNode.info);\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void hedgedFetchBlockByteRange(long blockStartOffset, long start,\n      long end, byte[] buf, int offset,\n      Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n      throws IOException {\n    final DfsClientConf conf \u003d dfsClient.getConf();\n    ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e futures \u003d new ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e();\n    CompletionService\u003cByteBuffer\u003e hedgedService \u003d\n        new ExecutorCompletionService\u003cByteBuffer\u003e(\n        dfsClient.getHedgedReadsThreadPool());\n    ArrayList\u003cDatanodeInfo\u003e ignored \u003d new ArrayList\u003cDatanodeInfo\u003e();\n    ByteBuffer bb \u003d null;\n    int len \u003d (int) (end - start + 1);\n    int hedgedReadId \u003d 0;\n    LocatedBlock block \u003d getBlockAt(blockStartOffset);\n    while (true) {\n      // see HDFS-6591, this metric is used to verify/catch unnecessary loops\n      hedgedReadOpsLoopNumForTesting++;\n      DNAddrPair chosenNode \u003d null;\n      // there is no request already executing.\n      if (futures.isEmpty()) {\n        // chooseDataNode is a commitment. If no node, we go to\n        // the NN to reget block locations. Only go here on first read.\n        chosenNode \u003d chooseDataNode(block, ignored);\n        bb \u003d ByteBuffer.wrap(buf, offset, len);\n        Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n            chosenNode, block.getStartOffset(), start, end, bb,\n            corruptedBlockMap, hedgedReadId++);\n        Future\u003cByteBuffer\u003e firstRequest \u003d hedgedService\n            .submit(getFromDataNodeCallable);\n        futures.add(firstRequest);\n        try {\n          Future\u003cByteBuffer\u003e future \u003d hedgedService.poll(\n              conf.getHedgedReadThresholdMillis(), TimeUnit.MILLISECONDS);\n          if (future !\u003d null) {\n            future.get();\n            return;\n          }\n          if (DFSClient.LOG.isDebugEnabled()) {\n            DFSClient.LOG.debug(\"Waited \" + conf.getHedgedReadThresholdMillis()\n                + \"ms to read from \" + chosenNode.info\n                + \"; spawning hedged read\");\n          }\n          // Ignore this node on next go around.\n          ignored.add(chosenNode.info);\n          dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n          continue; // no need to refresh block locations\n        } catch (InterruptedException e) {\n          // Ignore\n        } catch (ExecutionException e) {\n          // Ignore already logged in the call.\n        }\n      } else {\n        // We are starting up a \u0027hedged\u0027 read. We have a read already\n        // ongoing. Call getBestNodeDNAddrPair instead of chooseDataNode.\n        // If no nodes to do hedged reads against, pass.\n        try {\n          chosenNode \u003d getBestNodeDNAddrPair(block, ignored);\n          if (chosenNode \u003d\u003d null) {\n            chosenNode \u003d chooseDataNode(block, ignored);\n          }\n          bb \u003d ByteBuffer.allocate(len);\n          Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n              chosenNode, block.getStartOffset(), start, end, bb,\n              corruptedBlockMap, hedgedReadId++);\n          Future\u003cByteBuffer\u003e oneMoreRequest \u003d hedgedService\n              .submit(getFromDataNodeCallable);\n          futures.add(oneMoreRequest);\n        } catch (IOException ioe) {\n          if (DFSClient.LOG.isDebugEnabled()) {\n            DFSClient.LOG.debug(\"Failed getting node for hedged read: \"\n                + ioe.getMessage());\n          }\n        }\n        // if not succeeded. Submit callables for each datanode in a loop, wait\n        // for a fixed interval and get the result from the fastest one.\n        try {\n          ByteBuffer result \u003d getFirstToComplete(hedgedService, futures);\n          // cancel the rest.\n          cancelAll(futures);\n          if (result.array() !\u003d buf) { // compare the array pointers\n            dfsClient.getHedgedReadMetrics().incHedgedReadWins();\n            System.arraycopy(result.array(), result.position(), buf, offset,\n                len);\n          } else {\n            dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n          }\n          return;\n        } catch (InterruptedException ie) {\n          // Ignore and retry\n        }\n        // We got here if exception. Ignore this node on next go around IFF\n        // we found a chosenNode to hedge read against.\n        if (chosenNode !\u003d null \u0026\u0026 chosenNode.info !\u003d null) {\n          ignored.add(chosenNode.info);\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "75bbcc8bf3fa1daf54f56868dae737f6da12ab1f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8082. Move dfs.client.read.*, dfs.client.short.circuit.*, dfs.client.mmap.* and dfs.client.hedged.read.* conf from DFSConfigKeys to HdfsClientConfigKeys.\n",
      "commitDate": "16/04/15 1:22 PM",
      "commitName": "75bbcc8bf3fa1daf54f56868dae737f6da12ab1f",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "10/04/15 2:48 PM",
      "commitNameOld": "2cc9514ad643ae49d30524743420ee9744e571bd",
      "commitAuthorOld": "Tsz-Wo Nicholas Sze",
      "daysBetweenCommits": 5.94,
      "commitsBetweenForRepo": 36,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,98 +1,99 @@\n   private void hedgedFetchBlockByteRange(long blockStartOffset, long start,\n       long end, byte[] buf, int offset,\n       Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n       throws IOException {\n+    final DfsClientConf conf \u003d dfsClient.getConf();\n     ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e futures \u003d new ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e();\n     CompletionService\u003cByteBuffer\u003e hedgedService \u003d\n         new ExecutorCompletionService\u003cByteBuffer\u003e(\n         dfsClient.getHedgedReadsThreadPool());\n     ArrayList\u003cDatanodeInfo\u003e ignored \u003d new ArrayList\u003cDatanodeInfo\u003e();\n     ByteBuffer bb \u003d null;\n     int len \u003d (int) (end - start + 1);\n     int hedgedReadId \u003d 0;\n     LocatedBlock block \u003d getBlockAt(blockStartOffset);\n     while (true) {\n       // see HDFS-6591, this metric is used to verify/catch unnecessary loops\n       hedgedReadOpsLoopNumForTesting++;\n       DNAddrPair chosenNode \u003d null;\n       // there is no request already executing.\n       if (futures.isEmpty()) {\n         // chooseDataNode is a commitment. If no node, we go to\n         // the NN to reget block locations. Only go here on first read.\n         chosenNode \u003d chooseDataNode(block, ignored);\n         bb \u003d ByteBuffer.wrap(buf, offset, len);\n         Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n             chosenNode, block.getStartOffset(), start, end, bb,\n             corruptedBlockMap, hedgedReadId++);\n         Future\u003cByteBuffer\u003e firstRequest \u003d hedgedService\n             .submit(getFromDataNodeCallable);\n         futures.add(firstRequest);\n         try {\n           Future\u003cByteBuffer\u003e future \u003d hedgedService.poll(\n-              dfsClient.getHedgedReadTimeout(), TimeUnit.MILLISECONDS);\n+              conf.getHedgedReadThresholdMillis(), TimeUnit.MILLISECONDS);\n           if (future !\u003d null) {\n             future.get();\n             return;\n           }\n           if (DFSClient.LOG.isDebugEnabled()) {\n-            DFSClient.LOG.debug(\"Waited \" + dfsClient.getHedgedReadTimeout()\n+            DFSClient.LOG.debug(\"Waited \" + conf.getHedgedReadThresholdMillis()\n                 + \"ms to read from \" + chosenNode.info\n                 + \"; spawning hedged read\");\n           }\n           // Ignore this node on next go around.\n           ignored.add(chosenNode.info);\n           dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n           continue; // no need to refresh block locations\n         } catch (InterruptedException e) {\n           // Ignore\n         } catch (ExecutionException e) {\n           // Ignore already logged in the call.\n         }\n       } else {\n         // We are starting up a \u0027hedged\u0027 read. We have a read already\n         // ongoing. Call getBestNodeDNAddrPair instead of chooseDataNode.\n         // If no nodes to do hedged reads against, pass.\n         try {\n           try {\n             chosenNode \u003d getBestNodeDNAddrPair(block, ignored);\n           } catch (IOException ioe) {\n             chosenNode \u003d chooseDataNode(block, ignored);\n           }\n           bb \u003d ByteBuffer.allocate(len);\n           Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n               chosenNode, block.getStartOffset(), start, end, bb,\n               corruptedBlockMap, hedgedReadId++);\n           Future\u003cByteBuffer\u003e oneMoreRequest \u003d hedgedService\n               .submit(getFromDataNodeCallable);\n           futures.add(oneMoreRequest);\n         } catch (IOException ioe) {\n           if (DFSClient.LOG.isDebugEnabled()) {\n             DFSClient.LOG.debug(\"Failed getting node for hedged read: \"\n                 + ioe.getMessage());\n           }\n         }\n         // if not succeeded. Submit callables for each datanode in a loop, wait\n         // for a fixed interval and get the result from the fastest one.\n         try {\n           ByteBuffer result \u003d getFirstToComplete(hedgedService, futures);\n           // cancel the rest.\n           cancelAll(futures);\n           if (result.array() !\u003d buf) { // compare the array pointers\n             dfsClient.getHedgedReadMetrics().incHedgedReadWins();\n             System.arraycopy(result.array(), result.position(), buf, offset,\n                 len);\n           } else {\n             dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n           }\n           return;\n         } catch (InterruptedException ie) {\n           // Ignore and retry\n         }\n         // We got here if exception. Ignore this node on next go around IFF\n         // we found a chosenNode to hedge read against.\n         if (chosenNode !\u003d null \u0026\u0026 chosenNode.info !\u003d null) {\n           ignored.add(chosenNode.info);\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void hedgedFetchBlockByteRange(long blockStartOffset, long start,\n      long end, byte[] buf, int offset,\n      Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n      throws IOException {\n    final DfsClientConf conf \u003d dfsClient.getConf();\n    ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e futures \u003d new ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e();\n    CompletionService\u003cByteBuffer\u003e hedgedService \u003d\n        new ExecutorCompletionService\u003cByteBuffer\u003e(\n        dfsClient.getHedgedReadsThreadPool());\n    ArrayList\u003cDatanodeInfo\u003e ignored \u003d new ArrayList\u003cDatanodeInfo\u003e();\n    ByteBuffer bb \u003d null;\n    int len \u003d (int) (end - start + 1);\n    int hedgedReadId \u003d 0;\n    LocatedBlock block \u003d getBlockAt(blockStartOffset);\n    while (true) {\n      // see HDFS-6591, this metric is used to verify/catch unnecessary loops\n      hedgedReadOpsLoopNumForTesting++;\n      DNAddrPair chosenNode \u003d null;\n      // there is no request already executing.\n      if (futures.isEmpty()) {\n        // chooseDataNode is a commitment. If no node, we go to\n        // the NN to reget block locations. Only go here on first read.\n        chosenNode \u003d chooseDataNode(block, ignored);\n        bb \u003d ByteBuffer.wrap(buf, offset, len);\n        Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n            chosenNode, block.getStartOffset(), start, end, bb,\n            corruptedBlockMap, hedgedReadId++);\n        Future\u003cByteBuffer\u003e firstRequest \u003d hedgedService\n            .submit(getFromDataNodeCallable);\n        futures.add(firstRequest);\n        try {\n          Future\u003cByteBuffer\u003e future \u003d hedgedService.poll(\n              conf.getHedgedReadThresholdMillis(), TimeUnit.MILLISECONDS);\n          if (future !\u003d null) {\n            future.get();\n            return;\n          }\n          if (DFSClient.LOG.isDebugEnabled()) {\n            DFSClient.LOG.debug(\"Waited \" + conf.getHedgedReadThresholdMillis()\n                + \"ms to read from \" + chosenNode.info\n                + \"; spawning hedged read\");\n          }\n          // Ignore this node on next go around.\n          ignored.add(chosenNode.info);\n          dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n          continue; // no need to refresh block locations\n        } catch (InterruptedException e) {\n          // Ignore\n        } catch (ExecutionException e) {\n          // Ignore already logged in the call.\n        }\n      } else {\n        // We are starting up a \u0027hedged\u0027 read. We have a read already\n        // ongoing. Call getBestNodeDNAddrPair instead of chooseDataNode.\n        // If no nodes to do hedged reads against, pass.\n        try {\n          try {\n            chosenNode \u003d getBestNodeDNAddrPair(block, ignored);\n          } catch (IOException ioe) {\n            chosenNode \u003d chooseDataNode(block, ignored);\n          }\n          bb \u003d ByteBuffer.allocate(len);\n          Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n              chosenNode, block.getStartOffset(), start, end, bb,\n              corruptedBlockMap, hedgedReadId++);\n          Future\u003cByteBuffer\u003e oneMoreRequest \u003d hedgedService\n              .submit(getFromDataNodeCallable);\n          futures.add(oneMoreRequest);\n        } catch (IOException ioe) {\n          if (DFSClient.LOG.isDebugEnabled()) {\n            DFSClient.LOG.debug(\"Failed getting node for hedged read: \"\n                + ioe.getMessage());\n          }\n        }\n        // if not succeeded. Submit callables for each datanode in a loop, wait\n        // for a fixed interval and get the result from the fastest one.\n        try {\n          ByteBuffer result \u003d getFirstToComplete(hedgedService, futures);\n          // cancel the rest.\n          cancelAll(futures);\n          if (result.array() !\u003d buf) { // compare the array pointers\n            dfsClient.getHedgedReadMetrics().incHedgedReadWins();\n            System.arraycopy(result.array(), result.position(), buf, offset,\n                len);\n          } else {\n            dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n          }\n          return;\n        } catch (InterruptedException ie) {\n          // Ignore and retry\n        }\n        // We got here if exception. Ignore this node on next go around IFF\n        // we found a chosenNode to hedge read against.\n        if (chosenNode !\u003d null \u0026\u0026 chosenNode.info !\u003d null) {\n          ignored.add(chosenNode.info);\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "a42bb1cd915abe5dc33eda3c01e8c74c64f35748": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-8076. Code cleanup for DFSInputStream: use offset instead of LocatedBlock when possible. Contributed by Zhe Zhang.\n",
      "commitDate": "08/04/15 3:41 PM",
      "commitName": "a42bb1cd915abe5dc33eda3c01e8c74c64f35748",
      "commitAuthor": "Andrew Wang",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-8076. Code cleanup for DFSInputStream: use offset instead of LocatedBlock when possible. Contributed by Zhe Zhang.\n",
          "commitDate": "08/04/15 3:41 PM",
          "commitName": "a42bb1cd915abe5dc33eda3c01e8c74c64f35748",
          "commitAuthor": "Andrew Wang",
          "commitDateOld": "25/02/15 1:30 PM",
          "commitNameOld": "caa42adf208bfb5625d1b3ef665fbf334ffcccd9",
          "commitAuthorOld": "Colin Patrick Mccabe",
          "daysBetweenCommits": 42.05,
          "commitsBetweenForRepo": 367,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,98 +1,98 @@\n-  private void hedgedFetchBlockByteRange(LocatedBlock block, long start,\n+  private void hedgedFetchBlockByteRange(long blockStartOffset, long start,\n       long end, byte[] buf, int offset,\n       Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n       throws IOException {\n     ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e futures \u003d new ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e();\n     CompletionService\u003cByteBuffer\u003e hedgedService \u003d\n         new ExecutorCompletionService\u003cByteBuffer\u003e(\n         dfsClient.getHedgedReadsThreadPool());\n     ArrayList\u003cDatanodeInfo\u003e ignored \u003d new ArrayList\u003cDatanodeInfo\u003e();\n     ByteBuffer bb \u003d null;\n     int len \u003d (int) (end - start + 1);\n     int hedgedReadId \u003d 0;\n-    block \u003d getBlockAt(block.getStartOffset());\n+    LocatedBlock block \u003d getBlockAt(blockStartOffset);\n     while (true) {\n       // see HDFS-6591, this metric is used to verify/catch unnecessary loops\n       hedgedReadOpsLoopNumForTesting++;\n       DNAddrPair chosenNode \u003d null;\n       // there is no request already executing.\n       if (futures.isEmpty()) {\n         // chooseDataNode is a commitment. If no node, we go to\n         // the NN to reget block locations. Only go here on first read.\n         chosenNode \u003d chooseDataNode(block, ignored);\n         bb \u003d ByteBuffer.wrap(buf, offset, len);\n         Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n-            chosenNode, block, start, end, bb, corruptedBlockMap,\n-            hedgedReadId++);\n+            chosenNode, block.getStartOffset(), start, end, bb,\n+            corruptedBlockMap, hedgedReadId++);\n         Future\u003cByteBuffer\u003e firstRequest \u003d hedgedService\n             .submit(getFromDataNodeCallable);\n         futures.add(firstRequest);\n         try {\n           Future\u003cByteBuffer\u003e future \u003d hedgedService.poll(\n               dfsClient.getHedgedReadTimeout(), TimeUnit.MILLISECONDS);\n           if (future !\u003d null) {\n             future.get();\n             return;\n           }\n           if (DFSClient.LOG.isDebugEnabled()) {\n             DFSClient.LOG.debug(\"Waited \" + dfsClient.getHedgedReadTimeout()\n                 + \"ms to read from \" + chosenNode.info\n                 + \"; spawning hedged read\");\n           }\n           // Ignore this node on next go around.\n           ignored.add(chosenNode.info);\n           dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n           continue; // no need to refresh block locations\n         } catch (InterruptedException e) {\n           // Ignore\n         } catch (ExecutionException e) {\n           // Ignore already logged in the call.\n         }\n       } else {\n         // We are starting up a \u0027hedged\u0027 read. We have a read already\n         // ongoing. Call getBestNodeDNAddrPair instead of chooseDataNode.\n         // If no nodes to do hedged reads against, pass.\n         try {\n           try {\n             chosenNode \u003d getBestNodeDNAddrPair(block, ignored);\n           } catch (IOException ioe) {\n             chosenNode \u003d chooseDataNode(block, ignored);\n           }\n           bb \u003d ByteBuffer.allocate(len);\n           Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n-              chosenNode, block, start, end, bb, corruptedBlockMap,\n-              hedgedReadId++);\n+              chosenNode, block.getStartOffset(), start, end, bb,\n+              corruptedBlockMap, hedgedReadId++);\n           Future\u003cByteBuffer\u003e oneMoreRequest \u003d hedgedService\n               .submit(getFromDataNodeCallable);\n           futures.add(oneMoreRequest);\n         } catch (IOException ioe) {\n           if (DFSClient.LOG.isDebugEnabled()) {\n             DFSClient.LOG.debug(\"Failed getting node for hedged read: \"\n                 + ioe.getMessage());\n           }\n         }\n         // if not succeeded. Submit callables for each datanode in a loop, wait\n         // for a fixed interval and get the result from the fastest one.\n         try {\n           ByteBuffer result \u003d getFirstToComplete(hedgedService, futures);\n           // cancel the rest.\n           cancelAll(futures);\n           if (result.array() !\u003d buf) { // compare the array pointers\n             dfsClient.getHedgedReadMetrics().incHedgedReadWins();\n             System.arraycopy(result.array(), result.position(), buf, offset,\n                 len);\n           } else {\n             dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n           }\n           return;\n         } catch (InterruptedException ie) {\n           // Ignore and retry\n         }\n         // We got here if exception. Ignore this node on next go around IFF\n         // we found a chosenNode to hedge read against.\n         if (chosenNode !\u003d null \u0026\u0026 chosenNode.info !\u003d null) {\n           ignored.add(chosenNode.info);\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void hedgedFetchBlockByteRange(long blockStartOffset, long start,\n      long end, byte[] buf, int offset,\n      Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n      throws IOException {\n    ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e futures \u003d new ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e();\n    CompletionService\u003cByteBuffer\u003e hedgedService \u003d\n        new ExecutorCompletionService\u003cByteBuffer\u003e(\n        dfsClient.getHedgedReadsThreadPool());\n    ArrayList\u003cDatanodeInfo\u003e ignored \u003d new ArrayList\u003cDatanodeInfo\u003e();\n    ByteBuffer bb \u003d null;\n    int len \u003d (int) (end - start + 1);\n    int hedgedReadId \u003d 0;\n    LocatedBlock block \u003d getBlockAt(blockStartOffset);\n    while (true) {\n      // see HDFS-6591, this metric is used to verify/catch unnecessary loops\n      hedgedReadOpsLoopNumForTesting++;\n      DNAddrPair chosenNode \u003d null;\n      // there is no request already executing.\n      if (futures.isEmpty()) {\n        // chooseDataNode is a commitment. If no node, we go to\n        // the NN to reget block locations. Only go here on first read.\n        chosenNode \u003d chooseDataNode(block, ignored);\n        bb \u003d ByteBuffer.wrap(buf, offset, len);\n        Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n            chosenNode, block.getStartOffset(), start, end, bb,\n            corruptedBlockMap, hedgedReadId++);\n        Future\u003cByteBuffer\u003e firstRequest \u003d hedgedService\n            .submit(getFromDataNodeCallable);\n        futures.add(firstRequest);\n        try {\n          Future\u003cByteBuffer\u003e future \u003d hedgedService.poll(\n              dfsClient.getHedgedReadTimeout(), TimeUnit.MILLISECONDS);\n          if (future !\u003d null) {\n            future.get();\n            return;\n          }\n          if (DFSClient.LOG.isDebugEnabled()) {\n            DFSClient.LOG.debug(\"Waited \" + dfsClient.getHedgedReadTimeout()\n                + \"ms to read from \" + chosenNode.info\n                + \"; spawning hedged read\");\n          }\n          // Ignore this node on next go around.\n          ignored.add(chosenNode.info);\n          dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n          continue; // no need to refresh block locations\n        } catch (InterruptedException e) {\n          // Ignore\n        } catch (ExecutionException e) {\n          // Ignore already logged in the call.\n        }\n      } else {\n        // We are starting up a \u0027hedged\u0027 read. We have a read already\n        // ongoing. Call getBestNodeDNAddrPair instead of chooseDataNode.\n        // If no nodes to do hedged reads against, pass.\n        try {\n          try {\n            chosenNode \u003d getBestNodeDNAddrPair(block, ignored);\n          } catch (IOException ioe) {\n            chosenNode \u003d chooseDataNode(block, ignored);\n          }\n          bb \u003d ByteBuffer.allocate(len);\n          Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n              chosenNode, block.getStartOffset(), start, end, bb,\n              corruptedBlockMap, hedgedReadId++);\n          Future\u003cByteBuffer\u003e oneMoreRequest \u003d hedgedService\n              .submit(getFromDataNodeCallable);\n          futures.add(oneMoreRequest);\n        } catch (IOException ioe) {\n          if (DFSClient.LOG.isDebugEnabled()) {\n            DFSClient.LOG.debug(\"Failed getting node for hedged read: \"\n                + ioe.getMessage());\n          }\n        }\n        // if not succeeded. Submit callables for each datanode in a loop, wait\n        // for a fixed interval and get the result from the fastest one.\n        try {\n          ByteBuffer result \u003d getFirstToComplete(hedgedService, futures);\n          // cancel the rest.\n          cancelAll(futures);\n          if (result.array() !\u003d buf) { // compare the array pointers\n            dfsClient.getHedgedReadMetrics().incHedgedReadWins();\n            System.arraycopy(result.array(), result.position(), buf, offset,\n                len);\n          } else {\n            dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n          }\n          return;\n        } catch (InterruptedException ie) {\n          // Ignore and retry\n        }\n        // We got here if exception. Ignore this node on next go around IFF\n        // we found a chosenNode to hedge read against.\n        if (chosenNode !\u003d null \u0026\u0026 chosenNode.info !\u003d null) {\n          ignored.add(chosenNode.info);\n        }\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
          "extendedDetails": {
            "oldValue": "[block-LocatedBlock, start-long, end-long, buf-byte[], offset-int, corruptedBlockMap-Map\u003cExtendedBlock,Set\u003cDatanodeInfo\u003e\u003e]",
            "newValue": "[blockStartOffset-long, start-long, end-long, buf-byte[], offset-int, corruptedBlockMap-Map\u003cExtendedBlock,Set\u003cDatanodeInfo\u003e\u003e]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-8076. Code cleanup for DFSInputStream: use offset instead of LocatedBlock when possible. Contributed by Zhe Zhang.\n",
          "commitDate": "08/04/15 3:41 PM",
          "commitName": "a42bb1cd915abe5dc33eda3c01e8c74c64f35748",
          "commitAuthor": "Andrew Wang",
          "commitDateOld": "25/02/15 1:30 PM",
          "commitNameOld": "caa42adf208bfb5625d1b3ef665fbf334ffcccd9",
          "commitAuthorOld": "Colin Patrick Mccabe",
          "daysBetweenCommits": 42.05,
          "commitsBetweenForRepo": 367,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,98 +1,98 @@\n-  private void hedgedFetchBlockByteRange(LocatedBlock block, long start,\n+  private void hedgedFetchBlockByteRange(long blockStartOffset, long start,\n       long end, byte[] buf, int offset,\n       Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n       throws IOException {\n     ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e futures \u003d new ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e();\n     CompletionService\u003cByteBuffer\u003e hedgedService \u003d\n         new ExecutorCompletionService\u003cByteBuffer\u003e(\n         dfsClient.getHedgedReadsThreadPool());\n     ArrayList\u003cDatanodeInfo\u003e ignored \u003d new ArrayList\u003cDatanodeInfo\u003e();\n     ByteBuffer bb \u003d null;\n     int len \u003d (int) (end - start + 1);\n     int hedgedReadId \u003d 0;\n-    block \u003d getBlockAt(block.getStartOffset());\n+    LocatedBlock block \u003d getBlockAt(blockStartOffset);\n     while (true) {\n       // see HDFS-6591, this metric is used to verify/catch unnecessary loops\n       hedgedReadOpsLoopNumForTesting++;\n       DNAddrPair chosenNode \u003d null;\n       // there is no request already executing.\n       if (futures.isEmpty()) {\n         // chooseDataNode is a commitment. If no node, we go to\n         // the NN to reget block locations. Only go here on first read.\n         chosenNode \u003d chooseDataNode(block, ignored);\n         bb \u003d ByteBuffer.wrap(buf, offset, len);\n         Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n-            chosenNode, block, start, end, bb, corruptedBlockMap,\n-            hedgedReadId++);\n+            chosenNode, block.getStartOffset(), start, end, bb,\n+            corruptedBlockMap, hedgedReadId++);\n         Future\u003cByteBuffer\u003e firstRequest \u003d hedgedService\n             .submit(getFromDataNodeCallable);\n         futures.add(firstRequest);\n         try {\n           Future\u003cByteBuffer\u003e future \u003d hedgedService.poll(\n               dfsClient.getHedgedReadTimeout(), TimeUnit.MILLISECONDS);\n           if (future !\u003d null) {\n             future.get();\n             return;\n           }\n           if (DFSClient.LOG.isDebugEnabled()) {\n             DFSClient.LOG.debug(\"Waited \" + dfsClient.getHedgedReadTimeout()\n                 + \"ms to read from \" + chosenNode.info\n                 + \"; spawning hedged read\");\n           }\n           // Ignore this node on next go around.\n           ignored.add(chosenNode.info);\n           dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n           continue; // no need to refresh block locations\n         } catch (InterruptedException e) {\n           // Ignore\n         } catch (ExecutionException e) {\n           // Ignore already logged in the call.\n         }\n       } else {\n         // We are starting up a \u0027hedged\u0027 read. We have a read already\n         // ongoing. Call getBestNodeDNAddrPair instead of chooseDataNode.\n         // If no nodes to do hedged reads against, pass.\n         try {\n           try {\n             chosenNode \u003d getBestNodeDNAddrPair(block, ignored);\n           } catch (IOException ioe) {\n             chosenNode \u003d chooseDataNode(block, ignored);\n           }\n           bb \u003d ByteBuffer.allocate(len);\n           Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n-              chosenNode, block, start, end, bb, corruptedBlockMap,\n-              hedgedReadId++);\n+              chosenNode, block.getStartOffset(), start, end, bb,\n+              corruptedBlockMap, hedgedReadId++);\n           Future\u003cByteBuffer\u003e oneMoreRequest \u003d hedgedService\n               .submit(getFromDataNodeCallable);\n           futures.add(oneMoreRequest);\n         } catch (IOException ioe) {\n           if (DFSClient.LOG.isDebugEnabled()) {\n             DFSClient.LOG.debug(\"Failed getting node for hedged read: \"\n                 + ioe.getMessage());\n           }\n         }\n         // if not succeeded. Submit callables for each datanode in a loop, wait\n         // for a fixed interval and get the result from the fastest one.\n         try {\n           ByteBuffer result \u003d getFirstToComplete(hedgedService, futures);\n           // cancel the rest.\n           cancelAll(futures);\n           if (result.array() !\u003d buf) { // compare the array pointers\n             dfsClient.getHedgedReadMetrics().incHedgedReadWins();\n             System.arraycopy(result.array(), result.position(), buf, offset,\n                 len);\n           } else {\n             dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n           }\n           return;\n         } catch (InterruptedException ie) {\n           // Ignore and retry\n         }\n         // We got here if exception. Ignore this node on next go around IFF\n         // we found a chosenNode to hedge read against.\n         if (chosenNode !\u003d null \u0026\u0026 chosenNode.info !\u003d null) {\n           ignored.add(chosenNode.info);\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void hedgedFetchBlockByteRange(long blockStartOffset, long start,\n      long end, byte[] buf, int offset,\n      Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n      throws IOException {\n    ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e futures \u003d new ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e();\n    CompletionService\u003cByteBuffer\u003e hedgedService \u003d\n        new ExecutorCompletionService\u003cByteBuffer\u003e(\n        dfsClient.getHedgedReadsThreadPool());\n    ArrayList\u003cDatanodeInfo\u003e ignored \u003d new ArrayList\u003cDatanodeInfo\u003e();\n    ByteBuffer bb \u003d null;\n    int len \u003d (int) (end - start + 1);\n    int hedgedReadId \u003d 0;\n    LocatedBlock block \u003d getBlockAt(blockStartOffset);\n    while (true) {\n      // see HDFS-6591, this metric is used to verify/catch unnecessary loops\n      hedgedReadOpsLoopNumForTesting++;\n      DNAddrPair chosenNode \u003d null;\n      // there is no request already executing.\n      if (futures.isEmpty()) {\n        // chooseDataNode is a commitment. If no node, we go to\n        // the NN to reget block locations. Only go here on first read.\n        chosenNode \u003d chooseDataNode(block, ignored);\n        bb \u003d ByteBuffer.wrap(buf, offset, len);\n        Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n            chosenNode, block.getStartOffset(), start, end, bb,\n            corruptedBlockMap, hedgedReadId++);\n        Future\u003cByteBuffer\u003e firstRequest \u003d hedgedService\n            .submit(getFromDataNodeCallable);\n        futures.add(firstRequest);\n        try {\n          Future\u003cByteBuffer\u003e future \u003d hedgedService.poll(\n              dfsClient.getHedgedReadTimeout(), TimeUnit.MILLISECONDS);\n          if (future !\u003d null) {\n            future.get();\n            return;\n          }\n          if (DFSClient.LOG.isDebugEnabled()) {\n            DFSClient.LOG.debug(\"Waited \" + dfsClient.getHedgedReadTimeout()\n                + \"ms to read from \" + chosenNode.info\n                + \"; spawning hedged read\");\n          }\n          // Ignore this node on next go around.\n          ignored.add(chosenNode.info);\n          dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n          continue; // no need to refresh block locations\n        } catch (InterruptedException e) {\n          // Ignore\n        } catch (ExecutionException e) {\n          // Ignore already logged in the call.\n        }\n      } else {\n        // We are starting up a \u0027hedged\u0027 read. We have a read already\n        // ongoing. Call getBestNodeDNAddrPair instead of chooseDataNode.\n        // If no nodes to do hedged reads against, pass.\n        try {\n          try {\n            chosenNode \u003d getBestNodeDNAddrPair(block, ignored);\n          } catch (IOException ioe) {\n            chosenNode \u003d chooseDataNode(block, ignored);\n          }\n          bb \u003d ByteBuffer.allocate(len);\n          Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n              chosenNode, block.getStartOffset(), start, end, bb,\n              corruptedBlockMap, hedgedReadId++);\n          Future\u003cByteBuffer\u003e oneMoreRequest \u003d hedgedService\n              .submit(getFromDataNodeCallable);\n          futures.add(oneMoreRequest);\n        } catch (IOException ioe) {\n          if (DFSClient.LOG.isDebugEnabled()) {\n            DFSClient.LOG.debug(\"Failed getting node for hedged read: \"\n                + ioe.getMessage());\n          }\n        }\n        // if not succeeded. Submit callables for each datanode in a loop, wait\n        // for a fixed interval and get the result from the fastest one.\n        try {\n          ByteBuffer result \u003d getFirstToComplete(hedgedService, futures);\n          // cancel the rest.\n          cancelAll(futures);\n          if (result.array() !\u003d buf) { // compare the array pointers\n            dfsClient.getHedgedReadMetrics().incHedgedReadWins();\n            System.arraycopy(result.array(), result.position(), buf, offset,\n                len);\n          } else {\n            dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n          }\n          return;\n        } catch (InterruptedException ie) {\n          // Ignore and retry\n        }\n        // We got here if exception. Ignore this node on next go around IFF\n        // we found a chosenNode to hedge read against.\n        if (chosenNode !\u003d null \u0026\u0026 chosenNode.info !\u003d null) {\n          ignored.add(chosenNode.info);\n        }\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
          "extendedDetails": {}
        }
      ]
    },
    "caa42adf208bfb5625d1b3ef665fbf334ffcccd9": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7495. Remove updatePosition argument from DFSInputStream#getBlockAt() (cmccabe)\n",
      "commitDate": "25/02/15 1:30 PM",
      "commitName": "caa42adf208bfb5625d1b3ef665fbf334ffcccd9",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "21/02/15 3:38 PM",
      "commitNameOld": "8b465b4b8caed31ca9daeaae108f9a868a30a455",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 3.91,
      "commitsBetweenForRepo": 26,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,98 +1,98 @@\n   private void hedgedFetchBlockByteRange(LocatedBlock block, long start,\n       long end, byte[] buf, int offset,\n       Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n       throws IOException {\n     ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e futures \u003d new ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e();\n     CompletionService\u003cByteBuffer\u003e hedgedService \u003d\n         new ExecutorCompletionService\u003cByteBuffer\u003e(\n         dfsClient.getHedgedReadsThreadPool());\n     ArrayList\u003cDatanodeInfo\u003e ignored \u003d new ArrayList\u003cDatanodeInfo\u003e();\n     ByteBuffer bb \u003d null;\n     int len \u003d (int) (end - start + 1);\n     int hedgedReadId \u003d 0;\n-    block \u003d getBlockAt(block.getStartOffset(), false);\n+    block \u003d getBlockAt(block.getStartOffset());\n     while (true) {\n       // see HDFS-6591, this metric is used to verify/catch unnecessary loops\n       hedgedReadOpsLoopNumForTesting++;\n       DNAddrPair chosenNode \u003d null;\n       // there is no request already executing.\n       if (futures.isEmpty()) {\n         // chooseDataNode is a commitment. If no node, we go to\n         // the NN to reget block locations. Only go here on first read.\n         chosenNode \u003d chooseDataNode(block, ignored);\n         bb \u003d ByteBuffer.wrap(buf, offset, len);\n         Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n             chosenNode, block, start, end, bb, corruptedBlockMap,\n             hedgedReadId++);\n         Future\u003cByteBuffer\u003e firstRequest \u003d hedgedService\n             .submit(getFromDataNodeCallable);\n         futures.add(firstRequest);\n         try {\n           Future\u003cByteBuffer\u003e future \u003d hedgedService.poll(\n               dfsClient.getHedgedReadTimeout(), TimeUnit.MILLISECONDS);\n           if (future !\u003d null) {\n             future.get();\n             return;\n           }\n           if (DFSClient.LOG.isDebugEnabled()) {\n             DFSClient.LOG.debug(\"Waited \" + dfsClient.getHedgedReadTimeout()\n                 + \"ms to read from \" + chosenNode.info\n                 + \"; spawning hedged read\");\n           }\n           // Ignore this node on next go around.\n           ignored.add(chosenNode.info);\n           dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n           continue; // no need to refresh block locations\n         } catch (InterruptedException e) {\n           // Ignore\n         } catch (ExecutionException e) {\n           // Ignore already logged in the call.\n         }\n       } else {\n         // We are starting up a \u0027hedged\u0027 read. We have a read already\n         // ongoing. Call getBestNodeDNAddrPair instead of chooseDataNode.\n         // If no nodes to do hedged reads against, pass.\n         try {\n           try {\n             chosenNode \u003d getBestNodeDNAddrPair(block, ignored);\n           } catch (IOException ioe) {\n             chosenNode \u003d chooseDataNode(block, ignored);\n           }\n           bb \u003d ByteBuffer.allocate(len);\n           Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n               chosenNode, block, start, end, bb, corruptedBlockMap,\n               hedgedReadId++);\n           Future\u003cByteBuffer\u003e oneMoreRequest \u003d hedgedService\n               .submit(getFromDataNodeCallable);\n           futures.add(oneMoreRequest);\n         } catch (IOException ioe) {\n           if (DFSClient.LOG.isDebugEnabled()) {\n             DFSClient.LOG.debug(\"Failed getting node for hedged read: \"\n                 + ioe.getMessage());\n           }\n         }\n         // if not succeeded. Submit callables for each datanode in a loop, wait\n         // for a fixed interval and get the result from the fastest one.\n         try {\n           ByteBuffer result \u003d getFirstToComplete(hedgedService, futures);\n           // cancel the rest.\n           cancelAll(futures);\n           if (result.array() !\u003d buf) { // compare the array pointers\n             dfsClient.getHedgedReadMetrics().incHedgedReadWins();\n             System.arraycopy(result.array(), result.position(), buf, offset,\n                 len);\n           } else {\n             dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n           }\n           return;\n         } catch (InterruptedException ie) {\n           // Ignore and retry\n         }\n         // We got here if exception. Ignore this node on next go around IFF\n         // we found a chosenNode to hedge read against.\n         if (chosenNode !\u003d null \u0026\u0026 chosenNode.info !\u003d null) {\n           ignored.add(chosenNode.info);\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void hedgedFetchBlockByteRange(LocatedBlock block, long start,\n      long end, byte[] buf, int offset,\n      Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n      throws IOException {\n    ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e futures \u003d new ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e();\n    CompletionService\u003cByteBuffer\u003e hedgedService \u003d\n        new ExecutorCompletionService\u003cByteBuffer\u003e(\n        dfsClient.getHedgedReadsThreadPool());\n    ArrayList\u003cDatanodeInfo\u003e ignored \u003d new ArrayList\u003cDatanodeInfo\u003e();\n    ByteBuffer bb \u003d null;\n    int len \u003d (int) (end - start + 1);\n    int hedgedReadId \u003d 0;\n    block \u003d getBlockAt(block.getStartOffset());\n    while (true) {\n      // see HDFS-6591, this metric is used to verify/catch unnecessary loops\n      hedgedReadOpsLoopNumForTesting++;\n      DNAddrPair chosenNode \u003d null;\n      // there is no request already executing.\n      if (futures.isEmpty()) {\n        // chooseDataNode is a commitment. If no node, we go to\n        // the NN to reget block locations. Only go here on first read.\n        chosenNode \u003d chooseDataNode(block, ignored);\n        bb \u003d ByteBuffer.wrap(buf, offset, len);\n        Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n            chosenNode, block, start, end, bb, corruptedBlockMap,\n            hedgedReadId++);\n        Future\u003cByteBuffer\u003e firstRequest \u003d hedgedService\n            .submit(getFromDataNodeCallable);\n        futures.add(firstRequest);\n        try {\n          Future\u003cByteBuffer\u003e future \u003d hedgedService.poll(\n              dfsClient.getHedgedReadTimeout(), TimeUnit.MILLISECONDS);\n          if (future !\u003d null) {\n            future.get();\n            return;\n          }\n          if (DFSClient.LOG.isDebugEnabled()) {\n            DFSClient.LOG.debug(\"Waited \" + dfsClient.getHedgedReadTimeout()\n                + \"ms to read from \" + chosenNode.info\n                + \"; spawning hedged read\");\n          }\n          // Ignore this node on next go around.\n          ignored.add(chosenNode.info);\n          dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n          continue; // no need to refresh block locations\n        } catch (InterruptedException e) {\n          // Ignore\n        } catch (ExecutionException e) {\n          // Ignore already logged in the call.\n        }\n      } else {\n        // We are starting up a \u0027hedged\u0027 read. We have a read already\n        // ongoing. Call getBestNodeDNAddrPair instead of chooseDataNode.\n        // If no nodes to do hedged reads against, pass.\n        try {\n          try {\n            chosenNode \u003d getBestNodeDNAddrPair(block, ignored);\n          } catch (IOException ioe) {\n            chosenNode \u003d chooseDataNode(block, ignored);\n          }\n          bb \u003d ByteBuffer.allocate(len);\n          Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n              chosenNode, block, start, end, bb, corruptedBlockMap,\n              hedgedReadId++);\n          Future\u003cByteBuffer\u003e oneMoreRequest \u003d hedgedService\n              .submit(getFromDataNodeCallable);\n          futures.add(oneMoreRequest);\n        } catch (IOException ioe) {\n          if (DFSClient.LOG.isDebugEnabled()) {\n            DFSClient.LOG.debug(\"Failed getting node for hedged read: \"\n                + ioe.getMessage());\n          }\n        }\n        // if not succeeded. Submit callables for each datanode in a loop, wait\n        // for a fixed interval and get the result from the fastest one.\n        try {\n          ByteBuffer result \u003d getFirstToComplete(hedgedService, futures);\n          // cancel the rest.\n          cancelAll(futures);\n          if (result.array() !\u003d buf) { // compare the array pointers\n            dfsClient.getHedgedReadMetrics().incHedgedReadWins();\n            System.arraycopy(result.array(), result.position(), buf, offset,\n                len);\n          } else {\n            dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n          }\n          return;\n        } catch (InterruptedException ie) {\n          // Ignore and retry\n        }\n        // We got here if exception. Ignore this node on next go around IFF\n        // we found a chosenNode to hedge read against.\n        if (chosenNode !\u003d null \u0026\u0026 chosenNode.info !\u003d null) {\n          ignored.add(chosenNode.info);\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "463aec11718e47d4aabb86a7a539cb973460aae6": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6934. Move checksum computation off the hot path when writing to RAM disk. Contributed by Chris Nauroth.\n",
      "commitDate": "27/10/14 9:38 AM",
      "commitName": "463aec11718e47d4aabb86a7a539cb973460aae6",
      "commitAuthor": "cnauroth",
      "commitDateOld": "03/10/14 1:35 PM",
      "commitNameOld": "7f6ed7fe365166e8075359f1d0ad035fa876c70f",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 23.84,
      "commitsBetweenForRepo": 188,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,98 +1,98 @@\n   private void hedgedFetchBlockByteRange(LocatedBlock block, long start,\n       long end, byte[] buf, int offset,\n       Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n       throws IOException {\n     ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e futures \u003d new ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e();\n     CompletionService\u003cByteBuffer\u003e hedgedService \u003d\n         new ExecutorCompletionService\u003cByteBuffer\u003e(\n         dfsClient.getHedgedReadsThreadPool());\n     ArrayList\u003cDatanodeInfo\u003e ignored \u003d new ArrayList\u003cDatanodeInfo\u003e();\n     ByteBuffer bb \u003d null;\n     int len \u003d (int) (end - start + 1);\n     int hedgedReadId \u003d 0;\n     block \u003d getBlockAt(block.getStartOffset(), false);\n     while (true) {\n       // see HDFS-6591, this metric is used to verify/catch unnecessary loops\n       hedgedReadOpsLoopNumForTesting++;\n       DNAddrPair chosenNode \u003d null;\n       // there is no request already executing.\n       if (futures.isEmpty()) {\n         // chooseDataNode is a commitment. If no node, we go to\n         // the NN to reget block locations. Only go here on first read.\n         chosenNode \u003d chooseDataNode(block, ignored);\n         bb \u003d ByteBuffer.wrap(buf, offset, len);\n         Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n             chosenNode, block, start, end, bb, corruptedBlockMap,\n             hedgedReadId++);\n         Future\u003cByteBuffer\u003e firstRequest \u003d hedgedService\n             .submit(getFromDataNodeCallable);\n         futures.add(firstRequest);\n         try {\n           Future\u003cByteBuffer\u003e future \u003d hedgedService.poll(\n               dfsClient.getHedgedReadTimeout(), TimeUnit.MILLISECONDS);\n           if (future !\u003d null) {\n             future.get();\n             return;\n           }\n           if (DFSClient.LOG.isDebugEnabled()) {\n             DFSClient.LOG.debug(\"Waited \" + dfsClient.getHedgedReadTimeout()\n                 + \"ms to read from \" + chosenNode.info\n                 + \"; spawning hedged read\");\n           }\n           // Ignore this node on next go around.\n           ignored.add(chosenNode.info);\n           dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n           continue; // no need to refresh block locations\n         } catch (InterruptedException e) {\n           // Ignore\n         } catch (ExecutionException e) {\n           // Ignore already logged in the call.\n         }\n       } else {\n         // We are starting up a \u0027hedged\u0027 read. We have a read already\n         // ongoing. Call getBestNodeDNAddrPair instead of chooseDataNode.\n         // If no nodes to do hedged reads against, pass.\n         try {\n           try {\n-            chosenNode \u003d getBestNodeDNAddrPair(block.getLocations(), ignored);\n+            chosenNode \u003d getBestNodeDNAddrPair(block, ignored);\n           } catch (IOException ioe) {\n             chosenNode \u003d chooseDataNode(block, ignored);\n           }\n           bb \u003d ByteBuffer.allocate(len);\n           Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n               chosenNode, block, start, end, bb, corruptedBlockMap,\n               hedgedReadId++);\n           Future\u003cByteBuffer\u003e oneMoreRequest \u003d hedgedService\n               .submit(getFromDataNodeCallable);\n           futures.add(oneMoreRequest);\n         } catch (IOException ioe) {\n           if (DFSClient.LOG.isDebugEnabled()) {\n             DFSClient.LOG.debug(\"Failed getting node for hedged read: \"\n                 + ioe.getMessage());\n           }\n         }\n         // if not succeeded. Submit callables for each datanode in a loop, wait\n         // for a fixed interval and get the result from the fastest one.\n         try {\n           ByteBuffer result \u003d getFirstToComplete(hedgedService, futures);\n           // cancel the rest.\n           cancelAll(futures);\n           if (result.array() !\u003d buf) { // compare the array pointers\n             dfsClient.getHedgedReadMetrics().incHedgedReadWins();\n             System.arraycopy(result.array(), result.position(), buf, offset,\n                 len);\n           } else {\n             dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n           }\n           return;\n         } catch (InterruptedException ie) {\n           // Ignore and retry\n         }\n         // We got here if exception. Ignore this node on next go around IFF\n         // we found a chosenNode to hedge read against.\n         if (chosenNode !\u003d null \u0026\u0026 chosenNode.info !\u003d null) {\n           ignored.add(chosenNode.info);\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void hedgedFetchBlockByteRange(LocatedBlock block, long start,\n      long end, byte[] buf, int offset,\n      Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n      throws IOException {\n    ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e futures \u003d new ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e();\n    CompletionService\u003cByteBuffer\u003e hedgedService \u003d\n        new ExecutorCompletionService\u003cByteBuffer\u003e(\n        dfsClient.getHedgedReadsThreadPool());\n    ArrayList\u003cDatanodeInfo\u003e ignored \u003d new ArrayList\u003cDatanodeInfo\u003e();\n    ByteBuffer bb \u003d null;\n    int len \u003d (int) (end - start + 1);\n    int hedgedReadId \u003d 0;\n    block \u003d getBlockAt(block.getStartOffset(), false);\n    while (true) {\n      // see HDFS-6591, this metric is used to verify/catch unnecessary loops\n      hedgedReadOpsLoopNumForTesting++;\n      DNAddrPair chosenNode \u003d null;\n      // there is no request already executing.\n      if (futures.isEmpty()) {\n        // chooseDataNode is a commitment. If no node, we go to\n        // the NN to reget block locations. Only go here on first read.\n        chosenNode \u003d chooseDataNode(block, ignored);\n        bb \u003d ByteBuffer.wrap(buf, offset, len);\n        Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n            chosenNode, block, start, end, bb, corruptedBlockMap,\n            hedgedReadId++);\n        Future\u003cByteBuffer\u003e firstRequest \u003d hedgedService\n            .submit(getFromDataNodeCallable);\n        futures.add(firstRequest);\n        try {\n          Future\u003cByteBuffer\u003e future \u003d hedgedService.poll(\n              dfsClient.getHedgedReadTimeout(), TimeUnit.MILLISECONDS);\n          if (future !\u003d null) {\n            future.get();\n            return;\n          }\n          if (DFSClient.LOG.isDebugEnabled()) {\n            DFSClient.LOG.debug(\"Waited \" + dfsClient.getHedgedReadTimeout()\n                + \"ms to read from \" + chosenNode.info\n                + \"; spawning hedged read\");\n          }\n          // Ignore this node on next go around.\n          ignored.add(chosenNode.info);\n          dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n          continue; // no need to refresh block locations\n        } catch (InterruptedException e) {\n          // Ignore\n        } catch (ExecutionException e) {\n          // Ignore already logged in the call.\n        }\n      } else {\n        // We are starting up a \u0027hedged\u0027 read. We have a read already\n        // ongoing. Call getBestNodeDNAddrPair instead of chooseDataNode.\n        // If no nodes to do hedged reads against, pass.\n        try {\n          try {\n            chosenNode \u003d getBestNodeDNAddrPair(block, ignored);\n          } catch (IOException ioe) {\n            chosenNode \u003d chooseDataNode(block, ignored);\n          }\n          bb \u003d ByteBuffer.allocate(len);\n          Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n              chosenNode, block, start, end, bb, corruptedBlockMap,\n              hedgedReadId++);\n          Future\u003cByteBuffer\u003e oneMoreRequest \u003d hedgedService\n              .submit(getFromDataNodeCallable);\n          futures.add(oneMoreRequest);\n        } catch (IOException ioe) {\n          if (DFSClient.LOG.isDebugEnabled()) {\n            DFSClient.LOG.debug(\"Failed getting node for hedged read: \"\n                + ioe.getMessage());\n          }\n        }\n        // if not succeeded. Submit callables for each datanode in a loop, wait\n        // for a fixed interval and get the result from the fastest one.\n        try {\n          ByteBuffer result \u003d getFirstToComplete(hedgedService, futures);\n          // cancel the rest.\n          cancelAll(futures);\n          if (result.array() !\u003d buf) { // compare the array pointers\n            dfsClient.getHedgedReadMetrics().incHedgedReadWins();\n            System.arraycopy(result.array(), result.position(), buf, offset,\n                len);\n          } else {\n            dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n          }\n          return;\n        } catch (InterruptedException ie) {\n          // Ignore and retry\n        }\n        // We got here if exception. Ignore this node on next go around IFF\n        // we found a chosenNode to hedge read against.\n        if (chosenNode !\u003d null \u0026\u0026 chosenNode.info !\u003d null) {\n          ignored.add(chosenNode.info);\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "7f6ed7fe365166e8075359f1d0ad035fa876c70f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7055. Add tracing to DFSInputStream (cmccabe)\n",
      "commitDate": "03/10/14 1:35 PM",
      "commitName": "7f6ed7fe365166e8075359f1d0ad035fa876c70f",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "15/07/14 2:10 PM",
      "commitNameOld": "56c0bd4d37ab13b6cbcf860eda852da603ab2f62",
      "commitAuthorOld": "",
      "daysBetweenCommits": 79.98,
      "commitsBetweenForRepo": 820,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,95 +1,98 @@\n   private void hedgedFetchBlockByteRange(LocatedBlock block, long start,\n       long end, byte[] buf, int offset,\n       Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n       throws IOException {\n     ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e futures \u003d new ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e();\n     CompletionService\u003cByteBuffer\u003e hedgedService \u003d\n         new ExecutorCompletionService\u003cByteBuffer\u003e(\n         dfsClient.getHedgedReadsThreadPool());\n     ArrayList\u003cDatanodeInfo\u003e ignored \u003d new ArrayList\u003cDatanodeInfo\u003e();\n     ByteBuffer bb \u003d null;\n     int len \u003d (int) (end - start + 1);\n+    int hedgedReadId \u003d 0;\n     block \u003d getBlockAt(block.getStartOffset(), false);\n     while (true) {\n       // see HDFS-6591, this metric is used to verify/catch unnecessary loops\n       hedgedReadOpsLoopNumForTesting++;\n       DNAddrPair chosenNode \u003d null;\n       // there is no request already executing.\n       if (futures.isEmpty()) {\n         // chooseDataNode is a commitment. If no node, we go to\n         // the NN to reget block locations. Only go here on first read.\n         chosenNode \u003d chooseDataNode(block, ignored);\n         bb \u003d ByteBuffer.wrap(buf, offset, len);\n         Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n-            chosenNode, block, start, end, bb, corruptedBlockMap);\n+            chosenNode, block, start, end, bb, corruptedBlockMap,\n+            hedgedReadId++);\n         Future\u003cByteBuffer\u003e firstRequest \u003d hedgedService\n             .submit(getFromDataNodeCallable);\n         futures.add(firstRequest);\n         try {\n           Future\u003cByteBuffer\u003e future \u003d hedgedService.poll(\n               dfsClient.getHedgedReadTimeout(), TimeUnit.MILLISECONDS);\n           if (future !\u003d null) {\n             future.get();\n             return;\n           }\n           if (DFSClient.LOG.isDebugEnabled()) {\n             DFSClient.LOG.debug(\"Waited \" + dfsClient.getHedgedReadTimeout()\n                 + \"ms to read from \" + chosenNode.info\n                 + \"; spawning hedged read\");\n           }\n           // Ignore this node on next go around.\n           ignored.add(chosenNode.info);\n           dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n           continue; // no need to refresh block locations\n         } catch (InterruptedException e) {\n           // Ignore\n         } catch (ExecutionException e) {\n           // Ignore already logged in the call.\n         }\n       } else {\n         // We are starting up a \u0027hedged\u0027 read. We have a read already\n         // ongoing. Call getBestNodeDNAddrPair instead of chooseDataNode.\n         // If no nodes to do hedged reads against, pass.\n         try {\n           try {\n             chosenNode \u003d getBestNodeDNAddrPair(block.getLocations(), ignored);\n           } catch (IOException ioe) {\n             chosenNode \u003d chooseDataNode(block, ignored);\n           }\n           bb \u003d ByteBuffer.allocate(len);\n           Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n-              chosenNode, block, start, end, bb, corruptedBlockMap);\n+              chosenNode, block, start, end, bb, corruptedBlockMap,\n+              hedgedReadId++);\n           Future\u003cByteBuffer\u003e oneMoreRequest \u003d hedgedService\n               .submit(getFromDataNodeCallable);\n           futures.add(oneMoreRequest);\n         } catch (IOException ioe) {\n           if (DFSClient.LOG.isDebugEnabled()) {\n             DFSClient.LOG.debug(\"Failed getting node for hedged read: \"\n                 + ioe.getMessage());\n           }\n         }\n         // if not succeeded. Submit callables for each datanode in a loop, wait\n         // for a fixed interval and get the result from the fastest one.\n         try {\n           ByteBuffer result \u003d getFirstToComplete(hedgedService, futures);\n           // cancel the rest.\n           cancelAll(futures);\n           if (result.array() !\u003d buf) { // compare the array pointers\n             dfsClient.getHedgedReadMetrics().incHedgedReadWins();\n             System.arraycopy(result.array(), result.position(), buf, offset,\n                 len);\n           } else {\n             dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n           }\n           return;\n         } catch (InterruptedException ie) {\n           // Ignore and retry\n         }\n         // We got here if exception. Ignore this node on next go around IFF\n         // we found a chosenNode to hedge read against.\n         if (chosenNode !\u003d null \u0026\u0026 chosenNode.info !\u003d null) {\n           ignored.add(chosenNode.info);\n         }\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void hedgedFetchBlockByteRange(LocatedBlock block, long start,\n      long end, byte[] buf, int offset,\n      Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n      throws IOException {\n    ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e futures \u003d new ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e();\n    CompletionService\u003cByteBuffer\u003e hedgedService \u003d\n        new ExecutorCompletionService\u003cByteBuffer\u003e(\n        dfsClient.getHedgedReadsThreadPool());\n    ArrayList\u003cDatanodeInfo\u003e ignored \u003d new ArrayList\u003cDatanodeInfo\u003e();\n    ByteBuffer bb \u003d null;\n    int len \u003d (int) (end - start + 1);\n    int hedgedReadId \u003d 0;\n    block \u003d getBlockAt(block.getStartOffset(), false);\n    while (true) {\n      // see HDFS-6591, this metric is used to verify/catch unnecessary loops\n      hedgedReadOpsLoopNumForTesting++;\n      DNAddrPair chosenNode \u003d null;\n      // there is no request already executing.\n      if (futures.isEmpty()) {\n        // chooseDataNode is a commitment. If no node, we go to\n        // the NN to reget block locations. Only go here on first read.\n        chosenNode \u003d chooseDataNode(block, ignored);\n        bb \u003d ByteBuffer.wrap(buf, offset, len);\n        Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n            chosenNode, block, start, end, bb, corruptedBlockMap,\n            hedgedReadId++);\n        Future\u003cByteBuffer\u003e firstRequest \u003d hedgedService\n            .submit(getFromDataNodeCallable);\n        futures.add(firstRequest);\n        try {\n          Future\u003cByteBuffer\u003e future \u003d hedgedService.poll(\n              dfsClient.getHedgedReadTimeout(), TimeUnit.MILLISECONDS);\n          if (future !\u003d null) {\n            future.get();\n            return;\n          }\n          if (DFSClient.LOG.isDebugEnabled()) {\n            DFSClient.LOG.debug(\"Waited \" + dfsClient.getHedgedReadTimeout()\n                + \"ms to read from \" + chosenNode.info\n                + \"; spawning hedged read\");\n          }\n          // Ignore this node on next go around.\n          ignored.add(chosenNode.info);\n          dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n          continue; // no need to refresh block locations\n        } catch (InterruptedException e) {\n          // Ignore\n        } catch (ExecutionException e) {\n          // Ignore already logged in the call.\n        }\n      } else {\n        // We are starting up a \u0027hedged\u0027 read. We have a read already\n        // ongoing. Call getBestNodeDNAddrPair instead of chooseDataNode.\n        // If no nodes to do hedged reads against, pass.\n        try {\n          try {\n            chosenNode \u003d getBestNodeDNAddrPair(block.getLocations(), ignored);\n          } catch (IOException ioe) {\n            chosenNode \u003d chooseDataNode(block, ignored);\n          }\n          bb \u003d ByteBuffer.allocate(len);\n          Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n              chosenNode, block, start, end, bb, corruptedBlockMap,\n              hedgedReadId++);\n          Future\u003cByteBuffer\u003e oneMoreRequest \u003d hedgedService\n              .submit(getFromDataNodeCallable);\n          futures.add(oneMoreRequest);\n        } catch (IOException ioe) {\n          if (DFSClient.LOG.isDebugEnabled()) {\n            DFSClient.LOG.debug(\"Failed getting node for hedged read: \"\n                + ioe.getMessage());\n          }\n        }\n        // if not succeeded. Submit callables for each datanode in a loop, wait\n        // for a fixed interval and get the result from the fastest one.\n        try {\n          ByteBuffer result \u003d getFirstToComplete(hedgedService, futures);\n          // cancel the rest.\n          cancelAll(futures);\n          if (result.array() !\u003d buf) { // compare the array pointers\n            dfsClient.getHedgedReadMetrics().incHedgedReadWins();\n            System.arraycopy(result.array(), result.position(), buf, offset,\n                len);\n          } else {\n            dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n          }\n          return;\n        } catch (InterruptedException ie) {\n          // Ignore and retry\n        }\n        // We got here if exception. Ignore this node on next go around IFF\n        // we found a chosenNode to hedge read against.\n        if (chosenNode !\u003d null \u0026\u0026 chosenNode.info !\u003d null) {\n          ignored.add(chosenNode.info);\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "0ca41a8f35e4f05bb04805a2e0a617850707b4db": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6591. while loop is executed tens of thousands of times in Hedged Read. Contributed by Liang Xie.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1606927 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "30/06/14 1:46 PM",
      "commitName": "0ca41a8f35e4f05bb04805a2e0a617850707b4db",
      "commitAuthor": "Chris Nauroth",
      "commitDateOld": "28/04/14 1:20 PM",
      "commitNameOld": "71aa608b84afcbe19dc38ca7c43c6e750d5df97a",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 63.02,
      "commitsBetweenForRepo": 367,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,86 +1,95 @@\n   private void hedgedFetchBlockByteRange(LocatedBlock block, long start,\n       long end, byte[] buf, int offset,\n       Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n       throws IOException {\n     ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e futures \u003d new ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e();\n+    CompletionService\u003cByteBuffer\u003e hedgedService \u003d\n+        new ExecutorCompletionService\u003cByteBuffer\u003e(\n+        dfsClient.getHedgedReadsThreadPool());\n     ArrayList\u003cDatanodeInfo\u003e ignored \u003d new ArrayList\u003cDatanodeInfo\u003e();\n     ByteBuffer bb \u003d null;\n     int len \u003d (int) (end - start + 1);\n     block \u003d getBlockAt(block.getStartOffset(), false);\n-    // Latch shared by all outstanding reads.  First to finish closes\n-    CountDownLatch hasReceivedResult \u003d new CountDownLatch(1);\n     while (true) {\n+      // see HDFS-6591, this metric is used to verify/catch unnecessary loops\n+      hedgedReadOpsLoopNumForTesting++;\n       DNAddrPair chosenNode \u003d null;\n-      Future\u003cByteBuffer\u003e future \u003d null;\n-      // futures is null if there is no request already executing.\n+      // there is no request already executing.\n       if (futures.isEmpty()) {\n-        // chooseDataNode is a commitment.  If no node, we go to\n-        // the NN to reget block locations.  Only go here on first read.\n+        // chooseDataNode is a commitment. If no node, we go to\n+        // the NN to reget block locations. Only go here on first read.\n         chosenNode \u003d chooseDataNode(block, ignored);\n         bb \u003d ByteBuffer.wrap(buf, offset, len);\n-        future \u003d getHedgedReadFuture(chosenNode, block, start, end, bb,\n-          corruptedBlockMap, hasReceivedResult);\n+        Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n+            chosenNode, block, start, end, bb, corruptedBlockMap);\n+        Future\u003cByteBuffer\u003e firstRequest \u003d hedgedService\n+            .submit(getFromDataNodeCallable);\n+        futures.add(firstRequest);\n         try {\n-          future.get(dfsClient.getHedgedReadTimeout(), TimeUnit.MILLISECONDS);\n-          return;\n-        } catch (TimeoutException e) {\n+          Future\u003cByteBuffer\u003e future \u003d hedgedService.poll(\n+              dfsClient.getHedgedReadTimeout(), TimeUnit.MILLISECONDS);\n+          if (future !\u003d null) {\n+            future.get();\n+            return;\n+          }\n           if (DFSClient.LOG.isDebugEnabled()) {\n-            DFSClient.LOG.debug(\"Waited \" + dfsClient.getHedgedReadTimeout() +\n-              \"ms to read from \" + chosenNode.info + \"; spawning hedged read\");\n+            DFSClient.LOG.debug(\"Waited \" + dfsClient.getHedgedReadTimeout()\n+                + \"ms to read from \" + chosenNode.info\n+                + \"; spawning hedged read\");\n           }\n           // Ignore this node on next go around.\n           ignored.add(chosenNode.info);\n           dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n-          futures.add(future);\n           continue; // no need to refresh block locations\n         } catch (InterruptedException e) {\n           // Ignore\n         } catch (ExecutionException e) {\n           // Ignore already logged in the call.\n         }\n       } else {\n-        // We are starting up a \u0027hedged\u0027 read.  We have a read already\n+        // We are starting up a \u0027hedged\u0027 read. We have a read already\n         // ongoing. Call getBestNodeDNAddrPair instead of chooseDataNode.\n         // If no nodes to do hedged reads against, pass.\n         try {\n-          chosenNode \u003d getBestNodeDNAddrPair(block.getLocations(), ignored);\n+          try {\n+            chosenNode \u003d getBestNodeDNAddrPair(block.getLocations(), ignored);\n+          } catch (IOException ioe) {\n+            chosenNode \u003d chooseDataNode(block, ignored);\n+          }\n           bb \u003d ByteBuffer.allocate(len);\n-          future \u003d getHedgedReadFuture(chosenNode, block, start, end, bb,\n-            corruptedBlockMap, hasReceivedResult);\n-          futures.add(future);\n+          Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n+              chosenNode, block, start, end, bb, corruptedBlockMap);\n+          Future\u003cByteBuffer\u003e oneMoreRequest \u003d hedgedService\n+              .submit(getFromDataNodeCallable);\n+          futures.add(oneMoreRequest);\n         } catch (IOException ioe) {\n           if (DFSClient.LOG.isDebugEnabled()) {\n-            DFSClient.LOG.debug(\"Failed getting node for hedged read: \" +\n-              ioe.getMessage());\n+            DFSClient.LOG.debug(\"Failed getting node for hedged read: \"\n+                + ioe.getMessage());\n           }\n         }\n         // if not succeeded. Submit callables for each datanode in a loop, wait\n         // for a fixed interval and get the result from the fastest one.\n         try {\n-          ByteBuffer result \u003d getFirstToComplete(futures, hasReceivedResult);\n+          ByteBuffer result \u003d getFirstToComplete(hedgedService, futures);\n           // cancel the rest.\n           cancelAll(futures);\n           if (result.array() !\u003d buf) { // compare the array pointers\n             dfsClient.getHedgedReadMetrics().incHedgedReadWins();\n             System.arraycopy(result.array(), result.position(), buf, offset,\n                 len);\n           } else {\n             dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n           }\n           return;\n         } catch (InterruptedException ie) {\n-          // Ignore\n-        } catch (ExecutionException e) {\n-          // exception already handled in the call method. getFirstToComplete\n-          // will remove the failing future from the list. nothing more to do.\n+          // Ignore and retry\n         }\n-        // We got here if exception.  Ignore this node on next go around IFF\n+        // We got here if exception. Ignore this node on next go around IFF\n         // we found a chosenNode to hedge read against.\n         if (chosenNode !\u003d null \u0026\u0026 chosenNode.info !\u003d null) {\n           ignored.add(chosenNode.info);\n         }\n       }\n-      // executed if we get an error from a data node\n-      block \u003d getBlockAt(block.getStartOffset(), false);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void hedgedFetchBlockByteRange(LocatedBlock block, long start,\n      long end, byte[] buf, int offset,\n      Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n      throws IOException {\n    ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e futures \u003d new ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e();\n    CompletionService\u003cByteBuffer\u003e hedgedService \u003d\n        new ExecutorCompletionService\u003cByteBuffer\u003e(\n        dfsClient.getHedgedReadsThreadPool());\n    ArrayList\u003cDatanodeInfo\u003e ignored \u003d new ArrayList\u003cDatanodeInfo\u003e();\n    ByteBuffer bb \u003d null;\n    int len \u003d (int) (end - start + 1);\n    block \u003d getBlockAt(block.getStartOffset(), false);\n    while (true) {\n      // see HDFS-6591, this metric is used to verify/catch unnecessary loops\n      hedgedReadOpsLoopNumForTesting++;\n      DNAddrPair chosenNode \u003d null;\n      // there is no request already executing.\n      if (futures.isEmpty()) {\n        // chooseDataNode is a commitment. If no node, we go to\n        // the NN to reget block locations. Only go here on first read.\n        chosenNode \u003d chooseDataNode(block, ignored);\n        bb \u003d ByteBuffer.wrap(buf, offset, len);\n        Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n            chosenNode, block, start, end, bb, corruptedBlockMap);\n        Future\u003cByteBuffer\u003e firstRequest \u003d hedgedService\n            .submit(getFromDataNodeCallable);\n        futures.add(firstRequest);\n        try {\n          Future\u003cByteBuffer\u003e future \u003d hedgedService.poll(\n              dfsClient.getHedgedReadTimeout(), TimeUnit.MILLISECONDS);\n          if (future !\u003d null) {\n            future.get();\n            return;\n          }\n          if (DFSClient.LOG.isDebugEnabled()) {\n            DFSClient.LOG.debug(\"Waited \" + dfsClient.getHedgedReadTimeout()\n                + \"ms to read from \" + chosenNode.info\n                + \"; spawning hedged read\");\n          }\n          // Ignore this node on next go around.\n          ignored.add(chosenNode.info);\n          dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n          continue; // no need to refresh block locations\n        } catch (InterruptedException e) {\n          // Ignore\n        } catch (ExecutionException e) {\n          // Ignore already logged in the call.\n        }\n      } else {\n        // We are starting up a \u0027hedged\u0027 read. We have a read already\n        // ongoing. Call getBestNodeDNAddrPair instead of chooseDataNode.\n        // If no nodes to do hedged reads against, pass.\n        try {\n          try {\n            chosenNode \u003d getBestNodeDNAddrPair(block.getLocations(), ignored);\n          } catch (IOException ioe) {\n            chosenNode \u003d chooseDataNode(block, ignored);\n          }\n          bb \u003d ByteBuffer.allocate(len);\n          Callable\u003cByteBuffer\u003e getFromDataNodeCallable \u003d getFromOneDataNode(\n              chosenNode, block, start, end, bb, corruptedBlockMap);\n          Future\u003cByteBuffer\u003e oneMoreRequest \u003d hedgedService\n              .submit(getFromDataNodeCallable);\n          futures.add(oneMoreRequest);\n        } catch (IOException ioe) {\n          if (DFSClient.LOG.isDebugEnabled()) {\n            DFSClient.LOG.debug(\"Failed getting node for hedged read: \"\n                + ioe.getMessage());\n          }\n        }\n        // if not succeeded. Submit callables for each datanode in a loop, wait\n        // for a fixed interval and get the result from the fastest one.\n        try {\n          ByteBuffer result \u003d getFirstToComplete(hedgedService, futures);\n          // cancel the rest.\n          cancelAll(futures);\n          if (result.array() !\u003d buf) { // compare the array pointers\n            dfsClient.getHedgedReadMetrics().incHedgedReadWins();\n            System.arraycopy(result.array(), result.position(), buf, offset,\n                len);\n          } else {\n            dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n          }\n          return;\n        } catch (InterruptedException ie) {\n          // Ignore and retry\n        }\n        // We got here if exception. Ignore this node on next go around IFF\n        // we found a chosenNode to hedge read against.\n        if (chosenNode !\u003d null \u0026\u0026 chosenNode.info !\u003d null) {\n          ignored.add(chosenNode.info);\n        }\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "f8904ad299bbcd109e3460f9b8ab9fbb9cebdad4": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6231. DFSClient hangs infinitely if using hedged reads and all eligible datanodes die. Contributed by Chris Nauroth.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1586551 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/04/14 8:48 PM",
      "commitName": "f8904ad299bbcd109e3460f9b8ab9fbb9cebdad4",
      "commitAuthor": "Chris Nauroth",
      "commitDateOld": "01/04/14 10:09 PM",
      "commitNameOld": "f93d99990a9a02ce693cd74466c2e5f127c1f560",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 8.94,
      "commitsBetweenForRepo": 63,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,87 +1,86 @@\n   private void hedgedFetchBlockByteRange(LocatedBlock block, long start,\n       long end, byte[] buf, int offset,\n       Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n       throws IOException {\n-    ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e futures \u003d null;\n+    ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e futures \u003d new ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e();\n     ArrayList\u003cDatanodeInfo\u003e ignored \u003d new ArrayList\u003cDatanodeInfo\u003e();\n     ByteBuffer bb \u003d null;\n     int len \u003d (int) (end - start + 1);\n     block \u003d getBlockAt(block.getStartOffset(), false);\n     // Latch shared by all outstanding reads.  First to finish closes\n     CountDownLatch hasReceivedResult \u003d new CountDownLatch(1);\n     while (true) {\n       DNAddrPair chosenNode \u003d null;\n       Future\u003cByteBuffer\u003e future \u003d null;\n       // futures is null if there is no request already executing.\n-      if (futures \u003d\u003d null) {\n+      if (futures.isEmpty()) {\n         // chooseDataNode is a commitment.  If no node, we go to\n         // the NN to reget block locations.  Only go here on first read.\n         chosenNode \u003d chooseDataNode(block, ignored);\n         bb \u003d ByteBuffer.wrap(buf, offset, len);\n         future \u003d getHedgedReadFuture(chosenNode, block, start, end, bb,\n           corruptedBlockMap, hasReceivedResult);\n         try {\n           future.get(dfsClient.getHedgedReadTimeout(), TimeUnit.MILLISECONDS);\n           return;\n         } catch (TimeoutException e) {\n           if (DFSClient.LOG.isDebugEnabled()) {\n             DFSClient.LOG.debug(\"Waited \" + dfsClient.getHedgedReadTimeout() +\n               \"ms to read from \" + chosenNode.info + \"; spawning hedged read\");\n           }\n           // Ignore this node on next go around.\n           ignored.add(chosenNode.info);\n           dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n-          futures \u003d new ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e();\n           futures.add(future);\n           continue; // no need to refresh block locations\n         } catch (InterruptedException e) {\n           // Ignore\n         } catch (ExecutionException e) {\n           // Ignore already logged in the call.\n         }\n       } else {\n         // We are starting up a \u0027hedged\u0027 read.  We have a read already\n         // ongoing. Call getBestNodeDNAddrPair instead of chooseDataNode.\n         // If no nodes to do hedged reads against, pass.\n         try {\n           chosenNode \u003d getBestNodeDNAddrPair(block.getLocations(), ignored);\n           bb \u003d ByteBuffer.allocate(len);\n           future \u003d getHedgedReadFuture(chosenNode, block, start, end, bb,\n             corruptedBlockMap, hasReceivedResult);\n           futures.add(future);\n         } catch (IOException ioe) {\n           if (DFSClient.LOG.isDebugEnabled()) {\n             DFSClient.LOG.debug(\"Failed getting node for hedged read: \" +\n               ioe.getMessage());\n           }\n         }\n         // if not succeeded. Submit callables for each datanode in a loop, wait\n         // for a fixed interval and get the result from the fastest one.\n         try {\n           ByteBuffer result \u003d getFirstToComplete(futures, hasReceivedResult);\n           // cancel the rest.\n           cancelAll(futures);\n           if (result.array() !\u003d buf) { // compare the array pointers\n             dfsClient.getHedgedReadMetrics().incHedgedReadWins();\n             System.arraycopy(result.array(), result.position(), buf, offset,\n                 len);\n           } else {\n             dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n           }\n           return;\n         } catch (InterruptedException ie) {\n           // Ignore\n         } catch (ExecutionException e) {\n           // exception already handled in the call method. getFirstToComplete\n           // will remove the failing future from the list. nothing more to do.\n         }\n         // We got here if exception.  Ignore this node on next go around IFF\n         // we found a chosenNode to hedge read against.\n         if (chosenNode !\u003d null \u0026\u0026 chosenNode.info !\u003d null) {\n           ignored.add(chosenNode.info);\n         }\n       }\n       // executed if we get an error from a data node\n       block \u003d getBlockAt(block.getStartOffset(), false);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void hedgedFetchBlockByteRange(LocatedBlock block, long start,\n      long end, byte[] buf, int offset,\n      Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n      throws IOException {\n    ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e futures \u003d new ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e();\n    ArrayList\u003cDatanodeInfo\u003e ignored \u003d new ArrayList\u003cDatanodeInfo\u003e();\n    ByteBuffer bb \u003d null;\n    int len \u003d (int) (end - start + 1);\n    block \u003d getBlockAt(block.getStartOffset(), false);\n    // Latch shared by all outstanding reads.  First to finish closes\n    CountDownLatch hasReceivedResult \u003d new CountDownLatch(1);\n    while (true) {\n      DNAddrPair chosenNode \u003d null;\n      Future\u003cByteBuffer\u003e future \u003d null;\n      // futures is null if there is no request already executing.\n      if (futures.isEmpty()) {\n        // chooseDataNode is a commitment.  If no node, we go to\n        // the NN to reget block locations.  Only go here on first read.\n        chosenNode \u003d chooseDataNode(block, ignored);\n        bb \u003d ByteBuffer.wrap(buf, offset, len);\n        future \u003d getHedgedReadFuture(chosenNode, block, start, end, bb,\n          corruptedBlockMap, hasReceivedResult);\n        try {\n          future.get(dfsClient.getHedgedReadTimeout(), TimeUnit.MILLISECONDS);\n          return;\n        } catch (TimeoutException e) {\n          if (DFSClient.LOG.isDebugEnabled()) {\n            DFSClient.LOG.debug(\"Waited \" + dfsClient.getHedgedReadTimeout() +\n              \"ms to read from \" + chosenNode.info + \"; spawning hedged read\");\n          }\n          // Ignore this node on next go around.\n          ignored.add(chosenNode.info);\n          dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n          futures.add(future);\n          continue; // no need to refresh block locations\n        } catch (InterruptedException e) {\n          // Ignore\n        } catch (ExecutionException e) {\n          // Ignore already logged in the call.\n        }\n      } else {\n        // We are starting up a \u0027hedged\u0027 read.  We have a read already\n        // ongoing. Call getBestNodeDNAddrPair instead of chooseDataNode.\n        // If no nodes to do hedged reads against, pass.\n        try {\n          chosenNode \u003d getBestNodeDNAddrPair(block.getLocations(), ignored);\n          bb \u003d ByteBuffer.allocate(len);\n          future \u003d getHedgedReadFuture(chosenNode, block, start, end, bb,\n            corruptedBlockMap, hasReceivedResult);\n          futures.add(future);\n        } catch (IOException ioe) {\n          if (DFSClient.LOG.isDebugEnabled()) {\n            DFSClient.LOG.debug(\"Failed getting node for hedged read: \" +\n              ioe.getMessage());\n          }\n        }\n        // if not succeeded. Submit callables for each datanode in a loop, wait\n        // for a fixed interval and get the result from the fastest one.\n        try {\n          ByteBuffer result \u003d getFirstToComplete(futures, hasReceivedResult);\n          // cancel the rest.\n          cancelAll(futures);\n          if (result.array() !\u003d buf) { // compare the array pointers\n            dfsClient.getHedgedReadMetrics().incHedgedReadWins();\n            System.arraycopy(result.array(), result.position(), buf, offset,\n                len);\n          } else {\n            dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n          }\n          return;\n        } catch (InterruptedException ie) {\n          // Ignore\n        } catch (ExecutionException e) {\n          // exception already handled in the call method. getFirstToComplete\n          // will remove the failing future from the list. nothing more to do.\n        }\n        // We got here if exception.  Ignore this node on next go around IFF\n        // we found a chosenNode to hedge read against.\n        if (chosenNode !\u003d null \u0026\u0026 chosenNode.info !\u003d null) {\n          ignored.add(chosenNode.info);\n        }\n      }\n      // executed if we get an error from a data node\n      block \u003d getBlockAt(block.getStartOffset(), false);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "c0a903da22c65294b232c7530a6a684ee93daba4": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6047 TestPread NPE inside in DFSInputStream hedgedFetchBlockByteRange (stack)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1574205 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/03/14 1:07 PM",
      "commitName": "c0a903da22c65294b232c7530a6a684ee93daba4",
      "commitAuthor": "Michael Stack",
      "commitDateOld": "03/03/14 10:46 PM",
      "commitNameOld": "c94e43c6df096682138c9030274bbdd6d0d91817",
      "commitAuthorOld": "Colin McCabe",
      "daysBetweenCommits": 0.6,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,84 +1,87 @@\n   private void hedgedFetchBlockByteRange(LocatedBlock block, long start,\n       long end, byte[] buf, int offset,\n       Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n       throws IOException {\n     ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e futures \u003d null;\n     ArrayList\u003cDatanodeInfo\u003e ignored \u003d new ArrayList\u003cDatanodeInfo\u003e();\n     ByteBuffer bb \u003d null;\n     int len \u003d (int) (end - start + 1);\n     block \u003d getBlockAt(block.getStartOffset(), false);\n     // Latch shared by all outstanding reads.  First to finish closes\n     CountDownLatch hasReceivedResult \u003d new CountDownLatch(1);\n     while (true) {\n       DNAddrPair chosenNode \u003d null;\n       Future\u003cByteBuffer\u003e future \u003d null;\n       // futures is null if there is no request already executing.\n       if (futures \u003d\u003d null) {\n         // chooseDataNode is a commitment.  If no node, we go to\n         // the NN to reget block locations.  Only go here on first read.\n         chosenNode \u003d chooseDataNode(block, ignored);\n         bb \u003d ByteBuffer.wrap(buf, offset, len);\n         future \u003d getHedgedReadFuture(chosenNode, block, start, end, bb,\n           corruptedBlockMap, hasReceivedResult);\n         try {\n           future.get(dfsClient.getHedgedReadTimeout(), TimeUnit.MILLISECONDS);\n           return;\n         } catch (TimeoutException e) {\n           if (DFSClient.LOG.isDebugEnabled()) {\n             DFSClient.LOG.debug(\"Waited \" + dfsClient.getHedgedReadTimeout() +\n               \"ms to read from \" + chosenNode.info + \"; spawning hedged read\");\n           }\n           // Ignore this node on next go around.\n           ignored.add(chosenNode.info);\n           dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n           futures \u003d new ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e();\n           futures.add(future);\n           continue; // no need to refresh block locations\n         } catch (InterruptedException e) {\n           // Ignore\n         } catch (ExecutionException e) {\n           // Ignore already logged in the call.\n         }\n       } else {\n         // We are starting up a \u0027hedged\u0027 read.  We have a read already\n         // ongoing. Call getBestNodeDNAddrPair instead of chooseDataNode.\n         // If no nodes to do hedged reads against, pass.\n         try {\n           chosenNode \u003d getBestNodeDNAddrPair(block.getLocations(), ignored);\n           bb \u003d ByteBuffer.allocate(len);\n           future \u003d getHedgedReadFuture(chosenNode, block, start, end, bb,\n             corruptedBlockMap, hasReceivedResult);\n           futures.add(future);\n         } catch (IOException ioe) {\n           if (DFSClient.LOG.isDebugEnabled()) {\n             DFSClient.LOG.debug(\"Failed getting node for hedged read: \" +\n               ioe.getMessage());\n           }\n         }\n         // if not succeeded. Submit callables for each datanode in a loop, wait\n         // for a fixed interval and get the result from the fastest one.\n         try {\n           ByteBuffer result \u003d getFirstToComplete(futures, hasReceivedResult);\n           // cancel the rest.\n           cancelAll(futures);\n           if (result.array() !\u003d buf) { // compare the array pointers\n             dfsClient.getHedgedReadMetrics().incHedgedReadWins();\n             System.arraycopy(result.array(), result.position(), buf, offset,\n                 len);\n           } else {\n             dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n           }\n           return;\n         } catch (InterruptedException ie) {\n           // Ignore\n         } catch (ExecutionException e) {\n           // exception already handled in the call method. getFirstToComplete\n           // will remove the failing future from the list. nothing more to do.\n         }\n-        // We got here if exception.  Ignore this node on next go around.\n-        ignored.add(chosenNode.info);\n+        // We got here if exception.  Ignore this node on next go around IFF\n+        // we found a chosenNode to hedge read against.\n+        if (chosenNode !\u003d null \u0026\u0026 chosenNode.info !\u003d null) {\n+          ignored.add(chosenNode.info);\n+        }\n       }\n       // executed if we get an error from a data node\n       block \u003d getBlockAt(block.getStartOffset(), false);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void hedgedFetchBlockByteRange(LocatedBlock block, long start,\n      long end, byte[] buf, int offset,\n      Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n      throws IOException {\n    ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e futures \u003d null;\n    ArrayList\u003cDatanodeInfo\u003e ignored \u003d new ArrayList\u003cDatanodeInfo\u003e();\n    ByteBuffer bb \u003d null;\n    int len \u003d (int) (end - start + 1);\n    block \u003d getBlockAt(block.getStartOffset(), false);\n    // Latch shared by all outstanding reads.  First to finish closes\n    CountDownLatch hasReceivedResult \u003d new CountDownLatch(1);\n    while (true) {\n      DNAddrPair chosenNode \u003d null;\n      Future\u003cByteBuffer\u003e future \u003d null;\n      // futures is null if there is no request already executing.\n      if (futures \u003d\u003d null) {\n        // chooseDataNode is a commitment.  If no node, we go to\n        // the NN to reget block locations.  Only go here on first read.\n        chosenNode \u003d chooseDataNode(block, ignored);\n        bb \u003d ByteBuffer.wrap(buf, offset, len);\n        future \u003d getHedgedReadFuture(chosenNode, block, start, end, bb,\n          corruptedBlockMap, hasReceivedResult);\n        try {\n          future.get(dfsClient.getHedgedReadTimeout(), TimeUnit.MILLISECONDS);\n          return;\n        } catch (TimeoutException e) {\n          if (DFSClient.LOG.isDebugEnabled()) {\n            DFSClient.LOG.debug(\"Waited \" + dfsClient.getHedgedReadTimeout() +\n              \"ms to read from \" + chosenNode.info + \"; spawning hedged read\");\n          }\n          // Ignore this node on next go around.\n          ignored.add(chosenNode.info);\n          dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n          futures \u003d new ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e();\n          futures.add(future);\n          continue; // no need to refresh block locations\n        } catch (InterruptedException e) {\n          // Ignore\n        } catch (ExecutionException e) {\n          // Ignore already logged in the call.\n        }\n      } else {\n        // We are starting up a \u0027hedged\u0027 read.  We have a read already\n        // ongoing. Call getBestNodeDNAddrPair instead of chooseDataNode.\n        // If no nodes to do hedged reads against, pass.\n        try {\n          chosenNode \u003d getBestNodeDNAddrPair(block.getLocations(), ignored);\n          bb \u003d ByteBuffer.allocate(len);\n          future \u003d getHedgedReadFuture(chosenNode, block, start, end, bb,\n            corruptedBlockMap, hasReceivedResult);\n          futures.add(future);\n        } catch (IOException ioe) {\n          if (DFSClient.LOG.isDebugEnabled()) {\n            DFSClient.LOG.debug(\"Failed getting node for hedged read: \" +\n              ioe.getMessage());\n          }\n        }\n        // if not succeeded. Submit callables for each datanode in a loop, wait\n        // for a fixed interval and get the result from the fastest one.\n        try {\n          ByteBuffer result \u003d getFirstToComplete(futures, hasReceivedResult);\n          // cancel the rest.\n          cancelAll(futures);\n          if (result.array() !\u003d buf) { // compare the array pointers\n            dfsClient.getHedgedReadMetrics().incHedgedReadWins();\n            System.arraycopy(result.array(), result.position(), buf, offset,\n                len);\n          } else {\n            dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n          }\n          return;\n        } catch (InterruptedException ie) {\n          // Ignore\n        } catch (ExecutionException e) {\n          // exception already handled in the call method. getFirstToComplete\n          // will remove the failing future from the list. nothing more to do.\n        }\n        // We got here if exception.  Ignore this node on next go around IFF\n        // we found a chosenNode to hedge read against.\n        if (chosenNode !\u003d null \u0026\u0026 chosenNode.info !\u003d null) {\n          ignored.add(chosenNode.info);\n        }\n      }\n      // executed if we get an error from a data node\n      block \u003d getBlockAt(block.getStartOffset(), false);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "17db74a1c1972392a5aba48a3e0334dcd6c76487": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-5776 Support \u0027hedged\u0027 reads in DFSClient\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1571466 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/02/14 2:34 PM",
      "commitName": "17db74a1c1972392a5aba48a3e0334dcd6c76487",
      "commitAuthor": "Michael Stack",
      "diff": "@@ -0,0 +1,84 @@\n+  private void hedgedFetchBlockByteRange(LocatedBlock block, long start,\n+      long end, byte[] buf, int offset,\n+      Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n+      throws IOException {\n+    ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e futures \u003d null;\n+    ArrayList\u003cDatanodeInfo\u003e ignored \u003d new ArrayList\u003cDatanodeInfo\u003e();\n+    ByteBuffer bb \u003d null;\n+    int len \u003d (int) (end - start + 1);\n+    block \u003d getBlockAt(block.getStartOffset(), false);\n+    // Latch shared by all outstanding reads.  First to finish closes\n+    CountDownLatch hasReceivedResult \u003d new CountDownLatch(1);\n+    while (true) {\n+      DNAddrPair chosenNode \u003d null;\n+      Future\u003cByteBuffer\u003e future \u003d null;\n+      // futures is null if there is no request already executing.\n+      if (futures \u003d\u003d null) {\n+        // chooseDataNode is a commitment.  If no node, we go to\n+        // the NN to reget block locations.  Only go here on first read.\n+        chosenNode \u003d chooseDataNode(block, ignored);\n+        bb \u003d ByteBuffer.wrap(buf, offset, len);\n+        future \u003d getHedgedReadFuture(chosenNode, block, start, end, bb,\n+          corruptedBlockMap, hasReceivedResult);\n+        try {\n+          future.get(dfsClient.getHedgedReadTimeout(), TimeUnit.MILLISECONDS);\n+          return;\n+        } catch (TimeoutException e) {\n+          if (DFSClient.LOG.isDebugEnabled()) {\n+            DFSClient.LOG.debug(\"Waited \" + dfsClient.getHedgedReadTimeout() +\n+              \"ms to read from \" + chosenNode.info + \"; spawning hedged read\");\n+          }\n+          // Ignore this node on next go around.\n+          ignored.add(chosenNode.info);\n+          dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n+          futures \u003d new ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e();\n+          futures.add(future);\n+          continue; // no need to refresh block locations\n+        } catch (InterruptedException e) {\n+          // Ignore\n+        } catch (ExecutionException e) {\n+          // Ignore already logged in the call.\n+        }\n+      } else {\n+        // We are starting up a \u0027hedged\u0027 read.  We have a read already\n+        // ongoing. Call getBestNodeDNAddrPair instead of chooseDataNode.\n+        // If no nodes to do hedged reads against, pass.\n+        try {\n+          chosenNode \u003d getBestNodeDNAddrPair(block.getLocations(), ignored);\n+          bb \u003d ByteBuffer.allocate(len);\n+          future \u003d getHedgedReadFuture(chosenNode, block, start, end, bb,\n+            corruptedBlockMap, hasReceivedResult);\n+          futures.add(future);\n+        } catch (IOException ioe) {\n+          if (DFSClient.LOG.isDebugEnabled()) {\n+            DFSClient.LOG.debug(\"Failed getting node for hedged read: \" +\n+              ioe.getMessage());\n+          }\n+        }\n+        // if not succeeded. Submit callables for each datanode in a loop, wait\n+        // for a fixed interval and get the result from the fastest one.\n+        try {\n+          ByteBuffer result \u003d getFirstToComplete(futures, hasReceivedResult);\n+          // cancel the rest.\n+          cancelAll(futures);\n+          if (result.array() !\u003d buf) { // compare the array pointers\n+            dfsClient.getHedgedReadMetrics().incHedgedReadWins();\n+            System.arraycopy(result.array(), result.position(), buf, offset,\n+                len);\n+          } else {\n+            dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n+          }\n+          return;\n+        } catch (InterruptedException ie) {\n+          // Ignore\n+        } catch (ExecutionException e) {\n+          // exception already handled in the call method. getFirstToComplete\n+          // will remove the failing future from the list. nothing more to do.\n+        }\n+        // We got here if exception.  Ignore this node on next go around.\n+        ignored.add(chosenNode.info);\n+      }\n+      // executed if we get an error from a data node\n+      block \u003d getBlockAt(block.getStartOffset(), false);\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void hedgedFetchBlockByteRange(LocatedBlock block, long start,\n      long end, byte[] buf, int offset,\n      Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap)\n      throws IOException {\n    ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e futures \u003d null;\n    ArrayList\u003cDatanodeInfo\u003e ignored \u003d new ArrayList\u003cDatanodeInfo\u003e();\n    ByteBuffer bb \u003d null;\n    int len \u003d (int) (end - start + 1);\n    block \u003d getBlockAt(block.getStartOffset(), false);\n    // Latch shared by all outstanding reads.  First to finish closes\n    CountDownLatch hasReceivedResult \u003d new CountDownLatch(1);\n    while (true) {\n      DNAddrPair chosenNode \u003d null;\n      Future\u003cByteBuffer\u003e future \u003d null;\n      // futures is null if there is no request already executing.\n      if (futures \u003d\u003d null) {\n        // chooseDataNode is a commitment.  If no node, we go to\n        // the NN to reget block locations.  Only go here on first read.\n        chosenNode \u003d chooseDataNode(block, ignored);\n        bb \u003d ByteBuffer.wrap(buf, offset, len);\n        future \u003d getHedgedReadFuture(chosenNode, block, start, end, bb,\n          corruptedBlockMap, hasReceivedResult);\n        try {\n          future.get(dfsClient.getHedgedReadTimeout(), TimeUnit.MILLISECONDS);\n          return;\n        } catch (TimeoutException e) {\n          if (DFSClient.LOG.isDebugEnabled()) {\n            DFSClient.LOG.debug(\"Waited \" + dfsClient.getHedgedReadTimeout() +\n              \"ms to read from \" + chosenNode.info + \"; spawning hedged read\");\n          }\n          // Ignore this node on next go around.\n          ignored.add(chosenNode.info);\n          dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n          futures \u003d new ArrayList\u003cFuture\u003cByteBuffer\u003e\u003e();\n          futures.add(future);\n          continue; // no need to refresh block locations\n        } catch (InterruptedException e) {\n          // Ignore\n        } catch (ExecutionException e) {\n          // Ignore already logged in the call.\n        }\n      } else {\n        // We are starting up a \u0027hedged\u0027 read.  We have a read already\n        // ongoing. Call getBestNodeDNAddrPair instead of chooseDataNode.\n        // If no nodes to do hedged reads against, pass.\n        try {\n          chosenNode \u003d getBestNodeDNAddrPair(block.getLocations(), ignored);\n          bb \u003d ByteBuffer.allocate(len);\n          future \u003d getHedgedReadFuture(chosenNode, block, start, end, bb,\n            corruptedBlockMap, hasReceivedResult);\n          futures.add(future);\n        } catch (IOException ioe) {\n          if (DFSClient.LOG.isDebugEnabled()) {\n            DFSClient.LOG.debug(\"Failed getting node for hedged read: \" +\n              ioe.getMessage());\n          }\n        }\n        // if not succeeded. Submit callables for each datanode in a loop, wait\n        // for a fixed interval and get the result from the fastest one.\n        try {\n          ByteBuffer result \u003d getFirstToComplete(futures, hasReceivedResult);\n          // cancel the rest.\n          cancelAll(futures);\n          if (result.array() !\u003d buf) { // compare the array pointers\n            dfsClient.getHedgedReadMetrics().incHedgedReadWins();\n            System.arraycopy(result.array(), result.position(), buf, offset,\n                len);\n          } else {\n            dfsClient.getHedgedReadMetrics().incHedgedReadOps();\n          }\n          return;\n        } catch (InterruptedException ie) {\n          // Ignore\n        } catch (ExecutionException e) {\n          // exception already handled in the call method. getFirstToComplete\n          // will remove the failing future from the list. nothing more to do.\n        }\n        // We got here if exception.  Ignore this node on next go around.\n        ignored.add(chosenNode.info);\n      }\n      // executed if we get an error from a data node\n      block \u003d getBlockAt(block.getStartOffset(), false);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java"
    }
  }
}