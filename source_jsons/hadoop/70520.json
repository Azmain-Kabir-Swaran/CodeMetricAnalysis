{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "PageBlobInputStream.java",
  "functionName": "ensureDataInBuffer",
  "functionId": "ensureDataInBuffer",
  "sourceFilePath": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/PageBlobInputStream.java",
  "functionStartLine": 195,
  "functionEndLine": 226,
  "numCommitsSeen": 5,
  "timeTaken": 1347,
  "changeHistory": [
    "5b11b9fd413470e134ecdc7c50468f8c7b39fa50",
    "2217e2f8ff418b88eac6ad36cafe3a9795a11f40"
  ],
  "changeHistoryShort": {
    "5b11b9fd413470e134ecdc7c50468f8c7b39fa50": "Ybodychange",
    "2217e2f8ff418b88eac6ad36cafe3a9795a11f40": "Yintroduced"
  },
  "changeHistoryDetails": {
    "5b11b9fd413470e134ecdc7c50468f8c7b39fa50": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-15446. WASB: PageBlobInputStream.skip breaks HBASE replication.\nContributed by Thomas Marquardt\n",
      "commitDate": "07/05/18 3:54 AM",
      "commitName": "5b11b9fd413470e134ecdc7c50468f8c7b39fa50",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "28/11/17 3:52 AM",
      "commitNameOld": "0ea182d0faa35c726dcb37249d48786bfc8ca04c",
      "commitAuthorOld": "Steve Loughran",
      "daysBetweenCommits": 159.96,
      "commitsBetweenForRepo": 1638,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,34 +1,32 @@\n   private synchronized boolean ensureDataInBuffer() throws IOException {\n     if (dataAvailableInBuffer()) {\n       // We still have some data in our buffer.\n       return true;\n     }\n     currentBuffer \u003d null;\n+    currentBufferOffset \u003d 0;\n+    currentBufferLength \u003d 0;\n     if (numberOfPagesRemaining \u003d\u003d 0) {\n       // No more data to read.\n       return false;\n     }\n     final long pagesToRead \u003d Math.min(MAX_PAGES_PER_DOWNLOAD,\n         numberOfPagesRemaining);\n     final int bufferSize \u003d (int) (pagesToRead * PAGE_SIZE);\n  \n     // Download page to current buffer.\n     try {\n       // Create a byte array output stream to capture the results of the\n       // download.\n       ByteArrayOutputStream baos \u003d new ByteArrayOutputStream(bufferSize);\n       blob.downloadRange(currentOffsetInBlob, bufferSize, baos,\n           withMD5Checking(), opContext);\n-      currentBuffer \u003d baos.toByteArray();\n+      validateDataIntegrity(baos.toByteArray());\n     } catch (StorageException e) {\n       throw new IOException(e);\n     }\n     numberOfPagesRemaining -\u003d pagesToRead;\n     currentOffsetInBlob +\u003d bufferSize;\n-    currentOffsetInBuffer \u003d PAGE_HEADER_SIZE;\n-\n-    // Since we just downloaded a new buffer, validate its consistency.\n-    validateCurrentBufferConsistency();\n \n     return true;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized boolean ensureDataInBuffer() throws IOException {\n    if (dataAvailableInBuffer()) {\n      // We still have some data in our buffer.\n      return true;\n    }\n    currentBuffer \u003d null;\n    currentBufferOffset \u003d 0;\n    currentBufferLength \u003d 0;\n    if (numberOfPagesRemaining \u003d\u003d 0) {\n      // No more data to read.\n      return false;\n    }\n    final long pagesToRead \u003d Math.min(MAX_PAGES_PER_DOWNLOAD,\n        numberOfPagesRemaining);\n    final int bufferSize \u003d (int) (pagesToRead * PAGE_SIZE);\n \n    // Download page to current buffer.\n    try {\n      // Create a byte array output stream to capture the results of the\n      // download.\n      ByteArrayOutputStream baos \u003d new ByteArrayOutputStream(bufferSize);\n      blob.downloadRange(currentOffsetInBlob, bufferSize, baos,\n          withMD5Checking(), opContext);\n      validateDataIntegrity(baos.toByteArray());\n    } catch (StorageException e) {\n      throw new IOException(e);\n    }\n    numberOfPagesRemaining -\u003d pagesToRead;\n    currentOffsetInBlob +\u003d bufferSize;\n\n    return true;\n  }",
      "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/PageBlobInputStream.java",
      "extendedDetails": {}
    },
    "2217e2f8ff418b88eac6ad36cafe3a9795a11f40": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-10809. hadoop-azure: page blob support. Contributed by Dexter Bradshaw, Mostafa Elhemali, Eric Hanson, and Mike Liddell.\n",
      "commitDate": "08/10/14 2:20 PM",
      "commitName": "2217e2f8ff418b88eac6ad36cafe3a9795a11f40",
      "commitAuthor": "cnauroth",
      "diff": "@@ -0,0 +1,34 @@\n+  private synchronized boolean ensureDataInBuffer() throws IOException {\n+    if (dataAvailableInBuffer()) {\n+      // We still have some data in our buffer.\n+      return true;\n+    }\n+    currentBuffer \u003d null;\n+    if (numberOfPagesRemaining \u003d\u003d 0) {\n+      // No more data to read.\n+      return false;\n+    }\n+    final long pagesToRead \u003d Math.min(MAX_PAGES_PER_DOWNLOAD,\n+        numberOfPagesRemaining);\n+    final int bufferSize \u003d (int) (pagesToRead * PAGE_SIZE);\n+ \n+    // Download page to current buffer.\n+    try {\n+      // Create a byte array output stream to capture the results of the\n+      // download.\n+      ByteArrayOutputStream baos \u003d new ByteArrayOutputStream(bufferSize);\n+      blob.downloadRange(currentOffsetInBlob, bufferSize, baos,\n+          withMD5Checking(), opContext);\n+      currentBuffer \u003d baos.toByteArray();\n+    } catch (StorageException e) {\n+      throw new IOException(e);\n+    }\n+    numberOfPagesRemaining -\u003d pagesToRead;\n+    currentOffsetInBlob +\u003d bufferSize;\n+    currentOffsetInBuffer \u003d PAGE_HEADER_SIZE;\n+\n+    // Since we just downloaded a new buffer, validate its consistency.\n+    validateCurrentBufferConsistency();\n+\n+    return true;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private synchronized boolean ensureDataInBuffer() throws IOException {\n    if (dataAvailableInBuffer()) {\n      // We still have some data in our buffer.\n      return true;\n    }\n    currentBuffer \u003d null;\n    if (numberOfPagesRemaining \u003d\u003d 0) {\n      // No more data to read.\n      return false;\n    }\n    final long pagesToRead \u003d Math.min(MAX_PAGES_PER_DOWNLOAD,\n        numberOfPagesRemaining);\n    final int bufferSize \u003d (int) (pagesToRead * PAGE_SIZE);\n \n    // Download page to current buffer.\n    try {\n      // Create a byte array output stream to capture the results of the\n      // download.\n      ByteArrayOutputStream baos \u003d new ByteArrayOutputStream(bufferSize);\n      blob.downloadRange(currentOffsetInBlob, bufferSize, baos,\n          withMD5Checking(), opContext);\n      currentBuffer \u003d baos.toByteArray();\n    } catch (StorageException e) {\n      throw new IOException(e);\n    }\n    numberOfPagesRemaining -\u003d pagesToRead;\n    currentOffsetInBlob +\u003d bufferSize;\n    currentOffsetInBuffer \u003d PAGE_HEADER_SIZE;\n\n    // Since we just downloaded a new buffer, validate its consistency.\n    validateCurrentBufferConsistency();\n\n    return true;\n  }",
      "path": "hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azure/PageBlobInputStream.java"
    }
  }
}