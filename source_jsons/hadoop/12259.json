{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DataNode.java",
  "functionName": "handleVolumeFailures",
  "functionId": "handleVolumeFailures___unhealthyVolumes-Set__FsVolumeSpi__",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
  "functionStartLine": 3462,
  "functionEndLine": 3488,
  "numCommitsSeen": 358,
  "timeTaken": 2734,
  "changeHistory": [
    "6191d4b4a0919863fda78e549ab6c60022e3ebc2",
    "dfcb331ba3516264398121c9f23af3a79c0509cc"
  ],
  "changeHistoryShort": {
    "6191d4b4a0919863fda78e549ab6c60022e3ebc2": "Ybodychange",
    "dfcb331ba3516264398121c9f23af3a79c0509cc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "6191d4b4a0919863fda78e549ab6c60022e3ebc2": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-15158. The number of failed volumes mismatch with volumeFailures of Datanode metrics. Contributed by Yang Yun.\n",
      "commitDate": "09/02/20 10:02 AM",
      "commitName": "6191d4b4a0919863fda78e549ab6c60022e3ebc2",
      "commitAuthor": "Ayush Saxena",
      "commitDateOld": "07/02/20 1:21 AM",
      "commitNameOld": "7dac7e1d13eaf0eac04fe805c7502dcecd597979",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 2.36,
      "commitsBetweenForRepo": 7,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,27 +1,27 @@\n   private void handleVolumeFailures(Set\u003cFsVolumeSpi\u003e unhealthyVolumes) {\n     if (unhealthyVolumes.isEmpty()) {\n       LOG.debug(\"handleVolumeFailures done with empty \" +\n           \"unhealthyVolumes\");\n       return;\n     }\n \n     data.handleVolumeFailures(unhealthyVolumes);\n-    Set\u003cStorageLocation\u003e unhealthyLocations \u003d new HashSet\u003c\u003e(\n-        unhealthyVolumes.size());\n+    int failedNumber \u003d unhealthyVolumes.size();\n+    Set\u003cStorageLocation\u003e unhealthyLocations \u003d new HashSet\u003c\u003e(failedNumber);\n \n     StringBuilder sb \u003d new StringBuilder(\"DataNode failed volumes:\");\n     for (FsVolumeSpi vol : unhealthyVolumes) {\n       unhealthyLocations.add(vol.getStorageLocation());\n       sb.append(vol.getStorageLocation()).append(\";\");\n     }\n \n     try {\n       // Remove all unhealthy volumes from DataNode.\n       removeVolumes(unhealthyLocations, false);\n     } catch (IOException e) {\n       LOG.warn(\"Error occurred when removing unhealthy storage dirs\", e);\n     }\n     LOG.debug(\"{}\", sb);\n-      // send blockreport regarding volume failure\n-    handleDiskError(sb.toString());\n+    // send blockreport regarding volume failure\n+    handleDiskError(sb.toString(), failedNumber);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void handleVolumeFailures(Set\u003cFsVolumeSpi\u003e unhealthyVolumes) {\n    if (unhealthyVolumes.isEmpty()) {\n      LOG.debug(\"handleVolumeFailures done with empty \" +\n          \"unhealthyVolumes\");\n      return;\n    }\n\n    data.handleVolumeFailures(unhealthyVolumes);\n    int failedNumber \u003d unhealthyVolumes.size();\n    Set\u003cStorageLocation\u003e unhealthyLocations \u003d new HashSet\u003c\u003e(failedNumber);\n\n    StringBuilder sb \u003d new StringBuilder(\"DataNode failed volumes:\");\n    for (FsVolumeSpi vol : unhealthyVolumes) {\n      unhealthyLocations.add(vol.getStorageLocation());\n      sb.append(vol.getStorageLocation()).append(\";\");\n    }\n\n    try {\n      // Remove all unhealthy volumes from DataNode.\n      removeVolumes(unhealthyLocations, false);\n    } catch (IOException e) {\n      LOG.warn(\"Error occurred when removing unhealthy storage dirs\", e);\n    }\n    LOG.debug(\"{}\", sb);\n    // send blockreport regarding volume failure\n    handleDiskError(sb.toString(), failedNumber);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
      "extendedDetails": {}
    },
    "dfcb331ba3516264398121c9f23af3a79c0509cc": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-13076: [SPS]: Addendum. Resolve conflicts after rebasing branch to trunk. Contributed by Rakesh R.\n",
      "commitDate": "12/08/18 3:06 AM",
      "commitName": "dfcb331ba3516264398121c9f23af3a79c0509cc",
      "commitAuthor": "Rakesh Radhakrishnan",
      "diff": "@@ -0,0 +1,27 @@\n+  private void handleVolumeFailures(Set\u003cFsVolumeSpi\u003e unhealthyVolumes) {\n+    if (unhealthyVolumes.isEmpty()) {\n+      LOG.debug(\"handleVolumeFailures done with empty \" +\n+          \"unhealthyVolumes\");\n+      return;\n+    }\n+\n+    data.handleVolumeFailures(unhealthyVolumes);\n+    Set\u003cStorageLocation\u003e unhealthyLocations \u003d new HashSet\u003c\u003e(\n+        unhealthyVolumes.size());\n+\n+    StringBuilder sb \u003d new StringBuilder(\"DataNode failed volumes:\");\n+    for (FsVolumeSpi vol : unhealthyVolumes) {\n+      unhealthyLocations.add(vol.getStorageLocation());\n+      sb.append(vol.getStorageLocation()).append(\";\");\n+    }\n+\n+    try {\n+      // Remove all unhealthy volumes from DataNode.\n+      removeVolumes(unhealthyLocations, false);\n+    } catch (IOException e) {\n+      LOG.warn(\"Error occurred when removing unhealthy storage dirs\", e);\n+    }\n+    LOG.debug(\"{}\", sb);\n+      // send blockreport regarding volume failure\n+    handleDiskError(sb.toString());\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void handleVolumeFailures(Set\u003cFsVolumeSpi\u003e unhealthyVolumes) {\n    if (unhealthyVolumes.isEmpty()) {\n      LOG.debug(\"handleVolumeFailures done with empty \" +\n          \"unhealthyVolumes\");\n      return;\n    }\n\n    data.handleVolumeFailures(unhealthyVolumes);\n    Set\u003cStorageLocation\u003e unhealthyLocations \u003d new HashSet\u003c\u003e(\n        unhealthyVolumes.size());\n\n    StringBuilder sb \u003d new StringBuilder(\"DataNode failed volumes:\");\n    for (FsVolumeSpi vol : unhealthyVolumes) {\n      unhealthyLocations.add(vol.getStorageLocation());\n      sb.append(vol.getStorageLocation()).append(\";\");\n    }\n\n    try {\n      // Remove all unhealthy volumes from DataNode.\n      removeVolumes(unhealthyLocations, false);\n    } catch (IOException e) {\n      LOG.warn(\"Error occurred when removing unhealthy storage dirs\", e);\n    }\n    LOG.debug(\"{}\", sb);\n      // send blockreport regarding volume failure\n    handleDiskError(sb.toString());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java"
    }
  }
}