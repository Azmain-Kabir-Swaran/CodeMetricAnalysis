{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "FsDatasetImpl.java",
  "functionName": "getTmpInputStreams",
  "functionId": "getTmpInputStreams___b-ExtendedBlock__blkOffset-long__metaOffset-long",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
  "functionStartLine": 900,
  "functionEndLine": 920,
  "numCommitsSeen": 309,
  "timeTaken": 15356,
  "changeHistory": [
    "d7c136b9ed6d99e1b03f5b89723b3a20df359ba8",
    "6ba9587d370fbf39c129c08c00ebbb894ccc1389",
    "86c9862bec0248d671e657aa56094a2919b8ac14",
    "8c0638471f8f1dd47667b2d6727d4d2d54e4b48c",
    "d085eb15d7ca7b43a69bd70bad4e2ea601ba2ae0",
    "5a0051f4da6e102846d795a7965a6a18216d74f7",
    "b7f4a3156c0f5c600816c469637237ba6c9b330c",
    "5cdb7e5ce7f0c3129749be8f29e2f11c0e0f2269",
    "bc13dfb1426944ce45293cb8f444239a7406762c",
    "662b1887af4e39f3eadd7dda4953c7f2529b43bc",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "d7c136b9ed6d99e1b03f5b89723b3a20df359ba8": "Ybodychange",
    "6ba9587d370fbf39c129c08c00ebbb894ccc1389": "Ybodychange",
    "86c9862bec0248d671e657aa56094a2919b8ac14": "Ybodychange",
    "8c0638471f8f1dd47667b2d6727d4d2d54e4b48c": "Ymultichange(Ymodifierchange,Ybodychange)",
    "d085eb15d7ca7b43a69bd70bad4e2ea601ba2ae0": "Ymultichange(Yparameterchange,Ybodychange)",
    "5a0051f4da6e102846d795a7965a6a18216d74f7": "Ybodychange",
    "b7f4a3156c0f5c600816c469637237ba6c9b330c": "Ybodychange",
    "5cdb7e5ce7f0c3129749be8f29e2f11c0e0f2269": "Ybodychange",
    "bc13dfb1426944ce45293cb8f444239a7406762c": "Ymovefromfile",
    "662b1887af4e39f3eadd7dda4953c7f2529b43bc": "Ymultichange(Yreturntypechange,Ybodychange)",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "d7c136b9ed6d99e1b03f5b89723b3a20df359ba8": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-15150. Introduce read write lock to Datanode. Contributed Stephen O\u0027Donnell.\n\nSigned-off-by: Wei-Chiu Chuang \u003cweichiu@apache.org\u003e\n",
      "commitDate": "11/02/20 8:00 AM",
      "commitName": "d7c136b9ed6d99e1b03f5b89723b3a20df359ba8",
      "commitAuthor": "Stephen O\u0027Donnell",
      "commitDateOld": "28/01/20 10:10 AM",
      "commitNameOld": "1839c467f60cbb8592d446694ec3d7710cda5142",
      "commitAuthorOld": "Inigo Goiri",
      "daysBetweenCommits": 13.91,
      "commitsBetweenForRepo": 33,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,21 @@\n   public ReplicaInputStreams getTmpInputStreams(ExtendedBlock b,\n       long blkOffset, long metaOffset) throws IOException {\n-    try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n+    try (AutoCloseableLock lock \u003d datasetWriteLock.acquire()) {\n       ReplicaInfo info \u003d getReplicaInfo(b);\n       FsVolumeReference ref \u003d info.getVolume().obtainReference();\n       try {\n         InputStream blockInStream \u003d info.getDataInputStream(blkOffset);\n         try {\n           InputStream metaInStream \u003d info.getMetadataInputStream(metaOffset);\n           return new ReplicaInputStreams(\n               blockInStream, metaInStream, ref, datanode.getFileIoProvider());\n         } catch (IOException e) {\n           IOUtils.cleanup(null, blockInStream);\n           throw e;\n         }\n       } catch (IOException e) {\n         IOUtils.cleanup(null, ref);\n         throw e;\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public ReplicaInputStreams getTmpInputStreams(ExtendedBlock b,\n      long blkOffset, long metaOffset) throws IOException {\n    try (AutoCloseableLock lock \u003d datasetWriteLock.acquire()) {\n      ReplicaInfo info \u003d getReplicaInfo(b);\n      FsVolumeReference ref \u003d info.getVolume().obtainReference();\n      try {\n        InputStream blockInStream \u003d info.getDataInputStream(blkOffset);\n        try {\n          InputStream metaInStream \u003d info.getMetadataInputStream(metaOffset);\n          return new ReplicaInputStreams(\n              blockInStream, metaInStream, ref, datanode.getFileIoProvider());\n        } catch (IOException e) {\n          IOUtils.cleanup(null, blockInStream);\n          throw e;\n        }\n      } catch (IOException e) {\n        IOUtils.cleanup(null, ref);\n        throw e;\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "6ba9587d370fbf39c129c08c00ebbb894ccc1389": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10958. Add instrumentation hooks around Datanode disk IO.\n",
      "commitDate": "14/12/16 11:18 AM",
      "commitName": "6ba9587d370fbf39c129c08c00ebbb894ccc1389",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "06/12/16 11:05 AM",
      "commitNameOld": "df983b524ab68ea0c70cee9033bfff2d28052cbf",
      "commitAuthorOld": "Xiaoyu Yao",
      "daysBetweenCommits": 8.01,
      "commitsBetweenForRepo": 51,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,20 +1,21 @@\n   public ReplicaInputStreams getTmpInputStreams(ExtendedBlock b,\n       long blkOffset, long metaOffset) throws IOException {\n     try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n       ReplicaInfo info \u003d getReplicaInfo(b);\n       FsVolumeReference ref \u003d info.getVolume().obtainReference();\n       try {\n         InputStream blockInStream \u003d info.getDataInputStream(blkOffset);\n         try {\n           InputStream metaInStream \u003d info.getMetadataInputStream(metaOffset);\n-          return new ReplicaInputStreams(blockInStream, metaInStream, ref);\n+          return new ReplicaInputStreams(\n+              blockInStream, metaInStream, ref, datanode.getFileIoProvider());\n         } catch (IOException e) {\n           IOUtils.cleanup(null, blockInStream);\n           throw e;\n         }\n       } catch (IOException e) {\n         IOUtils.cleanup(null, ref);\n         throw e;\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public ReplicaInputStreams getTmpInputStreams(ExtendedBlock b,\n      long blkOffset, long metaOffset) throws IOException {\n    try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n      ReplicaInfo info \u003d getReplicaInfo(b);\n      FsVolumeReference ref \u003d info.getVolume().obtainReference();\n      try {\n        InputStream blockInStream \u003d info.getDataInputStream(blkOffset);\n        try {\n          InputStream metaInStream \u003d info.getMetadataInputStream(metaOffset);\n          return new ReplicaInputStreams(\n              blockInStream, metaInStream, ref, datanode.getFileIoProvider());\n        } catch (IOException e) {\n          IOUtils.cleanup(null, blockInStream);\n          throw e;\n        }\n      } catch (IOException e) {\n        IOUtils.cleanup(null, ref);\n        throw e;\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "86c9862bec0248d671e657aa56094a2919b8ac14": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10636. Modify ReplicaInfo to remove the assumption that replica metadata and data are stored in java.io.File. (Virajith Jalaparti via lei)\n",
      "commitDate": "13/09/16 12:54 PM",
      "commitName": "86c9862bec0248d671e657aa56094a2919b8ac14",
      "commitAuthor": "Lei Xu",
      "commitDateOld": "10/09/16 6:22 PM",
      "commitNameOld": "a99bf26a0899bcc4307c3a242c8414eaef555aa7",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 2.77,
      "commitsBetweenForRepo": 15,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,20 @@\n   public ReplicaInputStreams getTmpInputStreams(ExtendedBlock b,\n       long blkOffset, long metaOffset) throws IOException {\n     try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n       ReplicaInfo info \u003d getReplicaInfo(b);\n       FsVolumeReference ref \u003d info.getVolume().obtainReference();\n       try {\n-        InputStream blockInStream \u003d openAndSeek(info.getBlockFile(), blkOffset);\n+        InputStream blockInStream \u003d info.getDataInputStream(blkOffset);\n         try {\n-          InputStream metaInStream \u003d\n-              openAndSeek(info.getMetaFile(), metaOffset);\n+          InputStream metaInStream \u003d info.getMetadataInputStream(metaOffset);\n           return new ReplicaInputStreams(blockInStream, metaInStream, ref);\n         } catch (IOException e) {\n           IOUtils.cleanup(null, blockInStream);\n           throw e;\n         }\n       } catch (IOException e) {\n         IOUtils.cleanup(null, ref);\n         throw e;\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public ReplicaInputStreams getTmpInputStreams(ExtendedBlock b,\n      long blkOffset, long metaOffset) throws IOException {\n    try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n      ReplicaInfo info \u003d getReplicaInfo(b);\n      FsVolumeReference ref \u003d info.getVolume().obtainReference();\n      try {\n        InputStream blockInStream \u003d info.getDataInputStream(blkOffset);\n        try {\n          InputStream metaInStream \u003d info.getMetadataInputStream(metaOffset);\n          return new ReplicaInputStreams(blockInStream, metaInStream, ref);\n        } catch (IOException e) {\n          IOUtils.cleanup(null, blockInStream);\n          throw e;\n        }\n      } catch (IOException e) {\n        IOUtils.cleanup(null, ref);\n        throw e;\n      }\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "8c0638471f8f1dd47667b2d6727d4d2d54e4b48c": {
      "type": "Ymultichange(Ymodifierchange,Ybodychange)",
      "commitMessage": "HADOOP-10682. Replace FsDatasetImpl object lock with a separate lock object. (Chen Liang)\n",
      "commitDate": "08/08/16 12:02 PM",
      "commitName": "8c0638471f8f1dd47667b2d6727d4d2d54e4b48c",
      "commitAuthor": "Arpit Agarwal",
      "subchanges": [
        {
          "type": "Ymodifierchange",
          "commitMessage": "HADOOP-10682. Replace FsDatasetImpl object lock with a separate lock object. (Chen Liang)\n",
          "commitDate": "08/08/16 12:02 PM",
          "commitName": "8c0638471f8f1dd47667b2d6727d4d2d54e4b48c",
          "commitAuthor": "Arpit Agarwal",
          "commitDateOld": "08/07/16 7:40 PM",
          "commitNameOld": "da6f1b88dd47e22b24d44f6fc8bbee73e85746f7",
          "commitAuthorOld": "Yongjun Zhang",
          "daysBetweenCommits": 30.68,
          "commitsBetweenForRepo": 320,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,18 +1,21 @@\n-  public synchronized ReplicaInputStreams getTmpInputStreams(ExtendedBlock b,\n+  public ReplicaInputStreams getTmpInputStreams(ExtendedBlock b,\n       long blkOffset, long metaOffset) throws IOException {\n-    ReplicaInfo info \u003d getReplicaInfo(b);\n-    FsVolumeReference ref \u003d info.getVolume().obtainReference();\n-    try {\n-      InputStream blockInStream \u003d openAndSeek(info.getBlockFile(), blkOffset);\n+    try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n+      ReplicaInfo info \u003d getReplicaInfo(b);\n+      FsVolumeReference ref \u003d info.getVolume().obtainReference();\n       try {\n-        InputStream metaInStream \u003d openAndSeek(info.getMetaFile(), metaOffset);\n-        return new ReplicaInputStreams(blockInStream, metaInStream, ref);\n+        InputStream blockInStream \u003d openAndSeek(info.getBlockFile(), blkOffset);\n+        try {\n+          InputStream metaInStream \u003d\n+              openAndSeek(info.getMetaFile(), metaOffset);\n+          return new ReplicaInputStreams(blockInStream, metaInStream, ref);\n+        } catch (IOException e) {\n+          IOUtils.cleanup(null, blockInStream);\n+          throw e;\n+        }\n       } catch (IOException e) {\n-        IOUtils.cleanup(null, blockInStream);\n+        IOUtils.cleanup(null, ref);\n         throw e;\n       }\n-    } catch (IOException e) {\n-      IOUtils.cleanup(null, ref);\n-      throw e;\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public ReplicaInputStreams getTmpInputStreams(ExtendedBlock b,\n      long blkOffset, long metaOffset) throws IOException {\n    try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n      ReplicaInfo info \u003d getReplicaInfo(b);\n      FsVolumeReference ref \u003d info.getVolume().obtainReference();\n      try {\n        InputStream blockInStream \u003d openAndSeek(info.getBlockFile(), blkOffset);\n        try {\n          InputStream metaInStream \u003d\n              openAndSeek(info.getMetaFile(), metaOffset);\n          return new ReplicaInputStreams(blockInStream, metaInStream, ref);\n        } catch (IOException e) {\n          IOUtils.cleanup(null, blockInStream);\n          throw e;\n        }\n      } catch (IOException e) {\n        IOUtils.cleanup(null, ref);\n        throw e;\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
          "extendedDetails": {
            "oldValue": "[public, synchronized]",
            "newValue": "[public]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HADOOP-10682. Replace FsDatasetImpl object lock with a separate lock object. (Chen Liang)\n",
          "commitDate": "08/08/16 12:02 PM",
          "commitName": "8c0638471f8f1dd47667b2d6727d4d2d54e4b48c",
          "commitAuthor": "Arpit Agarwal",
          "commitDateOld": "08/07/16 7:40 PM",
          "commitNameOld": "da6f1b88dd47e22b24d44f6fc8bbee73e85746f7",
          "commitAuthorOld": "Yongjun Zhang",
          "daysBetweenCommits": 30.68,
          "commitsBetweenForRepo": 320,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,18 +1,21 @@\n-  public synchronized ReplicaInputStreams getTmpInputStreams(ExtendedBlock b,\n+  public ReplicaInputStreams getTmpInputStreams(ExtendedBlock b,\n       long blkOffset, long metaOffset) throws IOException {\n-    ReplicaInfo info \u003d getReplicaInfo(b);\n-    FsVolumeReference ref \u003d info.getVolume().obtainReference();\n-    try {\n-      InputStream blockInStream \u003d openAndSeek(info.getBlockFile(), blkOffset);\n+    try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n+      ReplicaInfo info \u003d getReplicaInfo(b);\n+      FsVolumeReference ref \u003d info.getVolume().obtainReference();\n       try {\n-        InputStream metaInStream \u003d openAndSeek(info.getMetaFile(), metaOffset);\n-        return new ReplicaInputStreams(blockInStream, metaInStream, ref);\n+        InputStream blockInStream \u003d openAndSeek(info.getBlockFile(), blkOffset);\n+        try {\n+          InputStream metaInStream \u003d\n+              openAndSeek(info.getMetaFile(), metaOffset);\n+          return new ReplicaInputStreams(blockInStream, metaInStream, ref);\n+        } catch (IOException e) {\n+          IOUtils.cleanup(null, blockInStream);\n+          throw e;\n+        }\n       } catch (IOException e) {\n-        IOUtils.cleanup(null, blockInStream);\n+        IOUtils.cleanup(null, ref);\n         throw e;\n       }\n-    } catch (IOException e) {\n-      IOUtils.cleanup(null, ref);\n-      throw e;\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public ReplicaInputStreams getTmpInputStreams(ExtendedBlock b,\n      long blkOffset, long metaOffset) throws IOException {\n    try (AutoCloseableLock lock \u003d datasetLock.acquire()) {\n      ReplicaInfo info \u003d getReplicaInfo(b);\n      FsVolumeReference ref \u003d info.getVolume().obtainReference();\n      try {\n        InputStream blockInStream \u003d openAndSeek(info.getBlockFile(), blkOffset);\n        try {\n          InputStream metaInStream \u003d\n              openAndSeek(info.getMetaFile(), metaOffset);\n          return new ReplicaInputStreams(blockInStream, metaInStream, ref);\n        } catch (IOException e) {\n          IOUtils.cleanup(null, blockInStream);\n          throw e;\n        }\n      } catch (IOException e) {\n        IOUtils.cleanup(null, ref);\n        throw e;\n      }\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
          "extendedDetails": {}
        }
      ]
    },
    "d085eb15d7ca7b43a69bd70bad4e2ea601ba2ae0": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-7696. In FsDatasetImpl, the getBlockInputStream(..) and getTmpInputStreams(..) methods may leak file descriptors.\n",
      "commitDate": "02/02/15 1:38 PM",
      "commitName": "d085eb15d7ca7b43a69bd70bad4e2ea601ba2ae0",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-7696. In FsDatasetImpl, the getBlockInputStream(..) and getTmpInputStreams(..) methods may leak file descriptors.\n",
          "commitDate": "02/02/15 1:38 PM",
          "commitName": "d085eb15d7ca7b43a69bd70bad4e2ea601ba2ae0",
          "commitAuthor": "Tsz-Wo Nicholas Sze",
          "commitDateOld": "28/01/15 4:00 PM",
          "commitNameOld": "5a0051f4da6e102846d795a7965a6a18216d74f7",
          "commitAuthorOld": "Tsz-Wo Nicholas Sze",
          "daysBetweenCommits": 4.9,
          "commitsBetweenForRepo": 29,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,28 +1,18 @@\n-  public synchronized ReplicaInputStreams getTmpInputStreams(ExtendedBlock b, \n-                          long blkOffset, long ckoff) throws IOException {\n+  public synchronized ReplicaInputStreams getTmpInputStreams(ExtendedBlock b,\n+      long blkOffset, long metaOffset) throws IOException {\n     ReplicaInfo info \u003d getReplicaInfo(b);\n     FsVolumeReference ref \u003d info.getVolume().obtainReference();\n     try {\n-      File blockFile \u003d info.getBlockFile();\n-      RandomAccessFile blockInFile \u003d new RandomAccessFile(blockFile, \"r\");\n-      if (blkOffset \u003e 0) {\n-        blockInFile.seek(blkOffset);\n-      }\n-      File metaFile \u003d info.getMetaFile();\n-      RandomAccessFile metaInFile \u003d new RandomAccessFile(metaFile, \"r\");\n-      if (ckoff \u003e 0) {\n-        metaInFile.seek(ckoff);\n-      }\n-      InputStream blockInStream \u003d new FileInputStream(blockInFile.getFD());\n+      InputStream blockInStream \u003d openAndSeek(info.getBlockFile(), blkOffset);\n       try {\n-        InputStream metaInStream \u003d new FileInputStream(metaInFile.getFD());\n+        InputStream metaInStream \u003d openAndSeek(info.getMetaFile(), metaOffset);\n         return new ReplicaInputStreams(blockInStream, metaInStream, ref);\n       } catch (IOException e) {\n         IOUtils.cleanup(null, blockInStream);\n         throw e;\n       }\n     } catch (IOException e) {\n       IOUtils.cleanup(null, ref);\n       throw e;\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public synchronized ReplicaInputStreams getTmpInputStreams(ExtendedBlock b,\n      long blkOffset, long metaOffset) throws IOException {\n    ReplicaInfo info \u003d getReplicaInfo(b);\n    FsVolumeReference ref \u003d info.getVolume().obtainReference();\n    try {\n      InputStream blockInStream \u003d openAndSeek(info.getBlockFile(), blkOffset);\n      try {\n        InputStream metaInStream \u003d openAndSeek(info.getMetaFile(), metaOffset);\n        return new ReplicaInputStreams(blockInStream, metaInStream, ref);\n      } catch (IOException e) {\n        IOUtils.cleanup(null, blockInStream);\n        throw e;\n      }\n    } catch (IOException e) {\n      IOUtils.cleanup(null, ref);\n      throw e;\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
          "extendedDetails": {
            "oldValue": "[b-ExtendedBlock, blkOffset-long, ckoff-long]",
            "newValue": "[b-ExtendedBlock, blkOffset-long, metaOffset-long]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-7696. In FsDatasetImpl, the getBlockInputStream(..) and getTmpInputStreams(..) methods may leak file descriptors.\n",
          "commitDate": "02/02/15 1:38 PM",
          "commitName": "d085eb15d7ca7b43a69bd70bad4e2ea601ba2ae0",
          "commitAuthor": "Tsz-Wo Nicholas Sze",
          "commitDateOld": "28/01/15 4:00 PM",
          "commitNameOld": "5a0051f4da6e102846d795a7965a6a18216d74f7",
          "commitAuthorOld": "Tsz-Wo Nicholas Sze",
          "daysBetweenCommits": 4.9,
          "commitsBetweenForRepo": 29,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,28 +1,18 @@\n-  public synchronized ReplicaInputStreams getTmpInputStreams(ExtendedBlock b, \n-                          long blkOffset, long ckoff) throws IOException {\n+  public synchronized ReplicaInputStreams getTmpInputStreams(ExtendedBlock b,\n+      long blkOffset, long metaOffset) throws IOException {\n     ReplicaInfo info \u003d getReplicaInfo(b);\n     FsVolumeReference ref \u003d info.getVolume().obtainReference();\n     try {\n-      File blockFile \u003d info.getBlockFile();\n-      RandomAccessFile blockInFile \u003d new RandomAccessFile(blockFile, \"r\");\n-      if (blkOffset \u003e 0) {\n-        blockInFile.seek(blkOffset);\n-      }\n-      File metaFile \u003d info.getMetaFile();\n-      RandomAccessFile metaInFile \u003d new RandomAccessFile(metaFile, \"r\");\n-      if (ckoff \u003e 0) {\n-        metaInFile.seek(ckoff);\n-      }\n-      InputStream blockInStream \u003d new FileInputStream(blockInFile.getFD());\n+      InputStream blockInStream \u003d openAndSeek(info.getBlockFile(), blkOffset);\n       try {\n-        InputStream metaInStream \u003d new FileInputStream(metaInFile.getFD());\n+        InputStream metaInStream \u003d openAndSeek(info.getMetaFile(), metaOffset);\n         return new ReplicaInputStreams(blockInStream, metaInStream, ref);\n       } catch (IOException e) {\n         IOUtils.cleanup(null, blockInStream);\n         throw e;\n       }\n     } catch (IOException e) {\n       IOUtils.cleanup(null, ref);\n       throw e;\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public synchronized ReplicaInputStreams getTmpInputStreams(ExtendedBlock b,\n      long blkOffset, long metaOffset) throws IOException {\n    ReplicaInfo info \u003d getReplicaInfo(b);\n    FsVolumeReference ref \u003d info.getVolume().obtainReference();\n    try {\n      InputStream blockInStream \u003d openAndSeek(info.getBlockFile(), blkOffset);\n      try {\n        InputStream metaInStream \u003d openAndSeek(info.getMetaFile(), metaOffset);\n        return new ReplicaInputStreams(blockInStream, metaInStream, ref);\n      } catch (IOException e) {\n        IOUtils.cleanup(null, blockInStream);\n        throw e;\n      }\n    } catch (IOException e) {\n      IOUtils.cleanup(null, ref);\n      throw e;\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
          "extendedDetails": {}
        }
      ]
    },
    "5a0051f4da6e102846d795a7965a6a18216d74f7": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7681. Change ReplicaInputStreams constructor to take InputStream(s) instead of FileDescriptor(s).  Contributed by Joe Pallas\n",
      "commitDate": "28/01/15 4:00 PM",
      "commitName": "5a0051f4da6e102846d795a7965a6a18216d74f7",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "21/01/15 7:00 PM",
      "commitNameOld": "6e62a1a6728b1f782f64065424f92b292c3f163a",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 6.87,
      "commitsBetweenForRepo": 52,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,22 +1,28 @@\n   public synchronized ReplicaInputStreams getTmpInputStreams(ExtendedBlock b, \n                           long blkOffset, long ckoff) throws IOException {\n     ReplicaInfo info \u003d getReplicaInfo(b);\n     FsVolumeReference ref \u003d info.getVolume().obtainReference();\n     try {\n       File blockFile \u003d info.getBlockFile();\n       RandomAccessFile blockInFile \u003d new RandomAccessFile(blockFile, \"r\");\n       if (blkOffset \u003e 0) {\n         blockInFile.seek(blkOffset);\n       }\n       File metaFile \u003d info.getMetaFile();\n       RandomAccessFile metaInFile \u003d new RandomAccessFile(metaFile, \"r\");\n       if (ckoff \u003e 0) {\n         metaInFile.seek(ckoff);\n       }\n-      return new ReplicaInputStreams(\n-          blockInFile.getFD(), metaInFile.getFD(), ref);\n+      InputStream blockInStream \u003d new FileInputStream(blockInFile.getFD());\n+      try {\n+        InputStream metaInStream \u003d new FileInputStream(metaInFile.getFD());\n+        return new ReplicaInputStreams(blockInStream, metaInStream, ref);\n+      } catch (IOException e) {\n+        IOUtils.cleanup(null, blockInStream);\n+        throw e;\n+      }\n     } catch (IOException e) {\n       IOUtils.cleanup(null, ref);\n       throw e;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized ReplicaInputStreams getTmpInputStreams(ExtendedBlock b, \n                          long blkOffset, long ckoff) throws IOException {\n    ReplicaInfo info \u003d getReplicaInfo(b);\n    FsVolumeReference ref \u003d info.getVolume().obtainReference();\n    try {\n      File blockFile \u003d info.getBlockFile();\n      RandomAccessFile blockInFile \u003d new RandomAccessFile(blockFile, \"r\");\n      if (blkOffset \u003e 0) {\n        blockInFile.seek(blkOffset);\n      }\n      File metaFile \u003d info.getMetaFile();\n      RandomAccessFile metaInFile \u003d new RandomAccessFile(metaFile, \"r\");\n      if (ckoff \u003e 0) {\n        metaInFile.seek(ckoff);\n      }\n      InputStream blockInStream \u003d new FileInputStream(blockInFile.getFD());\n      try {\n        InputStream metaInStream \u003d new FileInputStream(metaInFile.getFD());\n        return new ReplicaInputStreams(blockInStream, metaInStream, ref);\n      } catch (IOException e) {\n        IOUtils.cleanup(null, blockInStream);\n        throw e;\n      }\n    } catch (IOException e) {\n      IOUtils.cleanup(null, ref);\n      throw e;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "b7f4a3156c0f5c600816c469637237ba6c9b330c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7496. Fix FsVolume removal race conditions on the DataNode by reference-counting the volume instances (lei via cmccabe)\n",
      "commitDate": "20/01/15 7:05 PM",
      "commitName": "b7f4a3156c0f5c600816c469637237ba6c9b330c",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "13/01/15 12:24 AM",
      "commitNameOld": "08ac06283a3e9bf0d49d873823aabd419b08e41f",
      "commitAuthorOld": "Konstantin V Shvachko",
      "daysBetweenCommits": 7.78,
      "commitsBetweenForRepo": 49,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,15 +1,22 @@\n   public synchronized ReplicaInputStreams getTmpInputStreams(ExtendedBlock b, \n                           long blkOffset, long ckoff) throws IOException {\n     ReplicaInfo info \u003d getReplicaInfo(b);\n-    File blockFile \u003d info.getBlockFile();\n-    RandomAccessFile blockInFile \u003d new RandomAccessFile(blockFile, \"r\");\n-    if (blkOffset \u003e 0) {\n-      blockInFile.seek(blkOffset);\n+    FsVolumeReference ref \u003d info.getVolume().obtainReference();\n+    try {\n+      File blockFile \u003d info.getBlockFile();\n+      RandomAccessFile blockInFile \u003d new RandomAccessFile(blockFile, \"r\");\n+      if (blkOffset \u003e 0) {\n+        blockInFile.seek(blkOffset);\n+      }\n+      File metaFile \u003d info.getMetaFile();\n+      RandomAccessFile metaInFile \u003d new RandomAccessFile(metaFile, \"r\");\n+      if (ckoff \u003e 0) {\n+        metaInFile.seek(ckoff);\n+      }\n+      return new ReplicaInputStreams(\n+          blockInFile.getFD(), metaInFile.getFD(), ref);\n+    } catch (IOException e) {\n+      IOUtils.cleanup(null, ref);\n+      throw e;\n     }\n-    File metaFile \u003d info.getMetaFile();\n-    RandomAccessFile metaInFile \u003d new RandomAccessFile(metaFile, \"r\");\n-    if (ckoff \u003e 0) {\n-      metaInFile.seek(ckoff);\n-    }\n-    return new ReplicaInputStreams(blockInFile.getFD(), metaInFile.getFD());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized ReplicaInputStreams getTmpInputStreams(ExtendedBlock b, \n                          long blkOffset, long ckoff) throws IOException {\n    ReplicaInfo info \u003d getReplicaInfo(b);\n    FsVolumeReference ref \u003d info.getVolume().obtainReference();\n    try {\n      File blockFile \u003d info.getBlockFile();\n      RandomAccessFile blockInFile \u003d new RandomAccessFile(blockFile, \"r\");\n      if (blkOffset \u003e 0) {\n        blockInFile.seek(blkOffset);\n      }\n      File metaFile \u003d info.getMetaFile();\n      RandomAccessFile metaInFile \u003d new RandomAccessFile(metaFile, \"r\");\n      if (ckoff \u003e 0) {\n        metaInFile.seek(ckoff);\n      }\n      return new ReplicaInputStreams(\n          blockInFile.getFD(), metaInFile.getFD(), ref);\n    } catch (IOException e) {\n      IOUtils.cleanup(null, ref);\n      throw e;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "5cdb7e5ce7f0c3129749be8f29e2f11c0e0f2269": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4033. Miscellaneous findbugs 2 fixes. Contributed by Eli Collins\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1430534 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/01/13 1:05 PM",
      "commitName": "5cdb7e5ce7f0c3129749be8f29e2f11c0e0f2269",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "28/10/12 4:10 PM",
      "commitNameOld": "cea7bbc630deede93dbe6a1bbda56ad49de4f3de",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 71.91,
      "commitsBetweenForRepo": 301,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,16 +1,15 @@\n   public synchronized ReplicaInputStreams getTmpInputStreams(ExtendedBlock b, \n                           long blkOffset, long ckoff) throws IOException {\n     ReplicaInfo info \u003d getReplicaInfo(b);\n     File blockFile \u003d info.getBlockFile();\n     RandomAccessFile blockInFile \u003d new RandomAccessFile(blockFile, \"r\");\n     if (blkOffset \u003e 0) {\n       blockInFile.seek(blkOffset);\n     }\n     File metaFile \u003d info.getMetaFile();\n     RandomAccessFile metaInFile \u003d new RandomAccessFile(metaFile, \"r\");\n     if (ckoff \u003e 0) {\n       metaInFile.seek(ckoff);\n     }\n-    return new ReplicaInputStreams(new FileInputStream(blockInFile.getFD()),\n-                                new FileInputStream(metaInFile.getFD()));\n+    return new ReplicaInputStreams(blockInFile.getFD(), metaInFile.getFD());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized ReplicaInputStreams getTmpInputStreams(ExtendedBlock b, \n                          long blkOffset, long ckoff) throws IOException {\n    ReplicaInfo info \u003d getReplicaInfo(b);\n    File blockFile \u003d info.getBlockFile();\n    RandomAccessFile blockInFile \u003d new RandomAccessFile(blockFile, \"r\");\n    if (blkOffset \u003e 0) {\n      blockInFile.seek(blkOffset);\n    }\n    File metaFile \u003d info.getMetaFile();\n    RandomAccessFile metaInFile \u003d new RandomAccessFile(metaFile, \"r\");\n    if (ckoff \u003e 0) {\n      metaInFile.seek(ckoff);\n    }\n    return new ReplicaInputStreams(blockInFile.getFD(), metaInFile.getFD());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {}
    },
    "bc13dfb1426944ce45293cb8f444239a7406762c": {
      "type": "Ymovefromfile",
      "commitMessage": "HDFS-3130. Move fsdataset implementation to a package.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1308437 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "02/04/12 10:38 AM",
      "commitName": "bc13dfb1426944ce45293cb8f444239a7406762c",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "01/04/12 8:48 PM",
      "commitNameOld": "a4ccb8f504e79802f1b3c69acbcbb00b2343c529",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.58,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public synchronized ReplicaInputStreams getTmpInputStreams(ExtendedBlock b, \n                          long blkOffset, long ckoff) throws IOException {\n    ReplicaInfo info \u003d getReplicaInfo(b);\n    File blockFile \u003d info.getBlockFile();\n    RandomAccessFile blockInFile \u003d new RandomAccessFile(blockFile, \"r\");\n    if (blkOffset \u003e 0) {\n      blockInFile.seek(blkOffset);\n    }\n    File metaFile \u003d info.getMetaFile();\n    RandomAccessFile metaInFile \u003d new RandomAccessFile(metaFile, \"r\");\n    if (ckoff \u003e 0) {\n      metaInFile.seek(ckoff);\n    }\n    return new ReplicaInputStreams(new FileInputStream(blockInFile.getFD()),\n                                new FileInputStream(metaInFile.getFD()));\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
        "oldMethodName": "getTmpInputStreams",
        "newMethodName": "getTmpInputStreams"
      }
    },
    "662b1887af4e39f3eadd7dda4953c7f2529b43bc": {
      "type": "Ymultichange(Yreturntypechange,Ybodychange)",
      "commitMessage": "HDFS-3088. Move FSDatasetInterface inner classes to a package.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1301661 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "16/03/12 10:32 AM",
      "commitName": "662b1887af4e39f3eadd7dda4953c7f2529b43bc",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Yreturntypechange",
          "commitMessage": "HDFS-3088. Move FSDatasetInterface inner classes to a package.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1301661 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "16/03/12 10:32 AM",
          "commitName": "662b1887af4e39f3eadd7dda4953c7f2529b43bc",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "15/03/12 11:24 AM",
          "commitNameOld": "1177d4edc29f839b8df1038c4fbf37f56f56a2a0",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 0.96,
          "commitsBetweenForRepo": 16,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,16 +1,16 @@\n-  public synchronized BlockInputStreams getTmpInputStreams(ExtendedBlock b, \n+  public synchronized ReplicaInputStreams getTmpInputStreams(ExtendedBlock b, \n                           long blkOffset, long ckoff) throws IOException {\n     ReplicaInfo info \u003d getReplicaInfo(b);\n     File blockFile \u003d info.getBlockFile();\n     RandomAccessFile blockInFile \u003d new RandomAccessFile(blockFile, \"r\");\n     if (blkOffset \u003e 0) {\n       blockInFile.seek(blkOffset);\n     }\n     File metaFile \u003d info.getMetaFile();\n     RandomAccessFile metaInFile \u003d new RandomAccessFile(metaFile, \"r\");\n     if (ckoff \u003e 0) {\n       metaInFile.seek(ckoff);\n     }\n-    return new BlockInputStreams(new FileInputStream(blockInFile.getFD()),\n+    return new ReplicaInputStreams(new FileInputStream(blockInFile.getFD()),\n                                 new FileInputStream(metaInFile.getFD()));\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public synchronized ReplicaInputStreams getTmpInputStreams(ExtendedBlock b, \n                          long blkOffset, long ckoff) throws IOException {\n    ReplicaInfo info \u003d getReplicaInfo(b);\n    File blockFile \u003d info.getBlockFile();\n    RandomAccessFile blockInFile \u003d new RandomAccessFile(blockFile, \"r\");\n    if (blkOffset \u003e 0) {\n      blockInFile.seek(blkOffset);\n    }\n    File metaFile \u003d info.getMetaFile();\n    RandomAccessFile metaInFile \u003d new RandomAccessFile(metaFile, \"r\");\n    if (ckoff \u003e 0) {\n      metaInFile.seek(ckoff);\n    }\n    return new ReplicaInputStreams(new FileInputStream(blockInFile.getFD()),\n                                new FileInputStream(metaInFile.getFD()));\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
          "extendedDetails": {
            "oldValue": "BlockInputStreams",
            "newValue": "ReplicaInputStreams"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-3088. Move FSDatasetInterface inner classes to a package.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1301661 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "16/03/12 10:32 AM",
          "commitName": "662b1887af4e39f3eadd7dda4953c7f2529b43bc",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "15/03/12 11:24 AM",
          "commitNameOld": "1177d4edc29f839b8df1038c4fbf37f56f56a2a0",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 0.96,
          "commitsBetweenForRepo": 16,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,16 +1,16 @@\n-  public synchronized BlockInputStreams getTmpInputStreams(ExtendedBlock b, \n+  public synchronized ReplicaInputStreams getTmpInputStreams(ExtendedBlock b, \n                           long blkOffset, long ckoff) throws IOException {\n     ReplicaInfo info \u003d getReplicaInfo(b);\n     File blockFile \u003d info.getBlockFile();\n     RandomAccessFile blockInFile \u003d new RandomAccessFile(blockFile, \"r\");\n     if (blkOffset \u003e 0) {\n       blockInFile.seek(blkOffset);\n     }\n     File metaFile \u003d info.getMetaFile();\n     RandomAccessFile metaInFile \u003d new RandomAccessFile(metaFile, \"r\");\n     if (ckoff \u003e 0) {\n       metaInFile.seek(ckoff);\n     }\n-    return new BlockInputStreams(new FileInputStream(blockInFile.getFD()),\n+    return new ReplicaInputStreams(new FileInputStream(blockInFile.getFD()),\n                                 new FileInputStream(metaInFile.getFD()));\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public synchronized ReplicaInputStreams getTmpInputStreams(ExtendedBlock b, \n                          long blkOffset, long ckoff) throws IOException {\n    ReplicaInfo info \u003d getReplicaInfo(b);\n    File blockFile \u003d info.getBlockFile();\n    RandomAccessFile blockInFile \u003d new RandomAccessFile(blockFile, \"r\");\n    if (blkOffset \u003e 0) {\n      blockInFile.seek(blkOffset);\n    }\n    File metaFile \u003d info.getMetaFile();\n    RandomAccessFile metaInFile \u003d new RandomAccessFile(metaFile, \"r\");\n    if (ckoff \u003e 0) {\n      metaInFile.seek(ckoff);\n    }\n    return new ReplicaInputStreams(new FileInputStream(blockInFile.getFD()),\n                                new FileInputStream(metaInFile.getFD()));\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
          "extendedDetails": {}
        }
      ]
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public synchronized BlockInputStreams getTmpInputStreams(ExtendedBlock b, \n                          long blkOffset, long ckoff) throws IOException {\n    ReplicaInfo info \u003d getReplicaInfo(b);\n    File blockFile \u003d info.getBlockFile();\n    RandomAccessFile blockInFile \u003d new RandomAccessFile(blockFile, \"r\");\n    if (blkOffset \u003e 0) {\n      blockInFile.seek(blkOffset);\n    }\n    File metaFile \u003d info.getMetaFile();\n    RandomAccessFile metaInFile \u003d new RandomAccessFile(metaFile, \"r\");\n    if (ckoff \u003e 0) {\n      metaInFile.seek(ckoff);\n    }\n    return new BlockInputStreams(new FileInputStream(blockInFile.getFD()),\n                                new FileInputStream(metaInFile.getFD()));\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public synchronized BlockInputStreams getTmpInputStreams(ExtendedBlock b, \n                          long blkOffset, long ckoff) throws IOException {\n    ReplicaInfo info \u003d getReplicaInfo(b);\n    File blockFile \u003d info.getBlockFile();\n    RandomAccessFile blockInFile \u003d new RandomAccessFile(blockFile, \"r\");\n    if (blkOffset \u003e 0) {\n      blockInFile.seek(blkOffset);\n    }\n    File metaFile \u003d info.getMetaFile();\n    RandomAccessFile metaInFile \u003d new RandomAccessFile(metaFile, \"r\");\n    if (ckoff \u003e 0) {\n      metaInFile.seek(ckoff);\n    }\n    return new BlockInputStreams(new FileInputStream(blockInFile.getFD()),\n                                new FileInputStream(metaInFile.getFD()));\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,16 @@\n+  public synchronized BlockInputStreams getTmpInputStreams(ExtendedBlock b, \n+                          long blkOffset, long ckoff) throws IOException {\n+    ReplicaInfo info \u003d getReplicaInfo(b);\n+    File blockFile \u003d info.getBlockFile();\n+    RandomAccessFile blockInFile \u003d new RandomAccessFile(blockFile, \"r\");\n+    if (blkOffset \u003e 0) {\n+      blockInFile.seek(blkOffset);\n+    }\n+    File metaFile \u003d info.getMetaFile();\n+    RandomAccessFile metaInFile \u003d new RandomAccessFile(metaFile, \"r\");\n+    if (ckoff \u003e 0) {\n+      metaInFile.seek(ckoff);\n+    }\n+    return new BlockInputStreams(new FileInputStream(blockInFile.getFD()),\n+                                new FileInputStream(metaInFile.getFD()));\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized BlockInputStreams getTmpInputStreams(ExtendedBlock b, \n                          long blkOffset, long ckoff) throws IOException {\n    ReplicaInfo info \u003d getReplicaInfo(b);\n    File blockFile \u003d info.getBlockFile();\n    RandomAccessFile blockInFile \u003d new RandomAccessFile(blockFile, \"r\");\n    if (blkOffset \u003e 0) {\n      blockInFile.seek(blkOffset);\n    }\n    File metaFile \u003d info.getMetaFile();\n    RandomAccessFile metaInFile \u003d new RandomAccessFile(metaFile, \"r\");\n    if (ckoff \u003e 0) {\n      metaInFile.seek(ckoff);\n    }\n    return new BlockInputStreams(new FileInputStream(blockInFile.getFD()),\n                                new FileInputStream(metaInFile.getFD()));\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/datanode/FSDataset.java"
    }
  }
}