{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockManager.java",
  "functionName": "countNodes",
  "functionId": "countNodes___b-BlockInfo__inStartupSafeMode-boolean",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
  "functionStartLine": 4342,
  "functionEndLine": 4355,
  "numCommitsSeen": 1330,
  "timeTaken": 22412,
  "changeHistory": [
    "b61fb267b92b2736920b4bd0c673d31e7632ebb9",
    "47b92f2b6f2dafc129a41b247f35e77c8e47ffba",
    "5411dc559d5f73e4153e76fdff94a26869c17a37",
    "73b86a5046fe3262dde7b05be46b18575e35fd5f",
    "663eba0ab1c73b45f98e46ffc87ad8fd91584046",
    "de480d6c8945bd8b5b00e8657b7a72ce8dd9b6b5",
    "6e3fcffe291faec40fa9214f4880a35a952836c4",
    "f8f5887209a7d8e53c0a77abef275cbcaf1f7a5b",
    "2f341414dd4a052bee3907ff4a6db283a15f9d53",
    "4551da302d94cffea0313eac79479ab6f9b7cb34",
    "282be1b38e5cd141ed7e2b2194bfb67a7c2f7f15",
    "3f070e83b1f4e0211ece8c0ab508a61188ad352a",
    "31c91706f7d17da006ef2d6c541f8dd092fae077",
    "9a3f147fdd5421460889b266ead3a2300323cda2",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "09b6f98de431628c80bc8a6faf0070eeaf72ff2a",
    "97b6ca4dd7d1233e8f8c90b1c01e47228c044e13",
    "1bcfe45e47775b98cce5541f328c4fd46e5eb13d",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "b61fb267b92b2736920b4bd0c673d31e7632ebb9": "Ymodifierchange",
    "47b92f2b6f2dafc129a41b247f35e77c8e47ffba": "Ymultichange(Yparameterchange,Ymodifierchange,Ybodychange)",
    "5411dc559d5f73e4153e76fdff94a26869c17a37": "Ymultichange(Yparameterchange,Ybodychange)",
    "73b86a5046fe3262dde7b05be46b18575e35fd5f": "Ybodychange",
    "663eba0ab1c73b45f98e46ffc87ad8fd91584046": "Ybodychange",
    "de480d6c8945bd8b5b00e8657b7a72ce8dd9b6b5": "Ybodychange",
    "6e3fcffe291faec40fa9214f4880a35a952836c4": "Yparameterchange",
    "f8f5887209a7d8e53c0a77abef275cbcaf1f7a5b": "Ybodychange",
    "2f341414dd4a052bee3907ff4a6db283a15f9d53": "Ybodychange",
    "4551da302d94cffea0313eac79479ab6f9b7cb34": "Ybodychange",
    "282be1b38e5cd141ed7e2b2194bfb67a7c2f7f15": "Ybodychange",
    "3f070e83b1f4e0211ece8c0ab508a61188ad352a": "Ybodychange",
    "31c91706f7d17da006ef2d6c541f8dd092fae077": "Ybodychange",
    "9a3f147fdd5421460889b266ead3a2300323cda2": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "09b6f98de431628c80bc8a6faf0070eeaf72ff2a": "Ymultichange(Yfilerename,Ymodifierchange)",
    "97b6ca4dd7d1233e8f8c90b1c01e47228c044e13": "Ymultichange(Yfilerename,Ymodifierchange)",
    "1bcfe45e47775b98cce5541f328c4fd46e5eb13d": "Ymultichange(Yfilerename,Ymodifierchange)",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "b61fb267b92b2736920b4bd0c673d31e7632ebb9": {
      "type": "Ymodifierchange",
      "commitMessage": "HDFS-9390. Block management for maintenance states.\n",
      "commitDate": "17/10/16 5:45 PM",
      "commitName": "b61fb267b92b2736920b4bd0c673d31e7632ebb9",
      "commitAuthor": "Ming Ma",
      "commitDateOld": "14/10/16 6:13 PM",
      "commitNameOld": "391ce535a739dc92cb90017d759217265a4fd969",
      "commitAuthorOld": "Vinitha Reddy Gankidi",
      "daysBetweenCommits": 2.98,
      "commitsBetweenForRepo": 11,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,14 +1,14 @@\n-  private NumberReplicas countNodes(BlockInfo b, boolean inStartupSafeMode) {\n+  NumberReplicas countNodes(BlockInfo b, boolean inStartupSafeMode) {\n     NumberReplicas numberReplicas \u003d new NumberReplicas();\n     Collection\u003cDatanodeDescriptor\u003e nodesCorrupt \u003d corruptReplicas.getNodes(b);\n     if (b.isStriped()) {\n       countReplicasForStripedBlock(numberReplicas, (BlockInfoStriped) b,\n           nodesCorrupt, inStartupSafeMode);\n     } else {\n       for (DatanodeStorageInfo storage : blocksMap.getStorages(b)) {\n         checkReplicaOnStorage(numberReplicas, b, storage, nodesCorrupt,\n             inStartupSafeMode);\n       }\n     }\n     return numberReplicas;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  NumberReplicas countNodes(BlockInfo b, boolean inStartupSafeMode) {\n    NumberReplicas numberReplicas \u003d new NumberReplicas();\n    Collection\u003cDatanodeDescriptor\u003e nodesCorrupt \u003d corruptReplicas.getNodes(b);\n    if (b.isStriped()) {\n      countReplicasForStripedBlock(numberReplicas, (BlockInfoStriped) b,\n          nodesCorrupt, inStartupSafeMode);\n    } else {\n      for (DatanodeStorageInfo storage : blocksMap.getStorages(b)) {\n        checkReplicaOnStorage(numberReplicas, b, storage, nodesCorrupt,\n            inStartupSafeMode);\n      }\n    }\n    return numberReplicas;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {
        "oldValue": "[private]",
        "newValue": "[]"
      }
    },
    "47b92f2b6f2dafc129a41b247f35e77c8e47ffba": {
      "type": "Ymultichange(Yparameterchange,Ymodifierchange,Ybodychange)",
      "commitMessage": "HDFS-9837. BlockManager#countNodes should be able to detect duplicated internal blocks. Contributed by Jing Zhao.\n",
      "commitDate": "24/02/16 3:13 PM",
      "commitName": "47b92f2b6f2dafc129a41b247f35e77c8e47ffba",
      "commitAuthor": "Jing Zhao",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-9837. BlockManager#countNodes should be able to detect duplicated internal blocks. Contributed by Jing Zhao.\n",
          "commitDate": "24/02/16 3:13 PM",
          "commitName": "47b92f2b6f2dafc129a41b247f35e77c8e47ffba",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "20/02/16 11:19 PM",
          "commitNameOld": "d5abd293a890a8a1da48a166a291ae1c5644ad57",
          "commitAuthorOld": "Arpit Agarwal",
          "daysBetweenCommits": 3.66,
          "commitsBetweenForRepo": 32,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,39 +1,14 @@\n-  public NumberReplicas countNodes(Block b) {\n-    int decommissioned \u003d 0;\n-    int decommissioning \u003d 0;\n-    int live \u003d 0;\n-    int readonly \u003d 0;\n-    int corrupt \u003d 0;\n-    int excess \u003d 0;\n-    int stale \u003d 0;\n+  private NumberReplicas countNodes(BlockInfo b, boolean inStartupSafeMode) {\n+    NumberReplicas numberReplicas \u003d new NumberReplicas();\n     Collection\u003cDatanodeDescriptor\u003e nodesCorrupt \u003d corruptReplicas.getNodes(b);\n-    for(DatanodeStorageInfo storage : blocksMap.getStorages(b)) {\n-      if (storage.getState() \u003d\u003d State.FAILED) {\n-        continue;\n-      } else if (storage.getState() \u003d\u003d State.READ_ONLY_SHARED) {\n-        readonly++;\n-        continue;\n-      }\n-      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n-      if ((nodesCorrupt !\u003d null) \u0026\u0026 (nodesCorrupt.contains(node))) {\n-        corrupt++;\n-      } else if (node.isDecommissionInProgress()) {\n-        decommissioning++;\n-      } else if (node.isDecommissioned()) {\n-        decommissioned++;\n-      } else {\n-        LightWeightHashSet\u003cBlockInfo\u003e blocksExcess \u003d excessReplicateMap.get(\n-            node.getDatanodeUuid());\n-        if (blocksExcess !\u003d null \u0026\u0026 blocksExcess.contains(b)) {\n-          excess++;\n-        } else {\n-          live++;\n-        }\n-      }\n-      if (storage.areBlockContentsStale()) {\n-        stale++;\n+    if (b.isStriped()) {\n+      countReplicasForStripedBlock(numberReplicas, (BlockInfoStriped) b,\n+          nodesCorrupt, inStartupSafeMode);\n+    } else {\n+      for (DatanodeStorageInfo storage : blocksMap.getStorages(b)) {\n+        checkReplicaOnStorage(numberReplicas, b, storage, nodesCorrupt,\n+            inStartupSafeMode);\n       }\n     }\n-    return new NumberReplicas(live, readonly, decommissioned, decommissioning,\n-        corrupt, excess, stale);\n+    return numberReplicas;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private NumberReplicas countNodes(BlockInfo b, boolean inStartupSafeMode) {\n    NumberReplicas numberReplicas \u003d new NumberReplicas();\n    Collection\u003cDatanodeDescriptor\u003e nodesCorrupt \u003d corruptReplicas.getNodes(b);\n    if (b.isStriped()) {\n      countReplicasForStripedBlock(numberReplicas, (BlockInfoStriped) b,\n          nodesCorrupt, inStartupSafeMode);\n    } else {\n      for (DatanodeStorageInfo storage : blocksMap.getStorages(b)) {\n        checkReplicaOnStorage(numberReplicas, b, storage, nodesCorrupt,\n            inStartupSafeMode);\n      }\n    }\n    return numberReplicas;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldValue": "[b-Block]",
            "newValue": "[b-BlockInfo, inStartupSafeMode-boolean]"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-9837. BlockManager#countNodes should be able to detect duplicated internal blocks. Contributed by Jing Zhao.\n",
          "commitDate": "24/02/16 3:13 PM",
          "commitName": "47b92f2b6f2dafc129a41b247f35e77c8e47ffba",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "20/02/16 11:19 PM",
          "commitNameOld": "d5abd293a890a8a1da48a166a291ae1c5644ad57",
          "commitAuthorOld": "Arpit Agarwal",
          "daysBetweenCommits": 3.66,
          "commitsBetweenForRepo": 32,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,39 +1,14 @@\n-  public NumberReplicas countNodes(Block b) {\n-    int decommissioned \u003d 0;\n-    int decommissioning \u003d 0;\n-    int live \u003d 0;\n-    int readonly \u003d 0;\n-    int corrupt \u003d 0;\n-    int excess \u003d 0;\n-    int stale \u003d 0;\n+  private NumberReplicas countNodes(BlockInfo b, boolean inStartupSafeMode) {\n+    NumberReplicas numberReplicas \u003d new NumberReplicas();\n     Collection\u003cDatanodeDescriptor\u003e nodesCorrupt \u003d corruptReplicas.getNodes(b);\n-    for(DatanodeStorageInfo storage : blocksMap.getStorages(b)) {\n-      if (storage.getState() \u003d\u003d State.FAILED) {\n-        continue;\n-      } else if (storage.getState() \u003d\u003d State.READ_ONLY_SHARED) {\n-        readonly++;\n-        continue;\n-      }\n-      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n-      if ((nodesCorrupt !\u003d null) \u0026\u0026 (nodesCorrupt.contains(node))) {\n-        corrupt++;\n-      } else if (node.isDecommissionInProgress()) {\n-        decommissioning++;\n-      } else if (node.isDecommissioned()) {\n-        decommissioned++;\n-      } else {\n-        LightWeightHashSet\u003cBlockInfo\u003e blocksExcess \u003d excessReplicateMap.get(\n-            node.getDatanodeUuid());\n-        if (blocksExcess !\u003d null \u0026\u0026 blocksExcess.contains(b)) {\n-          excess++;\n-        } else {\n-          live++;\n-        }\n-      }\n-      if (storage.areBlockContentsStale()) {\n-        stale++;\n+    if (b.isStriped()) {\n+      countReplicasForStripedBlock(numberReplicas, (BlockInfoStriped) b,\n+          nodesCorrupt, inStartupSafeMode);\n+    } else {\n+      for (DatanodeStorageInfo storage : blocksMap.getStorages(b)) {\n+        checkReplicaOnStorage(numberReplicas, b, storage, nodesCorrupt,\n+            inStartupSafeMode);\n       }\n     }\n-    return new NumberReplicas(live, readonly, decommissioned, decommissioning,\n-        corrupt, excess, stale);\n+    return numberReplicas;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private NumberReplicas countNodes(BlockInfo b, boolean inStartupSafeMode) {\n    NumberReplicas numberReplicas \u003d new NumberReplicas();\n    Collection\u003cDatanodeDescriptor\u003e nodesCorrupt \u003d corruptReplicas.getNodes(b);\n    if (b.isStriped()) {\n      countReplicasForStripedBlock(numberReplicas, (BlockInfoStriped) b,\n          nodesCorrupt, inStartupSafeMode);\n    } else {\n      for (DatanodeStorageInfo storage : blocksMap.getStorages(b)) {\n        checkReplicaOnStorage(numberReplicas, b, storage, nodesCorrupt,\n            inStartupSafeMode);\n      }\n    }\n    return numberReplicas;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldValue": "[public]",
            "newValue": "[private]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-9837. BlockManager#countNodes should be able to detect duplicated internal blocks. Contributed by Jing Zhao.\n",
          "commitDate": "24/02/16 3:13 PM",
          "commitName": "47b92f2b6f2dafc129a41b247f35e77c8e47ffba",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "20/02/16 11:19 PM",
          "commitNameOld": "d5abd293a890a8a1da48a166a291ae1c5644ad57",
          "commitAuthorOld": "Arpit Agarwal",
          "daysBetweenCommits": 3.66,
          "commitsBetweenForRepo": 32,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,39 +1,14 @@\n-  public NumberReplicas countNodes(Block b) {\n-    int decommissioned \u003d 0;\n-    int decommissioning \u003d 0;\n-    int live \u003d 0;\n-    int readonly \u003d 0;\n-    int corrupt \u003d 0;\n-    int excess \u003d 0;\n-    int stale \u003d 0;\n+  private NumberReplicas countNodes(BlockInfo b, boolean inStartupSafeMode) {\n+    NumberReplicas numberReplicas \u003d new NumberReplicas();\n     Collection\u003cDatanodeDescriptor\u003e nodesCorrupt \u003d corruptReplicas.getNodes(b);\n-    for(DatanodeStorageInfo storage : blocksMap.getStorages(b)) {\n-      if (storage.getState() \u003d\u003d State.FAILED) {\n-        continue;\n-      } else if (storage.getState() \u003d\u003d State.READ_ONLY_SHARED) {\n-        readonly++;\n-        continue;\n-      }\n-      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n-      if ((nodesCorrupt !\u003d null) \u0026\u0026 (nodesCorrupt.contains(node))) {\n-        corrupt++;\n-      } else if (node.isDecommissionInProgress()) {\n-        decommissioning++;\n-      } else if (node.isDecommissioned()) {\n-        decommissioned++;\n-      } else {\n-        LightWeightHashSet\u003cBlockInfo\u003e blocksExcess \u003d excessReplicateMap.get(\n-            node.getDatanodeUuid());\n-        if (blocksExcess !\u003d null \u0026\u0026 blocksExcess.contains(b)) {\n-          excess++;\n-        } else {\n-          live++;\n-        }\n-      }\n-      if (storage.areBlockContentsStale()) {\n-        stale++;\n+    if (b.isStriped()) {\n+      countReplicasForStripedBlock(numberReplicas, (BlockInfoStriped) b,\n+          nodesCorrupt, inStartupSafeMode);\n+    } else {\n+      for (DatanodeStorageInfo storage : blocksMap.getStorages(b)) {\n+        checkReplicaOnStorage(numberReplicas, b, storage, nodesCorrupt,\n+            inStartupSafeMode);\n       }\n     }\n-    return new NumberReplicas(live, readonly, decommissioned, decommissioning,\n-        corrupt, excess, stale);\n+    return numberReplicas;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private NumberReplicas countNodes(BlockInfo b, boolean inStartupSafeMode) {\n    NumberReplicas numberReplicas \u003d new NumberReplicas();\n    Collection\u003cDatanodeDescriptor\u003e nodesCorrupt \u003d corruptReplicas.getNodes(b);\n    if (b.isStriped()) {\n      countReplicasForStripedBlock(numberReplicas, (BlockInfoStriped) b,\n          nodesCorrupt, inStartupSafeMode);\n    } else {\n      for (DatanodeStorageInfo storage : blocksMap.getStorages(b)) {\n        checkReplicaOnStorage(numberReplicas, b, storage, nodesCorrupt,\n            inStartupSafeMode);\n      }\n    }\n    return numberReplicas;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {}
        }
      ]
    },
    "5411dc559d5f73e4153e76fdff94a26869c17a37": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-9205. Do not schedule corrupt blocks for replication.  (szetszwo)\n",
      "commitDate": "15/10/15 3:07 AM",
      "commitName": "5411dc559d5f73e4153e76fdff94a26869c17a37",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-9205. Do not schedule corrupt blocks for replication.  (szetszwo)\n",
          "commitDate": "15/10/15 3:07 AM",
          "commitName": "5411dc559d5f73e4153e76fdff94a26869c17a37",
          "commitAuthor": "Tsz-Wo Nicholas Sze",
          "commitDateOld": "14/10/15 4:17 PM",
          "commitNameOld": "be7a0add8b6561d3c566237cc0370b06e7f32bb4",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 0.45,
          "commitsBetweenForRepo": 3,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,31 +1,39 @@\n-  public NumberReplicas countNodes(BlockInfo b) {\n+  public NumberReplicas countNodes(Block b) {\n     int decommissioned \u003d 0;\n     int decommissioning \u003d 0;\n     int live \u003d 0;\n+    int readonly \u003d 0;\n     int corrupt \u003d 0;\n     int excess \u003d 0;\n     int stale \u003d 0;\n     Collection\u003cDatanodeDescriptor\u003e nodesCorrupt \u003d corruptReplicas.getNodes(b);\n-    for(DatanodeStorageInfo storage : blocksMap.getStorages(b, State.NORMAL)) {\n+    for(DatanodeStorageInfo storage : blocksMap.getStorages(b)) {\n+      if (storage.getState() \u003d\u003d State.FAILED) {\n+        continue;\n+      } else if (storage.getState() \u003d\u003d State.READ_ONLY_SHARED) {\n+        readonly++;\n+        continue;\n+      }\n       final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n       if ((nodesCorrupt !\u003d null) \u0026\u0026 (nodesCorrupt.contains(node))) {\n         corrupt++;\n       } else if (node.isDecommissionInProgress()) {\n         decommissioning++;\n       } else if (node.isDecommissioned()) {\n         decommissioned++;\n       } else {\n         LightWeightHashSet\u003cBlockInfo\u003e blocksExcess \u003d excessReplicateMap.get(\n             node.getDatanodeUuid());\n         if (blocksExcess !\u003d null \u0026\u0026 blocksExcess.contains(b)) {\n           excess++;\n         } else {\n           live++;\n         }\n       }\n       if (storage.areBlockContentsStale()) {\n         stale++;\n       }\n     }\n-    return new NumberReplicas(live, decommissioned, decommissioning, corrupt, excess, stale);\n+    return new NumberReplicas(live, readonly, decommissioned, decommissioning,\n+        corrupt, excess, stale);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public NumberReplicas countNodes(Block b) {\n    int decommissioned \u003d 0;\n    int decommissioning \u003d 0;\n    int live \u003d 0;\n    int readonly \u003d 0;\n    int corrupt \u003d 0;\n    int excess \u003d 0;\n    int stale \u003d 0;\n    Collection\u003cDatanodeDescriptor\u003e nodesCorrupt \u003d corruptReplicas.getNodes(b);\n    for(DatanodeStorageInfo storage : blocksMap.getStorages(b)) {\n      if (storage.getState() \u003d\u003d State.FAILED) {\n        continue;\n      } else if (storage.getState() \u003d\u003d State.READ_ONLY_SHARED) {\n        readonly++;\n        continue;\n      }\n      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n      if ((nodesCorrupt !\u003d null) \u0026\u0026 (nodesCorrupt.contains(node))) {\n        corrupt++;\n      } else if (node.isDecommissionInProgress()) {\n        decommissioning++;\n      } else if (node.isDecommissioned()) {\n        decommissioned++;\n      } else {\n        LightWeightHashSet\u003cBlockInfo\u003e blocksExcess \u003d excessReplicateMap.get(\n            node.getDatanodeUuid());\n        if (blocksExcess !\u003d null \u0026\u0026 blocksExcess.contains(b)) {\n          excess++;\n        } else {\n          live++;\n        }\n      }\n      if (storage.areBlockContentsStale()) {\n        stale++;\n      }\n    }\n    return new NumberReplicas(live, readonly, decommissioned, decommissioning,\n        corrupt, excess, stale);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldValue": "[b-BlockInfo]",
            "newValue": "[b-Block]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-9205. Do not schedule corrupt blocks for replication.  (szetszwo)\n",
          "commitDate": "15/10/15 3:07 AM",
          "commitName": "5411dc559d5f73e4153e76fdff94a26869c17a37",
          "commitAuthor": "Tsz-Wo Nicholas Sze",
          "commitDateOld": "14/10/15 4:17 PM",
          "commitNameOld": "be7a0add8b6561d3c566237cc0370b06e7f32bb4",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 0.45,
          "commitsBetweenForRepo": 3,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,31 +1,39 @@\n-  public NumberReplicas countNodes(BlockInfo b) {\n+  public NumberReplicas countNodes(Block b) {\n     int decommissioned \u003d 0;\n     int decommissioning \u003d 0;\n     int live \u003d 0;\n+    int readonly \u003d 0;\n     int corrupt \u003d 0;\n     int excess \u003d 0;\n     int stale \u003d 0;\n     Collection\u003cDatanodeDescriptor\u003e nodesCorrupt \u003d corruptReplicas.getNodes(b);\n-    for(DatanodeStorageInfo storage : blocksMap.getStorages(b, State.NORMAL)) {\n+    for(DatanodeStorageInfo storage : blocksMap.getStorages(b)) {\n+      if (storage.getState() \u003d\u003d State.FAILED) {\n+        continue;\n+      } else if (storage.getState() \u003d\u003d State.READ_ONLY_SHARED) {\n+        readonly++;\n+        continue;\n+      }\n       final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n       if ((nodesCorrupt !\u003d null) \u0026\u0026 (nodesCorrupt.contains(node))) {\n         corrupt++;\n       } else if (node.isDecommissionInProgress()) {\n         decommissioning++;\n       } else if (node.isDecommissioned()) {\n         decommissioned++;\n       } else {\n         LightWeightHashSet\u003cBlockInfo\u003e blocksExcess \u003d excessReplicateMap.get(\n             node.getDatanodeUuid());\n         if (blocksExcess !\u003d null \u0026\u0026 blocksExcess.contains(b)) {\n           excess++;\n         } else {\n           live++;\n         }\n       }\n       if (storage.areBlockContentsStale()) {\n         stale++;\n       }\n     }\n-    return new NumberReplicas(live, decommissioned, decommissioning, corrupt, excess, stale);\n+    return new NumberReplicas(live, readonly, decommissioned, decommissioning,\n+        corrupt, excess, stale);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public NumberReplicas countNodes(Block b) {\n    int decommissioned \u003d 0;\n    int decommissioning \u003d 0;\n    int live \u003d 0;\n    int readonly \u003d 0;\n    int corrupt \u003d 0;\n    int excess \u003d 0;\n    int stale \u003d 0;\n    Collection\u003cDatanodeDescriptor\u003e nodesCorrupt \u003d corruptReplicas.getNodes(b);\n    for(DatanodeStorageInfo storage : blocksMap.getStorages(b)) {\n      if (storage.getState() \u003d\u003d State.FAILED) {\n        continue;\n      } else if (storage.getState() \u003d\u003d State.READ_ONLY_SHARED) {\n        readonly++;\n        continue;\n      }\n      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n      if ((nodesCorrupt !\u003d null) \u0026\u0026 (nodesCorrupt.contains(node))) {\n        corrupt++;\n      } else if (node.isDecommissionInProgress()) {\n        decommissioning++;\n      } else if (node.isDecommissioned()) {\n        decommissioned++;\n      } else {\n        LightWeightHashSet\u003cBlockInfo\u003e blocksExcess \u003d excessReplicateMap.get(\n            node.getDatanodeUuid());\n        if (blocksExcess !\u003d null \u0026\u0026 blocksExcess.contains(b)) {\n          excess++;\n        } else {\n          live++;\n        }\n      }\n      if (storage.areBlockContentsStale()) {\n        stale++;\n      }\n    }\n    return new NumberReplicas(live, readonly, decommissioned, decommissioning,\n        corrupt, excess, stale);\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {}
        }
      ]
    },
    "73b86a5046fe3262dde7b05be46b18575e35fd5f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8988. Use LightWeightHashSet instead of LightWeightLinkedSet in BlockManager#excessReplicateMap. (yliu)\n",
      "commitDate": "11/10/15 11:40 PM",
      "commitName": "73b86a5046fe3262dde7b05be46b18575e35fd5f",
      "commitAuthor": "yliu",
      "commitDateOld": "23/09/15 1:34 PM",
      "commitNameOld": "c09dc258a8f64fab852bf6f26187163480dbee3c",
      "commitAuthorOld": "Zhe Zhang",
      "daysBetweenCommits": 18.42,
      "commitsBetweenForRepo": 126,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,31 +1,31 @@\n   public NumberReplicas countNodes(BlockInfo b) {\n     int decommissioned \u003d 0;\n     int decommissioning \u003d 0;\n     int live \u003d 0;\n     int corrupt \u003d 0;\n     int excess \u003d 0;\n     int stale \u003d 0;\n     Collection\u003cDatanodeDescriptor\u003e nodesCorrupt \u003d corruptReplicas.getNodes(b);\n     for(DatanodeStorageInfo storage : blocksMap.getStorages(b, State.NORMAL)) {\n       final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n       if ((nodesCorrupt !\u003d null) \u0026\u0026 (nodesCorrupt.contains(node))) {\n         corrupt++;\n       } else if (node.isDecommissionInProgress()) {\n         decommissioning++;\n       } else if (node.isDecommissioned()) {\n         decommissioned++;\n       } else {\n-        LightWeightLinkedSet\u003cBlockInfo\u003e blocksExcess \u003d excessReplicateMap.get(\n+        LightWeightHashSet\u003cBlockInfo\u003e blocksExcess \u003d excessReplicateMap.get(\n             node.getDatanodeUuid());\n         if (blocksExcess !\u003d null \u0026\u0026 blocksExcess.contains(b)) {\n           excess++;\n         } else {\n           live++;\n         }\n       }\n       if (storage.areBlockContentsStale()) {\n         stale++;\n       }\n     }\n     return new NumberReplicas(live, decommissioned, decommissioning, corrupt, excess, stale);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public NumberReplicas countNodes(BlockInfo b) {\n    int decommissioned \u003d 0;\n    int decommissioning \u003d 0;\n    int live \u003d 0;\n    int corrupt \u003d 0;\n    int excess \u003d 0;\n    int stale \u003d 0;\n    Collection\u003cDatanodeDescriptor\u003e nodesCorrupt \u003d corruptReplicas.getNodes(b);\n    for(DatanodeStorageInfo storage : blocksMap.getStorages(b, State.NORMAL)) {\n      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n      if ((nodesCorrupt !\u003d null) \u0026\u0026 (nodesCorrupt.contains(node))) {\n        corrupt++;\n      } else if (node.isDecommissionInProgress()) {\n        decommissioning++;\n      } else if (node.isDecommissioned()) {\n        decommissioned++;\n      } else {\n        LightWeightHashSet\u003cBlockInfo\u003e blocksExcess \u003d excessReplicateMap.get(\n            node.getDatanodeUuid());\n        if (blocksExcess !\u003d null \u0026\u0026 blocksExcess.contains(b)) {\n          excess++;\n        } else {\n          live++;\n        }\n      }\n      if (storage.areBlockContentsStale()) {\n        stale++;\n      }\n    }\n    return new NumberReplicas(live, decommissioned, decommissioning, corrupt, excess, stale);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "663eba0ab1c73b45f98e46ffc87ad8fd91584046": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"HDFS-8623. Refactor NameNode handling of invalid, corrupt, and under-recovery blocks. Contributed by Zhe Zhang.\"\n\nThis reverts commit de480d6c8945bd8b5b00e8657b7a72ce8dd9b6b5.\n",
      "commitDate": "06/08/15 10:21 AM",
      "commitName": "663eba0ab1c73b45f98e46ffc87ad8fd91584046",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "31/07/15 4:15 PM",
      "commitNameOld": "d311a38a6b32bbb210bd8748cfb65463e9c0740e",
      "commitAuthorOld": "Xiaoyu Yao",
      "daysBetweenCommits": 5.75,
      "commitsBetweenForRepo": 23,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,31 +1,31 @@\n   public NumberReplicas countNodes(BlockInfo b) {\n     int decommissioned \u003d 0;\n     int decommissioning \u003d 0;\n     int live \u003d 0;\n     int corrupt \u003d 0;\n     int excess \u003d 0;\n     int stale \u003d 0;\n     Collection\u003cDatanodeDescriptor\u003e nodesCorrupt \u003d corruptReplicas.getNodes(b);\n     for(DatanodeStorageInfo storage : blocksMap.getStorages(b, State.NORMAL)) {\n       final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n       if ((nodesCorrupt !\u003d null) \u0026\u0026 (nodesCorrupt.contains(node))) {\n         corrupt++;\n       } else if (node.isDecommissionInProgress()) {\n         decommissioning++;\n       } else if (node.isDecommissioned()) {\n         decommissioned++;\n       } else {\n-        LightWeightLinkedSet\u003cBlockInfo\u003e blocksExcess \u003d\n-            excessReplicateMap.get(node.getDatanodeUuid());\n+        LightWeightLinkedSet\u003cBlock\u003e blocksExcess \u003d excessReplicateMap.get(node\n+            .getDatanodeUuid());\n         if (blocksExcess !\u003d null \u0026\u0026 blocksExcess.contains(b)) {\n           excess++;\n         } else {\n           live++;\n         }\n       }\n       if (storage.areBlockContentsStale()) {\n         stale++;\n       }\n     }\n     return new NumberReplicas(live, decommissioned, decommissioning, corrupt, excess, stale);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public NumberReplicas countNodes(BlockInfo b) {\n    int decommissioned \u003d 0;\n    int decommissioning \u003d 0;\n    int live \u003d 0;\n    int corrupt \u003d 0;\n    int excess \u003d 0;\n    int stale \u003d 0;\n    Collection\u003cDatanodeDescriptor\u003e nodesCorrupt \u003d corruptReplicas.getNodes(b);\n    for(DatanodeStorageInfo storage : blocksMap.getStorages(b, State.NORMAL)) {\n      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n      if ((nodesCorrupt !\u003d null) \u0026\u0026 (nodesCorrupt.contains(node))) {\n        corrupt++;\n      } else if (node.isDecommissionInProgress()) {\n        decommissioning++;\n      } else if (node.isDecommissioned()) {\n        decommissioned++;\n      } else {\n        LightWeightLinkedSet\u003cBlock\u003e blocksExcess \u003d excessReplicateMap.get(node\n            .getDatanodeUuid());\n        if (blocksExcess !\u003d null \u0026\u0026 blocksExcess.contains(b)) {\n          excess++;\n        } else {\n          live++;\n        }\n      }\n      if (storage.areBlockContentsStale()) {\n        stale++;\n      }\n    }\n    return new NumberReplicas(live, decommissioned, decommissioning, corrupt, excess, stale);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "de480d6c8945bd8b5b00e8657b7a72ce8dd9b6b5": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8623. Refactor NameNode handling of invalid, corrupt, and under-recovery blocks. Contributed by Zhe Zhang.\n",
      "commitDate": "26/06/15 10:49 AM",
      "commitName": "de480d6c8945bd8b5b00e8657b7a72ce8dd9b6b5",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "24/06/15 2:42 PM",
      "commitNameOld": "afe9ea3c12e1f5a71922400eadb642960bc87ca1",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 1.84,
      "commitsBetweenForRepo": 12,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,31 +1,31 @@\n   public NumberReplicas countNodes(BlockInfo b) {\n     int decommissioned \u003d 0;\n     int decommissioning \u003d 0;\n     int live \u003d 0;\n     int corrupt \u003d 0;\n     int excess \u003d 0;\n     int stale \u003d 0;\n     Collection\u003cDatanodeDescriptor\u003e nodesCorrupt \u003d corruptReplicas.getNodes(b);\n     for(DatanodeStorageInfo storage : blocksMap.getStorages(b, State.NORMAL)) {\n       final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n       if ((nodesCorrupt !\u003d null) \u0026\u0026 (nodesCorrupt.contains(node))) {\n         corrupt++;\n       } else if (node.isDecommissionInProgress()) {\n         decommissioning++;\n       } else if (node.isDecommissioned()) {\n         decommissioned++;\n       } else {\n-        LightWeightLinkedSet\u003cBlock\u003e blocksExcess \u003d excessReplicateMap.get(node\n-            .getDatanodeUuid());\n+        LightWeightLinkedSet\u003cBlockInfo\u003e blocksExcess \u003d\n+            excessReplicateMap.get(node.getDatanodeUuid());\n         if (blocksExcess !\u003d null \u0026\u0026 blocksExcess.contains(b)) {\n           excess++;\n         } else {\n           live++;\n         }\n       }\n       if (storage.areBlockContentsStale()) {\n         stale++;\n       }\n     }\n     return new NumberReplicas(live, decommissioned, decommissioning, corrupt, excess, stale);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public NumberReplicas countNodes(BlockInfo b) {\n    int decommissioned \u003d 0;\n    int decommissioning \u003d 0;\n    int live \u003d 0;\n    int corrupt \u003d 0;\n    int excess \u003d 0;\n    int stale \u003d 0;\n    Collection\u003cDatanodeDescriptor\u003e nodesCorrupt \u003d corruptReplicas.getNodes(b);\n    for(DatanodeStorageInfo storage : blocksMap.getStorages(b, State.NORMAL)) {\n      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n      if ((nodesCorrupt !\u003d null) \u0026\u0026 (nodesCorrupt.contains(node))) {\n        corrupt++;\n      } else if (node.isDecommissionInProgress()) {\n        decommissioning++;\n      } else if (node.isDecommissioned()) {\n        decommissioned++;\n      } else {\n        LightWeightLinkedSet\u003cBlockInfo\u003e blocksExcess \u003d\n            excessReplicateMap.get(node.getDatanodeUuid());\n        if (blocksExcess !\u003d null \u0026\u0026 blocksExcess.contains(b)) {\n          excess++;\n        } else {\n          live++;\n        }\n      }\n      if (storage.areBlockContentsStale()) {\n        stale++;\n      }\n    }\n    return new NumberReplicas(live, decommissioned, decommissioning, corrupt, excess, stale);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "6e3fcffe291faec40fa9214f4880a35a952836c4": {
      "type": "Yparameterchange",
      "commitMessage": "HDFS-8608. Merge HDFS-7912 to trunk and branch-2 (track BlockInfo instead of Block in UnderReplicatedBlocks and PendingReplicationBlocks). Contributed by Zhe Zhang.\n",
      "commitDate": "17/06/15 8:05 AM",
      "commitName": "6e3fcffe291faec40fa9214f4880a35a952836c4",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "12/06/15 11:38 AM",
      "commitNameOld": "c17439c2ddd921b63b1635e6f1cba634b8da8557",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 4.85,
      "commitsBetweenForRepo": 29,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,31 +1,31 @@\n-  public NumberReplicas countNodes(Block b) {\n+  public NumberReplicas countNodes(BlockInfo b) {\n     int decommissioned \u003d 0;\n     int decommissioning \u003d 0;\n     int live \u003d 0;\n     int corrupt \u003d 0;\n     int excess \u003d 0;\n     int stale \u003d 0;\n     Collection\u003cDatanodeDescriptor\u003e nodesCorrupt \u003d corruptReplicas.getNodes(b);\n     for(DatanodeStorageInfo storage : blocksMap.getStorages(b, State.NORMAL)) {\n       final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n       if ((nodesCorrupt !\u003d null) \u0026\u0026 (nodesCorrupt.contains(node))) {\n         corrupt++;\n       } else if (node.isDecommissionInProgress()) {\n         decommissioning++;\n       } else if (node.isDecommissioned()) {\n         decommissioned++;\n       } else {\n         LightWeightLinkedSet\u003cBlock\u003e blocksExcess \u003d excessReplicateMap.get(node\n             .getDatanodeUuid());\n         if (blocksExcess !\u003d null \u0026\u0026 blocksExcess.contains(b)) {\n           excess++;\n         } else {\n           live++;\n         }\n       }\n       if (storage.areBlockContentsStale()) {\n         stale++;\n       }\n     }\n     return new NumberReplicas(live, decommissioned, decommissioning, corrupt, excess, stale);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public NumberReplicas countNodes(BlockInfo b) {\n    int decommissioned \u003d 0;\n    int decommissioning \u003d 0;\n    int live \u003d 0;\n    int corrupt \u003d 0;\n    int excess \u003d 0;\n    int stale \u003d 0;\n    Collection\u003cDatanodeDescriptor\u003e nodesCorrupt \u003d corruptReplicas.getNodes(b);\n    for(DatanodeStorageInfo storage : blocksMap.getStorages(b, State.NORMAL)) {\n      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n      if ((nodesCorrupt !\u003d null) \u0026\u0026 (nodesCorrupt.contains(node))) {\n        corrupt++;\n      } else if (node.isDecommissionInProgress()) {\n        decommissioning++;\n      } else if (node.isDecommissioned()) {\n        decommissioned++;\n      } else {\n        LightWeightLinkedSet\u003cBlock\u003e blocksExcess \u003d excessReplicateMap.get(node\n            .getDatanodeUuid());\n        if (blocksExcess !\u003d null \u0026\u0026 blocksExcess.contains(b)) {\n          excess++;\n        } else {\n          live++;\n        }\n      }\n      if (storage.areBlockContentsStale()) {\n        stale++;\n      }\n    }\n    return new NumberReplicas(live, decommissioned, decommissioning, corrupt, excess, stale);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {
        "oldValue": "[b-Block]",
        "newValue": "[b-BlockInfo]"
      }
    },
    "f8f5887209a7d8e53c0a77abef275cbcaf1f7a5b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7933. fsck should also report decommissioning replicas. Contributed by Xiaoyu Yao.\n",
      "commitDate": "11/04/15 1:23 PM",
      "commitName": "f8f5887209a7d8e53c0a77abef275cbcaf1f7a5b",
      "commitAuthor": "cnauroth",
      "commitDateOld": "10/04/15 4:36 PM",
      "commitNameOld": "36e4cd3be6f7fec8db82d3d1bcb258af470ece2e",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 0.87,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,31 @@\n   public NumberReplicas countNodes(Block b) {\n     int decommissioned \u003d 0;\n+    int decommissioning \u003d 0;\n     int live \u003d 0;\n     int corrupt \u003d 0;\n     int excess \u003d 0;\n     int stale \u003d 0;\n     Collection\u003cDatanodeDescriptor\u003e nodesCorrupt \u003d corruptReplicas.getNodes(b);\n     for(DatanodeStorageInfo storage : blocksMap.getStorages(b, State.NORMAL)) {\n       final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n       if ((nodesCorrupt !\u003d null) \u0026\u0026 (nodesCorrupt.contains(node))) {\n         corrupt++;\n-      } else if (node.isDecommissionInProgress() || node.isDecommissioned()) {\n+      } else if (node.isDecommissionInProgress()) {\n+        decommissioning++;\n+      } else if (node.isDecommissioned()) {\n         decommissioned++;\n       } else {\n         LightWeightLinkedSet\u003cBlock\u003e blocksExcess \u003d excessReplicateMap.get(node\n             .getDatanodeUuid());\n         if (blocksExcess !\u003d null \u0026\u0026 blocksExcess.contains(b)) {\n           excess++;\n         } else {\n           live++;\n         }\n       }\n       if (storage.areBlockContentsStale()) {\n         stale++;\n       }\n     }\n-    return new NumberReplicas(live, decommissioned, corrupt, excess, stale);\n+    return new NumberReplicas(live, decommissioned, decommissioning, corrupt, excess, stale);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public NumberReplicas countNodes(Block b) {\n    int decommissioned \u003d 0;\n    int decommissioning \u003d 0;\n    int live \u003d 0;\n    int corrupt \u003d 0;\n    int excess \u003d 0;\n    int stale \u003d 0;\n    Collection\u003cDatanodeDescriptor\u003e nodesCorrupt \u003d corruptReplicas.getNodes(b);\n    for(DatanodeStorageInfo storage : blocksMap.getStorages(b, State.NORMAL)) {\n      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n      if ((nodesCorrupt !\u003d null) \u0026\u0026 (nodesCorrupt.contains(node))) {\n        corrupt++;\n      } else if (node.isDecommissionInProgress()) {\n        decommissioning++;\n      } else if (node.isDecommissioned()) {\n        decommissioned++;\n      } else {\n        LightWeightLinkedSet\u003cBlock\u003e blocksExcess \u003d excessReplicateMap.get(node\n            .getDatanodeUuid());\n        if (blocksExcess !\u003d null \u0026\u0026 blocksExcess.contains(b)) {\n          excess++;\n        } else {\n          live++;\n        }\n      }\n      if (storage.areBlockContentsStale()) {\n        stale++;\n      }\n    }\n    return new NumberReplicas(live, decommissioned, decommissioning, corrupt, excess, stale);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "2f341414dd4a052bee3907ff4a6db283a15f9d53": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5318. Support read-only and read-write paths to shared replicas. (Contributed by Eric Sirianni)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1569951 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/02/14 2:59 PM",
      "commitName": "2f341414dd4a052bee3907ff4a6db283a15f9d53",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "31/01/14 1:00 PM",
      "commitNameOld": "5beeb3016954a3ee0c1fb10a2083ffd540cd2c14",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 19.08,
      "commitsBetweenForRepo": 153,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,28 @@\n   public NumberReplicas countNodes(Block b) {\n     int decommissioned \u003d 0;\n     int live \u003d 0;\n     int corrupt \u003d 0;\n     int excess \u003d 0;\n     int stale \u003d 0;\n     Collection\u003cDatanodeDescriptor\u003e nodesCorrupt \u003d corruptReplicas.getNodes(b);\n-    for(DatanodeStorageInfo storage : blocksMap.getStorages(b)) {\n+    for(DatanodeStorageInfo storage : blocksMap.getStorages(b, State.NORMAL)) {\n       final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n       if ((nodesCorrupt !\u003d null) \u0026\u0026 (nodesCorrupt.contains(node))) {\n         corrupt++;\n       } else if (node.isDecommissionInProgress() || node.isDecommissioned()) {\n         decommissioned++;\n       } else {\n         LightWeightLinkedSet\u003cBlock\u003e blocksExcess \u003d excessReplicateMap.get(node\n             .getDatanodeUuid());\n         if (blocksExcess !\u003d null \u0026\u0026 blocksExcess.contains(b)) {\n           excess++;\n         } else {\n           live++;\n         }\n       }\n       if (storage.areBlockContentsStale()) {\n         stale++;\n       }\n     }\n     return new NumberReplicas(live, decommissioned, corrupt, excess, stale);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public NumberReplicas countNodes(Block b) {\n    int decommissioned \u003d 0;\n    int live \u003d 0;\n    int corrupt \u003d 0;\n    int excess \u003d 0;\n    int stale \u003d 0;\n    Collection\u003cDatanodeDescriptor\u003e nodesCorrupt \u003d corruptReplicas.getNodes(b);\n    for(DatanodeStorageInfo storage : blocksMap.getStorages(b, State.NORMAL)) {\n      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n      if ((nodesCorrupt !\u003d null) \u0026\u0026 (nodesCorrupt.contains(node))) {\n        corrupt++;\n      } else if (node.isDecommissionInProgress() || node.isDecommissioned()) {\n        decommissioned++;\n      } else {\n        LightWeightLinkedSet\u003cBlock\u003e blocksExcess \u003d excessReplicateMap.get(node\n            .getDatanodeUuid());\n        if (blocksExcess !\u003d null \u0026\u0026 blocksExcess.contains(b)) {\n          excess++;\n        } else {\n          live++;\n        }\n      }\n      if (storage.areBlockContentsStale()) {\n        stale++;\n      }\n    }\n    return new NumberReplicas(live, decommissioned, corrupt, excess, stale);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "4551da302d94cffea0313eac79479ab6f9b7cb34": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5233. Use Datanode UUID to identify Datanodes.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1525407 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "22/09/13 11:03 AM",
      "commitName": "4551da302d94cffea0313eac79479ab6f9b7cb34",
      "commitAuthor": "Arpit Agarwal",
      "commitDateOld": "18/09/13 8:12 AM",
      "commitNameOld": "abf09f090f77a7e54e331b7a07354e7926b60dc9",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 4.12,
      "commitsBetweenForRepo": 6,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,28 @@\n   public NumberReplicas countNodes(Block b) {\n     int decommissioned \u003d 0;\n     int live \u003d 0;\n     int corrupt \u003d 0;\n     int excess \u003d 0;\n     int stale \u003d 0;\n     Collection\u003cDatanodeDescriptor\u003e nodesCorrupt \u003d corruptReplicas.getNodes(b);\n     for(DatanodeStorageInfo storage : blocksMap.getStorages(b)) {\n       final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n       if ((nodesCorrupt !\u003d null) \u0026\u0026 (nodesCorrupt.contains(node))) {\n         corrupt++;\n       } else if (node.isDecommissionInProgress() || node.isDecommissioned()) {\n         decommissioned++;\n       } else {\n         LightWeightLinkedSet\u003cBlock\u003e blocksExcess \u003d excessReplicateMap.get(node\n-            .getStorageID());\n+            .getDatanodeUuid());\n         if (blocksExcess !\u003d null \u0026\u0026 blocksExcess.contains(b)) {\n           excess++;\n         } else {\n           live++;\n         }\n       }\n       if (storage.areBlockContentsStale()) {\n         stale++;\n       }\n     }\n     return new NumberReplicas(live, decommissioned, corrupt, excess, stale);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public NumberReplicas countNodes(Block b) {\n    int decommissioned \u003d 0;\n    int live \u003d 0;\n    int corrupt \u003d 0;\n    int excess \u003d 0;\n    int stale \u003d 0;\n    Collection\u003cDatanodeDescriptor\u003e nodesCorrupt \u003d corruptReplicas.getNodes(b);\n    for(DatanodeStorageInfo storage : blocksMap.getStorages(b)) {\n      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n      if ((nodesCorrupt !\u003d null) \u0026\u0026 (nodesCorrupt.contains(node))) {\n        corrupt++;\n      } else if (node.isDecommissionInProgress() || node.isDecommissioned()) {\n        decommissioned++;\n      } else {\n        LightWeightLinkedSet\u003cBlock\u003e blocksExcess \u003d excessReplicateMap.get(node\n            .getDatanodeUuid());\n        if (blocksExcess !\u003d null \u0026\u0026 blocksExcess.contains(b)) {\n          excess++;\n        } else {\n          live++;\n        }\n      }\n      if (storage.areBlockContentsStale()) {\n        stale++;\n      }\n    }\n    return new NumberReplicas(live, decommissioned, corrupt, excess, stale);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "282be1b38e5cd141ed7e2b2194bfb67a7c2f7f15": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5134. Move blockContentsStale, heartbeatedSinceFailover and firstBlockReport from DatanodeDescriptor to DatanodeStorageInfo; and fix a synchronization problem in DatanodeStorageInfo.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1520938 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "08/09/13 3:53 PM",
      "commitName": "282be1b38e5cd141ed7e2b2194bfb67a7c2f7f15",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "03/09/13 7:03 AM",
      "commitNameOld": "3f070e83b1f4e0211ece8c0ab508a61188ad352a",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 5.37,
      "commitsBetweenForRepo": 12,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,28 @@\n   public NumberReplicas countNodes(Block b) {\n     int decommissioned \u003d 0;\n     int live \u003d 0;\n     int corrupt \u003d 0;\n     int excess \u003d 0;\n     int stale \u003d 0;\n     Collection\u003cDatanodeDescriptor\u003e nodesCorrupt \u003d corruptReplicas.getNodes(b);\n     for(DatanodeStorageInfo storage : blocksMap.getStorages(b)) {\n       final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n       if ((nodesCorrupt !\u003d null) \u0026\u0026 (nodesCorrupt.contains(node))) {\n         corrupt++;\n       } else if (node.isDecommissionInProgress() || node.isDecommissioned()) {\n         decommissioned++;\n       } else {\n         LightWeightLinkedSet\u003cBlock\u003e blocksExcess \u003d excessReplicateMap.get(node\n             .getStorageID());\n         if (blocksExcess !\u003d null \u0026\u0026 blocksExcess.contains(b)) {\n           excess++;\n         } else {\n           live++;\n         }\n       }\n-      if (node.areBlockContentsStale()) {\n+      if (storage.areBlockContentsStale()) {\n         stale++;\n       }\n     }\n     return new NumberReplicas(live, decommissioned, corrupt, excess, stale);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public NumberReplicas countNodes(Block b) {\n    int decommissioned \u003d 0;\n    int live \u003d 0;\n    int corrupt \u003d 0;\n    int excess \u003d 0;\n    int stale \u003d 0;\n    Collection\u003cDatanodeDescriptor\u003e nodesCorrupt \u003d corruptReplicas.getNodes(b);\n    for(DatanodeStorageInfo storage : blocksMap.getStorages(b)) {\n      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n      if ((nodesCorrupt !\u003d null) \u0026\u0026 (nodesCorrupt.contains(node))) {\n        corrupt++;\n      } else if (node.isDecommissionInProgress() || node.isDecommissioned()) {\n        decommissioned++;\n      } else {\n        LightWeightLinkedSet\u003cBlock\u003e blocksExcess \u003d excessReplicateMap.get(node\n            .getStorageID());\n        if (blocksExcess !\u003d null \u0026\u0026 blocksExcess.contains(b)) {\n          excess++;\n        } else {\n          live++;\n        }\n      }\n      if (storage.areBlockContentsStale()) {\n        stale++;\n      }\n    }\n    return new NumberReplicas(live, decommissioned, corrupt, excess, stale);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "3f070e83b1f4e0211ece8c0ab508a61188ad352a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5009. Include storage information in the LocatedBlock.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1519691 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "03/09/13 7:03 AM",
      "commitName": "3f070e83b1f4e0211ece8c0ab508a61188ad352a",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "27/08/13 11:30 PM",
      "commitNameOld": "5d9d702607913685eab0d8ad077040ddc82bf085",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 6.31,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,29 +1,28 @@\n   public NumberReplicas countNodes(Block b) {\n     int decommissioned \u003d 0;\n     int live \u003d 0;\n     int corrupt \u003d 0;\n     int excess \u003d 0;\n     int stale \u003d 0;\n-    Iterator\u003cDatanodeDescriptor\u003e nodeIter \u003d blocksMap.nodeIterator(b);\n     Collection\u003cDatanodeDescriptor\u003e nodesCorrupt \u003d corruptReplicas.getNodes(b);\n-    while (nodeIter.hasNext()) {\n-      DatanodeDescriptor node \u003d nodeIter.next();\n+    for(DatanodeStorageInfo storage : blocksMap.getStorages(b)) {\n+      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n       if ((nodesCorrupt !\u003d null) \u0026\u0026 (nodesCorrupt.contains(node))) {\n         corrupt++;\n       } else if (node.isDecommissionInProgress() || node.isDecommissioned()) {\n         decommissioned++;\n       } else {\n         LightWeightLinkedSet\u003cBlock\u003e blocksExcess \u003d excessReplicateMap.get(node\n             .getStorageID());\n         if (blocksExcess !\u003d null \u0026\u0026 blocksExcess.contains(b)) {\n           excess++;\n         } else {\n           live++;\n         }\n       }\n       if (node.areBlockContentsStale()) {\n         stale++;\n       }\n     }\n     return new NumberReplicas(live, decommissioned, corrupt, excess, stale);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public NumberReplicas countNodes(Block b) {\n    int decommissioned \u003d 0;\n    int live \u003d 0;\n    int corrupt \u003d 0;\n    int excess \u003d 0;\n    int stale \u003d 0;\n    Collection\u003cDatanodeDescriptor\u003e nodesCorrupt \u003d corruptReplicas.getNodes(b);\n    for(DatanodeStorageInfo storage : blocksMap.getStorages(b)) {\n      final DatanodeDescriptor node \u003d storage.getDatanodeDescriptor();\n      if ((nodesCorrupt !\u003d null) \u0026\u0026 (nodesCorrupt.contains(node))) {\n        corrupt++;\n      } else if (node.isDecommissionInProgress() || node.isDecommissioned()) {\n        decommissioned++;\n      } else {\n        LightWeightLinkedSet\u003cBlock\u003e blocksExcess \u003d excessReplicateMap.get(node\n            .getStorageID());\n        if (blocksExcess !\u003d null \u0026\u0026 blocksExcess.contains(b)) {\n          excess++;\n        } else {\n          live++;\n        }\n      }\n      if (node.areBlockContentsStale()) {\n        stale++;\n      }\n    }\n    return new NumberReplicas(live, decommissioned, corrupt, excess, stale);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "31c91706f7d17da006ef2d6c541f8dd092fae077": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-1972. Fencing mechanism for block invalidations and replications. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1221608 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "20/12/11 8:32 PM",
      "commitName": "31c91706f7d17da006ef2d6c541f8dd092fae077",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "20/12/11 7:03 PM",
      "commitNameOld": "36d1c49486587c2dbb193e8538b1d4510c462fa6",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.06,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,25 +1,29 @@\n   public NumberReplicas countNodes(Block b) {\n-    int count \u003d 0;\n+    int decommissioned \u003d 0;\n     int live \u003d 0;\n     int corrupt \u003d 0;\n     int excess \u003d 0;\n+    int stale \u003d 0;\n     Iterator\u003cDatanodeDescriptor\u003e nodeIter \u003d blocksMap.nodeIterator(b);\n     Collection\u003cDatanodeDescriptor\u003e nodesCorrupt \u003d corruptReplicas.getNodes(b);\n     while (nodeIter.hasNext()) {\n       DatanodeDescriptor node \u003d nodeIter.next();\n       if ((nodesCorrupt !\u003d null) \u0026\u0026 (nodesCorrupt.contains(node))) {\n         corrupt++;\n       } else if (node.isDecommissionInProgress() || node.isDecommissioned()) {\n-        count++;\n+        decommissioned++;\n       } else {\n         LightWeightLinkedSet\u003cBlock\u003e blocksExcess \u003d excessReplicateMap.get(node\n             .getStorageID());\n         if (blocksExcess !\u003d null \u0026\u0026 blocksExcess.contains(b)) {\n           excess++;\n         } else {\n           live++;\n         }\n       }\n+      if (node.areBlockContentsStale()) {\n+        stale++;\n+      }\n     }\n-    return new NumberReplicas(live, count, corrupt, excess);\n+    return new NumberReplicas(live, decommissioned, corrupt, excess, stale);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public NumberReplicas countNodes(Block b) {\n    int decommissioned \u003d 0;\n    int live \u003d 0;\n    int corrupt \u003d 0;\n    int excess \u003d 0;\n    int stale \u003d 0;\n    Iterator\u003cDatanodeDescriptor\u003e nodeIter \u003d blocksMap.nodeIterator(b);\n    Collection\u003cDatanodeDescriptor\u003e nodesCorrupt \u003d corruptReplicas.getNodes(b);\n    while (nodeIter.hasNext()) {\n      DatanodeDescriptor node \u003d nodeIter.next();\n      if ((nodesCorrupt !\u003d null) \u0026\u0026 (nodesCorrupt.contains(node))) {\n        corrupt++;\n      } else if (node.isDecommissionInProgress() || node.isDecommissioned()) {\n        decommissioned++;\n      } else {\n        LightWeightLinkedSet\u003cBlock\u003e blocksExcess \u003d excessReplicateMap.get(node\n            .getStorageID());\n        if (blocksExcess !\u003d null \u0026\u0026 blocksExcess.contains(b)) {\n          excess++;\n        } else {\n          live++;\n        }\n      }\n      if (node.areBlockContentsStale()) {\n        stale++;\n      }\n    }\n    return new NumberReplicas(live, decommissioned, corrupt, excess, stale);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "9a3f147fdd5421460889b266ead3a2300323cda2": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2476. More CPU efficient data structure for under-replicated, over-replicated, and invalidated blocks. Contributed by Tomasz Nykiel.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1201991 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/11/11 5:13 PM",
      "commitName": "9a3f147fdd5421460889b266ead3a2300323cda2",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "07/11/11 4:16 PM",
      "commitNameOld": "6dff9329d5befb3af51b82194d26b244ea87f511",
      "commitAuthorOld": "Hairong Kuang",
      "daysBetweenCommits": 7.04,
      "commitsBetweenForRepo": 21,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,25 +1,25 @@\n   public NumberReplicas countNodes(Block b) {\n     int count \u003d 0;\n     int live \u003d 0;\n     int corrupt \u003d 0;\n     int excess \u003d 0;\n     Iterator\u003cDatanodeDescriptor\u003e nodeIter \u003d blocksMap.nodeIterator(b);\n     Collection\u003cDatanodeDescriptor\u003e nodesCorrupt \u003d corruptReplicas.getNodes(b);\n     while (nodeIter.hasNext()) {\n       DatanodeDescriptor node \u003d nodeIter.next();\n       if ((nodesCorrupt !\u003d null) \u0026\u0026 (nodesCorrupt.contains(node))) {\n         corrupt++;\n       } else if (node.isDecommissionInProgress() || node.isDecommissioned()) {\n         count++;\n       } else {\n-        Collection\u003cBlock\u003e blocksExcess \u003d\n-          excessReplicateMap.get(node.getStorageID());\n+        LightWeightLinkedSet\u003cBlock\u003e blocksExcess \u003d excessReplicateMap.get(node\n+            .getStorageID());\n         if (blocksExcess !\u003d null \u0026\u0026 blocksExcess.contains(b)) {\n           excess++;\n         } else {\n           live++;\n         }\n       }\n     }\n     return new NumberReplicas(live, count, corrupt, excess);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public NumberReplicas countNodes(Block b) {\n    int count \u003d 0;\n    int live \u003d 0;\n    int corrupt \u003d 0;\n    int excess \u003d 0;\n    Iterator\u003cDatanodeDescriptor\u003e nodeIter \u003d blocksMap.nodeIterator(b);\n    Collection\u003cDatanodeDescriptor\u003e nodesCorrupt \u003d corruptReplicas.getNodes(b);\n    while (nodeIter.hasNext()) {\n      DatanodeDescriptor node \u003d nodeIter.next();\n      if ((nodesCorrupt !\u003d null) \u0026\u0026 (nodesCorrupt.contains(node))) {\n        corrupt++;\n      } else if (node.isDecommissionInProgress() || node.isDecommissioned()) {\n        count++;\n      } else {\n        LightWeightLinkedSet\u003cBlock\u003e blocksExcess \u003d excessReplicateMap.get(node\n            .getStorageID());\n        if (blocksExcess !\u003d null \u0026\u0026 blocksExcess.contains(b)) {\n          excess++;\n        } else {\n          live++;\n        }\n      }\n    }\n    return new NumberReplicas(live, count, corrupt, excess);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public NumberReplicas countNodes(Block b) {\n    int count \u003d 0;\n    int live \u003d 0;\n    int corrupt \u003d 0;\n    int excess \u003d 0;\n    Iterator\u003cDatanodeDescriptor\u003e nodeIter \u003d blocksMap.nodeIterator(b);\n    Collection\u003cDatanodeDescriptor\u003e nodesCorrupt \u003d corruptReplicas.getNodes(b);\n    while (nodeIter.hasNext()) {\n      DatanodeDescriptor node \u003d nodeIter.next();\n      if ((nodesCorrupt !\u003d null) \u0026\u0026 (nodesCorrupt.contains(node))) {\n        corrupt++;\n      } else if (node.isDecommissionInProgress() || node.isDecommissioned()) {\n        count++;\n      } else {\n        Collection\u003cBlock\u003e blocksExcess \u003d\n          excessReplicateMap.get(node.getStorageID());\n        if (blocksExcess !\u003d null \u0026\u0026 blocksExcess.contains(b)) {\n          excess++;\n        } else {\n          live++;\n        }\n      }\n    }\n    return new NumberReplicas(live, count, corrupt, excess);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public NumberReplicas countNodes(Block b) {\n    int count \u003d 0;\n    int live \u003d 0;\n    int corrupt \u003d 0;\n    int excess \u003d 0;\n    Iterator\u003cDatanodeDescriptor\u003e nodeIter \u003d blocksMap.nodeIterator(b);\n    Collection\u003cDatanodeDescriptor\u003e nodesCorrupt \u003d corruptReplicas.getNodes(b);\n    while (nodeIter.hasNext()) {\n      DatanodeDescriptor node \u003d nodeIter.next();\n      if ((nodesCorrupt !\u003d null) \u0026\u0026 (nodesCorrupt.contains(node))) {\n        corrupt++;\n      } else if (node.isDecommissionInProgress() || node.isDecommissioned()) {\n        count++;\n      } else {\n        Collection\u003cBlock\u003e blocksExcess \u003d\n          excessReplicateMap.get(node.getStorageID());\n        if (blocksExcess !\u003d null \u0026\u0026 blocksExcess.contains(b)) {\n          excess++;\n        } else {\n          live++;\n        }\n      }\n    }\n    return new NumberReplicas(live, count, corrupt, excess);\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java"
      }
    },
    "09b6f98de431628c80bc8a6faf0070eeaf72ff2a": {
      "type": "Ymultichange(Yfilerename,Ymodifierchange)",
      "commitMessage": "HDFS-2107. Move block management code from o.a.h.h.s.namenode to a new package o.a.h.h.s.blockmanagement.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1140939 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "28/06/11 6:31 PM",
      "commitName": "09b6f98de431628c80bc8a6faf0070eeaf72ff2a",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Yfilerename",
          "commitMessage": "HDFS-2107. Move block management code from o.a.h.h.s.namenode to a new package o.a.h.h.s.blockmanagement.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1140939 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "28/06/11 6:31 PM",
          "commitName": "09b6f98de431628c80bc8a6faf0070eeaf72ff2a",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "28/06/11 5:26 PM",
          "commitNameOld": "97b6ca4dd7d1233e8f8c90b1c01e47228c044e13",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 0.04,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,25 +1,25 @@\n-  NumberReplicas countNodes(Block b) {\n+  public NumberReplicas countNodes(Block b) {\n     int count \u003d 0;\n     int live \u003d 0;\n     int corrupt \u003d 0;\n     int excess \u003d 0;\n     Iterator\u003cDatanodeDescriptor\u003e nodeIter \u003d blocksMap.nodeIterator(b);\n     Collection\u003cDatanodeDescriptor\u003e nodesCorrupt \u003d corruptReplicas.getNodes(b);\n     while (nodeIter.hasNext()) {\n       DatanodeDescriptor node \u003d nodeIter.next();\n       if ((nodesCorrupt !\u003d null) \u0026\u0026 (nodesCorrupt.contains(node))) {\n         corrupt++;\n       } else if (node.isDecommissionInProgress() || node.isDecommissioned()) {\n         count++;\n       } else {\n         Collection\u003cBlock\u003e blocksExcess \u003d\n           excessReplicateMap.get(node.getStorageID());\n         if (blocksExcess !\u003d null \u0026\u0026 blocksExcess.contains(b)) {\n           excess++;\n         } else {\n           live++;\n         }\n       }\n     }\n     return new NumberReplicas(live, count, corrupt, excess);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public NumberReplicas countNodes(Block b) {\n    int count \u003d 0;\n    int live \u003d 0;\n    int corrupt \u003d 0;\n    int excess \u003d 0;\n    Iterator\u003cDatanodeDescriptor\u003e nodeIter \u003d blocksMap.nodeIterator(b);\n    Collection\u003cDatanodeDescriptor\u003e nodesCorrupt \u003d corruptReplicas.getNodes(b);\n    while (nodeIter.hasNext()) {\n      DatanodeDescriptor node \u003d nodeIter.next();\n      if ((nodesCorrupt !\u003d null) \u0026\u0026 (nodesCorrupt.contains(node))) {\n        corrupt++;\n      } else if (node.isDecommissionInProgress() || node.isDecommissioned()) {\n        count++;\n      } else {\n        Collection\u003cBlock\u003e blocksExcess \u003d\n          excessReplicateMap.get(node.getStorageID());\n        if (blocksExcess !\u003d null \u0026\u0026 blocksExcess.contains(b)) {\n          excess++;\n        } else {\n          live++;\n        }\n      }\n    }\n    return new NumberReplicas(live, count, corrupt, excess);\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/BlockManager.java",
            "newPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-2107. Move block management code from o.a.h.h.s.namenode to a new package o.a.h.h.s.blockmanagement.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1140939 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "28/06/11 6:31 PM",
          "commitName": "09b6f98de431628c80bc8a6faf0070eeaf72ff2a",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "28/06/11 5:26 PM",
          "commitNameOld": "97b6ca4dd7d1233e8f8c90b1c01e47228c044e13",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 0.04,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,25 +1,25 @@\n-  NumberReplicas countNodes(Block b) {\n+  public NumberReplicas countNodes(Block b) {\n     int count \u003d 0;\n     int live \u003d 0;\n     int corrupt \u003d 0;\n     int excess \u003d 0;\n     Iterator\u003cDatanodeDescriptor\u003e nodeIter \u003d blocksMap.nodeIterator(b);\n     Collection\u003cDatanodeDescriptor\u003e nodesCorrupt \u003d corruptReplicas.getNodes(b);\n     while (nodeIter.hasNext()) {\n       DatanodeDescriptor node \u003d nodeIter.next();\n       if ((nodesCorrupt !\u003d null) \u0026\u0026 (nodesCorrupt.contains(node))) {\n         corrupt++;\n       } else if (node.isDecommissionInProgress() || node.isDecommissioned()) {\n         count++;\n       } else {\n         Collection\u003cBlock\u003e blocksExcess \u003d\n           excessReplicateMap.get(node.getStorageID());\n         if (blocksExcess !\u003d null \u0026\u0026 blocksExcess.contains(b)) {\n           excess++;\n         } else {\n           live++;\n         }\n       }\n     }\n     return new NumberReplicas(live, count, corrupt, excess);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public NumberReplicas countNodes(Block b) {\n    int count \u003d 0;\n    int live \u003d 0;\n    int corrupt \u003d 0;\n    int excess \u003d 0;\n    Iterator\u003cDatanodeDescriptor\u003e nodeIter \u003d blocksMap.nodeIterator(b);\n    Collection\u003cDatanodeDescriptor\u003e nodesCorrupt \u003d corruptReplicas.getNodes(b);\n    while (nodeIter.hasNext()) {\n      DatanodeDescriptor node \u003d nodeIter.next();\n      if ((nodesCorrupt !\u003d null) \u0026\u0026 (nodesCorrupt.contains(node))) {\n        corrupt++;\n      } else if (node.isDecommissionInProgress() || node.isDecommissioned()) {\n        count++;\n      } else {\n        Collection\u003cBlock\u003e blocksExcess \u003d\n          excessReplicateMap.get(node.getStorageID());\n        if (blocksExcess !\u003d null \u0026\u0026 blocksExcess.contains(b)) {\n          excess++;\n        } else {\n          live++;\n        }\n      }\n    }\n    return new NumberReplicas(live, count, corrupt, excess);\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldValue": "[]",
            "newValue": "[public]"
          }
        }
      ]
    },
    "97b6ca4dd7d1233e8f8c90b1c01e47228c044e13": {
      "type": "Ymultichange(Yfilerename,Ymodifierchange)",
      "commitMessage": "Revert 1140913 and 1140909 for HDFS-2107.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1140920 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "28/06/11 5:26 PM",
      "commitName": "97b6ca4dd7d1233e8f8c90b1c01e47228c044e13",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Yfilerename",
          "commitMessage": "Revert 1140913 and 1140909 for HDFS-2107.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1140920 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "28/06/11 5:26 PM",
          "commitName": "97b6ca4dd7d1233e8f8c90b1c01e47228c044e13",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "28/06/11 4:57 PM",
          "commitNameOld": "d58e3efe9269efe00c309ed0e9726d2f94bcd03a",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 0.02,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,25 +1,25 @@\n-  public NumberReplicas countNodes(Block b) {\n+  NumberReplicas countNodes(Block b) {\n     int count \u003d 0;\n     int live \u003d 0;\n     int corrupt \u003d 0;\n     int excess \u003d 0;\n     Iterator\u003cDatanodeDescriptor\u003e nodeIter \u003d blocksMap.nodeIterator(b);\n     Collection\u003cDatanodeDescriptor\u003e nodesCorrupt \u003d corruptReplicas.getNodes(b);\n     while (nodeIter.hasNext()) {\n       DatanodeDescriptor node \u003d nodeIter.next();\n       if ((nodesCorrupt !\u003d null) \u0026\u0026 (nodesCorrupt.contains(node))) {\n         corrupt++;\n       } else if (node.isDecommissionInProgress() || node.isDecommissioned()) {\n         count++;\n       } else {\n         Collection\u003cBlock\u003e blocksExcess \u003d\n           excessReplicateMap.get(node.getStorageID());\n         if (blocksExcess !\u003d null \u0026\u0026 blocksExcess.contains(b)) {\n           excess++;\n         } else {\n           live++;\n         }\n       }\n     }\n     return new NumberReplicas(live, count, corrupt, excess);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  NumberReplicas countNodes(Block b) {\n    int count \u003d 0;\n    int live \u003d 0;\n    int corrupt \u003d 0;\n    int excess \u003d 0;\n    Iterator\u003cDatanodeDescriptor\u003e nodeIter \u003d blocksMap.nodeIterator(b);\n    Collection\u003cDatanodeDescriptor\u003e nodesCorrupt \u003d corruptReplicas.getNodes(b);\n    while (nodeIter.hasNext()) {\n      DatanodeDescriptor node \u003d nodeIter.next();\n      if ((nodesCorrupt !\u003d null) \u0026\u0026 (nodesCorrupt.contains(node))) {\n        corrupt++;\n      } else if (node.isDecommissionInProgress() || node.isDecommissioned()) {\n        count++;\n      } else {\n        Collection\u003cBlock\u003e blocksExcess \u003d\n          excessReplicateMap.get(node.getStorageID());\n        if (blocksExcess !\u003d null \u0026\u0026 blocksExcess.contains(b)) {\n          excess++;\n        } else {\n          live++;\n        }\n      }\n    }\n    return new NumberReplicas(live, count, corrupt, excess);\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/BlockManager.java",
          "extendedDetails": {
            "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
            "newPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/BlockManager.java"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "Revert 1140913 and 1140909 for HDFS-2107.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1140920 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "28/06/11 5:26 PM",
          "commitName": "97b6ca4dd7d1233e8f8c90b1c01e47228c044e13",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "28/06/11 4:57 PM",
          "commitNameOld": "d58e3efe9269efe00c309ed0e9726d2f94bcd03a",
          "commitAuthorOld": "Tsz-wo Sze",
          "daysBetweenCommits": 0.02,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,25 +1,25 @@\n-  public NumberReplicas countNodes(Block b) {\n+  NumberReplicas countNodes(Block b) {\n     int count \u003d 0;\n     int live \u003d 0;\n     int corrupt \u003d 0;\n     int excess \u003d 0;\n     Iterator\u003cDatanodeDescriptor\u003e nodeIter \u003d blocksMap.nodeIterator(b);\n     Collection\u003cDatanodeDescriptor\u003e nodesCorrupt \u003d corruptReplicas.getNodes(b);\n     while (nodeIter.hasNext()) {\n       DatanodeDescriptor node \u003d nodeIter.next();\n       if ((nodesCorrupt !\u003d null) \u0026\u0026 (nodesCorrupt.contains(node))) {\n         corrupt++;\n       } else if (node.isDecommissionInProgress() || node.isDecommissioned()) {\n         count++;\n       } else {\n         Collection\u003cBlock\u003e blocksExcess \u003d\n           excessReplicateMap.get(node.getStorageID());\n         if (blocksExcess !\u003d null \u0026\u0026 blocksExcess.contains(b)) {\n           excess++;\n         } else {\n           live++;\n         }\n       }\n     }\n     return new NumberReplicas(live, count, corrupt, excess);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  NumberReplicas countNodes(Block b) {\n    int count \u003d 0;\n    int live \u003d 0;\n    int corrupt \u003d 0;\n    int excess \u003d 0;\n    Iterator\u003cDatanodeDescriptor\u003e nodeIter \u003d blocksMap.nodeIterator(b);\n    Collection\u003cDatanodeDescriptor\u003e nodesCorrupt \u003d corruptReplicas.getNodes(b);\n    while (nodeIter.hasNext()) {\n      DatanodeDescriptor node \u003d nodeIter.next();\n      if ((nodesCorrupt !\u003d null) \u0026\u0026 (nodesCorrupt.contains(node))) {\n        corrupt++;\n      } else if (node.isDecommissionInProgress() || node.isDecommissioned()) {\n        count++;\n      } else {\n        Collection\u003cBlock\u003e blocksExcess \u003d\n          excessReplicateMap.get(node.getStorageID());\n        if (blocksExcess !\u003d null \u0026\u0026 blocksExcess.contains(b)) {\n          excess++;\n        } else {\n          live++;\n        }\n      }\n    }\n    return new NumberReplicas(live, count, corrupt, excess);\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/BlockManager.java",
          "extendedDetails": {
            "oldValue": "[public]",
            "newValue": "[]"
          }
        }
      ]
    },
    "1bcfe45e47775b98cce5541f328c4fd46e5eb13d": {
      "type": "Ymultichange(Yfilerename,Ymodifierchange)",
      "commitMessage": "HDFS-2106. Move block management code from o.a.h.h.s.namenode to a new package o.a.h.h.s.blockmanagement.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1140909 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "28/06/11 4:43 PM",
      "commitName": "1bcfe45e47775b98cce5541f328c4fd46e5eb13d",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Yfilerename",
          "commitMessage": "HDFS-2106. Move block management code from o.a.h.h.s.namenode to a new package o.a.h.h.s.blockmanagement.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1140909 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "28/06/11 4:43 PM",
          "commitName": "1bcfe45e47775b98cce5541f328c4fd46e5eb13d",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "28/06/11 9:21 AM",
          "commitNameOld": "1834fb99f516b2f2cd5e0ab1f89d407f98a7237a",
          "commitAuthorOld": "Eli Collins",
          "daysBetweenCommits": 0.31,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,25 +1,25 @@\n-  NumberReplicas countNodes(Block b) {\n+  public NumberReplicas countNodes(Block b) {\n     int count \u003d 0;\n     int live \u003d 0;\n     int corrupt \u003d 0;\n     int excess \u003d 0;\n     Iterator\u003cDatanodeDescriptor\u003e nodeIter \u003d blocksMap.nodeIterator(b);\n     Collection\u003cDatanodeDescriptor\u003e nodesCorrupt \u003d corruptReplicas.getNodes(b);\n     while (nodeIter.hasNext()) {\n       DatanodeDescriptor node \u003d nodeIter.next();\n       if ((nodesCorrupt !\u003d null) \u0026\u0026 (nodesCorrupt.contains(node))) {\n         corrupt++;\n       } else if (node.isDecommissionInProgress() || node.isDecommissioned()) {\n         count++;\n       } else {\n         Collection\u003cBlock\u003e blocksExcess \u003d\n           excessReplicateMap.get(node.getStorageID());\n         if (blocksExcess !\u003d null \u0026\u0026 blocksExcess.contains(b)) {\n           excess++;\n         } else {\n           live++;\n         }\n       }\n     }\n     return new NumberReplicas(live, count, corrupt, excess);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public NumberReplicas countNodes(Block b) {\n    int count \u003d 0;\n    int live \u003d 0;\n    int corrupt \u003d 0;\n    int excess \u003d 0;\n    Iterator\u003cDatanodeDescriptor\u003e nodeIter \u003d blocksMap.nodeIterator(b);\n    Collection\u003cDatanodeDescriptor\u003e nodesCorrupt \u003d corruptReplicas.getNodes(b);\n    while (nodeIter.hasNext()) {\n      DatanodeDescriptor node \u003d nodeIter.next();\n      if ((nodesCorrupt !\u003d null) \u0026\u0026 (nodesCorrupt.contains(node))) {\n        corrupt++;\n      } else if (node.isDecommissionInProgress() || node.isDecommissioned()) {\n        count++;\n      } else {\n        Collection\u003cBlock\u003e blocksExcess \u003d\n          excessReplicateMap.get(node.getStorageID());\n        if (blocksExcess !\u003d null \u0026\u0026 blocksExcess.contains(b)) {\n          excess++;\n        } else {\n          live++;\n        }\n      }\n    }\n    return new NumberReplicas(live, count, corrupt, excess);\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/BlockManager.java",
            "newPath": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-2106. Move block management code from o.a.h.h.s.namenode to a new package o.a.h.h.s.blockmanagement.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1140909 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "28/06/11 4:43 PM",
          "commitName": "1bcfe45e47775b98cce5541f328c4fd46e5eb13d",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "28/06/11 9:21 AM",
          "commitNameOld": "1834fb99f516b2f2cd5e0ab1f89d407f98a7237a",
          "commitAuthorOld": "Eli Collins",
          "daysBetweenCommits": 0.31,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,25 +1,25 @@\n-  NumberReplicas countNodes(Block b) {\n+  public NumberReplicas countNodes(Block b) {\n     int count \u003d 0;\n     int live \u003d 0;\n     int corrupt \u003d 0;\n     int excess \u003d 0;\n     Iterator\u003cDatanodeDescriptor\u003e nodeIter \u003d blocksMap.nodeIterator(b);\n     Collection\u003cDatanodeDescriptor\u003e nodesCorrupt \u003d corruptReplicas.getNodes(b);\n     while (nodeIter.hasNext()) {\n       DatanodeDescriptor node \u003d nodeIter.next();\n       if ((nodesCorrupt !\u003d null) \u0026\u0026 (nodesCorrupt.contains(node))) {\n         corrupt++;\n       } else if (node.isDecommissionInProgress() || node.isDecommissioned()) {\n         count++;\n       } else {\n         Collection\u003cBlock\u003e blocksExcess \u003d\n           excessReplicateMap.get(node.getStorageID());\n         if (blocksExcess !\u003d null \u0026\u0026 blocksExcess.contains(b)) {\n           excess++;\n         } else {\n           live++;\n         }\n       }\n     }\n     return new NumberReplicas(live, count, corrupt, excess);\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public NumberReplicas countNodes(Block b) {\n    int count \u003d 0;\n    int live \u003d 0;\n    int corrupt \u003d 0;\n    int excess \u003d 0;\n    Iterator\u003cDatanodeDescriptor\u003e nodeIter \u003d blocksMap.nodeIterator(b);\n    Collection\u003cDatanodeDescriptor\u003e nodesCorrupt \u003d corruptReplicas.getNodes(b);\n    while (nodeIter.hasNext()) {\n      DatanodeDescriptor node \u003d nodeIter.next();\n      if ((nodesCorrupt !\u003d null) \u0026\u0026 (nodesCorrupt.contains(node))) {\n        corrupt++;\n      } else if (node.isDecommissionInProgress() || node.isDecommissioned()) {\n        count++;\n      } else {\n        Collection\u003cBlock\u003e blocksExcess \u003d\n          excessReplicateMap.get(node.getStorageID());\n        if (blocksExcess !\u003d null \u0026\u0026 blocksExcess.contains(b)) {\n          excess++;\n        } else {\n          live++;\n        }\n      }\n    }\n    return new NumberReplicas(live, count, corrupt, excess);\n  }",
          "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
          "extendedDetails": {
            "oldValue": "[]",
            "newValue": "[public]"
          }
        }
      ]
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,25 @@\n+  NumberReplicas countNodes(Block b) {\n+    int count \u003d 0;\n+    int live \u003d 0;\n+    int corrupt \u003d 0;\n+    int excess \u003d 0;\n+    Iterator\u003cDatanodeDescriptor\u003e nodeIter \u003d blocksMap.nodeIterator(b);\n+    Collection\u003cDatanodeDescriptor\u003e nodesCorrupt \u003d corruptReplicas.getNodes(b);\n+    while (nodeIter.hasNext()) {\n+      DatanodeDescriptor node \u003d nodeIter.next();\n+      if ((nodesCorrupt !\u003d null) \u0026\u0026 (nodesCorrupt.contains(node))) {\n+        corrupt++;\n+      } else if (node.isDecommissionInProgress() || node.isDecommissioned()) {\n+        count++;\n+      } else {\n+        Collection\u003cBlock\u003e blocksExcess \u003d\n+          excessReplicateMap.get(node.getStorageID());\n+        if (blocksExcess !\u003d null \u0026\u0026 blocksExcess.contains(b)) {\n+          excess++;\n+        } else {\n+          live++;\n+        }\n+      }\n+    }\n+    return new NumberReplicas(live, count, corrupt, excess);\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  NumberReplicas countNodes(Block b) {\n    int count \u003d 0;\n    int live \u003d 0;\n    int corrupt \u003d 0;\n    int excess \u003d 0;\n    Iterator\u003cDatanodeDescriptor\u003e nodeIter \u003d blocksMap.nodeIterator(b);\n    Collection\u003cDatanodeDescriptor\u003e nodesCorrupt \u003d corruptReplicas.getNodes(b);\n    while (nodeIter.hasNext()) {\n      DatanodeDescriptor node \u003d nodeIter.next();\n      if ((nodesCorrupt !\u003d null) \u0026\u0026 (nodesCorrupt.contains(node))) {\n        corrupt++;\n      } else if (node.isDecommissionInProgress() || node.isDecommissioned()) {\n        count++;\n      } else {\n        Collection\u003cBlock\u003e blocksExcess \u003d\n          excessReplicateMap.get(node.getStorageID());\n        if (blocksExcess !\u003d null \u0026\u0026 blocksExcess.contains(b)) {\n          excess++;\n        } else {\n          live++;\n        }\n      }\n    }\n    return new NumberReplicas(live, count, corrupt, excess);\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/server/namenode/BlockManager.java"
    }
  }
}