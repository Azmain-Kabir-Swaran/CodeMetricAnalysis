{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockTokenIdentifier.java",
  "functionName": "readFields",
  "functionId": "readFields___in-DataInput",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/security/token/block/BlockTokenIdentifier.java",
  "functionStartLine": 212,
  "functionEndLine": 236,
  "numCommitsSeen": 17,
  "timeTaken": 5782,
  "changeHistory": [
    "72ae371e7a6695f45f0d9cea5ae9aae83941d360",
    "4ed33e9ca3d85568e3904753a3ef61a85f801838",
    "5c97db07fb306842f49d73a67a90cecec19a7833",
    "98b00d7cc015555642068827e6c52eaed0740c94",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "72ae371e7a6695f45f0d9cea5ae9aae83941d360": "Ybodychange",
    "4ed33e9ca3d85568e3904753a3ef61a85f801838": "Ybodychange",
    "5c97db07fb306842f49d73a67a90cecec19a7833": "Yfilerename",
    "98b00d7cc015555642068827e6c52eaed0740c94": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "72ae371e7a6695f45f0d9cea5ae9aae83941d360": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-14509. DN throws InvalidToken due to inequality of password when upgrade NN 2.x to 3.x. Contributed by Yuxuan Wang and Konstantin Shvachko.\n",
      "commitDate": "08/10/19 11:56 AM",
      "commitName": "72ae371e7a6695f45f0d9cea5ae9aae83941d360",
      "commitAuthor": "Chen Liang",
      "commitDateOld": "11/07/19 1:23 PM",
      "commitNameOld": "8fb5ca3f405550828a17e689b9c60ddf7fb95ec1",
      "commitAuthorOld": "Chen Liang",
      "daysBetweenCommits": 88.94,
      "commitsBetweenForRepo": 790,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,16 +1,25 @@\n   public void readFields(DataInput in) throws IOException {\n     this.cache \u003d null;\n \n     final DataInputStream dis \u003d (DataInputStream)in;\n     if (!dis.markSupported()) {\n       throw new IOException(\"Could not peek first byte.\");\n     }\n+\n+    // this.cache should be assigned the raw bytes from the input data for\n+    // upgrading compatibility. If we won\u0027t mutate fields and call getBytes()\n+    // for something (e.g retrieve password), we should return the raw bytes\n+    // instead of serializing the instance self fields to bytes, because we may\n+    // lose newly added fields which we can\u0027t recognize\n+    this.cache \u003d IOUtils.readFullyToByteArray(dis);\n+    dis.reset();\n+\n     dis.mark(1);\n     final byte firstByte \u003d dis.readByte();\n     dis.reset();\n     if (firstByte \u003c\u003d 0) {\n       readFieldsLegacy(dis);\n     } else {\n       readFieldsProtobuf(dis);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void readFields(DataInput in) throws IOException {\n    this.cache \u003d null;\n\n    final DataInputStream dis \u003d (DataInputStream)in;\n    if (!dis.markSupported()) {\n      throw new IOException(\"Could not peek first byte.\");\n    }\n\n    // this.cache should be assigned the raw bytes from the input data for\n    // upgrading compatibility. If we won\u0027t mutate fields and call getBytes()\n    // for something (e.g retrieve password), we should return the raw bytes\n    // instead of serializing the instance self fields to bytes, because we may\n    // lose newly added fields which we can\u0027t recognize\n    this.cache \u003d IOUtils.readFullyToByteArray(dis);\n    dis.reset();\n\n    dis.mark(1);\n    final byte firstByte \u003d dis.readByte();\n    dis.reset();\n    if (firstByte \u003c\u003d 0) {\n      readFieldsLegacy(dis);\n    } else {\n      readFieldsProtobuf(dis);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/security/token/block/BlockTokenIdentifier.java",
      "extendedDetails": {}
    },
    "4ed33e9ca3d85568e3904753a3ef61a85f801838": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-11026. Convert BlockTokenIdentifier to use Protobuf. Contributed by Ewan Higgs.\n",
      "commitDate": "13/02/17 11:29 AM",
      "commitName": "4ed33e9ca3d85568e3904753a3ef61a85f801838",
      "commitAuthor": "Chris Douglas",
      "commitDateOld": "20/04/15 12:36 AM",
      "commitNameOld": "5c97db07fb306842f49d73a67a90cecec19a7833",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 665.49,
      "commitsBetweenForRepo": 4796,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,13 +1,16 @@\n   public void readFields(DataInput in) throws IOException {\n     this.cache \u003d null;\n-    expiryDate \u003d WritableUtils.readVLong(in);\n-    keyId \u003d WritableUtils.readVInt(in);\n-    userId \u003d WritableUtils.readString(in);\n-    blockPoolId \u003d WritableUtils.readString(in);\n-    blockId \u003d WritableUtils.readVLong(in);\n-    int length \u003d WritableUtils.readVIntInRange(in, 0,\n-        AccessMode.class.getEnumConstants().length);\n-    for (int i \u003d 0; i \u003c length; i++) {\n-      modes.add(WritableUtils.readEnum(in, AccessMode.class));\n+\n+    final DataInputStream dis \u003d (DataInputStream)in;\n+    if (!dis.markSupported()) {\n+      throw new IOException(\"Could not peek first byte.\");\n+    }\n+    dis.mark(1);\n+    final byte firstByte \u003d dis.readByte();\n+    dis.reset();\n+    if (firstByte \u003c\u003d 0) {\n+      readFieldsLegacy(dis);\n+    } else {\n+      readFieldsProtobuf(dis);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void readFields(DataInput in) throws IOException {\n    this.cache \u003d null;\n\n    final DataInputStream dis \u003d (DataInputStream)in;\n    if (!dis.markSupported()) {\n      throw new IOException(\"Could not peek first byte.\");\n    }\n    dis.mark(1);\n    final byte firstByte \u003d dis.readByte();\n    dis.reset();\n    if (firstByte \u003c\u003d 0) {\n      readFieldsLegacy(dis);\n    } else {\n      readFieldsProtobuf(dis);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/security/token/block/BlockTokenIdentifier.java",
      "extendedDetails": {}
    },
    "5c97db07fb306842f49d73a67a90cecec19a7833": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8169. Move LocatedBlocks and related classes to hdfs-client. Contributed by Haohui Mai.\n",
      "commitDate": "20/04/15 12:36 AM",
      "commitName": "5c97db07fb306842f49d73a67a90cecec19a7833",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "19/04/15 4:09 PM",
      "commitNameOld": "8511d80804de052618168a456a475ee0f7aa6d8c",
      "commitAuthorOld": "Arpit Agarwal",
      "daysBetweenCommits": 0.35,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public void readFields(DataInput in) throws IOException {\n    this.cache \u003d null;\n    expiryDate \u003d WritableUtils.readVLong(in);\n    keyId \u003d WritableUtils.readVInt(in);\n    userId \u003d WritableUtils.readString(in);\n    blockPoolId \u003d WritableUtils.readString(in);\n    blockId \u003d WritableUtils.readVLong(in);\n    int length \u003d WritableUtils.readVIntInRange(in, 0,\n        AccessMode.class.getEnumConstants().length);\n    for (int i \u003d 0; i \u003c length; i++) {\n      modes.add(WritableUtils.readEnum(in, AccessMode.class));\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/security/token/block/BlockTokenIdentifier.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/security/token/block/BlockTokenIdentifier.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/security/token/block/BlockTokenIdentifier.java"
      }
    },
    "98b00d7cc015555642068827e6c52eaed0740c94": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3134. harden edit log loader against malformed or malicious input. Contributed by Colin Patrick McCabe\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1336943 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/05/12 4:13 PM",
      "commitName": "98b00d7cc015555642068827e6c52eaed0740c94",
      "commitAuthor": "Eli Collins",
      "commitDateOld": "13/10/11 6:24 PM",
      "commitNameOld": "002dd6968b89ded6a77858ccb50c9b2df074c226",
      "commitAuthorOld": "Jitendra Nath Pandey",
      "daysBetweenCommits": 209.91,
      "commitsBetweenForRepo": 1508,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,12 +1,13 @@\n   public void readFields(DataInput in) throws IOException {\n     this.cache \u003d null;\n     expiryDate \u003d WritableUtils.readVLong(in);\n     keyId \u003d WritableUtils.readVInt(in);\n     userId \u003d WritableUtils.readString(in);\n     blockPoolId \u003d WritableUtils.readString(in);\n     blockId \u003d WritableUtils.readVLong(in);\n-    int length \u003d WritableUtils.readVInt(in);\n+    int length \u003d WritableUtils.readVIntInRange(in, 0,\n+        AccessMode.class.getEnumConstants().length);\n     for (int i \u003d 0; i \u003c length; i++) {\n       modes.add(WritableUtils.readEnum(in, AccessMode.class));\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public void readFields(DataInput in) throws IOException {\n    this.cache \u003d null;\n    expiryDate \u003d WritableUtils.readVLong(in);\n    keyId \u003d WritableUtils.readVInt(in);\n    userId \u003d WritableUtils.readString(in);\n    blockPoolId \u003d WritableUtils.readString(in);\n    blockId \u003d WritableUtils.readVLong(in);\n    int length \u003d WritableUtils.readVIntInRange(in, 0,\n        AccessMode.class.getEnumConstants().length);\n    for (int i \u003d 0; i \u003c length; i++) {\n      modes.add(WritableUtils.readEnum(in, AccessMode.class));\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/security/token/block/BlockTokenIdentifier.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public void readFields(DataInput in) throws IOException {\n    this.cache \u003d null;\n    expiryDate \u003d WritableUtils.readVLong(in);\n    keyId \u003d WritableUtils.readVInt(in);\n    userId \u003d WritableUtils.readString(in);\n    blockPoolId \u003d WritableUtils.readString(in);\n    blockId \u003d WritableUtils.readVLong(in);\n    int length \u003d WritableUtils.readVInt(in);\n    for (int i \u003d 0; i \u003c length; i++) {\n      modes.add(WritableUtils.readEnum(in, AccessMode.class));\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/security/token/block/BlockTokenIdentifier.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/security/token/block/BlockTokenIdentifier.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/security/token/block/BlockTokenIdentifier.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public void readFields(DataInput in) throws IOException {\n    this.cache \u003d null;\n    expiryDate \u003d WritableUtils.readVLong(in);\n    keyId \u003d WritableUtils.readVInt(in);\n    userId \u003d WritableUtils.readString(in);\n    blockPoolId \u003d WritableUtils.readString(in);\n    blockId \u003d WritableUtils.readVLong(in);\n    int length \u003d WritableUtils.readVInt(in);\n    for (int i \u003d 0; i \u003c length; i++) {\n      modes.add(WritableUtils.readEnum(in, AccessMode.class));\n    }\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/security/token/block/BlockTokenIdentifier.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/security/token/block/BlockTokenIdentifier.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/security/token/block/BlockTokenIdentifier.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,12 @@\n+  public void readFields(DataInput in) throws IOException {\n+    this.cache \u003d null;\n+    expiryDate \u003d WritableUtils.readVLong(in);\n+    keyId \u003d WritableUtils.readVInt(in);\n+    userId \u003d WritableUtils.readString(in);\n+    blockPoolId \u003d WritableUtils.readString(in);\n+    blockId \u003d WritableUtils.readVLong(in);\n+    int length \u003d WritableUtils.readVInt(in);\n+    for (int i \u003d 0; i \u003c length; i++) {\n+      modes.add(WritableUtils.readEnum(in, AccessMode.class));\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public void readFields(DataInput in) throws IOException {\n    this.cache \u003d null;\n    expiryDate \u003d WritableUtils.readVLong(in);\n    keyId \u003d WritableUtils.readVInt(in);\n    userId \u003d WritableUtils.readString(in);\n    blockPoolId \u003d WritableUtils.readString(in);\n    blockId \u003d WritableUtils.readVLong(in);\n    int length \u003d WritableUtils.readVInt(in);\n    for (int i \u003d 0; i \u003c length; i++) {\n      modes.add(WritableUtils.readEnum(in, AccessMode.class));\n    }\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/security/token/block/BlockTokenIdentifier.java"
    }
  }
}