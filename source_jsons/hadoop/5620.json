{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "CacheManager.java",
  "functionName": "processCacheReport",
  "functionId": "processCacheReport___datanodeID-DatanodeID(modifiers-final)__blockIds-List__Long__(modifiers-final)",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
  "functionStartLine": 975,
  "functionEndLine": 1008,
  "numCommitsSeen": 78,
  "timeTaken": 5791,
  "changeHistory": [
    "335a8139f5b9004414b2942eeac5a008283a6f75",
    "ff0b99eafeda035ebe0dc82cfe689808047a8893",
    "f741476146574550a1a208d58ef8be76639e5ddc",
    "be7a0add8b6561d3c566237cc0370b06e7f32bb4",
    "93e23a99157c30b51752fc49748c3c210745a187",
    "a3616c58dd2ddb16172ca3ab5d66fad52ec0e6d7",
    "d85c017d0488930d806f267141057fc73e68c728",
    "f9c08d02ebe4a5477cf5d753f0d9d48fc6f9fa48",
    "3cc7a38a53c8ae27ef6b2397cddc5d14a378203a",
    "40eb94ade3161d93e7a762a839004748f6d0ae89"
  ],
  "changeHistoryShort": {
    "335a8139f5b9004414b2942eeac5a008283a6f75": "Ybodychange",
    "ff0b99eafeda035ebe0dc82cfe689808047a8893": "Ybodychange",
    "f741476146574550a1a208d58ef8be76639e5ddc": "Ybodychange",
    "be7a0add8b6561d3c566237cc0370b06e7f32bb4": "Ybodychange",
    "93e23a99157c30b51752fc49748c3c210745a187": "Ybodychange",
    "a3616c58dd2ddb16172ca3ab5d66fad52ec0e6d7": "Ybodychange",
    "d85c017d0488930d806f267141057fc73e68c728": "Ybodychange",
    "f9c08d02ebe4a5477cf5d753f0d9d48fc6f9fa48": "Ymultichange(Yparameterchange,Ybodychange)",
    "3cc7a38a53c8ae27ef6b2397cddc5d14a378203a": "Ymultichange(Ymovefromfile,Ymodifierchange,Ybodychange,Yparameterchange)",
    "40eb94ade3161d93e7a762a839004748f6d0ae89": "Yintroduced"
  },
  "changeHistoryDetails": {
    "335a8139f5b9004414b2942eeac5a008283a6f75": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13820. Add an ability to disable CacheReplicationMonitor. Contributed by Hrishikesh Gadre.\n\nSigned-off-by: Xiao Chen \u003cxiao@apache.org\u003e\n",
      "commitDate": "07/09/18 2:59 PM",
      "commitName": "335a8139f5b9004414b2942eeac5a008283a6f75",
      "commitAuthor": "Hrishikesh Gadre",
      "commitDateOld": "10/05/17 12:15 PM",
      "commitNameOld": "ad1e3e4d9f105fac246ce1bdae80e92e013b8ba5",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 485.11,
      "commitsBetweenForRepo": 3998,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,34 @@\n   public final void processCacheReport(final DatanodeID datanodeID,\n       final List\u003cLong\u003e blockIds) throws IOException {\n+    if (!enabled) {\n+      LOG.debug(\"Ignoring cache report from {} because {} \u003d false. \" +\n+              \"number of blocks: {}\", datanodeID,\n+              DFS_NAMENODE_CACHING_ENABLED_KEY, blockIds.size());\n+      return;\n+    }\n     namesystem.writeLock();\n     final long startTime \u003d Time.monotonicNow();\n     final long endTime;\n     try {\n       final DatanodeDescriptor datanode \u003d \n           blockManager.getDatanodeManager().getDatanode(datanodeID);\n       if (datanode \u003d\u003d null || !datanode.isRegistered()) {\n         throw new IOException(\n             \"processCacheReport from dead or unregistered datanode: \" +\n             datanode);\n       }\n       processCacheReportImpl(datanode, blockIds);\n     } finally {\n       endTime \u003d Time.monotonicNow();\n       namesystem.writeUnlock(\"processCacheReport\");\n     }\n \n     // Log the block report processing stats from Namenode perspective\n     final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n     if (metrics !\u003d null) {\n       metrics.addCacheBlockReport((int) (endTime - startTime));\n     }\n     LOG.debug(\"Processed cache report from {}, blocks: {}, \" +\n         \"processing time: {} msecs\", datanodeID, blockIds.size(), \n         (endTime - startTime));\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public final void processCacheReport(final DatanodeID datanodeID,\n      final List\u003cLong\u003e blockIds) throws IOException {\n    if (!enabled) {\n      LOG.debug(\"Ignoring cache report from {} because {} \u003d false. \" +\n              \"number of blocks: {}\", datanodeID,\n              DFS_NAMENODE_CACHING_ENABLED_KEY, blockIds.size());\n      return;\n    }\n    namesystem.writeLock();\n    final long startTime \u003d Time.monotonicNow();\n    final long endTime;\n    try {\n      final DatanodeDescriptor datanode \u003d \n          blockManager.getDatanodeManager().getDatanode(datanodeID);\n      if (datanode \u003d\u003d null || !datanode.isRegistered()) {\n        throw new IOException(\n            \"processCacheReport from dead or unregistered datanode: \" +\n            datanode);\n      }\n      processCacheReportImpl(datanode, blockIds);\n    } finally {\n      endTime \u003d Time.monotonicNow();\n      namesystem.writeUnlock(\"processCacheReport\");\n    }\n\n    // Log the block report processing stats from Namenode perspective\n    final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n    if (metrics !\u003d null) {\n      metrics.addCacheBlockReport((int) (endTime - startTime));\n    }\n    LOG.debug(\"Processed cache report from {}, blocks: {}, \" +\n        \"processing time: {} msecs\", datanodeID, blockIds.size(), \n        (endTime - startTime));\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
      "extendedDetails": {}
    },
    "ff0b99eafeda035ebe0dc82cfe689808047a8893": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-10872. Add MutableRate metrics for FSNamesystemLock operations. Contributed by Erik Krogen.\n",
      "commitDate": "14/11/16 11:05 AM",
      "commitName": "ff0b99eafeda035ebe0dc82cfe689808047a8893",
      "commitAuthor": "Zhe Zhang",
      "commitDateOld": "24/10/16 3:14 PM",
      "commitNameOld": "9d175853b0170683ad5f21d9bcdeaac49fe89e04",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 20.87,
      "commitsBetweenForRepo": 229,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,28 @@\n   public final void processCacheReport(final DatanodeID datanodeID,\n       final List\u003cLong\u003e blockIds) throws IOException {\n     namesystem.writeLock();\n     final long startTime \u003d Time.monotonicNow();\n     final long endTime;\n     try {\n       final DatanodeDescriptor datanode \u003d \n           blockManager.getDatanodeManager().getDatanode(datanodeID);\n       if (datanode \u003d\u003d null || !datanode.isRegistered()) {\n         throw new IOException(\n             \"processCacheReport from dead or unregistered datanode: \" +\n             datanode);\n       }\n       processCacheReportImpl(datanode, blockIds);\n     } finally {\n       endTime \u003d Time.monotonicNow();\n-      namesystem.writeUnlock();\n+      namesystem.writeUnlock(\"processCacheReport\");\n     }\n \n     // Log the block report processing stats from Namenode perspective\n     final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n     if (metrics !\u003d null) {\n       metrics.addCacheBlockReport((int) (endTime - startTime));\n     }\n     LOG.debug(\"Processed cache report from {}, blocks: {}, \" +\n         \"processing time: {} msecs\", datanodeID, blockIds.size(), \n         (endTime - startTime));\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public final void processCacheReport(final DatanodeID datanodeID,\n      final List\u003cLong\u003e blockIds) throws IOException {\n    namesystem.writeLock();\n    final long startTime \u003d Time.monotonicNow();\n    final long endTime;\n    try {\n      final DatanodeDescriptor datanode \u003d \n          blockManager.getDatanodeManager().getDatanode(datanodeID);\n      if (datanode \u003d\u003d null || !datanode.isRegistered()) {\n        throw new IOException(\n            \"processCacheReport from dead or unregistered datanode: \" +\n            datanode);\n      }\n      processCacheReportImpl(datanode, blockIds);\n    } finally {\n      endTime \u003d Time.monotonicNow();\n      namesystem.writeUnlock(\"processCacheReport\");\n    }\n\n    // Log the block report processing stats from Namenode perspective\n    final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n    if (metrics !\u003d null) {\n      metrics.addCacheBlockReport((int) (endTime - startTime));\n    }\n    LOG.debug(\"Processed cache report from {}, blocks: {}, \" +\n        \"processing time: {} msecs\", datanodeID, blockIds.size(), \n        (endTime - startTime));\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
      "extendedDetails": {}
    },
    "f741476146574550a1a208d58ef8be76639e5ddc": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9198. Coalesce IBR processing in the NN. (Daryn Sharp via umamahesh)\n",
      "commitDate": "16/12/15 6:16 PM",
      "commitName": "f741476146574550a1a208d58ef8be76639e5ddc",
      "commitAuthor": "Uma Mahesh",
      "commitDateOld": "14/10/15 4:17 PM",
      "commitNameOld": "be7a0add8b6561d3c566237cc0370b06e7f32bb4",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 63.12,
      "commitsBetweenForRepo": 460,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,28 @@\n   public final void processCacheReport(final DatanodeID datanodeID,\n       final List\u003cLong\u003e blockIds) throws IOException {\n     namesystem.writeLock();\n     final long startTime \u003d Time.monotonicNow();\n     final long endTime;\n     try {\n       final DatanodeDescriptor datanode \u003d \n           blockManager.getDatanodeManager().getDatanode(datanodeID);\n-      if (datanode \u003d\u003d null || !datanode.isAlive()) {\n+      if (datanode \u003d\u003d null || !datanode.isRegistered()) {\n         throw new IOException(\n             \"processCacheReport from dead or unregistered datanode: \" +\n             datanode);\n       }\n       processCacheReportImpl(datanode, blockIds);\n     } finally {\n       endTime \u003d Time.monotonicNow();\n       namesystem.writeUnlock();\n     }\n \n     // Log the block report processing stats from Namenode perspective\n     final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n     if (metrics !\u003d null) {\n       metrics.addCacheBlockReport((int) (endTime - startTime));\n     }\n     LOG.debug(\"Processed cache report from {}, blocks: {}, \" +\n         \"processing time: {} msecs\", datanodeID, blockIds.size(), \n         (endTime - startTime));\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public final void processCacheReport(final DatanodeID datanodeID,\n      final List\u003cLong\u003e blockIds) throws IOException {\n    namesystem.writeLock();\n    final long startTime \u003d Time.monotonicNow();\n    final long endTime;\n    try {\n      final DatanodeDescriptor datanode \u003d \n          blockManager.getDatanodeManager().getDatanode(datanodeID);\n      if (datanode \u003d\u003d null || !datanode.isRegistered()) {\n        throw new IOException(\n            \"processCacheReport from dead or unregistered datanode: \" +\n            datanode);\n      }\n      processCacheReportImpl(datanode, blockIds);\n    } finally {\n      endTime \u003d Time.monotonicNow();\n      namesystem.writeUnlock();\n    }\n\n    // Log the block report processing stats from Namenode perspective\n    final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n    if (metrics !\u003d null) {\n      metrics.addCacheBlockReport((int) (endTime - startTime));\n    }\n    LOG.debug(\"Processed cache report from {}, blocks: {}, \" +\n        \"processing time: {} msecs\", datanodeID, blockIds.size(), \n        (endTime - startTime));\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
      "extendedDetails": {}
    },
    "be7a0add8b6561d3c566237cc0370b06e7f32bb4": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9223. Code cleanup for DatanodeDescriptor and HeartbeatManager. Contributed by Jing Zhao.\n",
      "commitDate": "14/10/15 4:17 PM",
      "commitName": "be7a0add8b6561d3c566237cc0370b06e7f32bb4",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "21/09/15 6:53 PM",
      "commitNameOld": "06022b8fdc40e50eaac63758246353058e8cfa6d",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 22.89,
      "commitsBetweenForRepo": 182,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,28 @@\n   public final void processCacheReport(final DatanodeID datanodeID,\n       final List\u003cLong\u003e blockIds) throws IOException {\n     namesystem.writeLock();\n     final long startTime \u003d Time.monotonicNow();\n     final long endTime;\n     try {\n       final DatanodeDescriptor datanode \u003d \n           blockManager.getDatanodeManager().getDatanode(datanodeID);\n-      if (datanode \u003d\u003d null || !datanode.isAlive) {\n+      if (datanode \u003d\u003d null || !datanode.isAlive()) {\n         throw new IOException(\n             \"processCacheReport from dead or unregistered datanode: \" +\n             datanode);\n       }\n       processCacheReportImpl(datanode, blockIds);\n     } finally {\n       endTime \u003d Time.monotonicNow();\n       namesystem.writeUnlock();\n     }\n \n     // Log the block report processing stats from Namenode perspective\n     final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n     if (metrics !\u003d null) {\n       metrics.addCacheBlockReport((int) (endTime - startTime));\n     }\n     LOG.debug(\"Processed cache report from {}, blocks: {}, \" +\n         \"processing time: {} msecs\", datanodeID, blockIds.size(), \n         (endTime - startTime));\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public final void processCacheReport(final DatanodeID datanodeID,\n      final List\u003cLong\u003e blockIds) throws IOException {\n    namesystem.writeLock();\n    final long startTime \u003d Time.monotonicNow();\n    final long endTime;\n    try {\n      final DatanodeDescriptor datanode \u003d \n          blockManager.getDatanodeManager().getDatanode(datanodeID);\n      if (datanode \u003d\u003d null || !datanode.isAlive()) {\n        throw new IOException(\n            \"processCacheReport from dead or unregistered datanode: \" +\n            datanode);\n      }\n      processCacheReportImpl(datanode, blockIds);\n    } finally {\n      endTime \u003d Time.monotonicNow();\n      namesystem.writeUnlock();\n    }\n\n    // Log the block report processing stats from Namenode perspective\n    final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n    if (metrics !\u003d null) {\n      metrics.addCacheBlockReport((int) (endTime - startTime));\n    }\n    LOG.debug(\"Processed cache report from {}, blocks: {}, \" +\n        \"processing time: {} msecs\", datanodeID, blockIds.size(), \n        (endTime - startTime));\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
      "extendedDetails": {}
    },
    "93e23a99157c30b51752fc49748c3c210745a187": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6613. Improve logging in caching classes. (wang)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1607697 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "03/07/14 10:13 AM",
      "commitName": "93e23a99157c30b51752fc49748c3c210745a187",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "15/05/14 6:18 PM",
      "commitNameOld": "8f48760663070529ff09927d1772010fffe5f438",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 48.66,
      "commitsBetweenForRepo": 297,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,30 +1,28 @@\n   public final void processCacheReport(final DatanodeID datanodeID,\n       final List\u003cLong\u003e blockIds) throws IOException {\n     namesystem.writeLock();\n     final long startTime \u003d Time.monotonicNow();\n     final long endTime;\n     try {\n       final DatanodeDescriptor datanode \u003d \n           blockManager.getDatanodeManager().getDatanode(datanodeID);\n       if (datanode \u003d\u003d null || !datanode.isAlive) {\n         throw new IOException(\n             \"processCacheReport from dead or unregistered datanode: \" +\n             datanode);\n       }\n       processCacheReportImpl(datanode, blockIds);\n     } finally {\n       endTime \u003d Time.monotonicNow();\n       namesystem.writeUnlock();\n     }\n \n     // Log the block report processing stats from Namenode perspective\n     final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n     if (metrics !\u003d null) {\n       metrics.addCacheBlockReport((int) (endTime - startTime));\n     }\n-    if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"Processed cache report from \"\n-          + datanodeID + \", blocks: \" + blockIds.size()\n-          + \", processing time: \" + (endTime - startTime) + \" msecs\");\n-    }\n+    LOG.debug(\"Processed cache report from {}, blocks: {}, \" +\n+        \"processing time: {} msecs\", datanodeID, blockIds.size(), \n+        (endTime - startTime));\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public final void processCacheReport(final DatanodeID datanodeID,\n      final List\u003cLong\u003e blockIds) throws IOException {\n    namesystem.writeLock();\n    final long startTime \u003d Time.monotonicNow();\n    final long endTime;\n    try {\n      final DatanodeDescriptor datanode \u003d \n          blockManager.getDatanodeManager().getDatanode(datanodeID);\n      if (datanode \u003d\u003d null || !datanode.isAlive) {\n        throw new IOException(\n            \"processCacheReport from dead or unregistered datanode: \" +\n            datanode);\n      }\n      processCacheReportImpl(datanode, blockIds);\n    } finally {\n      endTime \u003d Time.monotonicNow();\n      namesystem.writeUnlock();\n    }\n\n    // Log the block report processing stats from Namenode perspective\n    final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n    if (metrics !\u003d null) {\n      metrics.addCacheBlockReport((int) (endTime - startTime));\n    }\n    LOG.debug(\"Processed cache report from {}, blocks: {}, \" +\n        \"processing time: {} msecs\", datanodeID, blockIds.size(), \n        (endTime - startTime));\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
      "extendedDetails": {}
    },
    "a3616c58dd2ddb16172ca3ab5d66fad52ec0e6d7": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6086. Fix a case where zero-copy or no-checksum reads were not allowed even when the block was cached. (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1576533 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "11/03/14 3:41 PM",
      "commitName": "a3616c58dd2ddb16172ca3ab5d66fad52ec0e6d7",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "11/03/14 2:44 PM",
      "commitNameOld": "b027ef8858e6b8ce26635ffbadc2a461c555ff86",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 0.04,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,30 @@\n   public final void processCacheReport(final DatanodeID datanodeID,\n       final List\u003cLong\u003e blockIds) throws IOException {\n     namesystem.writeLock();\n     final long startTime \u003d Time.monotonicNow();\n     final long endTime;\n     try {\n       final DatanodeDescriptor datanode \u003d \n           blockManager.getDatanodeManager().getDatanode(datanodeID);\n       if (datanode \u003d\u003d null || !datanode.isAlive) {\n         throw new IOException(\n             \"processCacheReport from dead or unregistered datanode: \" +\n             datanode);\n       }\n       processCacheReportImpl(datanode, blockIds);\n     } finally {\n       endTime \u003d Time.monotonicNow();\n       namesystem.writeUnlock();\n     }\n \n     // Log the block report processing stats from Namenode perspective\n     final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n     if (metrics !\u003d null) {\n       metrics.addCacheBlockReport((int) (endTime - startTime));\n     }\n-    LOG.info(\"Processed cache report from \"\n-        + datanodeID + \", blocks: \" + blockIds.size()\n-        + \", processing time: \" + (endTime - startTime) + \" msecs\");\n+    if (LOG.isDebugEnabled()) {\n+      LOG.debug(\"Processed cache report from \"\n+          + datanodeID + \", blocks: \" + blockIds.size()\n+          + \", processing time: \" + (endTime - startTime) + \" msecs\");\n+    }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public final void processCacheReport(final DatanodeID datanodeID,\n      final List\u003cLong\u003e blockIds) throws IOException {\n    namesystem.writeLock();\n    final long startTime \u003d Time.monotonicNow();\n    final long endTime;\n    try {\n      final DatanodeDescriptor datanode \u003d \n          blockManager.getDatanodeManager().getDatanode(datanodeID);\n      if (datanode \u003d\u003d null || !datanode.isAlive) {\n        throw new IOException(\n            \"processCacheReport from dead or unregistered datanode: \" +\n            datanode);\n      }\n      processCacheReportImpl(datanode, blockIds);\n    } finally {\n      endTime \u003d Time.monotonicNow();\n      namesystem.writeUnlock();\n    }\n\n    // Log the block report processing stats from Namenode perspective\n    final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n    if (metrics !\u003d null) {\n      metrics.addCacheBlockReport((int) (endTime - startTime));\n    }\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Processed cache report from \"\n          + datanodeID + \", blocks: \" + blockIds.size()\n          + \", processing time: \" + (endTime - startTime) + \" msecs\");\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
      "extendedDetails": {}
    },
    "d85c017d0488930d806f267141057fc73e68c728": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5651. Remove dfs.namenode.caching.enabled and improve CRM locking. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1555002 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "02/01/14 6:45 PM",
      "commitName": "d85c017d0488930d806f267141057fc73e68c728",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "31/12/13 4:01 PM",
      "commitNameOld": "07e4fb1455abc33584fc666ef745abe256ebd7d1",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 2.11,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,34 +1,28 @@\n   public final void processCacheReport(final DatanodeID datanodeID,\n       final List\u003cLong\u003e blockIds) throws IOException {\n-    if (!enabled) {\n-      LOG.info(\"Ignoring cache report from \" + datanodeID +\n-          \" because \" + DFS_NAMENODE_CACHING_ENABLED_KEY + \" \u003d false. \" +\n-          \"number of blocks: \" + blockIds.size());\n-      return;\n-    }\n     namesystem.writeLock();\n     final long startTime \u003d Time.monotonicNow();\n     final long endTime;\n     try {\n       final DatanodeDescriptor datanode \u003d \n           blockManager.getDatanodeManager().getDatanode(datanodeID);\n       if (datanode \u003d\u003d null || !datanode.isAlive) {\n         throw new IOException(\n             \"processCacheReport from dead or unregistered datanode: \" +\n             datanode);\n       }\n       processCacheReportImpl(datanode, blockIds);\n     } finally {\n       endTime \u003d Time.monotonicNow();\n       namesystem.writeUnlock();\n     }\n \n     // Log the block report processing stats from Namenode perspective\n     final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n     if (metrics !\u003d null) {\n       metrics.addCacheBlockReport((int) (endTime - startTime));\n     }\n     LOG.info(\"Processed cache report from \"\n         + datanodeID + \", blocks: \" + blockIds.size()\n         + \", processing time: \" + (endTime - startTime) + \" msecs\");\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public final void processCacheReport(final DatanodeID datanodeID,\n      final List\u003cLong\u003e blockIds) throws IOException {\n    namesystem.writeLock();\n    final long startTime \u003d Time.monotonicNow();\n    final long endTime;\n    try {\n      final DatanodeDescriptor datanode \u003d \n          blockManager.getDatanodeManager().getDatanode(datanodeID);\n      if (datanode \u003d\u003d null || !datanode.isAlive) {\n        throw new IOException(\n            \"processCacheReport from dead or unregistered datanode: \" +\n            datanode);\n      }\n      processCacheReportImpl(datanode, blockIds);\n    } finally {\n      endTime \u003d Time.monotonicNow();\n      namesystem.writeUnlock();\n    }\n\n    // Log the block report processing stats from Namenode perspective\n    final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n    if (metrics !\u003d null) {\n      metrics.addCacheBlockReport((int) (endTime - startTime));\n    }\n    LOG.info(\"Processed cache report from \"\n        + datanodeID + \", blocks: \" + blockIds.size()\n        + \", processing time: \" + (endTime - startTime) + \" msecs\");\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
      "extendedDetails": {}
    },
    "f9c08d02ebe4a5477cf5d753f0d9d48fc6f9fa48": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-5378. In CacheReport, don\u0027t send genstamp and length on the wire (Contributed by Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1534334 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "21/10/13 12:29 PM",
      "commitName": "f9c08d02ebe4a5477cf5d753f0d9d48fc6f9fa48",
      "commitAuthor": "Colin McCabe",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-5378. In CacheReport, don\u0027t send genstamp and length on the wire (Contributed by Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1534334 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "21/10/13 12:29 PM",
          "commitName": "f9c08d02ebe4a5477cf5d753f0d9d48fc6f9fa48",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "18/10/13 3:15 PM",
          "commitNameOld": "d61af9781086073152113d97106f708ea1cf6e8c",
          "commitAuthorOld": "Colin McCabe",
          "daysBetweenCommits": 2.88,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,33 +1,33 @@\n   public final void processCacheReport(final DatanodeID datanodeID,\n-      final BlockListAsLongs report) throws IOException {\n+      final List\u003cLong\u003e blockIds) throws IOException {\n     if (!enabled) {\n       LOG.info(\"Ignoring cache report from \" + datanodeID +\n           \" because \" + DFS_NAMENODE_CACHING_ENABLED_KEY + \" \u003d false. \" +\n-          \"number of blocks: \" + report.getNumberOfBlocks());\n+          \"number of blocks: \" + blockIds.size());\n       return;\n     }\n     namesystem.writeLock();\n     final long startTime \u003d Time.monotonicNow();\n     final long endTime;\n     try {\n       final DatanodeDescriptor datanode \u003d \n           blockManager.getDatanodeManager().getDatanode(datanodeID);\n       if (datanode \u003d\u003d null || !datanode.isAlive) {\n         throw new IOException(\n             \"processCacheReport from dead or unregistered datanode: \" + datanode);\n       }\n-      processCacheReportImpl(datanode, report);\n+      processCacheReportImpl(datanode, blockIds);\n     } finally {\n       endTime \u003d Time.monotonicNow();\n       namesystem.writeUnlock();\n     }\n \n     // Log the block report processing stats from Namenode perspective\n     final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n     if (metrics !\u003d null) {\n       metrics.addCacheBlockReport((int) (endTime - startTime));\n     }\n     LOG.info(\"Processed cache report from \"\n-        + datanodeID + \", blocks: \" + report.getNumberOfBlocks()\n+        + datanodeID + \", blocks: \" + blockIds.size()\n         + \", processing time: \" + (endTime - startTime) + \" msecs\");\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public final void processCacheReport(final DatanodeID datanodeID,\n      final List\u003cLong\u003e blockIds) throws IOException {\n    if (!enabled) {\n      LOG.info(\"Ignoring cache report from \" + datanodeID +\n          \" because \" + DFS_NAMENODE_CACHING_ENABLED_KEY + \" \u003d false. \" +\n          \"number of blocks: \" + blockIds.size());\n      return;\n    }\n    namesystem.writeLock();\n    final long startTime \u003d Time.monotonicNow();\n    final long endTime;\n    try {\n      final DatanodeDescriptor datanode \u003d \n          blockManager.getDatanodeManager().getDatanode(datanodeID);\n      if (datanode \u003d\u003d null || !datanode.isAlive) {\n        throw new IOException(\n            \"processCacheReport from dead or unregistered datanode: \" + datanode);\n      }\n      processCacheReportImpl(datanode, blockIds);\n    } finally {\n      endTime \u003d Time.monotonicNow();\n      namesystem.writeUnlock();\n    }\n\n    // Log the block report processing stats from Namenode perspective\n    final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n    if (metrics !\u003d null) {\n      metrics.addCacheBlockReport((int) (endTime - startTime));\n    }\n    LOG.info(\"Processed cache report from \"\n        + datanodeID + \", blocks: \" + blockIds.size()\n        + \", processing time: \" + (endTime - startTime) + \" msecs\");\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
          "extendedDetails": {
            "oldValue": "[datanodeID-DatanodeID(modifiers-final), report-BlockListAsLongs(modifiers-final)]",
            "newValue": "[datanodeID-DatanodeID(modifiers-final), blockIds-List\u003cLong\u003e(modifiers-final)]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-5378. In CacheReport, don\u0027t send genstamp and length on the wire (Contributed by Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1534334 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "21/10/13 12:29 PM",
          "commitName": "f9c08d02ebe4a5477cf5d753f0d9d48fc6f9fa48",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "18/10/13 3:15 PM",
          "commitNameOld": "d61af9781086073152113d97106f708ea1cf6e8c",
          "commitAuthorOld": "Colin McCabe",
          "daysBetweenCommits": 2.88,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,33 +1,33 @@\n   public final void processCacheReport(final DatanodeID datanodeID,\n-      final BlockListAsLongs report) throws IOException {\n+      final List\u003cLong\u003e blockIds) throws IOException {\n     if (!enabled) {\n       LOG.info(\"Ignoring cache report from \" + datanodeID +\n           \" because \" + DFS_NAMENODE_CACHING_ENABLED_KEY + \" \u003d false. \" +\n-          \"number of blocks: \" + report.getNumberOfBlocks());\n+          \"number of blocks: \" + blockIds.size());\n       return;\n     }\n     namesystem.writeLock();\n     final long startTime \u003d Time.monotonicNow();\n     final long endTime;\n     try {\n       final DatanodeDescriptor datanode \u003d \n           blockManager.getDatanodeManager().getDatanode(datanodeID);\n       if (datanode \u003d\u003d null || !datanode.isAlive) {\n         throw new IOException(\n             \"processCacheReport from dead or unregistered datanode: \" + datanode);\n       }\n-      processCacheReportImpl(datanode, report);\n+      processCacheReportImpl(datanode, blockIds);\n     } finally {\n       endTime \u003d Time.monotonicNow();\n       namesystem.writeUnlock();\n     }\n \n     // Log the block report processing stats from Namenode perspective\n     final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n     if (metrics !\u003d null) {\n       metrics.addCacheBlockReport((int) (endTime - startTime));\n     }\n     LOG.info(\"Processed cache report from \"\n-        + datanodeID + \", blocks: \" + report.getNumberOfBlocks()\n+        + datanodeID + \", blocks: \" + blockIds.size()\n         + \", processing time: \" + (endTime - startTime) + \" msecs\");\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public final void processCacheReport(final DatanodeID datanodeID,\n      final List\u003cLong\u003e blockIds) throws IOException {\n    if (!enabled) {\n      LOG.info(\"Ignoring cache report from \" + datanodeID +\n          \" because \" + DFS_NAMENODE_CACHING_ENABLED_KEY + \" \u003d false. \" +\n          \"number of blocks: \" + blockIds.size());\n      return;\n    }\n    namesystem.writeLock();\n    final long startTime \u003d Time.monotonicNow();\n    final long endTime;\n    try {\n      final DatanodeDescriptor datanode \u003d \n          blockManager.getDatanodeManager().getDatanode(datanodeID);\n      if (datanode \u003d\u003d null || !datanode.isAlive) {\n        throw new IOException(\n            \"processCacheReport from dead or unregistered datanode: \" + datanode);\n      }\n      processCacheReportImpl(datanode, blockIds);\n    } finally {\n      endTime \u003d Time.monotonicNow();\n      namesystem.writeUnlock();\n    }\n\n    // Log the block report processing stats from Namenode perspective\n    final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n    if (metrics !\u003d null) {\n      metrics.addCacheBlockReport((int) (endTime - startTime));\n    }\n    LOG.info(\"Processed cache report from \"\n        + datanodeID + \", blocks: \" + blockIds.size()\n        + \", processing time: \" + (endTime - startTime) + \" msecs\");\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
          "extendedDetails": {}
        }
      ]
    },
    "3cc7a38a53c8ae27ef6b2397cddc5d14a378203a": {
      "type": "Ymultichange(Ymovefromfile,Ymodifierchange,Ybodychange,Yparameterchange)",
      "commitMessage": "HDFS-5096. Automatically cache new data added to a cached path (contributed by Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1532924 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "16/10/13 3:15 PM",
      "commitName": "3cc7a38a53c8ae27ef6b2397cddc5d14a378203a",
      "commitAuthor": "Colin McCabe",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "HDFS-5096. Automatically cache new data added to a cached path (contributed by Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1532924 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "16/10/13 3:15 PM",
          "commitName": "3cc7a38a53c8ae27ef6b2397cddc5d14a378203a",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "16/10/13 1:23 PM",
          "commitNameOld": "8da82eba1c84f828617a13a6f785a9b6cfc057a5",
          "commitAuthorOld": "Chris Nauroth",
          "daysBetweenCommits": 0.08,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,45 +1,33 @@\n-  public void processCacheReport(final DatanodeID nodeID, final String poolId,\n-      final BlockListAsLongs newReport) throws IOException {\n-    if (!isCachingEnabled) {\n-      String error \u003d \"cacheReport received from datanode \" + nodeID\n-          + \" but caching is disabled on the namenode (\"\n-          + DFSConfigKeys.DFS_NAMENODE_CACHING_ENABLED_KEY + \")\";\n-      LOG.warn(error + \", ignoring\");\n-      throw new IOException(error);\n+  public final void processCacheReport(final DatanodeID datanodeID,\n+      final BlockListAsLongs report) throws IOException {\n+    if (!enabled) {\n+      LOG.info(\"Ignoring cache report from \" + datanodeID +\n+          \" because \" + DFS_NAMENODE_CACHING_ENABLED_KEY + \" \u003d false. \" +\n+          \"number of blocks: \" + report.getNumberOfBlocks());\n+      return;\n     }\n     namesystem.writeLock();\n-    final long startTime \u003d Time.now(); //after acquiring write lock\n+    final long startTime \u003d Time.monotonicNow();\n     final long endTime;\n     try {\n-      final DatanodeDescriptor node \u003d datanodeManager.getDatanode(nodeID);\n-      if (node \u003d\u003d null || !node.isAlive) {\n+      final DatanodeDescriptor datanode \u003d \n+          blockManager.getDatanodeManager().getDatanode(datanodeID);\n+      if (datanode \u003d\u003d null || !datanode.isAlive) {\n         throw new IOException(\n-            \"processCacheReport from dead or unregistered node: \" + nodeID);\n+            \"processCacheReport from dead or unregistered datanode: \" + datanode);\n       }\n-\n-      // TODO: do an optimized initial cache report while in startup safemode\n-      if (namesystem.isInStartupSafeMode()) {\n-        blockLogInfo(\"#processCacheReport: \"\n-            + \"discarded cache report from \" + nodeID\n-            + \" because namenode still in startup phase\");\n-        return;\n-      }\n-\n-      processReport(node, newReport);\n-\n-      // TODO: process postponed blocks reported while a standby\n-      //rescanPostponedMisreplicatedBlocks();\n+      processCacheReportImpl(datanode, report);\n     } finally {\n-      endTime \u003d Time.now();\n+      endTime \u003d Time.monotonicNow();\n       namesystem.writeUnlock();\n     }\n \n     // Log the block report processing stats from Namenode perspective\n     final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n     if (metrics !\u003d null) {\n       metrics.addCacheBlockReport((int) (endTime - startTime));\n     }\n-    blockLogInfo(\"#processCacheReport: from \"\n-        + nodeID + \", blocks: \" + newReport.getNumberOfBlocks()\n+    LOG.info(\"Processed cache report from \"\n+        + datanodeID + \", blocks: \" + report.getNumberOfBlocks()\n         + \", processing time: \" + (endTime - startTime) + \" msecs\");\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public final void processCacheReport(final DatanodeID datanodeID,\n      final BlockListAsLongs report) throws IOException {\n    if (!enabled) {\n      LOG.info(\"Ignoring cache report from \" + datanodeID +\n          \" because \" + DFS_NAMENODE_CACHING_ENABLED_KEY + \" \u003d false. \" +\n          \"number of blocks: \" + report.getNumberOfBlocks());\n      return;\n    }\n    namesystem.writeLock();\n    final long startTime \u003d Time.monotonicNow();\n    final long endTime;\n    try {\n      final DatanodeDescriptor datanode \u003d \n          blockManager.getDatanodeManager().getDatanode(datanodeID);\n      if (datanode \u003d\u003d null || !datanode.isAlive) {\n        throw new IOException(\n            \"processCacheReport from dead or unregistered datanode: \" + datanode);\n      }\n      processCacheReportImpl(datanode, report);\n    } finally {\n      endTime \u003d Time.monotonicNow();\n      namesystem.writeUnlock();\n    }\n\n    // Log the block report processing stats from Namenode perspective\n    final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n    if (metrics !\u003d null) {\n      metrics.addCacheBlockReport((int) (endTime - startTime));\n    }\n    LOG.info(\"Processed cache report from \"\n        + datanodeID + \", blocks: \" + report.getNumberOfBlocks()\n        + \", processing time: \" + (endTime - startTime) + \" msecs\");\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationManager.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
            "oldMethodName": "processCacheReport",
            "newMethodName": "processCacheReport"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "HDFS-5096. Automatically cache new data added to a cached path (contributed by Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1532924 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "16/10/13 3:15 PM",
          "commitName": "3cc7a38a53c8ae27ef6b2397cddc5d14a378203a",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "16/10/13 1:23 PM",
          "commitNameOld": "8da82eba1c84f828617a13a6f785a9b6cfc057a5",
          "commitAuthorOld": "Chris Nauroth",
          "daysBetweenCommits": 0.08,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,45 +1,33 @@\n-  public void processCacheReport(final DatanodeID nodeID, final String poolId,\n-      final BlockListAsLongs newReport) throws IOException {\n-    if (!isCachingEnabled) {\n-      String error \u003d \"cacheReport received from datanode \" + nodeID\n-          + \" but caching is disabled on the namenode (\"\n-          + DFSConfigKeys.DFS_NAMENODE_CACHING_ENABLED_KEY + \")\";\n-      LOG.warn(error + \", ignoring\");\n-      throw new IOException(error);\n+  public final void processCacheReport(final DatanodeID datanodeID,\n+      final BlockListAsLongs report) throws IOException {\n+    if (!enabled) {\n+      LOG.info(\"Ignoring cache report from \" + datanodeID +\n+          \" because \" + DFS_NAMENODE_CACHING_ENABLED_KEY + \" \u003d false. \" +\n+          \"number of blocks: \" + report.getNumberOfBlocks());\n+      return;\n     }\n     namesystem.writeLock();\n-    final long startTime \u003d Time.now(); //after acquiring write lock\n+    final long startTime \u003d Time.monotonicNow();\n     final long endTime;\n     try {\n-      final DatanodeDescriptor node \u003d datanodeManager.getDatanode(nodeID);\n-      if (node \u003d\u003d null || !node.isAlive) {\n+      final DatanodeDescriptor datanode \u003d \n+          blockManager.getDatanodeManager().getDatanode(datanodeID);\n+      if (datanode \u003d\u003d null || !datanode.isAlive) {\n         throw new IOException(\n-            \"processCacheReport from dead or unregistered node: \" + nodeID);\n+            \"processCacheReport from dead or unregistered datanode: \" + datanode);\n       }\n-\n-      // TODO: do an optimized initial cache report while in startup safemode\n-      if (namesystem.isInStartupSafeMode()) {\n-        blockLogInfo(\"#processCacheReport: \"\n-            + \"discarded cache report from \" + nodeID\n-            + \" because namenode still in startup phase\");\n-        return;\n-      }\n-\n-      processReport(node, newReport);\n-\n-      // TODO: process postponed blocks reported while a standby\n-      //rescanPostponedMisreplicatedBlocks();\n+      processCacheReportImpl(datanode, report);\n     } finally {\n-      endTime \u003d Time.now();\n+      endTime \u003d Time.monotonicNow();\n       namesystem.writeUnlock();\n     }\n \n     // Log the block report processing stats from Namenode perspective\n     final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n     if (metrics !\u003d null) {\n       metrics.addCacheBlockReport((int) (endTime - startTime));\n     }\n-    blockLogInfo(\"#processCacheReport: from \"\n-        + nodeID + \", blocks: \" + newReport.getNumberOfBlocks()\n+    LOG.info(\"Processed cache report from \"\n+        + datanodeID + \", blocks: \" + report.getNumberOfBlocks()\n         + \", processing time: \" + (endTime - startTime) + \" msecs\");\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public final void processCacheReport(final DatanodeID datanodeID,\n      final BlockListAsLongs report) throws IOException {\n    if (!enabled) {\n      LOG.info(\"Ignoring cache report from \" + datanodeID +\n          \" because \" + DFS_NAMENODE_CACHING_ENABLED_KEY + \" \u003d false. \" +\n          \"number of blocks: \" + report.getNumberOfBlocks());\n      return;\n    }\n    namesystem.writeLock();\n    final long startTime \u003d Time.monotonicNow();\n    final long endTime;\n    try {\n      final DatanodeDescriptor datanode \u003d \n          blockManager.getDatanodeManager().getDatanode(datanodeID);\n      if (datanode \u003d\u003d null || !datanode.isAlive) {\n        throw new IOException(\n            \"processCacheReport from dead or unregistered datanode: \" + datanode);\n      }\n      processCacheReportImpl(datanode, report);\n    } finally {\n      endTime \u003d Time.monotonicNow();\n      namesystem.writeUnlock();\n    }\n\n    // Log the block report processing stats from Namenode perspective\n    final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n    if (metrics !\u003d null) {\n      metrics.addCacheBlockReport((int) (endTime - startTime));\n    }\n    LOG.info(\"Processed cache report from \"\n        + datanodeID + \", blocks: \" + report.getNumberOfBlocks()\n        + \", processing time: \" + (endTime - startTime) + \" msecs\");\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
          "extendedDetails": {
            "oldValue": "[public]",
            "newValue": "[public, final]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-5096. Automatically cache new data added to a cached path (contributed by Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1532924 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "16/10/13 3:15 PM",
          "commitName": "3cc7a38a53c8ae27ef6b2397cddc5d14a378203a",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "16/10/13 1:23 PM",
          "commitNameOld": "8da82eba1c84f828617a13a6f785a9b6cfc057a5",
          "commitAuthorOld": "Chris Nauroth",
          "daysBetweenCommits": 0.08,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,45 +1,33 @@\n-  public void processCacheReport(final DatanodeID nodeID, final String poolId,\n-      final BlockListAsLongs newReport) throws IOException {\n-    if (!isCachingEnabled) {\n-      String error \u003d \"cacheReport received from datanode \" + nodeID\n-          + \" but caching is disabled on the namenode (\"\n-          + DFSConfigKeys.DFS_NAMENODE_CACHING_ENABLED_KEY + \")\";\n-      LOG.warn(error + \", ignoring\");\n-      throw new IOException(error);\n+  public final void processCacheReport(final DatanodeID datanodeID,\n+      final BlockListAsLongs report) throws IOException {\n+    if (!enabled) {\n+      LOG.info(\"Ignoring cache report from \" + datanodeID +\n+          \" because \" + DFS_NAMENODE_CACHING_ENABLED_KEY + \" \u003d false. \" +\n+          \"number of blocks: \" + report.getNumberOfBlocks());\n+      return;\n     }\n     namesystem.writeLock();\n-    final long startTime \u003d Time.now(); //after acquiring write lock\n+    final long startTime \u003d Time.monotonicNow();\n     final long endTime;\n     try {\n-      final DatanodeDescriptor node \u003d datanodeManager.getDatanode(nodeID);\n-      if (node \u003d\u003d null || !node.isAlive) {\n+      final DatanodeDescriptor datanode \u003d \n+          blockManager.getDatanodeManager().getDatanode(datanodeID);\n+      if (datanode \u003d\u003d null || !datanode.isAlive) {\n         throw new IOException(\n-            \"processCacheReport from dead or unregistered node: \" + nodeID);\n+            \"processCacheReport from dead or unregistered datanode: \" + datanode);\n       }\n-\n-      // TODO: do an optimized initial cache report while in startup safemode\n-      if (namesystem.isInStartupSafeMode()) {\n-        blockLogInfo(\"#processCacheReport: \"\n-            + \"discarded cache report from \" + nodeID\n-            + \" because namenode still in startup phase\");\n-        return;\n-      }\n-\n-      processReport(node, newReport);\n-\n-      // TODO: process postponed blocks reported while a standby\n-      //rescanPostponedMisreplicatedBlocks();\n+      processCacheReportImpl(datanode, report);\n     } finally {\n-      endTime \u003d Time.now();\n+      endTime \u003d Time.monotonicNow();\n       namesystem.writeUnlock();\n     }\n \n     // Log the block report processing stats from Namenode perspective\n     final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n     if (metrics !\u003d null) {\n       metrics.addCacheBlockReport((int) (endTime - startTime));\n     }\n-    blockLogInfo(\"#processCacheReport: from \"\n-        + nodeID + \", blocks: \" + newReport.getNumberOfBlocks()\n+    LOG.info(\"Processed cache report from \"\n+        + datanodeID + \", blocks: \" + report.getNumberOfBlocks()\n         + \", processing time: \" + (endTime - startTime) + \" msecs\");\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public final void processCacheReport(final DatanodeID datanodeID,\n      final BlockListAsLongs report) throws IOException {\n    if (!enabled) {\n      LOG.info(\"Ignoring cache report from \" + datanodeID +\n          \" because \" + DFS_NAMENODE_CACHING_ENABLED_KEY + \" \u003d false. \" +\n          \"number of blocks: \" + report.getNumberOfBlocks());\n      return;\n    }\n    namesystem.writeLock();\n    final long startTime \u003d Time.monotonicNow();\n    final long endTime;\n    try {\n      final DatanodeDescriptor datanode \u003d \n          blockManager.getDatanodeManager().getDatanode(datanodeID);\n      if (datanode \u003d\u003d null || !datanode.isAlive) {\n        throw new IOException(\n            \"processCacheReport from dead or unregistered datanode: \" + datanode);\n      }\n      processCacheReportImpl(datanode, report);\n    } finally {\n      endTime \u003d Time.monotonicNow();\n      namesystem.writeUnlock();\n    }\n\n    // Log the block report processing stats from Namenode perspective\n    final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n    if (metrics !\u003d null) {\n      metrics.addCacheBlockReport((int) (endTime - startTime));\n    }\n    LOG.info(\"Processed cache report from \"\n        + datanodeID + \", blocks: \" + report.getNumberOfBlocks()\n        + \", processing time: \" + (endTime - startTime) + \" msecs\");\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
          "extendedDetails": {}
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-5096. Automatically cache new data added to a cached path (contributed by Colin Patrick McCabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1532924 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "16/10/13 3:15 PM",
          "commitName": "3cc7a38a53c8ae27ef6b2397cddc5d14a378203a",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "16/10/13 1:23 PM",
          "commitNameOld": "8da82eba1c84f828617a13a6f785a9b6cfc057a5",
          "commitAuthorOld": "Chris Nauroth",
          "daysBetweenCommits": 0.08,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,45 +1,33 @@\n-  public void processCacheReport(final DatanodeID nodeID, final String poolId,\n-      final BlockListAsLongs newReport) throws IOException {\n-    if (!isCachingEnabled) {\n-      String error \u003d \"cacheReport received from datanode \" + nodeID\n-          + \" but caching is disabled on the namenode (\"\n-          + DFSConfigKeys.DFS_NAMENODE_CACHING_ENABLED_KEY + \")\";\n-      LOG.warn(error + \", ignoring\");\n-      throw new IOException(error);\n+  public final void processCacheReport(final DatanodeID datanodeID,\n+      final BlockListAsLongs report) throws IOException {\n+    if (!enabled) {\n+      LOG.info(\"Ignoring cache report from \" + datanodeID +\n+          \" because \" + DFS_NAMENODE_CACHING_ENABLED_KEY + \" \u003d false. \" +\n+          \"number of blocks: \" + report.getNumberOfBlocks());\n+      return;\n     }\n     namesystem.writeLock();\n-    final long startTime \u003d Time.now(); //after acquiring write lock\n+    final long startTime \u003d Time.monotonicNow();\n     final long endTime;\n     try {\n-      final DatanodeDescriptor node \u003d datanodeManager.getDatanode(nodeID);\n-      if (node \u003d\u003d null || !node.isAlive) {\n+      final DatanodeDescriptor datanode \u003d \n+          blockManager.getDatanodeManager().getDatanode(datanodeID);\n+      if (datanode \u003d\u003d null || !datanode.isAlive) {\n         throw new IOException(\n-            \"processCacheReport from dead or unregistered node: \" + nodeID);\n+            \"processCacheReport from dead or unregistered datanode: \" + datanode);\n       }\n-\n-      // TODO: do an optimized initial cache report while in startup safemode\n-      if (namesystem.isInStartupSafeMode()) {\n-        blockLogInfo(\"#processCacheReport: \"\n-            + \"discarded cache report from \" + nodeID\n-            + \" because namenode still in startup phase\");\n-        return;\n-      }\n-\n-      processReport(node, newReport);\n-\n-      // TODO: process postponed blocks reported while a standby\n-      //rescanPostponedMisreplicatedBlocks();\n+      processCacheReportImpl(datanode, report);\n     } finally {\n-      endTime \u003d Time.now();\n+      endTime \u003d Time.monotonicNow();\n       namesystem.writeUnlock();\n     }\n \n     // Log the block report processing stats from Namenode perspective\n     final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n     if (metrics !\u003d null) {\n       metrics.addCacheBlockReport((int) (endTime - startTime));\n     }\n-    blockLogInfo(\"#processCacheReport: from \"\n-        + nodeID + \", blocks: \" + newReport.getNumberOfBlocks()\n+    LOG.info(\"Processed cache report from \"\n+        + datanodeID + \", blocks: \" + report.getNumberOfBlocks()\n         + \", processing time: \" + (endTime - startTime) + \" msecs\");\n   }\n\\ No newline at end of file\n",
          "actualSource": "  public final void processCacheReport(final DatanodeID datanodeID,\n      final BlockListAsLongs report) throws IOException {\n    if (!enabled) {\n      LOG.info(\"Ignoring cache report from \" + datanodeID +\n          \" because \" + DFS_NAMENODE_CACHING_ENABLED_KEY + \" \u003d false. \" +\n          \"number of blocks: \" + report.getNumberOfBlocks());\n      return;\n    }\n    namesystem.writeLock();\n    final long startTime \u003d Time.monotonicNow();\n    final long endTime;\n    try {\n      final DatanodeDescriptor datanode \u003d \n          blockManager.getDatanodeManager().getDatanode(datanodeID);\n      if (datanode \u003d\u003d null || !datanode.isAlive) {\n        throw new IOException(\n            \"processCacheReport from dead or unregistered datanode: \" + datanode);\n      }\n      processCacheReportImpl(datanode, report);\n    } finally {\n      endTime \u003d Time.monotonicNow();\n      namesystem.writeUnlock();\n    }\n\n    // Log the block report processing stats from Namenode perspective\n    final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n    if (metrics !\u003d null) {\n      metrics.addCacheBlockReport((int) (endTime - startTime));\n    }\n    LOG.info(\"Processed cache report from \"\n        + datanodeID + \", blocks: \" + report.getNumberOfBlocks()\n        + \", processing time: \" + (endTime - startTime) + \" msecs\");\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java",
          "extendedDetails": {
            "oldValue": "[nodeID-DatanodeID(modifiers-final), poolId-String(modifiers-final), newReport-BlockListAsLongs(modifiers-final)]",
            "newValue": "[datanodeID-DatanodeID(modifiers-final), report-BlockListAsLongs(modifiers-final)]"
          }
        }
      ]
    },
    "40eb94ade3161d93e7a762a839004748f6d0ae89": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-5053. NameNode should invoke DataNode APIs to coordinate caching. (Andrew Wang)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-4949@1523145 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/09/13 4:27 PM",
      "commitName": "40eb94ade3161d93e7a762a839004748f6d0ae89",
      "commitAuthor": "Andrew Wang",
      "diff": "@@ -0,0 +1,45 @@\n+  public void processCacheReport(final DatanodeID nodeID, final String poolId,\n+      final BlockListAsLongs newReport) throws IOException {\n+    if (!isCachingEnabled) {\n+      String error \u003d \"cacheReport received from datanode \" + nodeID\n+          + \" but caching is disabled on the namenode (\"\n+          + DFSConfigKeys.DFS_NAMENODE_CACHING_ENABLED_KEY + \")\";\n+      LOG.warn(error + \", ignoring\");\n+      throw new IOException(error);\n+    }\n+    namesystem.writeLock();\n+    final long startTime \u003d Time.now(); //after acquiring write lock\n+    final long endTime;\n+    try {\n+      final DatanodeDescriptor node \u003d datanodeManager.getDatanode(nodeID);\n+      if (node \u003d\u003d null || !node.isAlive) {\n+        throw new IOException(\n+            \"processCacheReport from dead or unregistered node: \" + nodeID);\n+      }\n+\n+      // TODO: do an optimized initial cache report while in startup safemode\n+      if (namesystem.isInStartupSafeMode()) {\n+        blockLogInfo(\"#processCacheReport: \"\n+            + \"discarded cache report from \" + nodeID\n+            + \" because namenode still in startup phase\");\n+        return;\n+      }\n+\n+      processReport(node, newReport);\n+\n+      // TODO: process postponed blocks reported while a standby\n+      //rescanPostponedMisreplicatedBlocks();\n+    } finally {\n+      endTime \u003d Time.now();\n+      namesystem.writeUnlock();\n+    }\n+\n+    // Log the block report processing stats from Namenode perspective\n+    final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n+    if (metrics !\u003d null) {\n+      metrics.addCacheBlockReport((int) (endTime - startTime));\n+    }\n+    blockLogInfo(\"#processCacheReport: from \"\n+        + nodeID + \", blocks: \" + newReport.getNumberOfBlocks()\n+        + \", processing time: \" + (endTime - startTime) + \" msecs\");\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public void processCacheReport(final DatanodeID nodeID, final String poolId,\n      final BlockListAsLongs newReport) throws IOException {\n    if (!isCachingEnabled) {\n      String error \u003d \"cacheReport received from datanode \" + nodeID\n          + \" but caching is disabled on the namenode (\"\n          + DFSConfigKeys.DFS_NAMENODE_CACHING_ENABLED_KEY + \")\";\n      LOG.warn(error + \", ignoring\");\n      throw new IOException(error);\n    }\n    namesystem.writeLock();\n    final long startTime \u003d Time.now(); //after acquiring write lock\n    final long endTime;\n    try {\n      final DatanodeDescriptor node \u003d datanodeManager.getDatanode(nodeID);\n      if (node \u003d\u003d null || !node.isAlive) {\n        throw new IOException(\n            \"processCacheReport from dead or unregistered node: \" + nodeID);\n      }\n\n      // TODO: do an optimized initial cache report while in startup safemode\n      if (namesystem.isInStartupSafeMode()) {\n        blockLogInfo(\"#processCacheReport: \"\n            + \"discarded cache report from \" + nodeID\n            + \" because namenode still in startup phase\");\n        return;\n      }\n\n      processReport(node, newReport);\n\n      // TODO: process postponed blocks reported while a standby\n      //rescanPostponedMisreplicatedBlocks();\n    } finally {\n      endTime \u003d Time.now();\n      namesystem.writeUnlock();\n    }\n\n    // Log the block report processing stats from Namenode perspective\n    final NameNodeMetrics metrics \u003d NameNode.getNameNodeMetrics();\n    if (metrics !\u003d null) {\n      metrics.addCacheBlockReport((int) (endTime - startTime));\n    }\n    blockLogInfo(\"#processCacheReport: from \"\n        + nodeID + \", blocks: \" + newReport.getNumberOfBlocks()\n        + \", processing time: \" + (endTime - startTime) + \" msecs\");\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationManager.java"
    }
  }
}