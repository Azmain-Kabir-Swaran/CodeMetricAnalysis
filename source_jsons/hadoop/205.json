{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "OpenFileCtx.java",
  "functionName": "processPerfectOverWrite",
  "functionId": "processPerfectOverWrite___dfsClient-DFSClient__offset-long__count-int__stableHow-WriteStableHow__data-byte[]__path-String__wccData-WccData__iug-IdMappingServiceProvider",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java",
  "functionStartLine": 691,
  "functionEndLine": 757,
  "numCommitsSeen": 62,
  "timeTaken": 2648,
  "changeHistory": [
    "f20dc0d5770d3876954faf0a6e8dcce6539ffc23",
    "f37849188b05a6251584de1aed5e66d5dfa7da4f",
    "72a556d3b0def0ab4e4509528cc513f6df06b084",
    "d71d40a63d198991077d5babd70be5e9787a53f1",
    "70be56d093022de9953e14a92dfa1a146bd9a290",
    "1c867b1de8a9f4c6bb118c08c7b714bd2b8356cd",
    "3bfd18c6b0483ae27eff6d53bda934e67dda5464",
    "027419832c1125d707b45ce852032d704ab79d88",
    "28e3d09230971b32f74284311931525cb7ad1b7c",
    "c9b89de0eacf15f21faa3a7ba30d4773f571c9a4",
    "37f587563a943a827fbff865f5302bac6d202415"
  ],
  "changeHistoryShort": {
    "f20dc0d5770d3876954faf0a6e8dcce6539ffc23": "Ybodychange",
    "f37849188b05a6251584de1aed5e66d5dfa7da4f": "Ybodychange",
    "72a556d3b0def0ab4e4509528cc513f6df06b084": "Yparameterchange",
    "d71d40a63d198991077d5babd70be5e9787a53f1": "Ybodychange",
    "70be56d093022de9953e14a92dfa1a146bd9a290": "Ybodychange",
    "1c867b1de8a9f4c6bb118c08c7b714bd2b8356cd": "Ybodychange",
    "3bfd18c6b0483ae27eff6d53bda934e67dda5464": "Ybodychange",
    "027419832c1125d707b45ce852032d704ab79d88": "Ybodychange",
    "28e3d09230971b32f74284311931525cb7ad1b7c": "Ybodychange",
    "c9b89de0eacf15f21faa3a7ba30d4773f571c9a4": "Ybodychange",
    "37f587563a943a827fbff865f5302bac6d202415": "Yintroduced"
  },
  "changeHistoryDetails": {
    "f20dc0d5770d3876954faf0a6e8dcce6539ffc23": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-10571. Use Log.*(Object, Throwable) overload to log exceptions.\nContributed by Andras Bokor.\n",
      "commitDate": "14/02/18 8:20 AM",
      "commitName": "f20dc0d5770d3876954faf0a6e8dcce6539ffc23",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "10/10/17 10:38 AM",
      "commitNameOld": "d6602b5f39833611b4afa4581552f6c4c37e23a8",
      "commitAuthorOld": "Jitendra Pandey",
      "daysBetweenCommits": 126.95,
      "commitsBetweenForRepo": 833,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,67 +1,67 @@\n   private WRITE3Response processPerfectOverWrite(DFSClient dfsClient,\n       long offset, int count, WriteStableHow stableHow, byte[] data,\n       String path, WccData wccData, IdMappingServiceProvider iug) {\n     WRITE3Response response;\n \n     // Read the content back\n     byte[] readbuffer \u003d new byte[count];\n \n     int readCount \u003d 0;\n     FSDataInputStream fis \u003d null;\n     try {\n       // Sync file data and length to avoid partial read failure\n       fos.hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));\n     } catch (ClosedChannelException closedException) {\n       LOG.info(\"The FSDataOutputStream has been closed. \"\n           + \"Continue processing the perfect overwrite.\");\n     } catch (IOException e) {\n-      LOG.info(\"hsync failed when processing possible perfect overwrite, path\u003d\"\n-          + path + \" error: \" + e);\n+      LOG.info(\"hsync failed when processing possible perfect overwrite, \" +\n+              \"path\u003d{} error: {}\", path, e.toString());\n       return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n           Nfs3Constant.WRITE_COMMIT_VERF);\n     }\n     \n     try {\n       fis \u003d dfsClient.createWrappedInputStream(dfsClient.open(path));\n       readCount \u003d fis.read(offset, readbuffer, 0, count);\n       if (readCount \u003c count) {\n-        LOG.error(\"Can\u0027t read back \" + count + \" bytes, partial read size: \"\n-            + readCount);\n+        LOG.error(\"Can\u0027t read back {} bytes, partial read size: {}\",\n+            count, readCount);\n         return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n             Nfs3Constant.WRITE_COMMIT_VERF);\n       }\n     } catch (IOException e) {\n-      LOG.info(\"Read failed when processing possible perfect overwrite, path\u003d\"\n-          + path, e);\n+      LOG.info(\"Read failed when processing possible perfect overwrite, \" +\n+              \"path\u003d{}\", path, e);\n       return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n           Nfs3Constant.WRITE_COMMIT_VERF);\n     } finally {\n-      IOUtils.cleanup(LOG, fis);\n+      IOUtils.cleanupWithLogger(LOG, fis);\n     }\n \n     // Compare with the request\n     Comparator comparator \u003d new Comparator();\n     if (comparator.compare(readbuffer, 0, readCount, data, 0, count) !\u003d 0) {\n       LOG.info(\"Perfect overwrite has different content\");\n       response \u003d new WRITE3Response(Nfs3Status.NFS3ERR_INVAL, wccData, 0,\n           stableHow, Nfs3Constant.WRITE_COMMIT_VERF);\n     } else {\n       LOG.info(\"Perfect overwrite has same content,\"\n           + \" updating the mtime, then return success\");\n       Nfs3FileAttributes postOpAttr \u003d null;\n       try {\n         dfsClient.setTimes(path, Time.monotonicNow(), -1);\n         postOpAttr \u003d Nfs3Utils.getFileAttr(dfsClient, path, iug);\n       } catch (IOException e) {\n-        LOG.info(\"Got error when processing perfect overwrite, path\u003d\" + path\n-            + \" error: \" + e);\n+        LOG.info(\"Got error when processing perfect overwrite, path\u003d{} \" +\n+            \"error: {}\", path, e.toString());\n         return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n             Nfs3Constant.WRITE_COMMIT_VERF);\n       }\n \n       wccData.setPostOpAttr(postOpAttr);\n       response \u003d new WRITE3Response(Nfs3Status.NFS3_OK, wccData, count,\n           stableHow, Nfs3Constant.WRITE_COMMIT_VERF);\n     }\n     return response;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private WRITE3Response processPerfectOverWrite(DFSClient dfsClient,\n      long offset, int count, WriteStableHow stableHow, byte[] data,\n      String path, WccData wccData, IdMappingServiceProvider iug) {\n    WRITE3Response response;\n\n    // Read the content back\n    byte[] readbuffer \u003d new byte[count];\n\n    int readCount \u003d 0;\n    FSDataInputStream fis \u003d null;\n    try {\n      // Sync file data and length to avoid partial read failure\n      fos.hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));\n    } catch (ClosedChannelException closedException) {\n      LOG.info(\"The FSDataOutputStream has been closed. \"\n          + \"Continue processing the perfect overwrite.\");\n    } catch (IOException e) {\n      LOG.info(\"hsync failed when processing possible perfect overwrite, \" +\n              \"path\u003d{} error: {}\", path, e.toString());\n      return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n          Nfs3Constant.WRITE_COMMIT_VERF);\n    }\n    \n    try {\n      fis \u003d dfsClient.createWrappedInputStream(dfsClient.open(path));\n      readCount \u003d fis.read(offset, readbuffer, 0, count);\n      if (readCount \u003c count) {\n        LOG.error(\"Can\u0027t read back {} bytes, partial read size: {}\",\n            count, readCount);\n        return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n            Nfs3Constant.WRITE_COMMIT_VERF);\n      }\n    } catch (IOException e) {\n      LOG.info(\"Read failed when processing possible perfect overwrite, \" +\n              \"path\u003d{}\", path, e);\n      return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n          Nfs3Constant.WRITE_COMMIT_VERF);\n    } finally {\n      IOUtils.cleanupWithLogger(LOG, fis);\n    }\n\n    // Compare with the request\n    Comparator comparator \u003d new Comparator();\n    if (comparator.compare(readbuffer, 0, readCount, data, 0, count) !\u003d 0) {\n      LOG.info(\"Perfect overwrite has different content\");\n      response \u003d new WRITE3Response(Nfs3Status.NFS3ERR_INVAL, wccData, 0,\n          stableHow, Nfs3Constant.WRITE_COMMIT_VERF);\n    } else {\n      LOG.info(\"Perfect overwrite has same content,\"\n          + \" updating the mtime, then return success\");\n      Nfs3FileAttributes postOpAttr \u003d null;\n      try {\n        dfsClient.setTimes(path, Time.monotonicNow(), -1);\n        postOpAttr \u003d Nfs3Utils.getFileAttr(dfsClient, path, iug);\n      } catch (IOException e) {\n        LOG.info(\"Got error when processing perfect overwrite, path\u003d{} \" +\n            \"error: {}\", path, e.toString());\n        return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n            Nfs3Constant.WRITE_COMMIT_VERF);\n      }\n\n      wccData.setPostOpAttr(postOpAttr);\n      response \u003d new WRITE3Response(Nfs3Status.NFS3_OK, wccData, count,\n          stableHow, Nfs3Constant.WRITE_COMMIT_VERF);\n    }\n    return response;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java",
      "extendedDetails": {}
    },
    "f37849188b05a6251584de1aed5e66d5dfa7da4f": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7423. various typos and message formatting fixes in nfs daemon and doc. (Charles Lamb via yliu)\n",
      "commitDate": "28/01/15 12:56 PM",
      "commitName": "f37849188b05a6251584de1aed5e66d5dfa7da4f",
      "commitAuthor": "yliu",
      "commitDateOld": "11/12/14 3:40 PM",
      "commitNameOld": "f6f2a3f1c73266bfedd802eacde60d8b19b81015",
      "commitAuthorOld": "Brandon Li",
      "daysBetweenCommits": 47.89,
      "commitsBetweenForRepo": 279,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,67 +1,67 @@\n   private WRITE3Response processPerfectOverWrite(DFSClient dfsClient,\n       long offset, int count, WriteStableHow stableHow, byte[] data,\n       String path, WccData wccData, IdMappingServiceProvider iug) {\n     WRITE3Response response;\n \n     // Read the content back\n     byte[] readbuffer \u003d new byte[count];\n \n     int readCount \u003d 0;\n     FSDataInputStream fis \u003d null;\n     try {\n       // Sync file data and length to avoid partial read failure\n       fos.hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));\n     } catch (ClosedChannelException closedException) {\n       LOG.info(\"The FSDataOutputStream has been closed. \"\n           + \"Continue processing the perfect overwrite.\");\n     } catch (IOException e) {\n       LOG.info(\"hsync failed when processing possible perfect overwrite, path\u003d\"\n-          + path + \" error:\" + e);\n+          + path + \" error: \" + e);\n       return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n           Nfs3Constant.WRITE_COMMIT_VERF);\n     }\n     \n     try {\n       fis \u003d dfsClient.createWrappedInputStream(dfsClient.open(path));\n       readCount \u003d fis.read(offset, readbuffer, 0, count);\n       if (readCount \u003c count) {\n-        LOG.error(\"Can\u0027t read back \" + count + \" bytes, partial read size:\"\n+        LOG.error(\"Can\u0027t read back \" + count + \" bytes, partial read size: \"\n             + readCount);\n         return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n             Nfs3Constant.WRITE_COMMIT_VERF);\n       }\n     } catch (IOException e) {\n       LOG.info(\"Read failed when processing possible perfect overwrite, path\u003d\"\n           + path, e);\n       return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n           Nfs3Constant.WRITE_COMMIT_VERF);\n     } finally {\n       IOUtils.cleanup(LOG, fis);\n     }\n \n     // Compare with the request\n     Comparator comparator \u003d new Comparator();\n     if (comparator.compare(readbuffer, 0, readCount, data, 0, count) !\u003d 0) {\n       LOG.info(\"Perfect overwrite has different content\");\n       response \u003d new WRITE3Response(Nfs3Status.NFS3ERR_INVAL, wccData, 0,\n           stableHow, Nfs3Constant.WRITE_COMMIT_VERF);\n     } else {\n       LOG.info(\"Perfect overwrite has same content,\"\n           + \" updating the mtime, then return success\");\n       Nfs3FileAttributes postOpAttr \u003d null;\n       try {\n         dfsClient.setTimes(path, Time.monotonicNow(), -1);\n         postOpAttr \u003d Nfs3Utils.getFileAttr(dfsClient, path, iug);\n       } catch (IOException e) {\n         LOG.info(\"Got error when processing perfect overwrite, path\u003d\" + path\n-            + \" error:\" + e);\n+            + \" error: \" + e);\n         return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n             Nfs3Constant.WRITE_COMMIT_VERF);\n       }\n \n       wccData.setPostOpAttr(postOpAttr);\n       response \u003d new WRITE3Response(Nfs3Status.NFS3_OK, wccData, count,\n           stableHow, Nfs3Constant.WRITE_COMMIT_VERF);\n     }\n     return response;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private WRITE3Response processPerfectOverWrite(DFSClient dfsClient,\n      long offset, int count, WriteStableHow stableHow, byte[] data,\n      String path, WccData wccData, IdMappingServiceProvider iug) {\n    WRITE3Response response;\n\n    // Read the content back\n    byte[] readbuffer \u003d new byte[count];\n\n    int readCount \u003d 0;\n    FSDataInputStream fis \u003d null;\n    try {\n      // Sync file data and length to avoid partial read failure\n      fos.hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));\n    } catch (ClosedChannelException closedException) {\n      LOG.info(\"The FSDataOutputStream has been closed. \"\n          + \"Continue processing the perfect overwrite.\");\n    } catch (IOException e) {\n      LOG.info(\"hsync failed when processing possible perfect overwrite, path\u003d\"\n          + path + \" error: \" + e);\n      return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n          Nfs3Constant.WRITE_COMMIT_VERF);\n    }\n    \n    try {\n      fis \u003d dfsClient.createWrappedInputStream(dfsClient.open(path));\n      readCount \u003d fis.read(offset, readbuffer, 0, count);\n      if (readCount \u003c count) {\n        LOG.error(\"Can\u0027t read back \" + count + \" bytes, partial read size: \"\n            + readCount);\n        return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n            Nfs3Constant.WRITE_COMMIT_VERF);\n      }\n    } catch (IOException e) {\n      LOG.info(\"Read failed when processing possible perfect overwrite, path\u003d\"\n          + path, e);\n      return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n          Nfs3Constant.WRITE_COMMIT_VERF);\n    } finally {\n      IOUtils.cleanup(LOG, fis);\n    }\n\n    // Compare with the request\n    Comparator comparator \u003d new Comparator();\n    if (comparator.compare(readbuffer, 0, readCount, data, 0, count) !\u003d 0) {\n      LOG.info(\"Perfect overwrite has different content\");\n      response \u003d new WRITE3Response(Nfs3Status.NFS3ERR_INVAL, wccData, 0,\n          stableHow, Nfs3Constant.WRITE_COMMIT_VERF);\n    } else {\n      LOG.info(\"Perfect overwrite has same content,\"\n          + \" updating the mtime, then return success\");\n      Nfs3FileAttributes postOpAttr \u003d null;\n      try {\n        dfsClient.setTimes(path, Time.monotonicNow(), -1);\n        postOpAttr \u003d Nfs3Utils.getFileAttr(dfsClient, path, iug);\n      } catch (IOException e) {\n        LOG.info(\"Got error when processing perfect overwrite, path\u003d\" + path\n            + \" error: \" + e);\n        return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n            Nfs3Constant.WRITE_COMMIT_VERF);\n      }\n\n      wccData.setPostOpAttr(postOpAttr);\n      response \u003d new WRITE3Response(Nfs3Status.NFS3_OK, wccData, count,\n          stableHow, Nfs3Constant.WRITE_COMMIT_VERF);\n    }\n    return response;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java",
      "extendedDetails": {}
    },
    "72a556d3b0def0ab4e4509528cc513f6df06b084": {
      "type": "Yparameterchange",
      "commitMessage": "HADOOP-11195. Move Id-Name mapping in NFS to the hadoop-common area for better maintenance. Contributed by Yongjun Zhang\n",
      "commitDate": "29/10/14 11:05 AM",
      "commitName": "72a556d3b0def0ab4e4509528cc513f6df06b084",
      "commitAuthor": "Brandon Li",
      "commitDateOld": "22/10/14 9:27 PM",
      "commitNameOld": "d71d40a63d198991077d5babd70be5e9787a53f1",
      "commitAuthorOld": "Brandon Li",
      "daysBetweenCommits": 6.57,
      "commitsBetweenForRepo": 67,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,67 +1,67 @@\n   private WRITE3Response processPerfectOverWrite(DFSClient dfsClient,\n       long offset, int count, WriteStableHow stableHow, byte[] data,\n-      String path, WccData wccData, IdUserGroup iug) {\n+      String path, WccData wccData, IdMappingServiceProvider iug) {\n     WRITE3Response response;\n \n     // Read the content back\n     byte[] readbuffer \u003d new byte[count];\n \n     int readCount \u003d 0;\n     FSDataInputStream fis \u003d null;\n     try {\n       // Sync file data and length to avoid partial read failure\n       fos.hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));\n     } catch (ClosedChannelException closedException) {\n       LOG.info(\"The FSDataOutputStream has been closed. \"\n           + \"Continue processing the perfect overwrite.\");\n     } catch (IOException e) {\n       LOG.info(\"hsync failed when processing possible perfect overwrite, path\u003d\"\n           + path + \" error:\" + e);\n       return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n           Nfs3Constant.WRITE_COMMIT_VERF);\n     }\n     \n     try {\n       fis \u003d dfsClient.createWrappedInputStream(dfsClient.open(path));\n       readCount \u003d fis.read(offset, readbuffer, 0, count);\n       if (readCount \u003c count) {\n         LOG.error(\"Can\u0027t read back \" + count + \" bytes, partial read size:\"\n             + readCount);\n         return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n             Nfs3Constant.WRITE_COMMIT_VERF);\n       }\n     } catch (IOException e) {\n       LOG.info(\"Read failed when processing possible perfect overwrite, path\u003d\"\n           + path, e);\n       return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n           Nfs3Constant.WRITE_COMMIT_VERF);\n     } finally {\n       IOUtils.cleanup(LOG, fis);\n     }\n \n     // Compare with the request\n     Comparator comparator \u003d new Comparator();\n     if (comparator.compare(readbuffer, 0, readCount, data, 0, count) !\u003d 0) {\n       LOG.info(\"Perfect overwrite has different content\");\n       response \u003d new WRITE3Response(Nfs3Status.NFS3ERR_INVAL, wccData, 0,\n           stableHow, Nfs3Constant.WRITE_COMMIT_VERF);\n     } else {\n       LOG.info(\"Perfect overwrite has same content,\"\n           + \" updating the mtime, then return success\");\n       Nfs3FileAttributes postOpAttr \u003d null;\n       try {\n         dfsClient.setTimes(path, Time.monotonicNow(), -1);\n         postOpAttr \u003d Nfs3Utils.getFileAttr(dfsClient, path, iug);\n       } catch (IOException e) {\n         LOG.info(\"Got error when processing perfect overwrite, path\u003d\" + path\n             + \" error:\" + e);\n         return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n             Nfs3Constant.WRITE_COMMIT_VERF);\n       }\n \n       wccData.setPostOpAttr(postOpAttr);\n       response \u003d new WRITE3Response(Nfs3Status.NFS3_OK, wccData, count,\n           stableHow, Nfs3Constant.WRITE_COMMIT_VERF);\n     }\n     return response;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private WRITE3Response processPerfectOverWrite(DFSClient dfsClient,\n      long offset, int count, WriteStableHow stableHow, byte[] data,\n      String path, WccData wccData, IdMappingServiceProvider iug) {\n    WRITE3Response response;\n\n    // Read the content back\n    byte[] readbuffer \u003d new byte[count];\n\n    int readCount \u003d 0;\n    FSDataInputStream fis \u003d null;\n    try {\n      // Sync file data and length to avoid partial read failure\n      fos.hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));\n    } catch (ClosedChannelException closedException) {\n      LOG.info(\"The FSDataOutputStream has been closed. \"\n          + \"Continue processing the perfect overwrite.\");\n    } catch (IOException e) {\n      LOG.info(\"hsync failed when processing possible perfect overwrite, path\u003d\"\n          + path + \" error:\" + e);\n      return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n          Nfs3Constant.WRITE_COMMIT_VERF);\n    }\n    \n    try {\n      fis \u003d dfsClient.createWrappedInputStream(dfsClient.open(path));\n      readCount \u003d fis.read(offset, readbuffer, 0, count);\n      if (readCount \u003c count) {\n        LOG.error(\"Can\u0027t read back \" + count + \" bytes, partial read size:\"\n            + readCount);\n        return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n            Nfs3Constant.WRITE_COMMIT_VERF);\n      }\n    } catch (IOException e) {\n      LOG.info(\"Read failed when processing possible perfect overwrite, path\u003d\"\n          + path, e);\n      return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n          Nfs3Constant.WRITE_COMMIT_VERF);\n    } finally {\n      IOUtils.cleanup(LOG, fis);\n    }\n\n    // Compare with the request\n    Comparator comparator \u003d new Comparator();\n    if (comparator.compare(readbuffer, 0, readCount, data, 0, count) !\u003d 0) {\n      LOG.info(\"Perfect overwrite has different content\");\n      response \u003d new WRITE3Response(Nfs3Status.NFS3ERR_INVAL, wccData, 0,\n          stableHow, Nfs3Constant.WRITE_COMMIT_VERF);\n    } else {\n      LOG.info(\"Perfect overwrite has same content,\"\n          + \" updating the mtime, then return success\");\n      Nfs3FileAttributes postOpAttr \u003d null;\n      try {\n        dfsClient.setTimes(path, Time.monotonicNow(), -1);\n        postOpAttr \u003d Nfs3Utils.getFileAttr(dfsClient, path, iug);\n      } catch (IOException e) {\n        LOG.info(\"Got error when processing perfect overwrite, path\u003d\" + path\n            + \" error:\" + e);\n        return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n            Nfs3Constant.WRITE_COMMIT_VERF);\n      }\n\n      wccData.setPostOpAttr(postOpAttr);\n      response \u003d new WRITE3Response(Nfs3Status.NFS3_OK, wccData, count,\n          stableHow, Nfs3Constant.WRITE_COMMIT_VERF);\n    }\n    return response;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java",
      "extendedDetails": {
        "oldValue": "[dfsClient-DFSClient, offset-long, count-int, stableHow-WriteStableHow, data-byte[], path-String, wccData-WccData, iug-IdUserGroup]",
        "newValue": "[dfsClient-DFSClient, offset-long, count-int, stableHow-WriteStableHow, data-byte[], path-String, wccData-WccData, iug-IdMappingServiceProvider]"
      }
    },
    "d71d40a63d198991077d5babd70be5e9787a53f1": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7180. NFSv3 gateway frequently gets stuck due to GC. Contributed by Brandon Li\n",
      "commitDate": "22/10/14 9:27 PM",
      "commitName": "d71d40a63d198991077d5babd70be5e9787a53f1",
      "commitAuthor": "Brandon Li",
      "commitDateOld": "21/10/14 10:20 AM",
      "commitNameOld": "b6f9d5538cf2b425652687e99503f3d566b2056a",
      "commitAuthorOld": "Brandon Li",
      "daysBetweenCommits": 1.46,
      "commitsBetweenForRepo": 23,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,67 +1,67 @@\n   private WRITE3Response processPerfectOverWrite(DFSClient dfsClient,\n       long offset, int count, WriteStableHow stableHow, byte[] data,\n       String path, WccData wccData, IdUserGroup iug) {\n-    WRITE3Response response \u003d null;\n+    WRITE3Response response;\n \n     // Read the content back\n     byte[] readbuffer \u003d new byte[count];\n \n     int readCount \u003d 0;\n     FSDataInputStream fis \u003d null;\n     try {\n       // Sync file data and length to avoid partial read failure\n       fos.hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));\n     } catch (ClosedChannelException closedException) {\n       LOG.info(\"The FSDataOutputStream has been closed. \"\n           + \"Continue processing the perfect overwrite.\");\n     } catch (IOException e) {\n       LOG.info(\"hsync failed when processing possible perfect overwrite, path\u003d\"\n           + path + \" error:\" + e);\n       return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n           Nfs3Constant.WRITE_COMMIT_VERF);\n     }\n     \n     try {\n       fis \u003d dfsClient.createWrappedInputStream(dfsClient.open(path));\n       readCount \u003d fis.read(offset, readbuffer, 0, count);\n       if (readCount \u003c count) {\n         LOG.error(\"Can\u0027t read back \" + count + \" bytes, partial read size:\"\n             + readCount);\n         return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n             Nfs3Constant.WRITE_COMMIT_VERF);\n       }\n     } catch (IOException e) {\n       LOG.info(\"Read failed when processing possible perfect overwrite, path\u003d\"\n           + path, e);\n       return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n           Nfs3Constant.WRITE_COMMIT_VERF);\n     } finally {\n       IOUtils.cleanup(LOG, fis);\n     }\n \n     // Compare with the request\n     Comparator comparator \u003d new Comparator();\n     if (comparator.compare(readbuffer, 0, readCount, data, 0, count) !\u003d 0) {\n       LOG.info(\"Perfect overwrite has different content\");\n       response \u003d new WRITE3Response(Nfs3Status.NFS3ERR_INVAL, wccData, 0,\n           stableHow, Nfs3Constant.WRITE_COMMIT_VERF);\n     } else {\n       LOG.info(\"Perfect overwrite has same content,\"\n           + \" updating the mtime, then return success\");\n       Nfs3FileAttributes postOpAttr \u003d null;\n       try {\n         dfsClient.setTimes(path, Time.monotonicNow(), -1);\n         postOpAttr \u003d Nfs3Utils.getFileAttr(dfsClient, path, iug);\n       } catch (IOException e) {\n         LOG.info(\"Got error when processing perfect overwrite, path\u003d\" + path\n             + \" error:\" + e);\n         return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n             Nfs3Constant.WRITE_COMMIT_VERF);\n       }\n \n       wccData.setPostOpAttr(postOpAttr);\n       response \u003d new WRITE3Response(Nfs3Status.NFS3_OK, wccData, count,\n           stableHow, Nfs3Constant.WRITE_COMMIT_VERF);\n     }\n     return response;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private WRITE3Response processPerfectOverWrite(DFSClient dfsClient,\n      long offset, int count, WriteStableHow stableHow, byte[] data,\n      String path, WccData wccData, IdUserGroup iug) {\n    WRITE3Response response;\n\n    // Read the content back\n    byte[] readbuffer \u003d new byte[count];\n\n    int readCount \u003d 0;\n    FSDataInputStream fis \u003d null;\n    try {\n      // Sync file data and length to avoid partial read failure\n      fos.hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));\n    } catch (ClosedChannelException closedException) {\n      LOG.info(\"The FSDataOutputStream has been closed. \"\n          + \"Continue processing the perfect overwrite.\");\n    } catch (IOException e) {\n      LOG.info(\"hsync failed when processing possible perfect overwrite, path\u003d\"\n          + path + \" error:\" + e);\n      return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n          Nfs3Constant.WRITE_COMMIT_VERF);\n    }\n    \n    try {\n      fis \u003d dfsClient.createWrappedInputStream(dfsClient.open(path));\n      readCount \u003d fis.read(offset, readbuffer, 0, count);\n      if (readCount \u003c count) {\n        LOG.error(\"Can\u0027t read back \" + count + \" bytes, partial read size:\"\n            + readCount);\n        return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n            Nfs3Constant.WRITE_COMMIT_VERF);\n      }\n    } catch (IOException e) {\n      LOG.info(\"Read failed when processing possible perfect overwrite, path\u003d\"\n          + path, e);\n      return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n          Nfs3Constant.WRITE_COMMIT_VERF);\n    } finally {\n      IOUtils.cleanup(LOG, fis);\n    }\n\n    // Compare with the request\n    Comparator comparator \u003d new Comparator();\n    if (comparator.compare(readbuffer, 0, readCount, data, 0, count) !\u003d 0) {\n      LOG.info(\"Perfect overwrite has different content\");\n      response \u003d new WRITE3Response(Nfs3Status.NFS3ERR_INVAL, wccData, 0,\n          stableHow, Nfs3Constant.WRITE_COMMIT_VERF);\n    } else {\n      LOG.info(\"Perfect overwrite has same content,\"\n          + \" updating the mtime, then return success\");\n      Nfs3FileAttributes postOpAttr \u003d null;\n      try {\n        dfsClient.setTimes(path, Time.monotonicNow(), -1);\n        postOpAttr \u003d Nfs3Utils.getFileAttr(dfsClient, path, iug);\n      } catch (IOException e) {\n        LOG.info(\"Got error when processing perfect overwrite, path\u003d\" + path\n            + \" error:\" + e);\n        return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n            Nfs3Constant.WRITE_COMMIT_VERF);\n      }\n\n      wccData.setPostOpAttr(postOpAttr);\n      response \u003d new WRITE3Response(Nfs3Status.NFS3_OK, wccData, count,\n          stableHow, Nfs3Constant.WRITE_COMMIT_VERF);\n    }\n    return response;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java",
      "extendedDetails": {}
    },
    "70be56d093022de9953e14a92dfa1a146bd9a290": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7003. Add NFS Gateway support for reading and writing to encryption zones. (clamb via wang)\n",
      "commitDate": "18/09/14 2:57 PM",
      "commitName": "70be56d093022de9953e14a92dfa1a146bd9a290",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "02/09/14 4:22 PM",
      "commitNameOld": "08a9ac7098cb4ae684f40cf2513e3137110cc7e4",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 15.94,
      "commitsBetweenForRepo": 155,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,67 +1,67 @@\n   private WRITE3Response processPerfectOverWrite(DFSClient dfsClient,\n       long offset, int count, WriteStableHow stableHow, byte[] data,\n       String path, WccData wccData, IdUserGroup iug) {\n     WRITE3Response response \u003d null;\n \n     // Read the content back\n     byte[] readbuffer \u003d new byte[count];\n \n     int readCount \u003d 0;\n     FSDataInputStream fis \u003d null;\n     try {\n       // Sync file data and length to avoid partial read failure\n       fos.hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));\n     } catch (ClosedChannelException closedException) {\n       LOG.info(\"The FSDataOutputStream has been closed. \"\n           + \"Continue processing the perfect overwrite.\");\n     } catch (IOException e) {\n       LOG.info(\"hsync failed when processing possible perfect overwrite, path\u003d\"\n           + path + \" error:\" + e);\n       return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n           Nfs3Constant.WRITE_COMMIT_VERF);\n     }\n     \n     try {\n-      fis \u003d new FSDataInputStream(dfsClient.open(path));\n+      fis \u003d dfsClient.createWrappedInputStream(dfsClient.open(path));\n       readCount \u003d fis.read(offset, readbuffer, 0, count);\n       if (readCount \u003c count) {\n         LOG.error(\"Can\u0027t read back \" + count + \" bytes, partial read size:\"\n             + readCount);\n         return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n             Nfs3Constant.WRITE_COMMIT_VERF);\n       }\n     } catch (IOException e) {\n       LOG.info(\"Read failed when processing possible perfect overwrite, path\u003d\"\n           + path, e);\n       return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n           Nfs3Constant.WRITE_COMMIT_VERF);\n     } finally {\n       IOUtils.cleanup(LOG, fis);\n     }\n \n     // Compare with the request\n     Comparator comparator \u003d new Comparator();\n     if (comparator.compare(readbuffer, 0, readCount, data, 0, count) !\u003d 0) {\n       LOG.info(\"Perfect overwrite has different content\");\n       response \u003d new WRITE3Response(Nfs3Status.NFS3ERR_INVAL, wccData, 0,\n           stableHow, Nfs3Constant.WRITE_COMMIT_VERF);\n     } else {\n       LOG.info(\"Perfect overwrite has same content,\"\n           + \" updating the mtime, then return success\");\n       Nfs3FileAttributes postOpAttr \u003d null;\n       try {\n         dfsClient.setTimes(path, Time.monotonicNow(), -1);\n         postOpAttr \u003d Nfs3Utils.getFileAttr(dfsClient, path, iug);\n       } catch (IOException e) {\n         LOG.info(\"Got error when processing perfect overwrite, path\u003d\" + path\n             + \" error:\" + e);\n         return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n             Nfs3Constant.WRITE_COMMIT_VERF);\n       }\n \n       wccData.setPostOpAttr(postOpAttr);\n       response \u003d new WRITE3Response(Nfs3Status.NFS3_OK, wccData, count,\n           stableHow, Nfs3Constant.WRITE_COMMIT_VERF);\n     }\n     return response;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private WRITE3Response processPerfectOverWrite(DFSClient dfsClient,\n      long offset, int count, WriteStableHow stableHow, byte[] data,\n      String path, WccData wccData, IdUserGroup iug) {\n    WRITE3Response response \u003d null;\n\n    // Read the content back\n    byte[] readbuffer \u003d new byte[count];\n\n    int readCount \u003d 0;\n    FSDataInputStream fis \u003d null;\n    try {\n      // Sync file data and length to avoid partial read failure\n      fos.hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));\n    } catch (ClosedChannelException closedException) {\n      LOG.info(\"The FSDataOutputStream has been closed. \"\n          + \"Continue processing the perfect overwrite.\");\n    } catch (IOException e) {\n      LOG.info(\"hsync failed when processing possible perfect overwrite, path\u003d\"\n          + path + \" error:\" + e);\n      return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n          Nfs3Constant.WRITE_COMMIT_VERF);\n    }\n    \n    try {\n      fis \u003d dfsClient.createWrappedInputStream(dfsClient.open(path));\n      readCount \u003d fis.read(offset, readbuffer, 0, count);\n      if (readCount \u003c count) {\n        LOG.error(\"Can\u0027t read back \" + count + \" bytes, partial read size:\"\n            + readCount);\n        return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n            Nfs3Constant.WRITE_COMMIT_VERF);\n      }\n    } catch (IOException e) {\n      LOG.info(\"Read failed when processing possible perfect overwrite, path\u003d\"\n          + path, e);\n      return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n          Nfs3Constant.WRITE_COMMIT_VERF);\n    } finally {\n      IOUtils.cleanup(LOG, fis);\n    }\n\n    // Compare with the request\n    Comparator comparator \u003d new Comparator();\n    if (comparator.compare(readbuffer, 0, readCount, data, 0, count) !\u003d 0) {\n      LOG.info(\"Perfect overwrite has different content\");\n      response \u003d new WRITE3Response(Nfs3Status.NFS3ERR_INVAL, wccData, 0,\n          stableHow, Nfs3Constant.WRITE_COMMIT_VERF);\n    } else {\n      LOG.info(\"Perfect overwrite has same content,\"\n          + \" updating the mtime, then return success\");\n      Nfs3FileAttributes postOpAttr \u003d null;\n      try {\n        dfsClient.setTimes(path, Time.monotonicNow(), -1);\n        postOpAttr \u003d Nfs3Utils.getFileAttr(dfsClient, path, iug);\n      } catch (IOException e) {\n        LOG.info(\"Got error when processing perfect overwrite, path\u003d\" + path\n            + \" error:\" + e);\n        return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n            Nfs3Constant.WRITE_COMMIT_VERF);\n      }\n\n      wccData.setPostOpAttr(postOpAttr);\n      response \u003d new WRITE3Response(Nfs3Status.NFS3_OK, wccData, count,\n          stableHow, Nfs3Constant.WRITE_COMMIT_VERF);\n    }\n    return response;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java",
      "extendedDetails": {}
    },
    "1c867b1de8a9f4c6bb118c08c7b714bd2b8356cd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6416. Use Time#monotonicNow in OpenFileCtx and OpenFileCtxCatch to avoid system clock bugs. Contributed by Abhiraj Butala\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1597868 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/05/14 1:21 PM",
      "commitName": "1c867b1de8a9f4c6bb118c08c7b714bd2b8356cd",
      "commitAuthor": "Brandon Li",
      "commitDateOld": "24/03/14 1:49 PM",
      "commitNameOld": "3bfd18c6b0483ae27eff6d53bda934e67dda5464",
      "commitAuthorOld": "Brandon Li",
      "daysBetweenCommits": 63.98,
      "commitsBetweenForRepo": 376,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,67 +1,67 @@\n   private WRITE3Response processPerfectOverWrite(DFSClient dfsClient,\n       long offset, int count, WriteStableHow stableHow, byte[] data,\n       String path, WccData wccData, IdUserGroup iug) {\n     WRITE3Response response \u003d null;\n \n     // Read the content back\n     byte[] readbuffer \u003d new byte[count];\n \n     int readCount \u003d 0;\n     FSDataInputStream fis \u003d null;\n     try {\n       // Sync file data and length to avoid partial read failure\n       fos.hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));\n     } catch (ClosedChannelException closedException) {\n       LOG.info(\"The FSDataOutputStream has been closed. \"\n           + \"Continue processing the perfect overwrite.\");\n     } catch (IOException e) {\n       LOG.info(\"hsync failed when processing possible perfect overwrite, path\u003d\"\n           + path + \" error:\" + e);\n       return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n           Nfs3Constant.WRITE_COMMIT_VERF);\n     }\n     \n     try {\n       fis \u003d new FSDataInputStream(dfsClient.open(path));\n       readCount \u003d fis.read(offset, readbuffer, 0, count);\n       if (readCount \u003c count) {\n         LOG.error(\"Can\u0027t read back \" + count + \" bytes, partial read size:\"\n             + readCount);\n         return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n             Nfs3Constant.WRITE_COMMIT_VERF);\n       }\n     } catch (IOException e) {\n       LOG.info(\"Read failed when processing possible perfect overwrite, path\u003d\"\n           + path, e);\n       return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n           Nfs3Constant.WRITE_COMMIT_VERF);\n     } finally {\n       IOUtils.cleanup(LOG, fis);\n     }\n \n     // Compare with the request\n     Comparator comparator \u003d new Comparator();\n     if (comparator.compare(readbuffer, 0, readCount, data, 0, count) !\u003d 0) {\n       LOG.info(\"Perfect overwrite has different content\");\n       response \u003d new WRITE3Response(Nfs3Status.NFS3ERR_INVAL, wccData, 0,\n           stableHow, Nfs3Constant.WRITE_COMMIT_VERF);\n     } else {\n       LOG.info(\"Perfect overwrite has same content,\"\n           + \" updating the mtime, then return success\");\n       Nfs3FileAttributes postOpAttr \u003d null;\n       try {\n-        dfsClient.setTimes(path, System.currentTimeMillis(), -1);\n+        dfsClient.setTimes(path, Time.monotonicNow(), -1);\n         postOpAttr \u003d Nfs3Utils.getFileAttr(dfsClient, path, iug);\n       } catch (IOException e) {\n         LOG.info(\"Got error when processing perfect overwrite, path\u003d\" + path\n             + \" error:\" + e);\n         return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n             Nfs3Constant.WRITE_COMMIT_VERF);\n       }\n \n       wccData.setPostOpAttr(postOpAttr);\n       response \u003d new WRITE3Response(Nfs3Status.NFS3_OK, wccData, count,\n           stableHow, Nfs3Constant.WRITE_COMMIT_VERF);\n     }\n     return response;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private WRITE3Response processPerfectOverWrite(DFSClient dfsClient,\n      long offset, int count, WriteStableHow stableHow, byte[] data,\n      String path, WccData wccData, IdUserGroup iug) {\n    WRITE3Response response \u003d null;\n\n    // Read the content back\n    byte[] readbuffer \u003d new byte[count];\n\n    int readCount \u003d 0;\n    FSDataInputStream fis \u003d null;\n    try {\n      // Sync file data and length to avoid partial read failure\n      fos.hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));\n    } catch (ClosedChannelException closedException) {\n      LOG.info(\"The FSDataOutputStream has been closed. \"\n          + \"Continue processing the perfect overwrite.\");\n    } catch (IOException e) {\n      LOG.info(\"hsync failed when processing possible perfect overwrite, path\u003d\"\n          + path + \" error:\" + e);\n      return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n          Nfs3Constant.WRITE_COMMIT_VERF);\n    }\n    \n    try {\n      fis \u003d new FSDataInputStream(dfsClient.open(path));\n      readCount \u003d fis.read(offset, readbuffer, 0, count);\n      if (readCount \u003c count) {\n        LOG.error(\"Can\u0027t read back \" + count + \" bytes, partial read size:\"\n            + readCount);\n        return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n            Nfs3Constant.WRITE_COMMIT_VERF);\n      }\n    } catch (IOException e) {\n      LOG.info(\"Read failed when processing possible perfect overwrite, path\u003d\"\n          + path, e);\n      return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n          Nfs3Constant.WRITE_COMMIT_VERF);\n    } finally {\n      IOUtils.cleanup(LOG, fis);\n    }\n\n    // Compare with the request\n    Comparator comparator \u003d new Comparator();\n    if (comparator.compare(readbuffer, 0, readCount, data, 0, count) !\u003d 0) {\n      LOG.info(\"Perfect overwrite has different content\");\n      response \u003d new WRITE3Response(Nfs3Status.NFS3ERR_INVAL, wccData, 0,\n          stableHow, Nfs3Constant.WRITE_COMMIT_VERF);\n    } else {\n      LOG.info(\"Perfect overwrite has same content,\"\n          + \" updating the mtime, then return success\");\n      Nfs3FileAttributes postOpAttr \u003d null;\n      try {\n        dfsClient.setTimes(path, Time.monotonicNow(), -1);\n        postOpAttr \u003d Nfs3Utils.getFileAttr(dfsClient, path, iug);\n      } catch (IOException e) {\n        LOG.info(\"Got error when processing perfect overwrite, path\u003d\" + path\n            + \" error:\" + e);\n        return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n            Nfs3Constant.WRITE_COMMIT_VERF);\n      }\n\n      wccData.setPostOpAttr(postOpAttr);\n      response \u003d new WRITE3Response(Nfs3Status.NFS3_OK, wccData, count,\n          stableHow, Nfs3Constant.WRITE_COMMIT_VERF);\n    }\n    return response;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java",
      "extendedDetails": {}
    },
    "3bfd18c6b0483ae27eff6d53bda934e67dda5464": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6050. NFS does not handle exceptions correctly in a few places. Contributed by Brandon Li\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1581055 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/03/14 1:49 PM",
      "commitName": "3bfd18c6b0483ae27eff6d53bda934e67dda5464",
      "commitAuthor": "Brandon Li",
      "commitDateOld": "17/12/13 12:40 PM",
      "commitNameOld": "5792d59da390842caec86ccaa8472d5be7933837",
      "commitAuthorOld": "Brandon Li",
      "daysBetweenCommits": 97.01,
      "commitsBetweenForRepo": 719,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,67 +1,67 @@\n   private WRITE3Response processPerfectOverWrite(DFSClient dfsClient,\n       long offset, int count, WriteStableHow stableHow, byte[] data,\n       String path, WccData wccData, IdUserGroup iug) {\n     WRITE3Response response \u003d null;\n \n     // Read the content back\n     byte[] readbuffer \u003d new byte[count];\n \n     int readCount \u003d 0;\n     FSDataInputStream fis \u003d null;\n     try {\n       // Sync file data and length to avoid partial read failure\n       fos.hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));\n     } catch (ClosedChannelException closedException) {\n-      LOG.info(\"The FSDataOutputStream has been closed. \" +\n-      \t\t\"Continue processing the perfect overwrite.\");\n+      LOG.info(\"The FSDataOutputStream has been closed. \"\n+          + \"Continue processing the perfect overwrite.\");\n     } catch (IOException e) {\n       LOG.info(\"hsync failed when processing possible perfect overwrite, path\u003d\"\n           + path + \" error:\" + e);\n       return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n           Nfs3Constant.WRITE_COMMIT_VERF);\n     }\n     \n     try {\n       fis \u003d new FSDataInputStream(dfsClient.open(path));\n       readCount \u003d fis.read(offset, readbuffer, 0, count);\n       if (readCount \u003c count) {\n         LOG.error(\"Can\u0027t read back \" + count + \" bytes, partial read size:\"\n             + readCount);\n         return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n             Nfs3Constant.WRITE_COMMIT_VERF);\n       }\n     } catch (IOException e) {\n       LOG.info(\"Read failed when processing possible perfect overwrite, path\u003d\"\n-          + path + \" error:\" + e);\n+          + path, e);\n       return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n           Nfs3Constant.WRITE_COMMIT_VERF);\n     } finally {\n       IOUtils.cleanup(LOG, fis);\n     }\n \n     // Compare with the request\n     Comparator comparator \u003d new Comparator();\n     if (comparator.compare(readbuffer, 0, readCount, data, 0, count) !\u003d 0) {\n       LOG.info(\"Perfect overwrite has different content\");\n       response \u003d new WRITE3Response(Nfs3Status.NFS3ERR_INVAL, wccData, 0,\n           stableHow, Nfs3Constant.WRITE_COMMIT_VERF);\n     } else {\n       LOG.info(\"Perfect overwrite has same content,\"\n           + \" updating the mtime, then return success\");\n       Nfs3FileAttributes postOpAttr \u003d null;\n       try {\n         dfsClient.setTimes(path, System.currentTimeMillis(), -1);\n         postOpAttr \u003d Nfs3Utils.getFileAttr(dfsClient, path, iug);\n       } catch (IOException e) {\n         LOG.info(\"Got error when processing perfect overwrite, path\u003d\" + path\n             + \" error:\" + e);\n         return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n             Nfs3Constant.WRITE_COMMIT_VERF);\n       }\n \n       wccData.setPostOpAttr(postOpAttr);\n       response \u003d new WRITE3Response(Nfs3Status.NFS3_OK, wccData, count,\n           stableHow, Nfs3Constant.WRITE_COMMIT_VERF);\n     }\n     return response;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private WRITE3Response processPerfectOverWrite(DFSClient dfsClient,\n      long offset, int count, WriteStableHow stableHow, byte[] data,\n      String path, WccData wccData, IdUserGroup iug) {\n    WRITE3Response response \u003d null;\n\n    // Read the content back\n    byte[] readbuffer \u003d new byte[count];\n\n    int readCount \u003d 0;\n    FSDataInputStream fis \u003d null;\n    try {\n      // Sync file data and length to avoid partial read failure\n      fos.hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));\n    } catch (ClosedChannelException closedException) {\n      LOG.info(\"The FSDataOutputStream has been closed. \"\n          + \"Continue processing the perfect overwrite.\");\n    } catch (IOException e) {\n      LOG.info(\"hsync failed when processing possible perfect overwrite, path\u003d\"\n          + path + \" error:\" + e);\n      return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n          Nfs3Constant.WRITE_COMMIT_VERF);\n    }\n    \n    try {\n      fis \u003d new FSDataInputStream(dfsClient.open(path));\n      readCount \u003d fis.read(offset, readbuffer, 0, count);\n      if (readCount \u003c count) {\n        LOG.error(\"Can\u0027t read back \" + count + \" bytes, partial read size:\"\n            + readCount);\n        return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n            Nfs3Constant.WRITE_COMMIT_VERF);\n      }\n    } catch (IOException e) {\n      LOG.info(\"Read failed when processing possible perfect overwrite, path\u003d\"\n          + path, e);\n      return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n          Nfs3Constant.WRITE_COMMIT_VERF);\n    } finally {\n      IOUtils.cleanup(LOG, fis);\n    }\n\n    // Compare with the request\n    Comparator comparator \u003d new Comparator();\n    if (comparator.compare(readbuffer, 0, readCount, data, 0, count) !\u003d 0) {\n      LOG.info(\"Perfect overwrite has different content\");\n      response \u003d new WRITE3Response(Nfs3Status.NFS3ERR_INVAL, wccData, 0,\n          stableHow, Nfs3Constant.WRITE_COMMIT_VERF);\n    } else {\n      LOG.info(\"Perfect overwrite has same content,\"\n          + \" updating the mtime, then return success\");\n      Nfs3FileAttributes postOpAttr \u003d null;\n      try {\n        dfsClient.setTimes(path, System.currentTimeMillis(), -1);\n        postOpAttr \u003d Nfs3Utils.getFileAttr(dfsClient, path, iug);\n      } catch (IOException e) {\n        LOG.info(\"Got error when processing perfect overwrite, path\u003d\" + path\n            + \" error:\" + e);\n        return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n            Nfs3Constant.WRITE_COMMIT_VERF);\n      }\n\n      wccData.setPostOpAttr(postOpAttr);\n      response \u003d new WRITE3Response(Nfs3Status.NFS3_OK, wccData, count,\n          stableHow, Nfs3Constant.WRITE_COMMIT_VERF);\n    }\n    return response;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java",
      "extendedDetails": {}
    },
    "027419832c1125d707b45ce852032d704ab79d88": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5268. NFS write commit verifier is not set in a few places. Contributed by Brandon Li\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1527087 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/09/13 2:28 PM",
      "commitName": "027419832c1125d707b45ce852032d704ab79d88",
      "commitAuthor": "Brandon Li",
      "commitDateOld": "23/09/13 9:11 PM",
      "commitNameOld": "e3088e4aef8cdfc0841858e7851f7276fab9b24b",
      "commitAuthorOld": "Brandon Li",
      "daysBetweenCommits": 3.72,
      "commitsBetweenForRepo": 32,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,67 +1,67 @@\n   private WRITE3Response processPerfectOverWrite(DFSClient dfsClient,\n       long offset, int count, WriteStableHow stableHow, byte[] data,\n       String path, WccData wccData, IdUserGroup iug) {\n     WRITE3Response response \u003d null;\n \n     // Read the content back\n     byte[] readbuffer \u003d new byte[count];\n \n     int readCount \u003d 0;\n     FSDataInputStream fis \u003d null;\n     try {\n       // Sync file data and length to avoid partial read failure\n       fos.hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));\n     } catch (ClosedChannelException closedException) {\n       LOG.info(\"The FSDataOutputStream has been closed. \" +\n       \t\t\"Continue processing the perfect overwrite.\");\n     } catch (IOException e) {\n       LOG.info(\"hsync failed when processing possible perfect overwrite, path\u003d\"\n           + path + \" error:\" + e);\n       return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n           Nfs3Constant.WRITE_COMMIT_VERF);\n     }\n     \n     try {\n       fis \u003d new FSDataInputStream(dfsClient.open(path));\n       readCount \u003d fis.read(offset, readbuffer, 0, count);\n       if (readCount \u003c count) {\n         LOG.error(\"Can\u0027t read back \" + count + \" bytes, partial read size:\"\n             + readCount);\n         return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n             Nfs3Constant.WRITE_COMMIT_VERF);\n       }\n     } catch (IOException e) {\n       LOG.info(\"Read failed when processing possible perfect overwrite, path\u003d\"\n           + path + \" error:\" + e);\n       return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n           Nfs3Constant.WRITE_COMMIT_VERF);\n     } finally {\n       IOUtils.cleanup(LOG, fis);\n     }\n \n     // Compare with the request\n     Comparator comparator \u003d new Comparator();\n     if (comparator.compare(readbuffer, 0, readCount, data, 0, count) !\u003d 0) {\n       LOG.info(\"Perfect overwrite has different content\");\n       response \u003d new WRITE3Response(Nfs3Status.NFS3ERR_INVAL, wccData, 0,\n-          stableHow, 0);\n+          stableHow, Nfs3Constant.WRITE_COMMIT_VERF);\n     } else {\n       LOG.info(\"Perfect overwrite has same content,\"\n           + \" updating the mtime, then return success\");\n       Nfs3FileAttributes postOpAttr \u003d null;\n       try {\n         dfsClient.setTimes(path, System.currentTimeMillis(), -1);\n         postOpAttr \u003d Nfs3Utils.getFileAttr(dfsClient, path, iug);\n       } catch (IOException e) {\n         LOG.info(\"Got error when processing perfect overwrite, path\u003d\" + path\n             + \" error:\" + e);\n         return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n-            0);\n+            Nfs3Constant.WRITE_COMMIT_VERF);\n       }\n \n       wccData.setPostOpAttr(postOpAttr);\n       response \u003d new WRITE3Response(Nfs3Status.NFS3_OK, wccData, count,\n-          stableHow, 0);\n+          stableHow, Nfs3Constant.WRITE_COMMIT_VERF);\n     }\n     return response;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private WRITE3Response processPerfectOverWrite(DFSClient dfsClient,\n      long offset, int count, WriteStableHow stableHow, byte[] data,\n      String path, WccData wccData, IdUserGroup iug) {\n    WRITE3Response response \u003d null;\n\n    // Read the content back\n    byte[] readbuffer \u003d new byte[count];\n\n    int readCount \u003d 0;\n    FSDataInputStream fis \u003d null;\n    try {\n      // Sync file data and length to avoid partial read failure\n      fos.hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));\n    } catch (ClosedChannelException closedException) {\n      LOG.info(\"The FSDataOutputStream has been closed. \" +\n      \t\t\"Continue processing the perfect overwrite.\");\n    } catch (IOException e) {\n      LOG.info(\"hsync failed when processing possible perfect overwrite, path\u003d\"\n          + path + \" error:\" + e);\n      return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n          Nfs3Constant.WRITE_COMMIT_VERF);\n    }\n    \n    try {\n      fis \u003d new FSDataInputStream(dfsClient.open(path));\n      readCount \u003d fis.read(offset, readbuffer, 0, count);\n      if (readCount \u003c count) {\n        LOG.error(\"Can\u0027t read back \" + count + \" bytes, partial read size:\"\n            + readCount);\n        return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n            Nfs3Constant.WRITE_COMMIT_VERF);\n      }\n    } catch (IOException e) {\n      LOG.info(\"Read failed when processing possible perfect overwrite, path\u003d\"\n          + path + \" error:\" + e);\n      return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n          Nfs3Constant.WRITE_COMMIT_VERF);\n    } finally {\n      IOUtils.cleanup(LOG, fis);\n    }\n\n    // Compare with the request\n    Comparator comparator \u003d new Comparator();\n    if (comparator.compare(readbuffer, 0, readCount, data, 0, count) !\u003d 0) {\n      LOG.info(\"Perfect overwrite has different content\");\n      response \u003d new WRITE3Response(Nfs3Status.NFS3ERR_INVAL, wccData, 0,\n          stableHow, Nfs3Constant.WRITE_COMMIT_VERF);\n    } else {\n      LOG.info(\"Perfect overwrite has same content,\"\n          + \" updating the mtime, then return success\");\n      Nfs3FileAttributes postOpAttr \u003d null;\n      try {\n        dfsClient.setTimes(path, System.currentTimeMillis(), -1);\n        postOpAttr \u003d Nfs3Utils.getFileAttr(dfsClient, path, iug);\n      } catch (IOException e) {\n        LOG.info(\"Got error when processing perfect overwrite, path\u003d\" + path\n            + \" error:\" + e);\n        return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n            Nfs3Constant.WRITE_COMMIT_VERF);\n      }\n\n      wccData.setPostOpAttr(postOpAttr);\n      response \u003d new WRITE3Response(Nfs3Status.NFS3_OK, wccData, count,\n          stableHow, Nfs3Constant.WRITE_COMMIT_VERF);\n    }\n    return response;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java",
      "extendedDetails": {}
    },
    "28e3d09230971b32f74284311931525cb7ad1b7c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4971. Move IO operations out of locking in OpenFileCtx. Contributed by Jing Zhao and Brandon Li.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1525681 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "23/09/13 1:02 PM",
      "commitName": "28e3d09230971b32f74284311931525cb7ad1b7c",
      "commitAuthor": "Jing Zhao",
      "commitDateOld": "17/09/13 11:08 PM",
      "commitNameOld": "5e18410e06dd63113c49029894007e0878312903",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 5.58,
      "commitsBetweenForRepo": 25,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,59 +1,67 @@\n   private WRITE3Response processPerfectOverWrite(DFSClient dfsClient,\n       long offset, int count, WriteStableHow stableHow, byte[] data,\n       String path, WccData wccData, IdUserGroup iug) {\n-    assert (ctxLock.isLocked());\n     WRITE3Response response \u003d null;\n \n     // Read the content back\n     byte[] readbuffer \u003d new byte[count];\n \n     int readCount \u003d 0;\n     FSDataInputStream fis \u003d null;\n     try {\n       // Sync file data and length to avoid partial read failure\n       fos.hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));\n-      \n+    } catch (ClosedChannelException closedException) {\n+      LOG.info(\"The FSDataOutputStream has been closed. \" +\n+      \t\t\"Continue processing the perfect overwrite.\");\n+    } catch (IOException e) {\n+      LOG.info(\"hsync failed when processing possible perfect overwrite, path\u003d\"\n+          + path + \" error:\" + e);\n+      return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n+          Nfs3Constant.WRITE_COMMIT_VERF);\n+    }\n+    \n+    try {\n       fis \u003d new FSDataInputStream(dfsClient.open(path));\n       readCount \u003d fis.read(offset, readbuffer, 0, count);\n       if (readCount \u003c count) {\n         LOG.error(\"Can\u0027t read back \" + count + \" bytes, partial read size:\"\n             + readCount);\n-        return response \u003d new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0,\n-            stableHow, Nfs3Constant.WRITE_COMMIT_VERF);\n+        return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n+            Nfs3Constant.WRITE_COMMIT_VERF);\n       }\n-\n     } catch (IOException e) {\n       LOG.info(\"Read failed when processing possible perfect overwrite, path\u003d\"\n           + path + \" error:\" + e);\n-      return response \u003d new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0,\n-          stableHow, Nfs3Constant.WRITE_COMMIT_VERF);\n+      return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n+          Nfs3Constant.WRITE_COMMIT_VERF);\n     } finally {\n       IOUtils.cleanup(LOG, fis);\n     }\n \n     // Compare with the request\n     Comparator comparator \u003d new Comparator();\n     if (comparator.compare(readbuffer, 0, readCount, data, 0, count) !\u003d 0) {\n       LOG.info(\"Perfect overwrite has different content\");\n       response \u003d new WRITE3Response(Nfs3Status.NFS3ERR_INVAL, wccData, 0,\n           stableHow, 0);\n     } else {\n       LOG.info(\"Perfect overwrite has same content,\"\n           + \" updating the mtime, then return success\");\n       Nfs3FileAttributes postOpAttr \u003d null;\n       try {\n         dfsClient.setTimes(path, System.currentTimeMillis(), -1);\n         postOpAttr \u003d Nfs3Utils.getFileAttr(dfsClient, path, iug);\n       } catch (IOException e) {\n         LOG.info(\"Got error when processing perfect overwrite, path\u003d\" + path\n             + \" error:\" + e);\n         return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n             0);\n       }\n \n       wccData.setPostOpAttr(postOpAttr);\n       response \u003d new WRITE3Response(Nfs3Status.NFS3_OK, wccData, count,\n           stableHow, 0);\n     }\n     return response;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private WRITE3Response processPerfectOverWrite(DFSClient dfsClient,\n      long offset, int count, WriteStableHow stableHow, byte[] data,\n      String path, WccData wccData, IdUserGroup iug) {\n    WRITE3Response response \u003d null;\n\n    // Read the content back\n    byte[] readbuffer \u003d new byte[count];\n\n    int readCount \u003d 0;\n    FSDataInputStream fis \u003d null;\n    try {\n      // Sync file data and length to avoid partial read failure\n      fos.hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));\n    } catch (ClosedChannelException closedException) {\n      LOG.info(\"The FSDataOutputStream has been closed. \" +\n      \t\t\"Continue processing the perfect overwrite.\");\n    } catch (IOException e) {\n      LOG.info(\"hsync failed when processing possible perfect overwrite, path\u003d\"\n          + path + \" error:\" + e);\n      return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n          Nfs3Constant.WRITE_COMMIT_VERF);\n    }\n    \n    try {\n      fis \u003d new FSDataInputStream(dfsClient.open(path));\n      readCount \u003d fis.read(offset, readbuffer, 0, count);\n      if (readCount \u003c count) {\n        LOG.error(\"Can\u0027t read back \" + count + \" bytes, partial read size:\"\n            + readCount);\n        return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n            Nfs3Constant.WRITE_COMMIT_VERF);\n      }\n    } catch (IOException e) {\n      LOG.info(\"Read failed when processing possible perfect overwrite, path\u003d\"\n          + path + \" error:\" + e);\n      return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n          Nfs3Constant.WRITE_COMMIT_VERF);\n    } finally {\n      IOUtils.cleanup(LOG, fis);\n    }\n\n    // Compare with the request\n    Comparator comparator \u003d new Comparator();\n    if (comparator.compare(readbuffer, 0, readCount, data, 0, count) !\u003d 0) {\n      LOG.info(\"Perfect overwrite has different content\");\n      response \u003d new WRITE3Response(Nfs3Status.NFS3ERR_INVAL, wccData, 0,\n          stableHow, 0);\n    } else {\n      LOG.info(\"Perfect overwrite has same content,\"\n          + \" updating the mtime, then return success\");\n      Nfs3FileAttributes postOpAttr \u003d null;\n      try {\n        dfsClient.setTimes(path, System.currentTimeMillis(), -1);\n        postOpAttr \u003d Nfs3Utils.getFileAttr(dfsClient, path, iug);\n      } catch (IOException e) {\n        LOG.info(\"Got error when processing perfect overwrite, path\u003d\" + path\n            + \" error:\" + e);\n        return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n            0);\n      }\n\n      wccData.setPostOpAttr(postOpAttr);\n      response \u003d new WRITE3Response(Nfs3Status.NFS3_OK, wccData, count,\n          stableHow, 0);\n    }\n    return response;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java",
      "extendedDetails": {}
    },
    "c9b89de0eacf15f21faa3a7ba30d4773f571c9a4": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5110 Change FSDataOutputStream to HdfsDataOutputStream for opened streams to fix type cast error. Contributed by Brandon Li\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1515624 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/13 2:54 PM",
      "commitName": "c9b89de0eacf15f21faa3a7ba30d4773f571c9a4",
      "commitAuthor": "Brandon Li",
      "commitDateOld": "10/07/13 10:01 AM",
      "commitNameOld": "58d75576c4d2a03d4954174bc223ed0334b34fee",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 40.2,
      "commitsBetweenForRepo": 257,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,59 +1,59 @@\n   private WRITE3Response processPerfectOverWrite(DFSClient dfsClient,\n       long offset, int count, WriteStableHow stableHow, byte[] data,\n       String path, WccData wccData, IdUserGroup iug) {\n     assert (ctxLock.isLocked());\n     WRITE3Response response \u003d null;\n \n     // Read the content back\n     byte[] readbuffer \u003d new byte[count];\n \n     int readCount \u003d 0;\n     FSDataInputStream fis \u003d null;\n     try {\n       // Sync file data and length to avoid partial read failure\n-      ((HdfsDataOutputStream) fos).hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));\n+      fos.hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));\n       \n       fis \u003d new FSDataInputStream(dfsClient.open(path));\n       readCount \u003d fis.read(offset, readbuffer, 0, count);\n       if (readCount \u003c count) {\n         LOG.error(\"Can\u0027t read back \" + count + \" bytes, partial read size:\"\n             + readCount);\n         return response \u003d new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0,\n             stableHow, Nfs3Constant.WRITE_COMMIT_VERF);\n       }\n \n     } catch (IOException e) {\n       LOG.info(\"Read failed when processing possible perfect overwrite, path\u003d\"\n           + path + \" error:\" + e);\n       return response \u003d new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0,\n           stableHow, Nfs3Constant.WRITE_COMMIT_VERF);\n     } finally {\n       IOUtils.cleanup(LOG, fis);\n     }\n \n     // Compare with the request\n     Comparator comparator \u003d new Comparator();\n     if (comparator.compare(readbuffer, 0, readCount, data, 0, count) !\u003d 0) {\n       LOG.info(\"Perfect overwrite has different content\");\n       response \u003d new WRITE3Response(Nfs3Status.NFS3ERR_INVAL, wccData, 0,\n           stableHow, 0);\n     } else {\n       LOG.info(\"Perfect overwrite has same content,\"\n           + \" updating the mtime, then return success\");\n       Nfs3FileAttributes postOpAttr \u003d null;\n       try {\n         dfsClient.setTimes(path, System.currentTimeMillis(), -1);\n         postOpAttr \u003d Nfs3Utils.getFileAttr(dfsClient, path, iug);\n       } catch (IOException e) {\n         LOG.info(\"Got error when processing perfect overwrite, path\u003d\" + path\n             + \" error:\" + e);\n         return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n             0);\n       }\n \n       wccData.setPostOpAttr(postOpAttr);\n       response \u003d new WRITE3Response(Nfs3Status.NFS3_OK, wccData, count,\n           stableHow, 0);\n     }\n     return response;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private WRITE3Response processPerfectOverWrite(DFSClient dfsClient,\n      long offset, int count, WriteStableHow stableHow, byte[] data,\n      String path, WccData wccData, IdUserGroup iug) {\n    assert (ctxLock.isLocked());\n    WRITE3Response response \u003d null;\n\n    // Read the content back\n    byte[] readbuffer \u003d new byte[count];\n\n    int readCount \u003d 0;\n    FSDataInputStream fis \u003d null;\n    try {\n      // Sync file data and length to avoid partial read failure\n      fos.hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));\n      \n      fis \u003d new FSDataInputStream(dfsClient.open(path));\n      readCount \u003d fis.read(offset, readbuffer, 0, count);\n      if (readCount \u003c count) {\n        LOG.error(\"Can\u0027t read back \" + count + \" bytes, partial read size:\"\n            + readCount);\n        return response \u003d new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0,\n            stableHow, Nfs3Constant.WRITE_COMMIT_VERF);\n      }\n\n    } catch (IOException e) {\n      LOG.info(\"Read failed when processing possible perfect overwrite, path\u003d\"\n          + path + \" error:\" + e);\n      return response \u003d new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0,\n          stableHow, Nfs3Constant.WRITE_COMMIT_VERF);\n    } finally {\n      IOUtils.cleanup(LOG, fis);\n    }\n\n    // Compare with the request\n    Comparator comparator \u003d new Comparator();\n    if (comparator.compare(readbuffer, 0, readCount, data, 0, count) !\u003d 0) {\n      LOG.info(\"Perfect overwrite has different content\");\n      response \u003d new WRITE3Response(Nfs3Status.NFS3ERR_INVAL, wccData, 0,\n          stableHow, 0);\n    } else {\n      LOG.info(\"Perfect overwrite has same content,\"\n          + \" updating the mtime, then return success\");\n      Nfs3FileAttributes postOpAttr \u003d null;\n      try {\n        dfsClient.setTimes(path, System.currentTimeMillis(), -1);\n        postOpAttr \u003d Nfs3Utils.getFileAttr(dfsClient, path, iug);\n      } catch (IOException e) {\n        LOG.info(\"Got error when processing perfect overwrite, path\u003d\" + path\n            + \" error:\" + e);\n        return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n            0);\n      }\n\n      wccData.setPostOpAttr(postOpAttr);\n      response \u003d new WRITE3Response(Nfs3Status.NFS3_OK, wccData, count,\n          stableHow, 0);\n    }\n    return response;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java",
      "extendedDetails": {}
    },
    "37f587563a943a827fbff865f5302bac6d202415": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-4762 Provide HDFS based NFSv3 and Mountd implementation. Contributed by Brandon Li\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1499029 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "02/07/13 10:31 AM",
      "commitName": "37f587563a943a827fbff865f5302bac6d202415",
      "commitAuthor": "Brandon Li",
      "diff": "@@ -0,0 +1,59 @@\n+  private WRITE3Response processPerfectOverWrite(DFSClient dfsClient,\n+      long offset, int count, WriteStableHow stableHow, byte[] data,\n+      String path, WccData wccData, IdUserGroup iug) {\n+    assert (ctxLock.isLocked());\n+    WRITE3Response response \u003d null;\n+\n+    // Read the content back\n+    byte[] readbuffer \u003d new byte[count];\n+\n+    int readCount \u003d 0;\n+    FSDataInputStream fis \u003d null;\n+    try {\n+      // Sync file data and length to avoid partial read failure\n+      ((HdfsDataOutputStream) fos).hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));\n+      \n+      fis \u003d new FSDataInputStream(dfsClient.open(path));\n+      readCount \u003d fis.read(offset, readbuffer, 0, count);\n+      if (readCount \u003c count) {\n+        LOG.error(\"Can\u0027t read back \" + count + \" bytes, partial read size:\"\n+            + readCount);\n+        return response \u003d new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0,\n+            stableHow, Nfs3Constant.WRITE_COMMIT_VERF);\n+      }\n+\n+    } catch (IOException e) {\n+      LOG.info(\"Read failed when processing possible perfect overwrite, path\u003d\"\n+          + path + \" error:\" + e);\n+      return response \u003d new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0,\n+          stableHow, Nfs3Constant.WRITE_COMMIT_VERF);\n+    } finally {\n+      IOUtils.cleanup(LOG, fis);\n+    }\n+\n+    // Compare with the request\n+    Comparator comparator \u003d new Comparator();\n+    if (comparator.compare(readbuffer, 0, readCount, data, 0, count) !\u003d 0) {\n+      LOG.info(\"Perfect overwrite has different content\");\n+      response \u003d new WRITE3Response(Nfs3Status.NFS3ERR_INVAL, wccData, 0,\n+          stableHow, 0);\n+    } else {\n+      LOG.info(\"Perfect overwrite has same content,\"\n+          + \" updating the mtime, then return success\");\n+      Nfs3FileAttributes postOpAttr \u003d null;\n+      try {\n+        dfsClient.setTimes(path, System.currentTimeMillis(), -1);\n+        postOpAttr \u003d Nfs3Utils.getFileAttr(dfsClient, path, iug);\n+      } catch (IOException e) {\n+        LOG.info(\"Got error when processing perfect overwrite, path\u003d\" + path\n+            + \" error:\" + e);\n+        return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n+            0);\n+      }\n+\n+      wccData.setPostOpAttr(postOpAttr);\n+      response \u003d new WRITE3Response(Nfs3Status.NFS3_OK, wccData, count,\n+          stableHow, 0);\n+    }\n+    return response;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private WRITE3Response processPerfectOverWrite(DFSClient dfsClient,\n      long offset, int count, WriteStableHow stableHow, byte[] data,\n      String path, WccData wccData, IdUserGroup iug) {\n    assert (ctxLock.isLocked());\n    WRITE3Response response \u003d null;\n\n    // Read the content back\n    byte[] readbuffer \u003d new byte[count];\n\n    int readCount \u003d 0;\n    FSDataInputStream fis \u003d null;\n    try {\n      // Sync file data and length to avoid partial read failure\n      ((HdfsDataOutputStream) fos).hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH));\n      \n      fis \u003d new FSDataInputStream(dfsClient.open(path));\n      readCount \u003d fis.read(offset, readbuffer, 0, count);\n      if (readCount \u003c count) {\n        LOG.error(\"Can\u0027t read back \" + count + \" bytes, partial read size:\"\n            + readCount);\n        return response \u003d new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0,\n            stableHow, Nfs3Constant.WRITE_COMMIT_VERF);\n      }\n\n    } catch (IOException e) {\n      LOG.info(\"Read failed when processing possible perfect overwrite, path\u003d\"\n          + path + \" error:\" + e);\n      return response \u003d new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0,\n          stableHow, Nfs3Constant.WRITE_COMMIT_VERF);\n    } finally {\n      IOUtils.cleanup(LOG, fis);\n    }\n\n    // Compare with the request\n    Comparator comparator \u003d new Comparator();\n    if (comparator.compare(readbuffer, 0, readCount, data, 0, count) !\u003d 0) {\n      LOG.info(\"Perfect overwrite has different content\");\n      response \u003d new WRITE3Response(Nfs3Status.NFS3ERR_INVAL, wccData, 0,\n          stableHow, 0);\n    } else {\n      LOG.info(\"Perfect overwrite has same content,\"\n          + \" updating the mtime, then return success\");\n      Nfs3FileAttributes postOpAttr \u003d null;\n      try {\n        dfsClient.setTimes(path, System.currentTimeMillis(), -1);\n        postOpAttr \u003d Nfs3Utils.getFileAttr(dfsClient, path, iug);\n      } catch (IOException e) {\n        LOG.info(\"Got error when processing perfect overwrite, path\u003d\" + path\n            + \" error:\" + e);\n        return new WRITE3Response(Nfs3Status.NFS3ERR_IO, wccData, 0, stableHow,\n            0);\n      }\n\n      wccData.setPostOpAttr(postOpAttr);\n      response \u003d new WRITE3Response(Nfs3Status.NFS3_OK, wccData, count,\n          stableHow, 0);\n    }\n    return response;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java"
    }
  }
}