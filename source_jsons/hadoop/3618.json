{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DFSInputStream.java",
  "functionName": "reportCheckSumFailure",
  "functionId": "reportCheckSumFailure___corruptedBlocks-CorruptedBlocks__dataNodeCount-int__isStriped-boolean",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
  "functionStartLine": 1543,
  "functionEndLine": 1571,
  "numCommitsSeen": 160,
  "timeTaken": 8431,
  "changeHistory": [
    "6ba99741086170b83c38d3e7e715d9e8046a1e00",
    "8808779db351fe444388d4acb3094766b5980718",
    "95363bcc7dae28ba9ae2cd7ee9a258fcb58cd932",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
    "bff5999d07e9416a22846c849487e509ede55040",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "6ba99741086170b83c38d3e7e715d9e8046a1e00": "Ybodychange",
    "8808779db351fe444388d4acb3094766b5980718": "Ymultichange(Yparameterchange,Ybodychange)",
    "95363bcc7dae28ba9ae2cd7ee9a258fcb58cd932": "Ymultichange(Yparameterchange,Ybodychange)",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": "Yfilerename",
    "bff5999d07e9416a22846c849487e509ede55040": "Ymodifierchange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "6ba99741086170b83c38d3e7e715d9e8046a1e00": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13703. Avoid allocation of CorruptedBlocks hashmap when no corrupted blocks are hit. Contributed by Todd Lipcon.\n",
      "commitDate": "02/07/18 3:02 AM",
      "commitName": "6ba99741086170b83c38d3e7e715d9e8046a1e00",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "05/06/18 9:25 PM",
      "commitNameOld": "774c1f199e11d886d0c0a1069325f0284da35deb",
      "commitAuthorOld": "Xiao Chen",
      "daysBetweenCommits": 26.23,
      "commitsBetweenForRepo": 143,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,29 +1,29 @@\n   protected void reportCheckSumFailure(CorruptedBlocks corruptedBlocks,\n       int dataNodeCount, boolean isStriped) {\n \n     Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap \u003d\n         corruptedBlocks.getCorruptionMap();\n-    if (corruptedBlockMap.isEmpty()) {\n+    if (corruptedBlockMap \u003d\u003d null) {\n       return;\n     }\n     List\u003cLocatedBlock\u003e reportList \u003d new ArrayList\u003c\u003e(corruptedBlockMap.size());\n     for (Map.Entry\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e entry :\n         corruptedBlockMap.entrySet()) {\n       ExtendedBlock blk \u003d entry.getKey();\n       Set\u003cDatanodeInfo\u003e dnSet \u003d entry.getValue();\n       if (isStriped || ((dnSet.size() \u003c dataNodeCount) \u0026\u0026 (dnSet.size() \u003e 0))\n           || ((dataNodeCount \u003d\u003d 1) \u0026\u0026 (dnSet.size() \u003d\u003d dataNodeCount))) {\n         DatanodeInfo[] locs \u003d new DatanodeInfo[dnSet.size()];\n         int i \u003d 0;\n         for (DatanodeInfo dn:dnSet) {\n           locs[i++] \u003d dn;\n         }\n         reportList.add(new LocatedBlock(blk, locs));\n       }\n     }\n     if (reportList.size() \u003e 0) {\n       dfsClient.reportChecksumFailure(src,\n           reportList.toArray(new LocatedBlock[reportList.size()]));\n     }\n     corruptedBlockMap.clear();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void reportCheckSumFailure(CorruptedBlocks corruptedBlocks,\n      int dataNodeCount, boolean isStriped) {\n\n    Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap \u003d\n        corruptedBlocks.getCorruptionMap();\n    if (corruptedBlockMap \u003d\u003d null) {\n      return;\n    }\n    List\u003cLocatedBlock\u003e reportList \u003d new ArrayList\u003c\u003e(corruptedBlockMap.size());\n    for (Map.Entry\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e entry :\n        corruptedBlockMap.entrySet()) {\n      ExtendedBlock blk \u003d entry.getKey();\n      Set\u003cDatanodeInfo\u003e dnSet \u003d entry.getValue();\n      if (isStriped || ((dnSet.size() \u003c dataNodeCount) \u0026\u0026 (dnSet.size() \u003e 0))\n          || ((dataNodeCount \u003d\u003d 1) \u0026\u0026 (dnSet.size() \u003d\u003d dataNodeCount))) {\n        DatanodeInfo[] locs \u003d new DatanodeInfo[dnSet.size()];\n        int i \u003d 0;\n        for (DatanodeInfo dn:dnSet) {\n          locs[i++] \u003d dn;\n        }\n        reportList.add(new LocatedBlock(blk, locs));\n      }\n    }\n    if (reportList.size() \u003e 0) {\n      dfsClient.reportChecksumFailure(src,\n          reportList.toArray(new LocatedBlock[reportList.size()]));\n    }\n    corruptedBlockMap.clear();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {}
    },
    "8808779db351fe444388d4acb3094766b5980718": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-9734. Refactoring of checksum failure report related codes. Contributed by Kai Zheng.\n\nChange-Id: Ie69a77e3498a360959f8e213c51fb2b17c28b64a\n",
      "commitDate": "25/02/16 9:55 AM",
      "commitName": "8808779db351fe444388d4acb3094766b5980718",
      "commitAuthor": "Zhe Zhang",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-9734. Refactoring of checksum failure report related codes. Contributed by Kai Zheng.\n\nChange-Id: Ie69a77e3498a360959f8e213c51fb2b17c28b64a\n",
          "commitDate": "25/02/16 9:55 AM",
          "commitName": "8808779db351fe444388d4acb3094766b5980718",
          "commitAuthor": "Zhe Zhang",
          "commitDateOld": "22/01/16 9:46 AM",
          "commitNameOld": "95363bcc7dae28ba9ae2cd7ee9a258fcb58cd932",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 34.01,
          "commitsBetweenForRepo": 234,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,27 +1,29 @@\n-  protected void reportCheckSumFailure(\n-      Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap,\n+  protected void reportCheckSumFailure(CorruptedBlocks corruptedBlocks,\n       int dataNodeCount, boolean isStriped) {\n+\n+    Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap \u003d\n+        corruptedBlocks.getCorruptionMap();\n     if (corruptedBlockMap.isEmpty()) {\n       return;\n     }\n     List\u003cLocatedBlock\u003e reportList \u003d new ArrayList\u003c\u003e(corruptedBlockMap.size());\n     for (Map.Entry\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e entry :\n         corruptedBlockMap.entrySet()) {\n       ExtendedBlock blk \u003d entry.getKey();\n       Set\u003cDatanodeInfo\u003e dnSet \u003d entry.getValue();\n       if (isStriped || ((dnSet.size() \u003c dataNodeCount) \u0026\u0026 (dnSet.size() \u003e 0))\n           || ((dataNodeCount \u003d\u003d 1) \u0026\u0026 (dnSet.size() \u003d\u003d dataNodeCount))) {\n         DatanodeInfo[] locs \u003d new DatanodeInfo[dnSet.size()];\n         int i \u003d 0;\n         for (DatanodeInfo dn:dnSet) {\n           locs[i++] \u003d dn;\n         }\n         reportList.add(new LocatedBlock(blk, locs));\n       }\n     }\n     if (reportList.size() \u003e 0) {\n       dfsClient.reportChecksumFailure(src,\n           reportList.toArray(new LocatedBlock[reportList.size()]));\n     }\n     corruptedBlockMap.clear();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected void reportCheckSumFailure(CorruptedBlocks corruptedBlocks,\n      int dataNodeCount, boolean isStriped) {\n\n    Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap \u003d\n        corruptedBlocks.getCorruptionMap();\n    if (corruptedBlockMap.isEmpty()) {\n      return;\n    }\n    List\u003cLocatedBlock\u003e reportList \u003d new ArrayList\u003c\u003e(corruptedBlockMap.size());\n    for (Map.Entry\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e entry :\n        corruptedBlockMap.entrySet()) {\n      ExtendedBlock blk \u003d entry.getKey();\n      Set\u003cDatanodeInfo\u003e dnSet \u003d entry.getValue();\n      if (isStriped || ((dnSet.size() \u003c dataNodeCount) \u0026\u0026 (dnSet.size() \u003e 0))\n          || ((dataNodeCount \u003d\u003d 1) \u0026\u0026 (dnSet.size() \u003d\u003d dataNodeCount))) {\n        DatanodeInfo[] locs \u003d new DatanodeInfo[dnSet.size()];\n        int i \u003d 0;\n        for (DatanodeInfo dn:dnSet) {\n          locs[i++] \u003d dn;\n        }\n        reportList.add(new LocatedBlock(blk, locs));\n      }\n    }\n    if (reportList.size() \u003e 0) {\n      dfsClient.reportChecksumFailure(src,\n          reportList.toArray(new LocatedBlock[reportList.size()]));\n    }\n    corruptedBlockMap.clear();\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
          "extendedDetails": {
            "oldValue": "[corruptedBlockMap-Map\u003cExtendedBlock,Set\u003cDatanodeInfo\u003e\u003e, dataNodeCount-int, isStriped-boolean]",
            "newValue": "[corruptedBlocks-CorruptedBlocks, dataNodeCount-int, isStriped-boolean]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-9734. Refactoring of checksum failure report related codes. Contributed by Kai Zheng.\n\nChange-Id: Ie69a77e3498a360959f8e213c51fb2b17c28b64a\n",
          "commitDate": "25/02/16 9:55 AM",
          "commitName": "8808779db351fe444388d4acb3094766b5980718",
          "commitAuthor": "Zhe Zhang",
          "commitDateOld": "22/01/16 9:46 AM",
          "commitNameOld": "95363bcc7dae28ba9ae2cd7ee9a258fcb58cd932",
          "commitAuthorOld": "Jing Zhao",
          "daysBetweenCommits": 34.01,
          "commitsBetweenForRepo": 234,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,27 +1,29 @@\n-  protected void reportCheckSumFailure(\n-      Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap,\n+  protected void reportCheckSumFailure(CorruptedBlocks corruptedBlocks,\n       int dataNodeCount, boolean isStriped) {\n+\n+    Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap \u003d\n+        corruptedBlocks.getCorruptionMap();\n     if (corruptedBlockMap.isEmpty()) {\n       return;\n     }\n     List\u003cLocatedBlock\u003e reportList \u003d new ArrayList\u003c\u003e(corruptedBlockMap.size());\n     for (Map.Entry\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e entry :\n         corruptedBlockMap.entrySet()) {\n       ExtendedBlock blk \u003d entry.getKey();\n       Set\u003cDatanodeInfo\u003e dnSet \u003d entry.getValue();\n       if (isStriped || ((dnSet.size() \u003c dataNodeCount) \u0026\u0026 (dnSet.size() \u003e 0))\n           || ((dataNodeCount \u003d\u003d 1) \u0026\u0026 (dnSet.size() \u003d\u003d dataNodeCount))) {\n         DatanodeInfo[] locs \u003d new DatanodeInfo[dnSet.size()];\n         int i \u003d 0;\n         for (DatanodeInfo dn:dnSet) {\n           locs[i++] \u003d dn;\n         }\n         reportList.add(new LocatedBlock(blk, locs));\n       }\n     }\n     if (reportList.size() \u003e 0) {\n       dfsClient.reportChecksumFailure(src,\n           reportList.toArray(new LocatedBlock[reportList.size()]));\n     }\n     corruptedBlockMap.clear();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected void reportCheckSumFailure(CorruptedBlocks corruptedBlocks,\n      int dataNodeCount, boolean isStriped) {\n\n    Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap \u003d\n        corruptedBlocks.getCorruptionMap();\n    if (corruptedBlockMap.isEmpty()) {\n      return;\n    }\n    List\u003cLocatedBlock\u003e reportList \u003d new ArrayList\u003c\u003e(corruptedBlockMap.size());\n    for (Map.Entry\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e entry :\n        corruptedBlockMap.entrySet()) {\n      ExtendedBlock blk \u003d entry.getKey();\n      Set\u003cDatanodeInfo\u003e dnSet \u003d entry.getValue();\n      if (isStriped || ((dnSet.size() \u003c dataNodeCount) \u0026\u0026 (dnSet.size() \u003e 0))\n          || ((dataNodeCount \u003d\u003d 1) \u0026\u0026 (dnSet.size() \u003d\u003d dataNodeCount))) {\n        DatanodeInfo[] locs \u003d new DatanodeInfo[dnSet.size()];\n        int i \u003d 0;\n        for (DatanodeInfo dn:dnSet) {\n          locs[i++] \u003d dn;\n        }\n        reportList.add(new LocatedBlock(blk, locs));\n      }\n    }\n    if (reportList.size() \u003e 0) {\n      dfsClient.reportChecksumFailure(src,\n          reportList.toArray(new LocatedBlock[reportList.size()]));\n    }\n    corruptedBlockMap.clear();\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
          "extendedDetails": {}
        }
      ]
    },
    "95363bcc7dae28ba9ae2cd7ee9a258fcb58cd932": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-9646. ErasureCodingWorker may fail when recovering data blocks with length less than the first internal block. Contributed by Jing Zhao.\n",
      "commitDate": "22/01/16 9:46 AM",
      "commitName": "95363bcc7dae28ba9ae2cd7ee9a258fcb58cd932",
      "commitAuthor": "Jing Zhao",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-9646. ErasureCodingWorker may fail when recovering data blocks with length less than the first internal block. Contributed by Jing Zhao.\n",
          "commitDate": "22/01/16 9:46 AM",
          "commitName": "95363bcc7dae28ba9ae2cd7ee9a258fcb58cd932",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "20/01/16 11:26 AM",
          "commitNameOld": "7905788db94d560e6668af0d4bed22b326961aaf",
          "commitAuthorOld": "Colin Patrick Mccabe",
          "daysBetweenCommits": 1.93,
          "commitsBetweenForRepo": 26,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,23 +1,27 @@\n   protected void reportCheckSumFailure(\n       Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap,\n-      int dataNodeCount) {\n+      int dataNodeCount, boolean isStriped) {\n     if (corruptedBlockMap.isEmpty()) {\n       return;\n     }\n-    Iterator\u003cEntry\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e\u003e it \u003d corruptedBlockMap\n-        .entrySet().iterator();\n-    Entry\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e entry \u003d it.next();\n-    ExtendedBlock blk \u003d entry.getKey();\n-    Set\u003cDatanodeInfo\u003e dnSet \u003d entry.getValue();\n-    if (((dnSet.size() \u003c dataNodeCount) \u0026\u0026 (dnSet.size() \u003e 0))\n-        || ((dataNodeCount \u003d\u003d 1) \u0026\u0026 (dnSet.size() \u003d\u003d dataNodeCount))) {\n-      DatanodeInfo[] locs \u003d new DatanodeInfo[dnSet.size()];\n-      int i \u003d 0;\n-      for (DatanodeInfo dn:dnSet) {\n-        locs[i++] \u003d dn;\n+    List\u003cLocatedBlock\u003e reportList \u003d new ArrayList\u003c\u003e(corruptedBlockMap.size());\n+    for (Map.Entry\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e entry :\n+        corruptedBlockMap.entrySet()) {\n+      ExtendedBlock blk \u003d entry.getKey();\n+      Set\u003cDatanodeInfo\u003e dnSet \u003d entry.getValue();\n+      if (isStriped || ((dnSet.size() \u003c dataNodeCount) \u0026\u0026 (dnSet.size() \u003e 0))\n+          || ((dataNodeCount \u003d\u003d 1) \u0026\u0026 (dnSet.size() \u003d\u003d dataNodeCount))) {\n+        DatanodeInfo[] locs \u003d new DatanodeInfo[dnSet.size()];\n+        int i \u003d 0;\n+        for (DatanodeInfo dn:dnSet) {\n+          locs[i++] \u003d dn;\n+        }\n+        reportList.add(new LocatedBlock(blk, locs));\n       }\n-      LocatedBlock [] lblocks \u003d { new LocatedBlock(blk, locs) };\n-      dfsClient.reportChecksumFailure(src, lblocks);\n+    }\n+    if (reportList.size() \u003e 0) {\n+      dfsClient.reportChecksumFailure(src,\n+          reportList.toArray(new LocatedBlock[reportList.size()]));\n     }\n     corruptedBlockMap.clear();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected void reportCheckSumFailure(\n      Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap,\n      int dataNodeCount, boolean isStriped) {\n    if (corruptedBlockMap.isEmpty()) {\n      return;\n    }\n    List\u003cLocatedBlock\u003e reportList \u003d new ArrayList\u003c\u003e(corruptedBlockMap.size());\n    for (Map.Entry\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e entry :\n        corruptedBlockMap.entrySet()) {\n      ExtendedBlock blk \u003d entry.getKey();\n      Set\u003cDatanodeInfo\u003e dnSet \u003d entry.getValue();\n      if (isStriped || ((dnSet.size() \u003c dataNodeCount) \u0026\u0026 (dnSet.size() \u003e 0))\n          || ((dataNodeCount \u003d\u003d 1) \u0026\u0026 (dnSet.size() \u003d\u003d dataNodeCount))) {\n        DatanodeInfo[] locs \u003d new DatanodeInfo[dnSet.size()];\n        int i \u003d 0;\n        for (DatanodeInfo dn:dnSet) {\n          locs[i++] \u003d dn;\n        }\n        reportList.add(new LocatedBlock(blk, locs));\n      }\n    }\n    if (reportList.size() \u003e 0) {\n      dfsClient.reportChecksumFailure(src,\n          reportList.toArray(new LocatedBlock[reportList.size()]));\n    }\n    corruptedBlockMap.clear();\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
          "extendedDetails": {
            "oldValue": "[corruptedBlockMap-Map\u003cExtendedBlock,Set\u003cDatanodeInfo\u003e\u003e, dataNodeCount-int]",
            "newValue": "[corruptedBlockMap-Map\u003cExtendedBlock,Set\u003cDatanodeInfo\u003e\u003e, dataNodeCount-int, isStriped-boolean]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-9646. ErasureCodingWorker may fail when recovering data blocks with length less than the first internal block. Contributed by Jing Zhao.\n",
          "commitDate": "22/01/16 9:46 AM",
          "commitName": "95363bcc7dae28ba9ae2cd7ee9a258fcb58cd932",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "20/01/16 11:26 AM",
          "commitNameOld": "7905788db94d560e6668af0d4bed22b326961aaf",
          "commitAuthorOld": "Colin Patrick Mccabe",
          "daysBetweenCommits": 1.93,
          "commitsBetweenForRepo": 26,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,23 +1,27 @@\n   protected void reportCheckSumFailure(\n       Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap,\n-      int dataNodeCount) {\n+      int dataNodeCount, boolean isStriped) {\n     if (corruptedBlockMap.isEmpty()) {\n       return;\n     }\n-    Iterator\u003cEntry\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e\u003e it \u003d corruptedBlockMap\n-        .entrySet().iterator();\n-    Entry\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e entry \u003d it.next();\n-    ExtendedBlock blk \u003d entry.getKey();\n-    Set\u003cDatanodeInfo\u003e dnSet \u003d entry.getValue();\n-    if (((dnSet.size() \u003c dataNodeCount) \u0026\u0026 (dnSet.size() \u003e 0))\n-        || ((dataNodeCount \u003d\u003d 1) \u0026\u0026 (dnSet.size() \u003d\u003d dataNodeCount))) {\n-      DatanodeInfo[] locs \u003d new DatanodeInfo[dnSet.size()];\n-      int i \u003d 0;\n-      for (DatanodeInfo dn:dnSet) {\n-        locs[i++] \u003d dn;\n+    List\u003cLocatedBlock\u003e reportList \u003d new ArrayList\u003c\u003e(corruptedBlockMap.size());\n+    for (Map.Entry\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e entry :\n+        corruptedBlockMap.entrySet()) {\n+      ExtendedBlock blk \u003d entry.getKey();\n+      Set\u003cDatanodeInfo\u003e dnSet \u003d entry.getValue();\n+      if (isStriped || ((dnSet.size() \u003c dataNodeCount) \u0026\u0026 (dnSet.size() \u003e 0))\n+          || ((dataNodeCount \u003d\u003d 1) \u0026\u0026 (dnSet.size() \u003d\u003d dataNodeCount))) {\n+        DatanodeInfo[] locs \u003d new DatanodeInfo[dnSet.size()];\n+        int i \u003d 0;\n+        for (DatanodeInfo dn:dnSet) {\n+          locs[i++] \u003d dn;\n+        }\n+        reportList.add(new LocatedBlock(blk, locs));\n       }\n-      LocatedBlock [] lblocks \u003d { new LocatedBlock(blk, locs) };\n-      dfsClient.reportChecksumFailure(src, lblocks);\n+    }\n+    if (reportList.size() \u003e 0) {\n+      dfsClient.reportChecksumFailure(src,\n+          reportList.toArray(new LocatedBlock[reportList.size()]));\n     }\n     corruptedBlockMap.clear();\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected void reportCheckSumFailure(\n      Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap,\n      int dataNodeCount, boolean isStriped) {\n    if (corruptedBlockMap.isEmpty()) {\n      return;\n    }\n    List\u003cLocatedBlock\u003e reportList \u003d new ArrayList\u003c\u003e(corruptedBlockMap.size());\n    for (Map.Entry\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e entry :\n        corruptedBlockMap.entrySet()) {\n      ExtendedBlock blk \u003d entry.getKey();\n      Set\u003cDatanodeInfo\u003e dnSet \u003d entry.getValue();\n      if (isStriped || ((dnSet.size() \u003c dataNodeCount) \u0026\u0026 (dnSet.size() \u003e 0))\n          || ((dataNodeCount \u003d\u003d 1) \u0026\u0026 (dnSet.size() \u003d\u003d dataNodeCount))) {\n        DatanodeInfo[] locs \u003d new DatanodeInfo[dnSet.size()];\n        int i \u003d 0;\n        for (DatanodeInfo dn:dnSet) {\n          locs[i++] \u003d dn;\n        }\n        reportList.add(new LocatedBlock(blk, locs));\n      }\n    }\n    if (reportList.size() \u003e 0) {\n      dfsClient.reportChecksumFailure(src,\n          reportList.toArray(new LocatedBlock[reportList.size()]));\n    }\n    corruptedBlockMap.clear();\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
          "extendedDetails": {}
        }
      ]
    },
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8053. Move DFSIn/OutputStream and related classes to hadoop-hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "26/09/15 11:08 AM",
      "commitName": "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "26/09/15 9:06 AM",
      "commitNameOld": "861b52db242f238d7e36ad75c158025be959a696",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 0.08,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  protected void reportCheckSumFailure(\n      Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap, \n      int dataNodeCount) {\n    if (corruptedBlockMap.isEmpty()) {\n      return;\n    }\n    Iterator\u003cEntry\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e\u003e it \u003d corruptedBlockMap\n        .entrySet().iterator();\n    Entry\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e entry \u003d it.next();\n    ExtendedBlock blk \u003d entry.getKey();\n    Set\u003cDatanodeInfo\u003e dnSet \u003d entry.getValue();\n    if (((dnSet.size() \u003c dataNodeCount) \u0026\u0026 (dnSet.size() \u003e 0))\n        || ((dataNodeCount \u003d\u003d 1) \u0026\u0026 (dnSet.size() \u003d\u003d dataNodeCount))) {\n      DatanodeInfo[] locs \u003d new DatanodeInfo[dnSet.size()];\n      int i \u003d 0;\n      for (DatanodeInfo dn:dnSet) {\n        locs[i++] \u003d dn;\n      }\n      LocatedBlock [] lblocks \u003d { new LocatedBlock(blk, locs) };\n      dfsClient.reportChecksumFailure(src, lblocks);\n    }\n    corruptedBlockMap.clear();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java"
      }
    },
    "bff5999d07e9416a22846c849487e509ede55040": {
      "type": "Ymodifierchange",
      "commitMessage": "HDFS-8703. Merge refactor of DFSInputStream from ErasureCoding branch (Contributed by Vinayakumar B)\n",
      "commitDate": "02/07/15 3:41 AM",
      "commitName": "bff5999d07e9416a22846c849487e509ede55040",
      "commitAuthor": "Vinayakumar B",
      "commitDateOld": "04/06/15 10:51 AM",
      "commitNameOld": "ade6d9a61eb2e57a975f0efcdf8828d51ffec5fd",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 27.7,
      "commitsBetweenForRepo": 196,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,23 +1,23 @@\n-  private void reportCheckSumFailure(\n+  protected void reportCheckSumFailure(\n       Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap, \n       int dataNodeCount) {\n     if (corruptedBlockMap.isEmpty()) {\n       return;\n     }\n     Iterator\u003cEntry\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e\u003e it \u003d corruptedBlockMap\n         .entrySet().iterator();\n     Entry\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e entry \u003d it.next();\n     ExtendedBlock blk \u003d entry.getKey();\n     Set\u003cDatanodeInfo\u003e dnSet \u003d entry.getValue();\n     if (((dnSet.size() \u003c dataNodeCount) \u0026\u0026 (dnSet.size() \u003e 0))\n         || ((dataNodeCount \u003d\u003d 1) \u0026\u0026 (dnSet.size() \u003d\u003d dataNodeCount))) {\n       DatanodeInfo[] locs \u003d new DatanodeInfo[dnSet.size()];\n       int i \u003d 0;\n       for (DatanodeInfo dn:dnSet) {\n         locs[i++] \u003d dn;\n       }\n       LocatedBlock [] lblocks \u003d { new LocatedBlock(blk, locs) };\n       dfsClient.reportChecksumFailure(src, lblocks);\n     }\n     corruptedBlockMap.clear();\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void reportCheckSumFailure(\n      Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap, \n      int dataNodeCount) {\n    if (corruptedBlockMap.isEmpty()) {\n      return;\n    }\n    Iterator\u003cEntry\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e\u003e it \u003d corruptedBlockMap\n        .entrySet().iterator();\n    Entry\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e entry \u003d it.next();\n    ExtendedBlock blk \u003d entry.getKey();\n    Set\u003cDatanodeInfo\u003e dnSet \u003d entry.getValue();\n    if (((dnSet.size() \u003c dataNodeCount) \u0026\u0026 (dnSet.size() \u003e 0))\n        || ((dataNodeCount \u003d\u003d 1) \u0026\u0026 (dnSet.size() \u003d\u003d dataNodeCount))) {\n      DatanodeInfo[] locs \u003d new DatanodeInfo[dnSet.size()];\n      int i \u003d 0;\n      for (DatanodeInfo dn:dnSet) {\n        locs[i++] \u003d dn;\n      }\n      LocatedBlock [] lblocks \u003d { new LocatedBlock(blk, locs) };\n      dfsClient.reportChecksumFailure(src, lblocks);\n    }\n    corruptedBlockMap.clear();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {
        "oldValue": "[private]",
        "newValue": "[protected]"
      }
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private void reportCheckSumFailure(\n      Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap, \n      int dataNodeCount) {\n    if (corruptedBlockMap.isEmpty()) {\n      return;\n    }\n    Iterator\u003cEntry\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e\u003e it \u003d corruptedBlockMap\n        .entrySet().iterator();\n    Entry\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e entry \u003d it.next();\n    ExtendedBlock blk \u003d entry.getKey();\n    Set\u003cDatanodeInfo\u003e dnSet \u003d entry.getValue();\n    if (((dnSet.size() \u003c dataNodeCount) \u0026\u0026 (dnSet.size() \u003e 0))\n        || ((dataNodeCount \u003d\u003d 1) \u0026\u0026 (dnSet.size() \u003d\u003d dataNodeCount))) {\n      DatanodeInfo[] locs \u003d new DatanodeInfo[dnSet.size()];\n      int i \u003d 0;\n      for (DatanodeInfo dn:dnSet) {\n        locs[i++] \u003d dn;\n      }\n      LocatedBlock [] lblocks \u003d { new LocatedBlock(blk, locs) };\n      dfsClient.reportChecksumFailure(src, lblocks);\n    }\n    corruptedBlockMap.clear();\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private void reportCheckSumFailure(\n      Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap, \n      int dataNodeCount) {\n    if (corruptedBlockMap.isEmpty()) {\n      return;\n    }\n    Iterator\u003cEntry\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e\u003e it \u003d corruptedBlockMap\n        .entrySet().iterator();\n    Entry\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e entry \u003d it.next();\n    ExtendedBlock blk \u003d entry.getKey();\n    Set\u003cDatanodeInfo\u003e dnSet \u003d entry.getValue();\n    if (((dnSet.size() \u003c dataNodeCount) \u0026\u0026 (dnSet.size() \u003e 0))\n        || ((dataNodeCount \u003d\u003d 1) \u0026\u0026 (dnSet.size() \u003d\u003d dataNodeCount))) {\n      DatanodeInfo[] locs \u003d new DatanodeInfo[dnSet.size()];\n      int i \u003d 0;\n      for (DatanodeInfo dn:dnSet) {\n        locs[i++] \u003d dn;\n      }\n      LocatedBlock [] lblocks \u003d { new LocatedBlock(blk, locs) };\n      dfsClient.reportChecksumFailure(src, lblocks);\n    }\n    corruptedBlockMap.clear();\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/DFSInputStream.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,23 @@\n+  private void reportCheckSumFailure(\n+      Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap, \n+      int dataNodeCount) {\n+    if (corruptedBlockMap.isEmpty()) {\n+      return;\n+    }\n+    Iterator\u003cEntry\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e\u003e it \u003d corruptedBlockMap\n+        .entrySet().iterator();\n+    Entry\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e entry \u003d it.next();\n+    ExtendedBlock blk \u003d entry.getKey();\n+    Set\u003cDatanodeInfo\u003e dnSet \u003d entry.getValue();\n+    if (((dnSet.size() \u003c dataNodeCount) \u0026\u0026 (dnSet.size() \u003e 0))\n+        || ((dataNodeCount \u003d\u003d 1) \u0026\u0026 (dnSet.size() \u003d\u003d dataNodeCount))) {\n+      DatanodeInfo[] locs \u003d new DatanodeInfo[dnSet.size()];\n+      int i \u003d 0;\n+      for (DatanodeInfo dn:dnSet) {\n+        locs[i++] \u003d dn;\n+      }\n+      LocatedBlock [] lblocks \u003d { new LocatedBlock(blk, locs) };\n+      dfsClient.reportChecksumFailure(src, lblocks);\n+    }\n+    corruptedBlockMap.clear();\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void reportCheckSumFailure(\n      Map\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e corruptedBlockMap, \n      int dataNodeCount) {\n    if (corruptedBlockMap.isEmpty()) {\n      return;\n    }\n    Iterator\u003cEntry\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e\u003e it \u003d corruptedBlockMap\n        .entrySet().iterator();\n    Entry\u003cExtendedBlock, Set\u003cDatanodeInfo\u003e\u003e entry \u003d it.next();\n    ExtendedBlock blk \u003d entry.getKey();\n    Set\u003cDatanodeInfo\u003e dnSet \u003d entry.getValue();\n    if (((dnSet.size() \u003c dataNodeCount) \u0026\u0026 (dnSet.size() \u003e 0))\n        || ((dataNodeCount \u003d\u003d 1) \u0026\u0026 (dnSet.size() \u003d\u003d dataNodeCount))) {\n      DatanodeInfo[] locs \u003d new DatanodeInfo[dnSet.size()];\n      int i \u003d 0;\n      for (DatanodeInfo dn:dnSet) {\n        locs[i++] \u003d dn;\n      }\n      LocatedBlock [] lblocks \u003d { new LocatedBlock(blk, locs) };\n      dfsClient.reportChecksumFailure(src, lblocks);\n    }\n    corruptedBlockMap.clear();\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/DFSInputStream.java"
    }
  }
}