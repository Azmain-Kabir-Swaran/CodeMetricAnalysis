{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "RMContainerAllocator.java",
  "functionName": "handleEvent",
  "functionId": "handleEvent___event-ContainerAllocatorEvent",
  "sourceFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java",
  "functionStartLine": 375,
  "functionEndLine": 419,
  "numCommitsSeen": 94,
  "timeTaken": 10124,
  "changeHistory": [
    "fc2b69eba1c5df59f6175205c27dc7b584df50c0",
    "819224dcf9c683aa52f58633ac8e13663f1916d8",
    "42f90ab885d9693fcc1e52f9637f7de4111110ae",
    "3164e7d83875aa6b7435d1dfe61ac280aa277f1c",
    "376233cdd4a4ddbde5a92a0627f78338cb4c38b7",
    "875592220fb250ff9d0bba73c8ace9858fd369fd",
    "7b9c074b7635e3dcdc38d4e7fb1afbff7145e698",
    "9ca394d54dd24e67867c845a58150f6b51761512",
    "079ed1c9e1ab0a902e183dca2a5a9d79a7201264",
    "28a2eb9d722bb8cbbeee87a1c43b4dc4ef4467ce",
    "08f8abf5639d39167952dc5120b44fe35c63ff7a",
    "fffdf661e30afd10331d2153ff052c141b7ebe4b",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517"
  ],
  "changeHistoryShort": {
    "fc2b69eba1c5df59f6175205c27dc7b584df50c0": "Ybodychange",
    "819224dcf9c683aa52f58633ac8e13663f1916d8": "Ybodychange",
    "42f90ab885d9693fcc1e52f9637f7de4111110ae": "Ybodychange",
    "3164e7d83875aa6b7435d1dfe61ac280aa277f1c": "Ybodychange",
    "376233cdd4a4ddbde5a92a0627f78338cb4c38b7": "Ybodychange",
    "875592220fb250ff9d0bba73c8ace9858fd369fd": "Ybodychange",
    "7b9c074b7635e3dcdc38d4e7fb1afbff7145e698": "Ybodychange",
    "9ca394d54dd24e67867c845a58150f6b51761512": "Ybodychange",
    "079ed1c9e1ab0a902e183dca2a5a9d79a7201264": "Ybodychange",
    "28a2eb9d722bb8cbbeee87a1c43b4dc4ef4467ce": "Ybodychange",
    "08f8abf5639d39167952dc5120b44fe35c63ff7a": "Ymultichange(Yrename,Ymodifierchange,Ybodychange)",
    "fffdf661e30afd10331d2153ff052c141b7ebe4b": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Yintroduced"
  },
  "changeHistoryDetails": {
    "fc2b69eba1c5df59f6175205c27dc7b584df50c0": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-6765. MR should not schedule container requests in cases where reducer or mapper containers demand resource larger than the maximum supported (haibochen via rkanter)\n",
      "commitDate": "01/11/16 8:47 PM",
      "commitName": "fc2b69eba1c5df59f6175205c27dc7b584df50c0",
      "commitAuthor": "Robert Kanter",
      "commitDateOld": "27/10/16 5:33 AM",
      "commitNameOld": "060558c6f221ded0b014189d5b82eee4cc7b576b",
      "commitAuthorOld": "Naganarasimha",
      "daysBetweenCommits": 5.63,
      "commitsBetweenForRepo": 67,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,104 +1,45 @@\n   protected synchronized void handleEvent(ContainerAllocatorEvent event) {\n     recalculateReduceSchedule \u003d true;\n     if (event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_REQ) {\n       ContainerRequestEvent reqEvent \u003d (ContainerRequestEvent) event;\n-      JobId jobId \u003d getJob().getID();\n-      Resource supportedMaxContainerCapability \u003d getMaxContainerCapability();\n-      if (reqEvent.getAttemptID().getTaskId().getTaskType().equals(TaskType.MAP)) {\n-        if (mapResourceRequest.equals(Resources.none())) {\n-          mapResourceRequest \u003d reqEvent.getCapability();\n-          eventHandler.handle(new JobHistoryEvent(jobId,\n-            new NormalizedResourceEvent(\n-              org.apache.hadoop.mapreduce.TaskType.MAP, mapResourceRequest\n-                .getMemorySize())));\n-          LOG.info(\"mapResourceRequest:\" + mapResourceRequest);\n-          if (mapResourceRequest.getMemorySize() \u003e supportedMaxContainerCapability\n-            .getMemorySize()\n-              || mapResourceRequest.getVirtualCores() \u003e supportedMaxContainerCapability\n-                .getVirtualCores()) {\n-            String diagMsg \u003d\n-                \"MAP capability required is more than the supported \"\n-                    + \"max container capability in the cluster. Killing the Job. mapResourceRequest: \"\n-                    + mapResourceRequest + \" maxContainerCapability:\"\n-                    + supportedMaxContainerCapability;\n-            LOG.info(diagMsg);\n-            eventHandler.handle(new JobDiagnosticsUpdateEvent(jobId, diagMsg));\n-            eventHandler.handle(new JobEvent(jobId, JobEventType.JOB_KILL));\n-          }\n-        }\n-        // set the resources\n-        reqEvent.getCapability().setMemorySize(mapResourceRequest.getMemorySize());\n-        reqEvent.getCapability().setVirtualCores(\n-          mapResourceRequest.getVirtualCores());\n-        scheduledRequests.addMap(reqEvent);//maps are immediately scheduled\n+      boolean isMap \u003d reqEvent.getAttemptID().getTaskId().getTaskType().\n+          equals(TaskType.MAP);\n+      if (isMap) {\n+        handleMapContainerRequest(reqEvent);\n       } else {\n-        if (reduceResourceRequest.equals(Resources.none())) {\n-          reduceResourceRequest \u003d reqEvent.getCapability();\n-          eventHandler.handle(new JobHistoryEvent(jobId,\n-            new NormalizedResourceEvent(\n-              org.apache.hadoop.mapreduce.TaskType.REDUCE,\n-              reduceResourceRequest.getMemorySize())));\n-          LOG.info(\"reduceResourceRequest:\" + reduceResourceRequest);\n-          if (reduceResourceRequest.getMemorySize() \u003e supportedMaxContainerCapability\n-            .getMemorySize()\n-              || reduceResourceRequest.getVirtualCores() \u003e supportedMaxContainerCapability\n-                .getVirtualCores()) {\n-            String diagMsg \u003d\n-                \"REDUCE capability required is more than the \"\n-                    + \"supported max container capability in the cluster. Killing the \"\n-                    + \"Job. reduceResourceRequest: \" + reduceResourceRequest\n-                    + \" maxContainerCapability:\"\n-                    + supportedMaxContainerCapability;\n-            LOG.info(diagMsg);\n-            eventHandler.handle(new JobDiagnosticsUpdateEvent(jobId, diagMsg));\n-            eventHandler.handle(new JobEvent(jobId, JobEventType.JOB_KILL));\n-          }\n-        }\n-        // set the resources\n-        reqEvent.getCapability().setMemorySize(reduceResourceRequest.getMemorySize());\n-        reqEvent.getCapability().setVirtualCores(\n-          reduceResourceRequest.getVirtualCores());\n-        if (reqEvent.getEarlierAttemptFailed()) {\n-          //add to the front of queue for fail fast\n-          pendingReduces.addFirst(new ContainerRequest(reqEvent,\n-              PRIORITY_REDUCE, reduceNodeLabelExpression));\n-        } else {\n-          pendingReduces.add(new ContainerRequest(reqEvent, PRIORITY_REDUCE,\n-              reduceNodeLabelExpression));\n-          //reduces are added to pending and are slowly ramped up\n-        }\n+        handleReduceContainerRequest(reqEvent);\n       }\n       \n     } else if (\n       event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_DEALLOCATE) {\n   \n       LOG.info(\"Processing the event \" + event.toString());\n \n       TaskAttemptId aId \u003d event.getAttemptID();\n       \n       boolean removed \u003d scheduledRequests.remove(aId);\n       if (!removed) {\n         ContainerId containerId \u003d assignedRequests.get(aId);\n         if (containerId !\u003d null) {\n           removed \u003d true;\n           assignedRequests.remove(aId);\n           containersReleased++;\n           pendingRelease.add(containerId);\n           release(containerId);\n         }\n       }\n       if (!removed) {\n         LOG.error(\"Could not deallocate container for task attemptId \" + \n             aId);\n       }\n       preemptionPolicy.handleCompletedContainer(event.getAttemptID());\n     } else if (\n         event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_FAILED) {\n       ContainerFailedEvent fEv \u003d (ContainerFailedEvent) event;\n       String host \u003d getHost(fEv.getContMgrAddress());\n       containerFailedOnHost(host);\n       // propagate failures to preemption policy to discard checkpoints for\n       // failed tasks\n       preemptionPolicy.handleFailedContainer(event.getAttemptID());\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized void handleEvent(ContainerAllocatorEvent event) {\n    recalculateReduceSchedule \u003d true;\n    if (event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_REQ) {\n      ContainerRequestEvent reqEvent \u003d (ContainerRequestEvent) event;\n      boolean isMap \u003d reqEvent.getAttemptID().getTaskId().getTaskType().\n          equals(TaskType.MAP);\n      if (isMap) {\n        handleMapContainerRequest(reqEvent);\n      } else {\n        handleReduceContainerRequest(reqEvent);\n      }\n      \n    } else if (\n      event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_DEALLOCATE) {\n  \n      LOG.info(\"Processing the event \" + event.toString());\n\n      TaskAttemptId aId \u003d event.getAttemptID();\n      \n      boolean removed \u003d scheduledRequests.remove(aId);\n      if (!removed) {\n        ContainerId containerId \u003d assignedRequests.get(aId);\n        if (containerId !\u003d null) {\n          removed \u003d true;\n          assignedRequests.remove(aId);\n          containersReleased++;\n          pendingRelease.add(containerId);\n          release(containerId);\n        }\n      }\n      if (!removed) {\n        LOG.error(\"Could not deallocate container for task attemptId \" + \n            aId);\n      }\n      preemptionPolicy.handleCompletedContainer(event.getAttemptID());\n    } else if (\n        event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_FAILED) {\n      ContainerFailedEvent fEv \u003d (ContainerFailedEvent) event;\n      String host \u003d getHost(fEv.getContMgrAddress());\n      containerFailedOnHost(host);\n      // propagate failures to preemption policy to discard checkpoints for\n      // failed tasks\n      preemptionPolicy.handleFailedContainer(event.getAttemptID());\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java",
      "extendedDetails": {}
    },
    "819224dcf9c683aa52f58633ac8e13663f1916d8": {
      "type": "Ybodychange",
      "commitMessage": "YARN-5270. Solve miscellaneous issues caused by YARN-4844. Contributed by Wangda Tan\n",
      "commitDate": "11/07/16 10:36 PM",
      "commitName": "819224dcf9c683aa52f58633ac8e13663f1916d8",
      "commitAuthor": "Jian He",
      "commitDateOld": "10/07/16 8:46 AM",
      "commitNameOld": "6cf6ab7b780de2b0c2c9ea730e1f366965a0d682",
      "commitAuthorOld": "Sangjin Lee",
      "daysBetweenCommits": 1.58,
      "commitsBetweenForRepo": 5,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,104 +1,104 @@\n   protected synchronized void handleEvent(ContainerAllocatorEvent event) {\n     recalculateReduceSchedule \u003d true;\n     if (event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_REQ) {\n       ContainerRequestEvent reqEvent \u003d (ContainerRequestEvent) event;\n       JobId jobId \u003d getJob().getID();\n       Resource supportedMaxContainerCapability \u003d getMaxContainerCapability();\n       if (reqEvent.getAttemptID().getTaskId().getTaskType().equals(TaskType.MAP)) {\n         if (mapResourceRequest.equals(Resources.none())) {\n           mapResourceRequest \u003d reqEvent.getCapability();\n           eventHandler.handle(new JobHistoryEvent(jobId,\n             new NormalizedResourceEvent(\n               org.apache.hadoop.mapreduce.TaskType.MAP, mapResourceRequest\n                 .getMemorySize())));\n           LOG.info(\"mapResourceRequest:\" + mapResourceRequest);\n           if (mapResourceRequest.getMemorySize() \u003e supportedMaxContainerCapability\n             .getMemorySize()\n               || mapResourceRequest.getVirtualCores() \u003e supportedMaxContainerCapability\n                 .getVirtualCores()) {\n             String diagMsg \u003d\n                 \"MAP capability required is more than the supported \"\n                     + \"max container capability in the cluster. Killing the Job. mapResourceRequest: \"\n                     + mapResourceRequest + \" maxContainerCapability:\"\n                     + supportedMaxContainerCapability;\n             LOG.info(diagMsg);\n             eventHandler.handle(new JobDiagnosticsUpdateEvent(jobId, diagMsg));\n             eventHandler.handle(new JobEvent(jobId, JobEventType.JOB_KILL));\n           }\n         }\n         // set the resources\n-        reqEvent.getCapability().setMemory(mapResourceRequest.getMemorySize());\n+        reqEvent.getCapability().setMemorySize(mapResourceRequest.getMemorySize());\n         reqEvent.getCapability().setVirtualCores(\n           mapResourceRequest.getVirtualCores());\n         scheduledRequests.addMap(reqEvent);//maps are immediately scheduled\n       } else {\n         if (reduceResourceRequest.equals(Resources.none())) {\n           reduceResourceRequest \u003d reqEvent.getCapability();\n           eventHandler.handle(new JobHistoryEvent(jobId,\n             new NormalizedResourceEvent(\n               org.apache.hadoop.mapreduce.TaskType.REDUCE,\n               reduceResourceRequest.getMemorySize())));\n           LOG.info(\"reduceResourceRequest:\" + reduceResourceRequest);\n           if (reduceResourceRequest.getMemorySize() \u003e supportedMaxContainerCapability\n             .getMemorySize()\n               || reduceResourceRequest.getVirtualCores() \u003e supportedMaxContainerCapability\n                 .getVirtualCores()) {\n             String diagMsg \u003d\n                 \"REDUCE capability required is more than the \"\n                     + \"supported max container capability in the cluster. Killing the \"\n                     + \"Job. reduceResourceRequest: \" + reduceResourceRequest\n                     + \" maxContainerCapability:\"\n                     + supportedMaxContainerCapability;\n             LOG.info(diagMsg);\n             eventHandler.handle(new JobDiagnosticsUpdateEvent(jobId, diagMsg));\n             eventHandler.handle(new JobEvent(jobId, JobEventType.JOB_KILL));\n           }\n         }\n         // set the resources\n-        reqEvent.getCapability().setMemory(reduceResourceRequest.getMemorySize());\n+        reqEvent.getCapability().setMemorySize(reduceResourceRequest.getMemorySize());\n         reqEvent.getCapability().setVirtualCores(\n           reduceResourceRequest.getVirtualCores());\n         if (reqEvent.getEarlierAttemptFailed()) {\n           //add to the front of queue for fail fast\n           pendingReduces.addFirst(new ContainerRequest(reqEvent,\n               PRIORITY_REDUCE, reduceNodeLabelExpression));\n         } else {\n           pendingReduces.add(new ContainerRequest(reqEvent, PRIORITY_REDUCE,\n               reduceNodeLabelExpression));\n           //reduces are added to pending and are slowly ramped up\n         }\n       }\n       \n     } else if (\n       event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_DEALLOCATE) {\n   \n       LOG.info(\"Processing the event \" + event.toString());\n \n       TaskAttemptId aId \u003d event.getAttemptID();\n       \n       boolean removed \u003d scheduledRequests.remove(aId);\n       if (!removed) {\n         ContainerId containerId \u003d assignedRequests.get(aId);\n         if (containerId !\u003d null) {\n           removed \u003d true;\n           assignedRequests.remove(aId);\n           containersReleased++;\n           pendingRelease.add(containerId);\n           release(containerId);\n         }\n       }\n       if (!removed) {\n         LOG.error(\"Could not deallocate container for task attemptId \" + \n             aId);\n       }\n       preemptionPolicy.handleCompletedContainer(event.getAttemptID());\n     } else if (\n         event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_FAILED) {\n       ContainerFailedEvent fEv \u003d (ContainerFailedEvent) event;\n       String host \u003d getHost(fEv.getContMgrAddress());\n       containerFailedOnHost(host);\n       // propagate failures to preemption policy to discard checkpoints for\n       // failed tasks\n       preemptionPolicy.handleFailedContainer(event.getAttemptID());\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized void handleEvent(ContainerAllocatorEvent event) {\n    recalculateReduceSchedule \u003d true;\n    if (event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_REQ) {\n      ContainerRequestEvent reqEvent \u003d (ContainerRequestEvent) event;\n      JobId jobId \u003d getJob().getID();\n      Resource supportedMaxContainerCapability \u003d getMaxContainerCapability();\n      if (reqEvent.getAttemptID().getTaskId().getTaskType().equals(TaskType.MAP)) {\n        if (mapResourceRequest.equals(Resources.none())) {\n          mapResourceRequest \u003d reqEvent.getCapability();\n          eventHandler.handle(new JobHistoryEvent(jobId,\n            new NormalizedResourceEvent(\n              org.apache.hadoop.mapreduce.TaskType.MAP, mapResourceRequest\n                .getMemorySize())));\n          LOG.info(\"mapResourceRequest:\" + mapResourceRequest);\n          if (mapResourceRequest.getMemorySize() \u003e supportedMaxContainerCapability\n            .getMemorySize()\n              || mapResourceRequest.getVirtualCores() \u003e supportedMaxContainerCapability\n                .getVirtualCores()) {\n            String diagMsg \u003d\n                \"MAP capability required is more than the supported \"\n                    + \"max container capability in the cluster. Killing the Job. mapResourceRequest: \"\n                    + mapResourceRequest + \" maxContainerCapability:\"\n                    + supportedMaxContainerCapability;\n            LOG.info(diagMsg);\n            eventHandler.handle(new JobDiagnosticsUpdateEvent(jobId, diagMsg));\n            eventHandler.handle(new JobEvent(jobId, JobEventType.JOB_KILL));\n          }\n        }\n        // set the resources\n        reqEvent.getCapability().setMemorySize(mapResourceRequest.getMemorySize());\n        reqEvent.getCapability().setVirtualCores(\n          mapResourceRequest.getVirtualCores());\n        scheduledRequests.addMap(reqEvent);//maps are immediately scheduled\n      } else {\n        if (reduceResourceRequest.equals(Resources.none())) {\n          reduceResourceRequest \u003d reqEvent.getCapability();\n          eventHandler.handle(new JobHistoryEvent(jobId,\n            new NormalizedResourceEvent(\n              org.apache.hadoop.mapreduce.TaskType.REDUCE,\n              reduceResourceRequest.getMemorySize())));\n          LOG.info(\"reduceResourceRequest:\" + reduceResourceRequest);\n          if (reduceResourceRequest.getMemorySize() \u003e supportedMaxContainerCapability\n            .getMemorySize()\n              || reduceResourceRequest.getVirtualCores() \u003e supportedMaxContainerCapability\n                .getVirtualCores()) {\n            String diagMsg \u003d\n                \"REDUCE capability required is more than the \"\n                    + \"supported max container capability in the cluster. Killing the \"\n                    + \"Job. reduceResourceRequest: \" + reduceResourceRequest\n                    + \" maxContainerCapability:\"\n                    + supportedMaxContainerCapability;\n            LOG.info(diagMsg);\n            eventHandler.handle(new JobDiagnosticsUpdateEvent(jobId, diagMsg));\n            eventHandler.handle(new JobEvent(jobId, JobEventType.JOB_KILL));\n          }\n        }\n        // set the resources\n        reqEvent.getCapability().setMemorySize(reduceResourceRequest.getMemorySize());\n        reqEvent.getCapability().setVirtualCores(\n          reduceResourceRequest.getVirtualCores());\n        if (reqEvent.getEarlierAttemptFailed()) {\n          //add to the front of queue for fail fast\n          pendingReduces.addFirst(new ContainerRequest(reqEvent,\n              PRIORITY_REDUCE, reduceNodeLabelExpression));\n        } else {\n          pendingReduces.add(new ContainerRequest(reqEvent, PRIORITY_REDUCE,\n              reduceNodeLabelExpression));\n          //reduces are added to pending and are slowly ramped up\n        }\n      }\n      \n    } else if (\n      event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_DEALLOCATE) {\n  \n      LOG.info(\"Processing the event \" + event.toString());\n\n      TaskAttemptId aId \u003d event.getAttemptID();\n      \n      boolean removed \u003d scheduledRequests.remove(aId);\n      if (!removed) {\n        ContainerId containerId \u003d assignedRequests.get(aId);\n        if (containerId !\u003d null) {\n          removed \u003d true;\n          assignedRequests.remove(aId);\n          containersReleased++;\n          pendingRelease.add(containerId);\n          release(containerId);\n        }\n      }\n      if (!removed) {\n        LOG.error(\"Could not deallocate container for task attemptId \" + \n            aId);\n      }\n      preemptionPolicy.handleCompletedContainer(event.getAttemptID());\n    } else if (\n        event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_FAILED) {\n      ContainerFailedEvent fEv \u003d (ContainerFailedEvent) event;\n      String host \u003d getHost(fEv.getContMgrAddress());\n      containerFailedOnHost(host);\n      // propagate failures to preemption policy to discard checkpoints for\n      // failed tasks\n      preemptionPolicy.handleFailedContainer(event.getAttemptID());\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java",
      "extendedDetails": {}
    },
    "42f90ab885d9693fcc1e52f9637f7de4111110ae": {
      "type": "Ybodychange",
      "commitMessage": "YARN-4844. Add getMemorySize/getVirtualCoresSize to o.a.h.y.api.records.Resource. Contributed by Wangda Tan.\n",
      "commitDate": "29/05/16 8:54 AM",
      "commitName": "42f90ab885d9693fcc1e52f9637f7de4111110ae",
      "commitAuthor": "Varun Vasudev",
      "commitDateOld": "24/05/16 7:47 PM",
      "commitNameOld": "ae353ea96993ec664090c5d84f6675c29d9f0f5f",
      "commitAuthorOld": "Jian He",
      "daysBetweenCommits": 4.55,
      "commitsBetweenForRepo": 37,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,104 +1,104 @@\n   protected synchronized void handleEvent(ContainerAllocatorEvent event) {\n     recalculateReduceSchedule \u003d true;\n     if (event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_REQ) {\n       ContainerRequestEvent reqEvent \u003d (ContainerRequestEvent) event;\n       JobId jobId \u003d getJob().getID();\n       Resource supportedMaxContainerCapability \u003d getMaxContainerCapability();\n       if (reqEvent.getAttemptID().getTaskId().getTaskType().equals(TaskType.MAP)) {\n         if (mapResourceRequest.equals(Resources.none())) {\n           mapResourceRequest \u003d reqEvent.getCapability();\n           eventHandler.handle(new JobHistoryEvent(jobId,\n             new NormalizedResourceEvent(\n               org.apache.hadoop.mapreduce.TaskType.MAP, mapResourceRequest\n-                .getMemory())));\n+                .getMemorySize())));\n           LOG.info(\"mapResourceRequest:\" + mapResourceRequest);\n-          if (mapResourceRequest.getMemory() \u003e supportedMaxContainerCapability\n-            .getMemory()\n+          if (mapResourceRequest.getMemorySize() \u003e supportedMaxContainerCapability\n+            .getMemorySize()\n               || mapResourceRequest.getVirtualCores() \u003e supportedMaxContainerCapability\n                 .getVirtualCores()) {\n             String diagMsg \u003d\n                 \"MAP capability required is more than the supported \"\n                     + \"max container capability in the cluster. Killing the Job. mapResourceRequest: \"\n                     + mapResourceRequest + \" maxContainerCapability:\"\n                     + supportedMaxContainerCapability;\n             LOG.info(diagMsg);\n             eventHandler.handle(new JobDiagnosticsUpdateEvent(jobId, diagMsg));\n             eventHandler.handle(new JobEvent(jobId, JobEventType.JOB_KILL));\n           }\n         }\n         // set the resources\n-        reqEvent.getCapability().setMemory(mapResourceRequest.getMemory());\n+        reqEvent.getCapability().setMemory(mapResourceRequest.getMemorySize());\n         reqEvent.getCapability().setVirtualCores(\n           mapResourceRequest.getVirtualCores());\n         scheduledRequests.addMap(reqEvent);//maps are immediately scheduled\n       } else {\n         if (reduceResourceRequest.equals(Resources.none())) {\n           reduceResourceRequest \u003d reqEvent.getCapability();\n           eventHandler.handle(new JobHistoryEvent(jobId,\n             new NormalizedResourceEvent(\n               org.apache.hadoop.mapreduce.TaskType.REDUCE,\n-              reduceResourceRequest.getMemory())));\n+              reduceResourceRequest.getMemorySize())));\n           LOG.info(\"reduceResourceRequest:\" + reduceResourceRequest);\n-          if (reduceResourceRequest.getMemory() \u003e supportedMaxContainerCapability\n-            .getMemory()\n+          if (reduceResourceRequest.getMemorySize() \u003e supportedMaxContainerCapability\n+            .getMemorySize()\n               || reduceResourceRequest.getVirtualCores() \u003e supportedMaxContainerCapability\n                 .getVirtualCores()) {\n             String diagMsg \u003d\n                 \"REDUCE capability required is more than the \"\n                     + \"supported max container capability in the cluster. Killing the \"\n                     + \"Job. reduceResourceRequest: \" + reduceResourceRequest\n                     + \" maxContainerCapability:\"\n                     + supportedMaxContainerCapability;\n             LOG.info(diagMsg);\n             eventHandler.handle(new JobDiagnosticsUpdateEvent(jobId, diagMsg));\n             eventHandler.handle(new JobEvent(jobId, JobEventType.JOB_KILL));\n           }\n         }\n         // set the resources\n-        reqEvent.getCapability().setMemory(reduceResourceRequest.getMemory());\n+        reqEvent.getCapability().setMemory(reduceResourceRequest.getMemorySize());\n         reqEvent.getCapability().setVirtualCores(\n           reduceResourceRequest.getVirtualCores());\n         if (reqEvent.getEarlierAttemptFailed()) {\n           //add to the front of queue for fail fast\n           pendingReduces.addFirst(new ContainerRequest(reqEvent,\n               PRIORITY_REDUCE, reduceNodeLabelExpression));\n         } else {\n           pendingReduces.add(new ContainerRequest(reqEvent, PRIORITY_REDUCE,\n               reduceNodeLabelExpression));\n           //reduces are added to pending and are slowly ramped up\n         }\n       }\n       \n     } else if (\n       event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_DEALLOCATE) {\n   \n       LOG.info(\"Processing the event \" + event.toString());\n \n       TaskAttemptId aId \u003d event.getAttemptID();\n       \n       boolean removed \u003d scheduledRequests.remove(aId);\n       if (!removed) {\n         ContainerId containerId \u003d assignedRequests.get(aId);\n         if (containerId !\u003d null) {\n           removed \u003d true;\n           assignedRequests.remove(aId);\n           containersReleased++;\n           pendingRelease.add(containerId);\n           release(containerId);\n         }\n       }\n       if (!removed) {\n         LOG.error(\"Could not deallocate container for task attemptId \" + \n             aId);\n       }\n       preemptionPolicy.handleCompletedContainer(event.getAttemptID());\n     } else if (\n         event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_FAILED) {\n       ContainerFailedEvent fEv \u003d (ContainerFailedEvent) event;\n       String host \u003d getHost(fEv.getContMgrAddress());\n       containerFailedOnHost(host);\n       // propagate failures to preemption policy to discard checkpoints for\n       // failed tasks\n       preemptionPolicy.handleFailedContainer(event.getAttemptID());\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized void handleEvent(ContainerAllocatorEvent event) {\n    recalculateReduceSchedule \u003d true;\n    if (event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_REQ) {\n      ContainerRequestEvent reqEvent \u003d (ContainerRequestEvent) event;\n      JobId jobId \u003d getJob().getID();\n      Resource supportedMaxContainerCapability \u003d getMaxContainerCapability();\n      if (reqEvent.getAttemptID().getTaskId().getTaskType().equals(TaskType.MAP)) {\n        if (mapResourceRequest.equals(Resources.none())) {\n          mapResourceRequest \u003d reqEvent.getCapability();\n          eventHandler.handle(new JobHistoryEvent(jobId,\n            new NormalizedResourceEvent(\n              org.apache.hadoop.mapreduce.TaskType.MAP, mapResourceRequest\n                .getMemorySize())));\n          LOG.info(\"mapResourceRequest:\" + mapResourceRequest);\n          if (mapResourceRequest.getMemorySize() \u003e supportedMaxContainerCapability\n            .getMemorySize()\n              || mapResourceRequest.getVirtualCores() \u003e supportedMaxContainerCapability\n                .getVirtualCores()) {\n            String diagMsg \u003d\n                \"MAP capability required is more than the supported \"\n                    + \"max container capability in the cluster. Killing the Job. mapResourceRequest: \"\n                    + mapResourceRequest + \" maxContainerCapability:\"\n                    + supportedMaxContainerCapability;\n            LOG.info(diagMsg);\n            eventHandler.handle(new JobDiagnosticsUpdateEvent(jobId, diagMsg));\n            eventHandler.handle(new JobEvent(jobId, JobEventType.JOB_KILL));\n          }\n        }\n        // set the resources\n        reqEvent.getCapability().setMemory(mapResourceRequest.getMemorySize());\n        reqEvent.getCapability().setVirtualCores(\n          mapResourceRequest.getVirtualCores());\n        scheduledRequests.addMap(reqEvent);//maps are immediately scheduled\n      } else {\n        if (reduceResourceRequest.equals(Resources.none())) {\n          reduceResourceRequest \u003d reqEvent.getCapability();\n          eventHandler.handle(new JobHistoryEvent(jobId,\n            new NormalizedResourceEvent(\n              org.apache.hadoop.mapreduce.TaskType.REDUCE,\n              reduceResourceRequest.getMemorySize())));\n          LOG.info(\"reduceResourceRequest:\" + reduceResourceRequest);\n          if (reduceResourceRequest.getMemorySize() \u003e supportedMaxContainerCapability\n            .getMemorySize()\n              || reduceResourceRequest.getVirtualCores() \u003e supportedMaxContainerCapability\n                .getVirtualCores()) {\n            String diagMsg \u003d\n                \"REDUCE capability required is more than the \"\n                    + \"supported max container capability in the cluster. Killing the \"\n                    + \"Job. reduceResourceRequest: \" + reduceResourceRequest\n                    + \" maxContainerCapability:\"\n                    + supportedMaxContainerCapability;\n            LOG.info(diagMsg);\n            eventHandler.handle(new JobDiagnosticsUpdateEvent(jobId, diagMsg));\n            eventHandler.handle(new JobEvent(jobId, JobEventType.JOB_KILL));\n          }\n        }\n        // set the resources\n        reqEvent.getCapability().setMemory(reduceResourceRequest.getMemorySize());\n        reqEvent.getCapability().setVirtualCores(\n          reduceResourceRequest.getVirtualCores());\n        if (reqEvent.getEarlierAttemptFailed()) {\n          //add to the front of queue for fail fast\n          pendingReduces.addFirst(new ContainerRequest(reqEvent,\n              PRIORITY_REDUCE, reduceNodeLabelExpression));\n        } else {\n          pendingReduces.add(new ContainerRequest(reqEvent, PRIORITY_REDUCE,\n              reduceNodeLabelExpression));\n          //reduces are added to pending and are slowly ramped up\n        }\n      }\n      \n    } else if (\n      event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_DEALLOCATE) {\n  \n      LOG.info(\"Processing the event \" + event.toString());\n\n      TaskAttemptId aId \u003d event.getAttemptID();\n      \n      boolean removed \u003d scheduledRequests.remove(aId);\n      if (!removed) {\n        ContainerId containerId \u003d assignedRequests.get(aId);\n        if (containerId !\u003d null) {\n          removed \u003d true;\n          assignedRequests.remove(aId);\n          containersReleased++;\n          pendingRelease.add(containerId);\n          release(containerId);\n        }\n      }\n      if (!removed) {\n        LOG.error(\"Could not deallocate container for task attemptId \" + \n            aId);\n      }\n      preemptionPolicy.handleCompletedContainer(event.getAttemptID());\n    } else if (\n        event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_FAILED) {\n      ContainerFailedEvent fEv \u003d (ContainerFailedEvent) event;\n      String host \u003d getHost(fEv.getContMgrAddress());\n      containerFailedOnHost(host);\n      // propagate failures to preemption policy to discard checkpoints for\n      // failed tasks\n      preemptionPolicy.handleFailedContainer(event.getAttemptID());\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java",
      "extendedDetails": {}
    },
    "3164e7d83875aa6b7435d1dfe61ac280aa277f1c": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-6304. Specifying node labels when submitting MR jobs. (Naganarasimha G R via wangda)\n",
      "commitDate": "27/05/15 2:26 PM",
      "commitName": "3164e7d83875aa6b7435d1dfe61ac280aa277f1c",
      "commitAuthor": "Wangda Tan",
      "commitDateOld": "18/03/15 12:30 PM",
      "commitNameOld": "30da99cbaf36aeef38a858251ce8ffa5eb657b38",
      "commitAuthorOld": "Jason Lowe",
      "daysBetweenCommits": 70.08,
      "commitsBetweenForRepo": 677,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,102 +1,104 @@\n   protected synchronized void handleEvent(ContainerAllocatorEvent event) {\n     recalculateReduceSchedule \u003d true;\n     if (event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_REQ) {\n       ContainerRequestEvent reqEvent \u003d (ContainerRequestEvent) event;\n       JobId jobId \u003d getJob().getID();\n       Resource supportedMaxContainerCapability \u003d getMaxContainerCapability();\n       if (reqEvent.getAttemptID().getTaskId().getTaskType().equals(TaskType.MAP)) {\n         if (mapResourceRequest.equals(Resources.none())) {\n           mapResourceRequest \u003d reqEvent.getCapability();\n           eventHandler.handle(new JobHistoryEvent(jobId,\n             new NormalizedResourceEvent(\n               org.apache.hadoop.mapreduce.TaskType.MAP, mapResourceRequest\n                 .getMemory())));\n           LOG.info(\"mapResourceRequest:\" + mapResourceRequest);\n           if (mapResourceRequest.getMemory() \u003e supportedMaxContainerCapability\n             .getMemory()\n               || mapResourceRequest.getVirtualCores() \u003e supportedMaxContainerCapability\n                 .getVirtualCores()) {\n             String diagMsg \u003d\n                 \"MAP capability required is more than the supported \"\n                     + \"max container capability in the cluster. Killing the Job. mapResourceRequest: \"\n                     + mapResourceRequest + \" maxContainerCapability:\"\n                     + supportedMaxContainerCapability;\n             LOG.info(diagMsg);\n             eventHandler.handle(new JobDiagnosticsUpdateEvent(jobId, diagMsg));\n             eventHandler.handle(new JobEvent(jobId, JobEventType.JOB_KILL));\n           }\n         }\n         // set the resources\n         reqEvent.getCapability().setMemory(mapResourceRequest.getMemory());\n         reqEvent.getCapability().setVirtualCores(\n           mapResourceRequest.getVirtualCores());\n         scheduledRequests.addMap(reqEvent);//maps are immediately scheduled\n       } else {\n         if (reduceResourceRequest.equals(Resources.none())) {\n           reduceResourceRequest \u003d reqEvent.getCapability();\n           eventHandler.handle(new JobHistoryEvent(jobId,\n             new NormalizedResourceEvent(\n               org.apache.hadoop.mapreduce.TaskType.REDUCE,\n               reduceResourceRequest.getMemory())));\n           LOG.info(\"reduceResourceRequest:\" + reduceResourceRequest);\n           if (reduceResourceRequest.getMemory() \u003e supportedMaxContainerCapability\n             .getMemory()\n               || reduceResourceRequest.getVirtualCores() \u003e supportedMaxContainerCapability\n                 .getVirtualCores()) {\n             String diagMsg \u003d\n                 \"REDUCE capability required is more than the \"\n                     + \"supported max container capability in the cluster. Killing the \"\n                     + \"Job. reduceResourceRequest: \" + reduceResourceRequest\n                     + \" maxContainerCapability:\"\n                     + supportedMaxContainerCapability;\n             LOG.info(diagMsg);\n             eventHandler.handle(new JobDiagnosticsUpdateEvent(jobId, diagMsg));\n             eventHandler.handle(new JobEvent(jobId, JobEventType.JOB_KILL));\n           }\n         }\n         // set the resources\n         reqEvent.getCapability().setMemory(reduceResourceRequest.getMemory());\n         reqEvent.getCapability().setVirtualCores(\n           reduceResourceRequest.getVirtualCores());\n         if (reqEvent.getEarlierAttemptFailed()) {\n           //add to the front of queue for fail fast\n-          pendingReduces.addFirst(new ContainerRequest(reqEvent, PRIORITY_REDUCE));\n+          pendingReduces.addFirst(new ContainerRequest(reqEvent,\n+              PRIORITY_REDUCE, reduceNodeLabelExpression));\n         } else {\n-          pendingReduces.add(new ContainerRequest(reqEvent, PRIORITY_REDUCE));\n+          pendingReduces.add(new ContainerRequest(reqEvent, PRIORITY_REDUCE,\n+              reduceNodeLabelExpression));\n           //reduces are added to pending and are slowly ramped up\n         }\n       }\n       \n     } else if (\n       event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_DEALLOCATE) {\n   \n       LOG.info(\"Processing the event \" + event.toString());\n \n       TaskAttemptId aId \u003d event.getAttemptID();\n       \n       boolean removed \u003d scheduledRequests.remove(aId);\n       if (!removed) {\n         ContainerId containerId \u003d assignedRequests.get(aId);\n         if (containerId !\u003d null) {\n           removed \u003d true;\n           assignedRequests.remove(aId);\n           containersReleased++;\n           pendingRelease.add(containerId);\n           release(containerId);\n         }\n       }\n       if (!removed) {\n         LOG.error(\"Could not deallocate container for task attemptId \" + \n             aId);\n       }\n       preemptionPolicy.handleCompletedContainer(event.getAttemptID());\n     } else if (\n         event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_FAILED) {\n       ContainerFailedEvent fEv \u003d (ContainerFailedEvent) event;\n       String host \u003d getHost(fEv.getContMgrAddress());\n       containerFailedOnHost(host);\n       // propagate failures to preemption policy to discard checkpoints for\n       // failed tasks\n       preemptionPolicy.handleFailedContainer(event.getAttemptID());\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized void handleEvent(ContainerAllocatorEvent event) {\n    recalculateReduceSchedule \u003d true;\n    if (event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_REQ) {\n      ContainerRequestEvent reqEvent \u003d (ContainerRequestEvent) event;\n      JobId jobId \u003d getJob().getID();\n      Resource supportedMaxContainerCapability \u003d getMaxContainerCapability();\n      if (reqEvent.getAttemptID().getTaskId().getTaskType().equals(TaskType.MAP)) {\n        if (mapResourceRequest.equals(Resources.none())) {\n          mapResourceRequest \u003d reqEvent.getCapability();\n          eventHandler.handle(new JobHistoryEvent(jobId,\n            new NormalizedResourceEvent(\n              org.apache.hadoop.mapreduce.TaskType.MAP, mapResourceRequest\n                .getMemory())));\n          LOG.info(\"mapResourceRequest:\" + mapResourceRequest);\n          if (mapResourceRequest.getMemory() \u003e supportedMaxContainerCapability\n            .getMemory()\n              || mapResourceRequest.getVirtualCores() \u003e supportedMaxContainerCapability\n                .getVirtualCores()) {\n            String diagMsg \u003d\n                \"MAP capability required is more than the supported \"\n                    + \"max container capability in the cluster. Killing the Job. mapResourceRequest: \"\n                    + mapResourceRequest + \" maxContainerCapability:\"\n                    + supportedMaxContainerCapability;\n            LOG.info(diagMsg);\n            eventHandler.handle(new JobDiagnosticsUpdateEvent(jobId, diagMsg));\n            eventHandler.handle(new JobEvent(jobId, JobEventType.JOB_KILL));\n          }\n        }\n        // set the resources\n        reqEvent.getCapability().setMemory(mapResourceRequest.getMemory());\n        reqEvent.getCapability().setVirtualCores(\n          mapResourceRequest.getVirtualCores());\n        scheduledRequests.addMap(reqEvent);//maps are immediately scheduled\n      } else {\n        if (reduceResourceRequest.equals(Resources.none())) {\n          reduceResourceRequest \u003d reqEvent.getCapability();\n          eventHandler.handle(new JobHistoryEvent(jobId,\n            new NormalizedResourceEvent(\n              org.apache.hadoop.mapreduce.TaskType.REDUCE,\n              reduceResourceRequest.getMemory())));\n          LOG.info(\"reduceResourceRequest:\" + reduceResourceRequest);\n          if (reduceResourceRequest.getMemory() \u003e supportedMaxContainerCapability\n            .getMemory()\n              || reduceResourceRequest.getVirtualCores() \u003e supportedMaxContainerCapability\n                .getVirtualCores()) {\n            String diagMsg \u003d\n                \"REDUCE capability required is more than the \"\n                    + \"supported max container capability in the cluster. Killing the \"\n                    + \"Job. reduceResourceRequest: \" + reduceResourceRequest\n                    + \" maxContainerCapability:\"\n                    + supportedMaxContainerCapability;\n            LOG.info(diagMsg);\n            eventHandler.handle(new JobDiagnosticsUpdateEvent(jobId, diagMsg));\n            eventHandler.handle(new JobEvent(jobId, JobEventType.JOB_KILL));\n          }\n        }\n        // set the resources\n        reqEvent.getCapability().setMemory(reduceResourceRequest.getMemory());\n        reqEvent.getCapability().setVirtualCores(\n          reduceResourceRequest.getVirtualCores());\n        if (reqEvent.getEarlierAttemptFailed()) {\n          //add to the front of queue for fail fast\n          pendingReduces.addFirst(new ContainerRequest(reqEvent,\n              PRIORITY_REDUCE, reduceNodeLabelExpression));\n        } else {\n          pendingReduces.add(new ContainerRequest(reqEvent, PRIORITY_REDUCE,\n              reduceNodeLabelExpression));\n          //reduces are added to pending and are slowly ramped up\n        }\n      }\n      \n    } else if (\n      event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_DEALLOCATE) {\n  \n      LOG.info(\"Processing the event \" + event.toString());\n\n      TaskAttemptId aId \u003d event.getAttemptID();\n      \n      boolean removed \u003d scheduledRequests.remove(aId);\n      if (!removed) {\n        ContainerId containerId \u003d assignedRequests.get(aId);\n        if (containerId !\u003d null) {\n          removed \u003d true;\n          assignedRequests.remove(aId);\n          containersReleased++;\n          pendingRelease.add(containerId);\n          release(containerId);\n        }\n      }\n      if (!removed) {\n        LOG.error(\"Could not deallocate container for task attemptId \" + \n            aId);\n      }\n      preemptionPolicy.handleCompletedContainer(event.getAttemptID());\n    } else if (\n        event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_FAILED) {\n      ContainerFailedEvent fEv \u003d (ContainerFailedEvent) event;\n      String host \u003d getHost(fEv.getContMgrAddress());\n      containerFailedOnHost(host);\n      // propagate failures to preemption policy to discard checkpoints for\n      // failed tasks\n      preemptionPolicy.handleFailedContainer(event.getAttemptID());\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java",
      "extendedDetails": {}
    },
    "376233cdd4a4ddbde5a92a0627f78338cb4c38b7": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5279. Made MR headroom calculation honor cpu dimension when YARN scheduler resource type is memory plus cpu. Contributed by Peng Zhang and Varun Vasudev.\n",
      "commitDate": "22/09/14 9:28 AM",
      "commitName": "376233cdd4a4ddbde5a92a0627f78338cb4c38b7",
      "commitAuthor": "Zhijie Shen",
      "commitDateOld": "08/08/14 2:38 PM",
      "commitNameOld": "eeb4acd955802e2a84ea94cecf2e2341b83d5efb",
      "commitAuthorOld": "Xuan Gong",
      "daysBetweenCommits": 44.78,
      "commitsBetweenForRepo": 414,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,90 +1,102 @@\n   protected synchronized void handleEvent(ContainerAllocatorEvent event) {\n     recalculateReduceSchedule \u003d true;\n     if (event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_REQ) {\n       ContainerRequestEvent reqEvent \u003d (ContainerRequestEvent) event;\n       JobId jobId \u003d getJob().getID();\n-      int supportedMaxContainerCapability \u003d\n-          getMaxContainerCapability().getMemory();\n+      Resource supportedMaxContainerCapability \u003d getMaxContainerCapability();\n       if (reqEvent.getAttemptID().getTaskId().getTaskType().equals(TaskType.MAP)) {\n-        if (mapResourceRequest \u003d\u003d 0) {\n-          mapResourceRequest \u003d reqEvent.getCapability().getMemory();\n-          eventHandler.handle(new JobHistoryEvent(jobId, \n-              new NormalizedResourceEvent(org.apache.hadoop.mapreduce.TaskType.MAP,\n-                  mapResourceRequest)));\n-          LOG.info(\"mapResourceRequest:\"+ mapResourceRequest);\n-          if (mapResourceRequest \u003e supportedMaxContainerCapability) {\n-            String diagMsg \u003d \"MAP capability required is more than the supported \" +\n-            \"max container capability in the cluster. Killing the Job. mapResourceRequest: \" +\n-                mapResourceRequest + \" maxContainerCapability:\" + supportedMaxContainerCapability;\n+        if (mapResourceRequest.equals(Resources.none())) {\n+          mapResourceRequest \u003d reqEvent.getCapability();\n+          eventHandler.handle(new JobHistoryEvent(jobId,\n+            new NormalizedResourceEvent(\n+              org.apache.hadoop.mapreduce.TaskType.MAP, mapResourceRequest\n+                .getMemory())));\n+          LOG.info(\"mapResourceRequest:\" + mapResourceRequest);\n+          if (mapResourceRequest.getMemory() \u003e supportedMaxContainerCapability\n+            .getMemory()\n+              || mapResourceRequest.getVirtualCores() \u003e supportedMaxContainerCapability\n+                .getVirtualCores()) {\n+            String diagMsg \u003d\n+                \"MAP capability required is more than the supported \"\n+                    + \"max container capability in the cluster. Killing the Job. mapResourceRequest: \"\n+                    + mapResourceRequest + \" maxContainerCapability:\"\n+                    + supportedMaxContainerCapability;\n             LOG.info(diagMsg);\n-            eventHandler.handle(new JobDiagnosticsUpdateEvent(\n-                jobId, diagMsg));\n+            eventHandler.handle(new JobDiagnosticsUpdateEvent(jobId, diagMsg));\n             eventHandler.handle(new JobEvent(jobId, JobEventType.JOB_KILL));\n           }\n         }\n-        //set the rounded off memory\n-        reqEvent.getCapability().setMemory(mapResourceRequest);\n+        // set the resources\n+        reqEvent.getCapability().setMemory(mapResourceRequest.getMemory());\n+        reqEvent.getCapability().setVirtualCores(\n+          mapResourceRequest.getVirtualCores());\n         scheduledRequests.addMap(reqEvent);//maps are immediately scheduled\n       } else {\n-        if (reduceResourceRequest \u003d\u003d 0) {\n-          reduceResourceRequest \u003d reqEvent.getCapability().getMemory();\n-          eventHandler.handle(new JobHistoryEvent(jobId, \n-              new NormalizedResourceEvent(\n-                  org.apache.hadoop.mapreduce.TaskType.REDUCE,\n-                  reduceResourceRequest)));\n-          LOG.info(\"reduceResourceRequest:\"+ reduceResourceRequest);\n-          if (reduceResourceRequest \u003e supportedMaxContainerCapability) {\n-            String diagMsg \u003d \"REDUCE capability required is more than the \" +\n-            \t\t\"supported max container capability in the cluster. Killing the \" +\n-            \t\t\"Job. reduceResourceRequest: \" + reduceResourceRequest +\n-            \t\t\" maxContainerCapability:\" + supportedMaxContainerCapability;\n+        if (reduceResourceRequest.equals(Resources.none())) {\n+          reduceResourceRequest \u003d reqEvent.getCapability();\n+          eventHandler.handle(new JobHistoryEvent(jobId,\n+            new NormalizedResourceEvent(\n+              org.apache.hadoop.mapreduce.TaskType.REDUCE,\n+              reduceResourceRequest.getMemory())));\n+          LOG.info(\"reduceResourceRequest:\" + reduceResourceRequest);\n+          if (reduceResourceRequest.getMemory() \u003e supportedMaxContainerCapability\n+            .getMemory()\n+              || reduceResourceRequest.getVirtualCores() \u003e supportedMaxContainerCapability\n+                .getVirtualCores()) {\n+            String diagMsg \u003d\n+                \"REDUCE capability required is more than the \"\n+                    + \"supported max container capability in the cluster. Killing the \"\n+                    + \"Job. reduceResourceRequest: \" + reduceResourceRequest\n+                    + \" maxContainerCapability:\"\n+                    + supportedMaxContainerCapability;\n             LOG.info(diagMsg);\n-            eventHandler.handle(new JobDiagnosticsUpdateEvent(\n-                jobId, diagMsg));\n+            eventHandler.handle(new JobDiagnosticsUpdateEvent(jobId, diagMsg));\n             eventHandler.handle(new JobEvent(jobId, JobEventType.JOB_KILL));\n           }\n         }\n-        //set the rounded off memory\n-        reqEvent.getCapability().setMemory(reduceResourceRequest);\n+        // set the resources\n+        reqEvent.getCapability().setMemory(reduceResourceRequest.getMemory());\n+        reqEvent.getCapability().setVirtualCores(\n+          reduceResourceRequest.getVirtualCores());\n         if (reqEvent.getEarlierAttemptFailed()) {\n           //add to the front of queue for fail fast\n           pendingReduces.addFirst(new ContainerRequest(reqEvent, PRIORITY_REDUCE));\n         } else {\n           pendingReduces.add(new ContainerRequest(reqEvent, PRIORITY_REDUCE));\n           //reduces are added to pending and are slowly ramped up\n         }\n       }\n       \n     } else if (\n       event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_DEALLOCATE) {\n   \n       LOG.info(\"Processing the event \" + event.toString());\n \n       TaskAttemptId aId \u003d event.getAttemptID();\n       \n       boolean removed \u003d scheduledRequests.remove(aId);\n       if (!removed) {\n         ContainerId containerId \u003d assignedRequests.get(aId);\n         if (containerId !\u003d null) {\n           removed \u003d true;\n           assignedRequests.remove(aId);\n           containersReleased++;\n           pendingRelease.add(containerId);\n           release(containerId);\n         }\n       }\n       if (!removed) {\n         LOG.error(\"Could not deallocate container for task attemptId \" + \n             aId);\n       }\n       preemptionPolicy.handleCompletedContainer(event.getAttemptID());\n     } else if (\n         event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_FAILED) {\n       ContainerFailedEvent fEv \u003d (ContainerFailedEvent) event;\n       String host \u003d getHost(fEv.getContMgrAddress());\n       containerFailedOnHost(host);\n       // propagate failures to preemption policy to discard checkpoints for\n       // failed tasks\n       preemptionPolicy.handleFailedContainer(event.getAttemptID());\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized void handleEvent(ContainerAllocatorEvent event) {\n    recalculateReduceSchedule \u003d true;\n    if (event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_REQ) {\n      ContainerRequestEvent reqEvent \u003d (ContainerRequestEvent) event;\n      JobId jobId \u003d getJob().getID();\n      Resource supportedMaxContainerCapability \u003d getMaxContainerCapability();\n      if (reqEvent.getAttemptID().getTaskId().getTaskType().equals(TaskType.MAP)) {\n        if (mapResourceRequest.equals(Resources.none())) {\n          mapResourceRequest \u003d reqEvent.getCapability();\n          eventHandler.handle(new JobHistoryEvent(jobId,\n            new NormalizedResourceEvent(\n              org.apache.hadoop.mapreduce.TaskType.MAP, mapResourceRequest\n                .getMemory())));\n          LOG.info(\"mapResourceRequest:\" + mapResourceRequest);\n          if (mapResourceRequest.getMemory() \u003e supportedMaxContainerCapability\n            .getMemory()\n              || mapResourceRequest.getVirtualCores() \u003e supportedMaxContainerCapability\n                .getVirtualCores()) {\n            String diagMsg \u003d\n                \"MAP capability required is more than the supported \"\n                    + \"max container capability in the cluster. Killing the Job. mapResourceRequest: \"\n                    + mapResourceRequest + \" maxContainerCapability:\"\n                    + supportedMaxContainerCapability;\n            LOG.info(diagMsg);\n            eventHandler.handle(new JobDiagnosticsUpdateEvent(jobId, diagMsg));\n            eventHandler.handle(new JobEvent(jobId, JobEventType.JOB_KILL));\n          }\n        }\n        // set the resources\n        reqEvent.getCapability().setMemory(mapResourceRequest.getMemory());\n        reqEvent.getCapability().setVirtualCores(\n          mapResourceRequest.getVirtualCores());\n        scheduledRequests.addMap(reqEvent);//maps are immediately scheduled\n      } else {\n        if (reduceResourceRequest.equals(Resources.none())) {\n          reduceResourceRequest \u003d reqEvent.getCapability();\n          eventHandler.handle(new JobHistoryEvent(jobId,\n            new NormalizedResourceEvent(\n              org.apache.hadoop.mapreduce.TaskType.REDUCE,\n              reduceResourceRequest.getMemory())));\n          LOG.info(\"reduceResourceRequest:\" + reduceResourceRequest);\n          if (reduceResourceRequest.getMemory() \u003e supportedMaxContainerCapability\n            .getMemory()\n              || reduceResourceRequest.getVirtualCores() \u003e supportedMaxContainerCapability\n                .getVirtualCores()) {\n            String diagMsg \u003d\n                \"REDUCE capability required is more than the \"\n                    + \"supported max container capability in the cluster. Killing the \"\n                    + \"Job. reduceResourceRequest: \" + reduceResourceRequest\n                    + \" maxContainerCapability:\"\n                    + supportedMaxContainerCapability;\n            LOG.info(diagMsg);\n            eventHandler.handle(new JobDiagnosticsUpdateEvent(jobId, diagMsg));\n            eventHandler.handle(new JobEvent(jobId, JobEventType.JOB_KILL));\n          }\n        }\n        // set the resources\n        reqEvent.getCapability().setMemory(reduceResourceRequest.getMemory());\n        reqEvent.getCapability().setVirtualCores(\n          reduceResourceRequest.getVirtualCores());\n        if (reqEvent.getEarlierAttemptFailed()) {\n          //add to the front of queue for fail fast\n          pendingReduces.addFirst(new ContainerRequest(reqEvent, PRIORITY_REDUCE));\n        } else {\n          pendingReduces.add(new ContainerRequest(reqEvent, PRIORITY_REDUCE));\n          //reduces are added to pending and are slowly ramped up\n        }\n      }\n      \n    } else if (\n      event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_DEALLOCATE) {\n  \n      LOG.info(\"Processing the event \" + event.toString());\n\n      TaskAttemptId aId \u003d event.getAttemptID();\n      \n      boolean removed \u003d scheduledRequests.remove(aId);\n      if (!removed) {\n        ContainerId containerId \u003d assignedRequests.get(aId);\n        if (containerId !\u003d null) {\n          removed \u003d true;\n          assignedRequests.remove(aId);\n          containersReleased++;\n          pendingRelease.add(containerId);\n          release(containerId);\n        }\n      }\n      if (!removed) {\n        LOG.error(\"Could not deallocate container for task attemptId \" + \n            aId);\n      }\n      preemptionPolicy.handleCompletedContainer(event.getAttemptID());\n    } else if (\n        event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_FAILED) {\n      ContainerFailedEvent fEv \u003d (ContainerFailedEvent) event;\n      String host \u003d getHost(fEv.getContMgrAddress());\n      containerFailedOnHost(host);\n      // propagate failures to preemption policy to discard checkpoints for\n      // failed tasks\n      preemptionPolicy.handleFailedContainer(event.getAttemptID());\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java",
      "extendedDetails": {}
    },
    "875592220fb250ff9d0bba73c8ace9858fd369fd": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5910. Make MR AM resync with RM in case of work-preserving RM-restart. Contributed by Rohith\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1611434 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "17/07/14 11:46 AM",
      "commitName": "875592220fb250ff9d0bba73c8ace9858fd369fd",
      "commitAuthor": "Jian He",
      "commitDateOld": "02/07/14 6:43 PM",
      "commitNameOld": "c6a09d2110286632e6cfcee00abf8e79894381ec",
      "commitAuthorOld": "Zhijie Shen",
      "daysBetweenCommits": 14.71,
      "commitsBetweenForRepo": 100,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,89 +1,90 @@\n   protected synchronized void handleEvent(ContainerAllocatorEvent event) {\n     recalculateReduceSchedule \u003d true;\n     if (event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_REQ) {\n       ContainerRequestEvent reqEvent \u003d (ContainerRequestEvent) event;\n       JobId jobId \u003d getJob().getID();\n       int supportedMaxContainerCapability \u003d\n           getMaxContainerCapability().getMemory();\n       if (reqEvent.getAttemptID().getTaskId().getTaskType().equals(TaskType.MAP)) {\n         if (mapResourceRequest \u003d\u003d 0) {\n           mapResourceRequest \u003d reqEvent.getCapability().getMemory();\n           eventHandler.handle(new JobHistoryEvent(jobId, \n               new NormalizedResourceEvent(org.apache.hadoop.mapreduce.TaskType.MAP,\n                   mapResourceRequest)));\n           LOG.info(\"mapResourceRequest:\"+ mapResourceRequest);\n           if (mapResourceRequest \u003e supportedMaxContainerCapability) {\n             String diagMsg \u003d \"MAP capability required is more than the supported \" +\n             \"max container capability in the cluster. Killing the Job. mapResourceRequest: \" +\n                 mapResourceRequest + \" maxContainerCapability:\" + supportedMaxContainerCapability;\n             LOG.info(diagMsg);\n             eventHandler.handle(new JobDiagnosticsUpdateEvent(\n                 jobId, diagMsg));\n             eventHandler.handle(new JobEvent(jobId, JobEventType.JOB_KILL));\n           }\n         }\n         //set the rounded off memory\n         reqEvent.getCapability().setMemory(mapResourceRequest);\n         scheduledRequests.addMap(reqEvent);//maps are immediately scheduled\n       } else {\n         if (reduceResourceRequest \u003d\u003d 0) {\n           reduceResourceRequest \u003d reqEvent.getCapability().getMemory();\n           eventHandler.handle(new JobHistoryEvent(jobId, \n               new NormalizedResourceEvent(\n                   org.apache.hadoop.mapreduce.TaskType.REDUCE,\n                   reduceResourceRequest)));\n           LOG.info(\"reduceResourceRequest:\"+ reduceResourceRequest);\n           if (reduceResourceRequest \u003e supportedMaxContainerCapability) {\n             String diagMsg \u003d \"REDUCE capability required is more than the \" +\n             \t\t\"supported max container capability in the cluster. Killing the \" +\n             \t\t\"Job. reduceResourceRequest: \" + reduceResourceRequest +\n             \t\t\" maxContainerCapability:\" + supportedMaxContainerCapability;\n             LOG.info(diagMsg);\n             eventHandler.handle(new JobDiagnosticsUpdateEvent(\n                 jobId, diagMsg));\n             eventHandler.handle(new JobEvent(jobId, JobEventType.JOB_KILL));\n           }\n         }\n         //set the rounded off memory\n         reqEvent.getCapability().setMemory(reduceResourceRequest);\n         if (reqEvent.getEarlierAttemptFailed()) {\n           //add to the front of queue for fail fast\n           pendingReduces.addFirst(new ContainerRequest(reqEvent, PRIORITY_REDUCE));\n         } else {\n           pendingReduces.add(new ContainerRequest(reqEvent, PRIORITY_REDUCE));\n           //reduces are added to pending and are slowly ramped up\n         }\n       }\n       \n     } else if (\n       event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_DEALLOCATE) {\n   \n       LOG.info(\"Processing the event \" + event.toString());\n \n       TaskAttemptId aId \u003d event.getAttemptID();\n       \n       boolean removed \u003d scheduledRequests.remove(aId);\n       if (!removed) {\n         ContainerId containerId \u003d assignedRequests.get(aId);\n         if (containerId !\u003d null) {\n           removed \u003d true;\n           assignedRequests.remove(aId);\n           containersReleased++;\n+          pendingRelease.add(containerId);\n           release(containerId);\n         }\n       }\n       if (!removed) {\n         LOG.error(\"Could not deallocate container for task attemptId \" + \n             aId);\n       }\n       preemptionPolicy.handleCompletedContainer(event.getAttemptID());\n     } else if (\n         event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_FAILED) {\n       ContainerFailedEvent fEv \u003d (ContainerFailedEvent) event;\n       String host \u003d getHost(fEv.getContMgrAddress());\n       containerFailedOnHost(host);\n       // propagate failures to preemption policy to discard checkpoints for\n       // failed tasks\n       preemptionPolicy.handleFailedContainer(event.getAttemptID());\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized void handleEvent(ContainerAllocatorEvent event) {\n    recalculateReduceSchedule \u003d true;\n    if (event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_REQ) {\n      ContainerRequestEvent reqEvent \u003d (ContainerRequestEvent) event;\n      JobId jobId \u003d getJob().getID();\n      int supportedMaxContainerCapability \u003d\n          getMaxContainerCapability().getMemory();\n      if (reqEvent.getAttemptID().getTaskId().getTaskType().equals(TaskType.MAP)) {\n        if (mapResourceRequest \u003d\u003d 0) {\n          mapResourceRequest \u003d reqEvent.getCapability().getMemory();\n          eventHandler.handle(new JobHistoryEvent(jobId, \n              new NormalizedResourceEvent(org.apache.hadoop.mapreduce.TaskType.MAP,\n                  mapResourceRequest)));\n          LOG.info(\"mapResourceRequest:\"+ mapResourceRequest);\n          if (mapResourceRequest \u003e supportedMaxContainerCapability) {\n            String diagMsg \u003d \"MAP capability required is more than the supported \" +\n            \"max container capability in the cluster. Killing the Job. mapResourceRequest: \" +\n                mapResourceRequest + \" maxContainerCapability:\" + supportedMaxContainerCapability;\n            LOG.info(diagMsg);\n            eventHandler.handle(new JobDiagnosticsUpdateEvent(\n                jobId, diagMsg));\n            eventHandler.handle(new JobEvent(jobId, JobEventType.JOB_KILL));\n          }\n        }\n        //set the rounded off memory\n        reqEvent.getCapability().setMemory(mapResourceRequest);\n        scheduledRequests.addMap(reqEvent);//maps are immediately scheduled\n      } else {\n        if (reduceResourceRequest \u003d\u003d 0) {\n          reduceResourceRequest \u003d reqEvent.getCapability().getMemory();\n          eventHandler.handle(new JobHistoryEvent(jobId, \n              new NormalizedResourceEvent(\n                  org.apache.hadoop.mapreduce.TaskType.REDUCE,\n                  reduceResourceRequest)));\n          LOG.info(\"reduceResourceRequest:\"+ reduceResourceRequest);\n          if (reduceResourceRequest \u003e supportedMaxContainerCapability) {\n            String diagMsg \u003d \"REDUCE capability required is more than the \" +\n            \t\t\"supported max container capability in the cluster. Killing the \" +\n            \t\t\"Job. reduceResourceRequest: \" + reduceResourceRequest +\n            \t\t\" maxContainerCapability:\" + supportedMaxContainerCapability;\n            LOG.info(diagMsg);\n            eventHandler.handle(new JobDiagnosticsUpdateEvent(\n                jobId, diagMsg));\n            eventHandler.handle(new JobEvent(jobId, JobEventType.JOB_KILL));\n          }\n        }\n        //set the rounded off memory\n        reqEvent.getCapability().setMemory(reduceResourceRequest);\n        if (reqEvent.getEarlierAttemptFailed()) {\n          //add to the front of queue for fail fast\n          pendingReduces.addFirst(new ContainerRequest(reqEvent, PRIORITY_REDUCE));\n        } else {\n          pendingReduces.add(new ContainerRequest(reqEvent, PRIORITY_REDUCE));\n          //reduces are added to pending and are slowly ramped up\n        }\n      }\n      \n    } else if (\n      event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_DEALLOCATE) {\n  \n      LOG.info(\"Processing the event \" + event.toString());\n\n      TaskAttemptId aId \u003d event.getAttemptID();\n      \n      boolean removed \u003d scheduledRequests.remove(aId);\n      if (!removed) {\n        ContainerId containerId \u003d assignedRequests.get(aId);\n        if (containerId !\u003d null) {\n          removed \u003d true;\n          assignedRequests.remove(aId);\n          containersReleased++;\n          pendingRelease.add(containerId);\n          release(containerId);\n        }\n      }\n      if (!removed) {\n        LOG.error(\"Could not deallocate container for task attemptId \" + \n            aId);\n      }\n      preemptionPolicy.handleCompletedContainer(event.getAttemptID());\n    } else if (\n        event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_FAILED) {\n      ContainerFailedEvent fEv \u003d (ContainerFailedEvent) event;\n      String host \u003d getHost(fEv.getContMgrAddress());\n      containerFailedOnHost(host);\n      // propagate failures to preemption policy to discard checkpoints for\n      // failed tasks\n      preemptionPolicy.handleFailedContainer(event.getAttemptID());\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java",
      "extendedDetails": {}
    },
    "7b9c074b7635e3dcdc38d4e7fb1afbff7145e698": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5844. Add a configurable delay to reducer-preemption. (Maysam Yabandeh via kasha)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1603957 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/06/14 10:22 AM",
      "commitName": "7b9c074b7635e3dcdc38d4e7fb1afbff7145e698",
      "commitAuthor": "Karthik Kambatla",
      "commitDateOld": "22/04/14 11:44 AM",
      "commitNameOld": "4a91b876db768e7997e740d754aeea66c3b7a36c",
      "commitAuthorOld": "Chris Nauroth",
      "daysBetweenCommits": 57.94,
      "commitsBetweenForRepo": 333,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,89 +1,89 @@\n   protected synchronized void handleEvent(ContainerAllocatorEvent event) {\n     recalculateReduceSchedule \u003d true;\n     if (event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_REQ) {\n       ContainerRequestEvent reqEvent \u003d (ContainerRequestEvent) event;\n       JobId jobId \u003d getJob().getID();\n       int supportedMaxContainerCapability \u003d\n           getMaxContainerCapability().getMemory();\n       if (reqEvent.getAttemptID().getTaskId().getTaskType().equals(TaskType.MAP)) {\n-        if (mapResourceReqt \u003d\u003d 0) {\n-          mapResourceReqt \u003d reqEvent.getCapability().getMemory();\n+        if (mapResourceRequest \u003d\u003d 0) {\n+          mapResourceRequest \u003d reqEvent.getCapability().getMemory();\n           eventHandler.handle(new JobHistoryEvent(jobId, \n               new NormalizedResourceEvent(org.apache.hadoop.mapreduce.TaskType.MAP,\n-              mapResourceReqt)));\n-          LOG.info(\"mapResourceReqt:\"+mapResourceReqt);\n-          if (mapResourceReqt \u003e supportedMaxContainerCapability) {\n+                  mapResourceRequest)));\n+          LOG.info(\"mapResourceRequest:\"+ mapResourceRequest);\n+          if (mapResourceRequest \u003e supportedMaxContainerCapability) {\n             String diagMsg \u003d \"MAP capability required is more than the supported \" +\n-            \"max container capability in the cluster. Killing the Job. mapResourceReqt: \" + \n-            mapResourceReqt + \" maxContainerCapability:\" + supportedMaxContainerCapability;\n+            \"max container capability in the cluster. Killing the Job. mapResourceRequest: \" +\n+                mapResourceRequest + \" maxContainerCapability:\" + supportedMaxContainerCapability;\n             LOG.info(diagMsg);\n             eventHandler.handle(new JobDiagnosticsUpdateEvent(\n                 jobId, diagMsg));\n             eventHandler.handle(new JobEvent(jobId, JobEventType.JOB_KILL));\n           }\n         }\n         //set the rounded off memory\n-        reqEvent.getCapability().setMemory(mapResourceReqt);\n+        reqEvent.getCapability().setMemory(mapResourceRequest);\n         scheduledRequests.addMap(reqEvent);//maps are immediately scheduled\n       } else {\n-        if (reduceResourceReqt \u003d\u003d 0) {\n-          reduceResourceReqt \u003d reqEvent.getCapability().getMemory();\n+        if (reduceResourceRequest \u003d\u003d 0) {\n+          reduceResourceRequest \u003d reqEvent.getCapability().getMemory();\n           eventHandler.handle(new JobHistoryEvent(jobId, \n               new NormalizedResourceEvent(\n                   org.apache.hadoop.mapreduce.TaskType.REDUCE,\n-              reduceResourceReqt)));\n-          LOG.info(\"reduceResourceReqt:\"+reduceResourceReqt);\n-          if (reduceResourceReqt \u003e supportedMaxContainerCapability) {\n+                  reduceResourceRequest)));\n+          LOG.info(\"reduceResourceRequest:\"+ reduceResourceRequest);\n+          if (reduceResourceRequest \u003e supportedMaxContainerCapability) {\n             String diagMsg \u003d \"REDUCE capability required is more than the \" +\n             \t\t\"supported max container capability in the cluster. Killing the \" +\n-            \t\t\"Job. reduceResourceReqt: \" + reduceResourceReqt +\n+            \t\t\"Job. reduceResourceRequest: \" + reduceResourceRequest +\n             \t\t\" maxContainerCapability:\" + supportedMaxContainerCapability;\n             LOG.info(diagMsg);\n             eventHandler.handle(new JobDiagnosticsUpdateEvent(\n                 jobId, diagMsg));\n             eventHandler.handle(new JobEvent(jobId, JobEventType.JOB_KILL));\n           }\n         }\n         //set the rounded off memory\n-        reqEvent.getCapability().setMemory(reduceResourceReqt);\n+        reqEvent.getCapability().setMemory(reduceResourceRequest);\n         if (reqEvent.getEarlierAttemptFailed()) {\n           //add to the front of queue for fail fast\n           pendingReduces.addFirst(new ContainerRequest(reqEvent, PRIORITY_REDUCE));\n         } else {\n           pendingReduces.add(new ContainerRequest(reqEvent, PRIORITY_REDUCE));\n           //reduces are added to pending and are slowly ramped up\n         }\n       }\n       \n     } else if (\n       event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_DEALLOCATE) {\n   \n       LOG.info(\"Processing the event \" + event.toString());\n \n       TaskAttemptId aId \u003d event.getAttemptID();\n       \n       boolean removed \u003d scheduledRequests.remove(aId);\n       if (!removed) {\n         ContainerId containerId \u003d assignedRequests.get(aId);\n         if (containerId !\u003d null) {\n           removed \u003d true;\n           assignedRequests.remove(aId);\n           containersReleased++;\n           release(containerId);\n         }\n       }\n       if (!removed) {\n         LOG.error(\"Could not deallocate container for task attemptId \" + \n             aId);\n       }\n       preemptionPolicy.handleCompletedContainer(event.getAttemptID());\n     } else if (\n         event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_FAILED) {\n       ContainerFailedEvent fEv \u003d (ContainerFailedEvent) event;\n       String host \u003d getHost(fEv.getContMgrAddress());\n       containerFailedOnHost(host);\n       // propagate failures to preemption policy to discard checkpoints for\n       // failed tasks\n       preemptionPolicy.handleFailedContainer(event.getAttemptID());\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized void handleEvent(ContainerAllocatorEvent event) {\n    recalculateReduceSchedule \u003d true;\n    if (event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_REQ) {\n      ContainerRequestEvent reqEvent \u003d (ContainerRequestEvent) event;\n      JobId jobId \u003d getJob().getID();\n      int supportedMaxContainerCapability \u003d\n          getMaxContainerCapability().getMemory();\n      if (reqEvent.getAttemptID().getTaskId().getTaskType().equals(TaskType.MAP)) {\n        if (mapResourceRequest \u003d\u003d 0) {\n          mapResourceRequest \u003d reqEvent.getCapability().getMemory();\n          eventHandler.handle(new JobHistoryEvent(jobId, \n              new NormalizedResourceEvent(org.apache.hadoop.mapreduce.TaskType.MAP,\n                  mapResourceRequest)));\n          LOG.info(\"mapResourceRequest:\"+ mapResourceRequest);\n          if (mapResourceRequest \u003e supportedMaxContainerCapability) {\n            String diagMsg \u003d \"MAP capability required is more than the supported \" +\n            \"max container capability in the cluster. Killing the Job. mapResourceRequest: \" +\n                mapResourceRequest + \" maxContainerCapability:\" + supportedMaxContainerCapability;\n            LOG.info(diagMsg);\n            eventHandler.handle(new JobDiagnosticsUpdateEvent(\n                jobId, diagMsg));\n            eventHandler.handle(new JobEvent(jobId, JobEventType.JOB_KILL));\n          }\n        }\n        //set the rounded off memory\n        reqEvent.getCapability().setMemory(mapResourceRequest);\n        scheduledRequests.addMap(reqEvent);//maps are immediately scheduled\n      } else {\n        if (reduceResourceRequest \u003d\u003d 0) {\n          reduceResourceRequest \u003d reqEvent.getCapability().getMemory();\n          eventHandler.handle(new JobHistoryEvent(jobId, \n              new NormalizedResourceEvent(\n                  org.apache.hadoop.mapreduce.TaskType.REDUCE,\n                  reduceResourceRequest)));\n          LOG.info(\"reduceResourceRequest:\"+ reduceResourceRequest);\n          if (reduceResourceRequest \u003e supportedMaxContainerCapability) {\n            String diagMsg \u003d \"REDUCE capability required is more than the \" +\n            \t\t\"supported max container capability in the cluster. Killing the \" +\n            \t\t\"Job. reduceResourceRequest: \" + reduceResourceRequest +\n            \t\t\" maxContainerCapability:\" + supportedMaxContainerCapability;\n            LOG.info(diagMsg);\n            eventHandler.handle(new JobDiagnosticsUpdateEvent(\n                jobId, diagMsg));\n            eventHandler.handle(new JobEvent(jobId, JobEventType.JOB_KILL));\n          }\n        }\n        //set the rounded off memory\n        reqEvent.getCapability().setMemory(reduceResourceRequest);\n        if (reqEvent.getEarlierAttemptFailed()) {\n          //add to the front of queue for fail fast\n          pendingReduces.addFirst(new ContainerRequest(reqEvent, PRIORITY_REDUCE));\n        } else {\n          pendingReduces.add(new ContainerRequest(reqEvent, PRIORITY_REDUCE));\n          //reduces are added to pending and are slowly ramped up\n        }\n      }\n      \n    } else if (\n      event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_DEALLOCATE) {\n  \n      LOG.info(\"Processing the event \" + event.toString());\n\n      TaskAttemptId aId \u003d event.getAttemptID();\n      \n      boolean removed \u003d scheduledRequests.remove(aId);\n      if (!removed) {\n        ContainerId containerId \u003d assignedRequests.get(aId);\n        if (containerId !\u003d null) {\n          removed \u003d true;\n          assignedRequests.remove(aId);\n          containersReleased++;\n          release(containerId);\n        }\n      }\n      if (!removed) {\n        LOG.error(\"Could not deallocate container for task attemptId \" + \n            aId);\n      }\n      preemptionPolicy.handleCompletedContainer(event.getAttemptID());\n    } else if (\n        event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_FAILED) {\n      ContainerFailedEvent fEv \u003d (ContainerFailedEvent) event;\n      String host \u003d getHost(fEv.getContMgrAddress());\n      containerFailedOnHost(host);\n      // propagate failures to preemption policy to discard checkpoints for\n      // failed tasks\n      preemptionPolicy.handleFailedContainer(event.getAttemptID());\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java",
      "extendedDetails": {}
    },
    "9ca394d54dd24e67867c845a58150f6b51761512": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5189. Add policies and wiring to respond to preemption requests\nfrom YARN. Contributed by Carlo Curino.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1551748 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "17/12/13 2:54 PM",
      "commitName": "9ca394d54dd24e67867c845a58150f6b51761512",
      "commitAuthor": "Christopher Douglas",
      "commitDateOld": "18/07/13 5:57 PM",
      "commitNameOld": "ac914f79bc80b152e71e7de5497b73f22824f4a7",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 151.91,
      "commitsBetweenForRepo": 987,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,85 +1,89 @@\n   protected synchronized void handleEvent(ContainerAllocatorEvent event) {\n     recalculateReduceSchedule \u003d true;\n     if (event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_REQ) {\n       ContainerRequestEvent reqEvent \u003d (ContainerRequestEvent) event;\n       JobId jobId \u003d getJob().getID();\n       int supportedMaxContainerCapability \u003d\n           getMaxContainerCapability().getMemory();\n       if (reqEvent.getAttemptID().getTaskId().getTaskType().equals(TaskType.MAP)) {\n         if (mapResourceReqt \u003d\u003d 0) {\n           mapResourceReqt \u003d reqEvent.getCapability().getMemory();\n           eventHandler.handle(new JobHistoryEvent(jobId, \n               new NormalizedResourceEvent(org.apache.hadoop.mapreduce.TaskType.MAP,\n               mapResourceReqt)));\n           LOG.info(\"mapResourceReqt:\"+mapResourceReqt);\n           if (mapResourceReqt \u003e supportedMaxContainerCapability) {\n             String diagMsg \u003d \"MAP capability required is more than the supported \" +\n             \"max container capability in the cluster. Killing the Job. mapResourceReqt: \" + \n             mapResourceReqt + \" maxContainerCapability:\" + supportedMaxContainerCapability;\n             LOG.info(diagMsg);\n             eventHandler.handle(new JobDiagnosticsUpdateEvent(\n                 jobId, diagMsg));\n             eventHandler.handle(new JobEvent(jobId, JobEventType.JOB_KILL));\n           }\n         }\n         //set the rounded off memory\n         reqEvent.getCapability().setMemory(mapResourceReqt);\n         scheduledRequests.addMap(reqEvent);//maps are immediately scheduled\n       } else {\n         if (reduceResourceReqt \u003d\u003d 0) {\n           reduceResourceReqt \u003d reqEvent.getCapability().getMemory();\n           eventHandler.handle(new JobHistoryEvent(jobId, \n               new NormalizedResourceEvent(\n                   org.apache.hadoop.mapreduce.TaskType.REDUCE,\n               reduceResourceReqt)));\n           LOG.info(\"reduceResourceReqt:\"+reduceResourceReqt);\n           if (reduceResourceReqt \u003e supportedMaxContainerCapability) {\n             String diagMsg \u003d \"REDUCE capability required is more than the \" +\n             \t\t\"supported max container capability in the cluster. Killing the \" +\n             \t\t\"Job. reduceResourceReqt: \" + reduceResourceReqt +\n             \t\t\" maxContainerCapability:\" + supportedMaxContainerCapability;\n             LOG.info(diagMsg);\n             eventHandler.handle(new JobDiagnosticsUpdateEvent(\n                 jobId, diagMsg));\n             eventHandler.handle(new JobEvent(jobId, JobEventType.JOB_KILL));\n           }\n         }\n         //set the rounded off memory\n         reqEvent.getCapability().setMemory(reduceResourceReqt);\n         if (reqEvent.getEarlierAttemptFailed()) {\n           //add to the front of queue for fail fast\n           pendingReduces.addFirst(new ContainerRequest(reqEvent, PRIORITY_REDUCE));\n         } else {\n           pendingReduces.add(new ContainerRequest(reqEvent, PRIORITY_REDUCE));\n           //reduces are added to pending and are slowly ramped up\n         }\n       }\n       \n     } else if (\n         event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_DEALLOCATE) {\n   \n       LOG.info(\"Processing the event \" + event.toString());\n \n       TaskAttemptId aId \u003d event.getAttemptID();\n       \n       boolean removed \u003d scheduledRequests.remove(aId);\n       if (!removed) {\n         ContainerId containerId \u003d assignedRequests.get(aId);\n         if (containerId !\u003d null) {\n           removed \u003d true;\n           assignedRequests.remove(aId);\n           containersReleased++;\n           release(containerId);\n         }\n       }\n       if (!removed) {\n         LOG.error(\"Could not deallocate container for task attemptId \" + \n             aId);\n       }\n+      preemptionPolicy.handleCompletedContainer(event.getAttemptID());\n     } else if (\n         event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_FAILED) {\n       ContainerFailedEvent fEv \u003d (ContainerFailedEvent) event;\n       String host \u003d getHost(fEv.getContMgrAddress());\n       containerFailedOnHost(host);\n+      // propagate failures to preemption policy to discard checkpoints for\n+      // failed tasks\n+      preemptionPolicy.handleFailedContainer(event.getAttemptID());\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized void handleEvent(ContainerAllocatorEvent event) {\n    recalculateReduceSchedule \u003d true;\n    if (event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_REQ) {\n      ContainerRequestEvent reqEvent \u003d (ContainerRequestEvent) event;\n      JobId jobId \u003d getJob().getID();\n      int supportedMaxContainerCapability \u003d\n          getMaxContainerCapability().getMemory();\n      if (reqEvent.getAttemptID().getTaskId().getTaskType().equals(TaskType.MAP)) {\n        if (mapResourceReqt \u003d\u003d 0) {\n          mapResourceReqt \u003d reqEvent.getCapability().getMemory();\n          eventHandler.handle(new JobHistoryEvent(jobId, \n              new NormalizedResourceEvent(org.apache.hadoop.mapreduce.TaskType.MAP,\n              mapResourceReqt)));\n          LOG.info(\"mapResourceReqt:\"+mapResourceReqt);\n          if (mapResourceReqt \u003e supportedMaxContainerCapability) {\n            String diagMsg \u003d \"MAP capability required is more than the supported \" +\n            \"max container capability in the cluster. Killing the Job. mapResourceReqt: \" + \n            mapResourceReqt + \" maxContainerCapability:\" + supportedMaxContainerCapability;\n            LOG.info(diagMsg);\n            eventHandler.handle(new JobDiagnosticsUpdateEvent(\n                jobId, diagMsg));\n            eventHandler.handle(new JobEvent(jobId, JobEventType.JOB_KILL));\n          }\n        }\n        //set the rounded off memory\n        reqEvent.getCapability().setMemory(mapResourceReqt);\n        scheduledRequests.addMap(reqEvent);//maps are immediately scheduled\n      } else {\n        if (reduceResourceReqt \u003d\u003d 0) {\n          reduceResourceReqt \u003d reqEvent.getCapability().getMemory();\n          eventHandler.handle(new JobHistoryEvent(jobId, \n              new NormalizedResourceEvent(\n                  org.apache.hadoop.mapreduce.TaskType.REDUCE,\n              reduceResourceReqt)));\n          LOG.info(\"reduceResourceReqt:\"+reduceResourceReqt);\n          if (reduceResourceReqt \u003e supportedMaxContainerCapability) {\n            String diagMsg \u003d \"REDUCE capability required is more than the \" +\n            \t\t\"supported max container capability in the cluster. Killing the \" +\n            \t\t\"Job. reduceResourceReqt: \" + reduceResourceReqt +\n            \t\t\" maxContainerCapability:\" + supportedMaxContainerCapability;\n            LOG.info(diagMsg);\n            eventHandler.handle(new JobDiagnosticsUpdateEvent(\n                jobId, diagMsg));\n            eventHandler.handle(new JobEvent(jobId, JobEventType.JOB_KILL));\n          }\n        }\n        //set the rounded off memory\n        reqEvent.getCapability().setMemory(reduceResourceReqt);\n        if (reqEvent.getEarlierAttemptFailed()) {\n          //add to the front of queue for fail fast\n          pendingReduces.addFirst(new ContainerRequest(reqEvent, PRIORITY_REDUCE));\n        } else {\n          pendingReduces.add(new ContainerRequest(reqEvent, PRIORITY_REDUCE));\n          //reduces are added to pending and are slowly ramped up\n        }\n      }\n      \n    } else if (\n        event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_DEALLOCATE) {\n  \n      LOG.info(\"Processing the event \" + event.toString());\n\n      TaskAttemptId aId \u003d event.getAttemptID();\n      \n      boolean removed \u003d scheduledRequests.remove(aId);\n      if (!removed) {\n        ContainerId containerId \u003d assignedRequests.get(aId);\n        if (containerId !\u003d null) {\n          removed \u003d true;\n          assignedRequests.remove(aId);\n          containersReleased++;\n          release(containerId);\n        }\n      }\n      if (!removed) {\n        LOG.error(\"Could not deallocate container for task attemptId \" + \n            aId);\n      }\n      preemptionPolicy.handleCompletedContainer(event.getAttemptID());\n    } else if (\n        event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_FAILED) {\n      ContainerFailedEvent fEv \u003d (ContainerFailedEvent) event;\n      String host \u003d getHost(fEv.getContMgrAddress());\n      containerFailedOnHost(host);\n      // propagate failures to preemption policy to discard checkpoints for\n      // failed tasks\n      preemptionPolicy.handleFailedContainer(event.getAttemptID());\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java",
      "extendedDetails": {}
    },
    "079ed1c9e1ab0a902e183dca2a5a9d79a7201264": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5310. MRAM should not normalize allocation request capabilities. (tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1493207 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "14/06/13 12:06 PM",
      "commitName": "079ed1c9e1ab0a902e183dca2a5a9d79a7201264",
      "commitAuthor": "Alejandro Abdelnur",
      "commitDateOld": "14/06/13 9:10 AM",
      "commitNameOld": "fdc9412a810564c79fbebf5eb730cb1018a95c6c",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 0.12,
      "commitsBetweenForRepo": 3,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,92 +1,85 @@\n   protected synchronized void handleEvent(ContainerAllocatorEvent event) {\n     recalculateReduceSchedule \u003d true;\n     if (event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_REQ) {\n       ContainerRequestEvent reqEvent \u003d (ContainerRequestEvent) event;\n       JobId jobId \u003d getJob().getID();\n       int supportedMaxContainerCapability \u003d\n           getMaxContainerCapability().getMemory();\n       if (reqEvent.getAttemptID().getTaskId().getTaskType().equals(TaskType.MAP)) {\n         if (mapResourceReqt \u003d\u003d 0) {\n           mapResourceReqt \u003d reqEvent.getCapability().getMemory();\n-          int minSlotMemSize \u003d getMinContainerCapability().getMemory();\n-          mapResourceReqt \u003d (int) Math.ceil((float) mapResourceReqt/minSlotMemSize)\n-              * minSlotMemSize;\n           eventHandler.handle(new JobHistoryEvent(jobId, \n               new NormalizedResourceEvent(org.apache.hadoop.mapreduce.TaskType.MAP,\n               mapResourceReqt)));\n           LOG.info(\"mapResourceReqt:\"+mapResourceReqt);\n           if (mapResourceReqt \u003e supportedMaxContainerCapability) {\n             String diagMsg \u003d \"MAP capability required is more than the supported \" +\n             \"max container capability in the cluster. Killing the Job. mapResourceReqt: \" + \n             mapResourceReqt + \" maxContainerCapability:\" + supportedMaxContainerCapability;\n             LOG.info(diagMsg);\n             eventHandler.handle(new JobDiagnosticsUpdateEvent(\n                 jobId, diagMsg));\n             eventHandler.handle(new JobEvent(jobId, JobEventType.JOB_KILL));\n           }\n         }\n         //set the rounded off memory\n         reqEvent.getCapability().setMemory(mapResourceReqt);\n         scheduledRequests.addMap(reqEvent);//maps are immediately scheduled\n       } else {\n         if (reduceResourceReqt \u003d\u003d 0) {\n           reduceResourceReqt \u003d reqEvent.getCapability().getMemory();\n-          int minSlotMemSize \u003d getMinContainerCapability().getMemory();\n-          //round off on slotsize\n-          reduceResourceReqt \u003d (int) Math.ceil((float) \n-              reduceResourceReqt/minSlotMemSize) * minSlotMemSize;\n           eventHandler.handle(new JobHistoryEvent(jobId, \n               new NormalizedResourceEvent(\n                   org.apache.hadoop.mapreduce.TaskType.REDUCE,\n               reduceResourceReqt)));\n           LOG.info(\"reduceResourceReqt:\"+reduceResourceReqt);\n           if (reduceResourceReqt \u003e supportedMaxContainerCapability) {\n             String diagMsg \u003d \"REDUCE capability required is more than the \" +\n             \t\t\"supported max container capability in the cluster. Killing the \" +\n             \t\t\"Job. reduceResourceReqt: \" + reduceResourceReqt +\n             \t\t\" maxContainerCapability:\" + supportedMaxContainerCapability;\n             LOG.info(diagMsg);\n             eventHandler.handle(new JobDiagnosticsUpdateEvent(\n                 jobId, diagMsg));\n             eventHandler.handle(new JobEvent(jobId, JobEventType.JOB_KILL));\n           }\n         }\n         //set the rounded off memory\n         reqEvent.getCapability().setMemory(reduceResourceReqt);\n         if (reqEvent.getEarlierAttemptFailed()) {\n           //add to the front of queue for fail fast\n           pendingReduces.addFirst(new ContainerRequest(reqEvent, PRIORITY_REDUCE));\n         } else {\n           pendingReduces.add(new ContainerRequest(reqEvent, PRIORITY_REDUCE));\n           //reduces are added to pending and are slowly ramped up\n         }\n       }\n       \n     } else if (\n         event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_DEALLOCATE) {\n   \n       LOG.info(\"Processing the event \" + event.toString());\n \n       TaskAttemptId aId \u003d event.getAttemptID();\n       \n       boolean removed \u003d scheduledRequests.remove(aId);\n       if (!removed) {\n         ContainerId containerId \u003d assignedRequests.get(aId);\n         if (containerId !\u003d null) {\n           removed \u003d true;\n           assignedRequests.remove(aId);\n           containersReleased++;\n           release(containerId);\n         }\n       }\n       if (!removed) {\n         LOG.error(\"Could not deallocate container for task attemptId \" + \n             aId);\n       }\n     } else if (\n         event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_FAILED) {\n       ContainerFailedEvent fEv \u003d (ContainerFailedEvent) event;\n       String host \u003d getHost(fEv.getContMgrAddress());\n       containerFailedOnHost(host);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized void handleEvent(ContainerAllocatorEvent event) {\n    recalculateReduceSchedule \u003d true;\n    if (event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_REQ) {\n      ContainerRequestEvent reqEvent \u003d (ContainerRequestEvent) event;\n      JobId jobId \u003d getJob().getID();\n      int supportedMaxContainerCapability \u003d\n          getMaxContainerCapability().getMemory();\n      if (reqEvent.getAttemptID().getTaskId().getTaskType().equals(TaskType.MAP)) {\n        if (mapResourceReqt \u003d\u003d 0) {\n          mapResourceReqt \u003d reqEvent.getCapability().getMemory();\n          eventHandler.handle(new JobHistoryEvent(jobId, \n              new NormalizedResourceEvent(org.apache.hadoop.mapreduce.TaskType.MAP,\n              mapResourceReqt)));\n          LOG.info(\"mapResourceReqt:\"+mapResourceReqt);\n          if (mapResourceReqt \u003e supportedMaxContainerCapability) {\n            String diagMsg \u003d \"MAP capability required is more than the supported \" +\n            \"max container capability in the cluster. Killing the Job. mapResourceReqt: \" + \n            mapResourceReqt + \" maxContainerCapability:\" + supportedMaxContainerCapability;\n            LOG.info(diagMsg);\n            eventHandler.handle(new JobDiagnosticsUpdateEvent(\n                jobId, diagMsg));\n            eventHandler.handle(new JobEvent(jobId, JobEventType.JOB_KILL));\n          }\n        }\n        //set the rounded off memory\n        reqEvent.getCapability().setMemory(mapResourceReqt);\n        scheduledRequests.addMap(reqEvent);//maps are immediately scheduled\n      } else {\n        if (reduceResourceReqt \u003d\u003d 0) {\n          reduceResourceReqt \u003d reqEvent.getCapability().getMemory();\n          eventHandler.handle(new JobHistoryEvent(jobId, \n              new NormalizedResourceEvent(\n                  org.apache.hadoop.mapreduce.TaskType.REDUCE,\n              reduceResourceReqt)));\n          LOG.info(\"reduceResourceReqt:\"+reduceResourceReqt);\n          if (reduceResourceReqt \u003e supportedMaxContainerCapability) {\n            String diagMsg \u003d \"REDUCE capability required is more than the \" +\n            \t\t\"supported max container capability in the cluster. Killing the \" +\n            \t\t\"Job. reduceResourceReqt: \" + reduceResourceReqt +\n            \t\t\" maxContainerCapability:\" + supportedMaxContainerCapability;\n            LOG.info(diagMsg);\n            eventHandler.handle(new JobDiagnosticsUpdateEvent(\n                jobId, diagMsg));\n            eventHandler.handle(new JobEvent(jobId, JobEventType.JOB_KILL));\n          }\n        }\n        //set the rounded off memory\n        reqEvent.getCapability().setMemory(reduceResourceReqt);\n        if (reqEvent.getEarlierAttemptFailed()) {\n          //add to the front of queue for fail fast\n          pendingReduces.addFirst(new ContainerRequest(reqEvent, PRIORITY_REDUCE));\n        } else {\n          pendingReduces.add(new ContainerRequest(reqEvent, PRIORITY_REDUCE));\n          //reduces are added to pending and are slowly ramped up\n        }\n      }\n      \n    } else if (\n        event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_DEALLOCATE) {\n  \n      LOG.info(\"Processing the event \" + event.toString());\n\n      TaskAttemptId aId \u003d event.getAttemptID();\n      \n      boolean removed \u003d scheduledRequests.remove(aId);\n      if (!removed) {\n        ContainerId containerId \u003d assignedRequests.get(aId);\n        if (containerId !\u003d null) {\n          removed \u003d true;\n          assignedRequests.remove(aId);\n          containersReleased++;\n          release(containerId);\n        }\n      }\n      if (!removed) {\n        LOG.error(\"Could not deallocate container for task attemptId \" + \n            aId);\n      }\n    } else if (\n        event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_FAILED) {\n      ContainerFailedEvent fEv \u003d (ContainerFailedEvent) event;\n      String host \u003d getHost(fEv.getContMgrAddress());\n      containerFailedOnHost(host);\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java",
      "extendedDetails": {}
    },
    "28a2eb9d722bb8cbbeee87a1c43b4dc4ef4467ce": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-3810. Performance tweaks - reduced logging in AM and defined hascode/equals for ResourceRequest \u0026 Priority. Contributed by Vinod K V. \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1241205 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "06/02/12 2:01 PM",
      "commitName": "28a2eb9d722bb8cbbeee87a1c43b4dc4ef4467ce",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "03/02/12 4:04 PM",
      "commitNameOld": "94242c93857a06fb9c56ee571a47d6ca18f00f48",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 2.91,
      "commitsBetweenForRepo": 18,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,87 +1,92 @@\n   protected synchronized void handleEvent(ContainerAllocatorEvent event) {\n-    LOG.info(\"Processing the event \" + event.toString());\n     recalculateReduceSchedule \u003d true;\n     if (event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_REQ) {\n       ContainerRequestEvent reqEvent \u003d (ContainerRequestEvent) event;\n+      JobId jobId \u003d getJob().getID();\n+      int supportedMaxContainerCapability \u003d\n+          getMaxContainerCapability().getMemory();\n       if (reqEvent.getAttemptID().getTaskId().getTaskType().equals(TaskType.MAP)) {\n         if (mapResourceReqt \u003d\u003d 0) {\n           mapResourceReqt \u003d reqEvent.getCapability().getMemory();\n           int minSlotMemSize \u003d getMinContainerCapability().getMemory();\n           mapResourceReqt \u003d (int) Math.ceil((float) mapResourceReqt/minSlotMemSize)\n               * minSlotMemSize;\n-          eventHandler.handle(new JobHistoryEvent(getJob().getID(), \n+          eventHandler.handle(new JobHistoryEvent(jobId, \n               new NormalizedResourceEvent(org.apache.hadoop.mapreduce.TaskType.MAP,\n               mapResourceReqt)));\n           LOG.info(\"mapResourceReqt:\"+mapResourceReqt);\n-          if (mapResourceReqt \u003e getMaxContainerCapability().getMemory()) {\n+          if (mapResourceReqt \u003e supportedMaxContainerCapability) {\n             String diagMsg \u003d \"MAP capability required is more than the supported \" +\n             \"max container capability in the cluster. Killing the Job. mapResourceReqt: \" + \n-            mapResourceReqt + \" maxContainerCapability:\" + getMaxContainerCapability().getMemory();\n+            mapResourceReqt + \" maxContainerCapability:\" + supportedMaxContainerCapability;\n             LOG.info(diagMsg);\n             eventHandler.handle(new JobDiagnosticsUpdateEvent(\n-                getJob().getID(), diagMsg));\n-            eventHandler.handle(new JobEvent(getJob().getID(), JobEventType.JOB_KILL));\n+                jobId, diagMsg));\n+            eventHandler.handle(new JobEvent(jobId, JobEventType.JOB_KILL));\n           }\n         }\n         //set the rounded off memory\n         reqEvent.getCapability().setMemory(mapResourceReqt);\n         scheduledRequests.addMap(reqEvent);//maps are immediately scheduled\n       } else {\n         if (reduceResourceReqt \u003d\u003d 0) {\n           reduceResourceReqt \u003d reqEvent.getCapability().getMemory();\n           int minSlotMemSize \u003d getMinContainerCapability().getMemory();\n           //round off on slotsize\n           reduceResourceReqt \u003d (int) Math.ceil((float) \n               reduceResourceReqt/minSlotMemSize) * minSlotMemSize;\n-          eventHandler.handle(new JobHistoryEvent(getJob().getID(), \n+          eventHandler.handle(new JobHistoryEvent(jobId, \n               new NormalizedResourceEvent(\n                   org.apache.hadoop.mapreduce.TaskType.REDUCE,\n               reduceResourceReqt)));\n           LOG.info(\"reduceResourceReqt:\"+reduceResourceReqt);\n-          if (reduceResourceReqt \u003e getMaxContainerCapability().getMemory()) {\n+          if (reduceResourceReqt \u003e supportedMaxContainerCapability) {\n             String diagMsg \u003d \"REDUCE capability required is more than the \" +\n             \t\t\"supported max container capability in the cluster. Killing the \" +\n             \t\t\"Job. reduceResourceReqt: \" + reduceResourceReqt +\n-            \t\t\" maxContainerCapability:\" + getMaxContainerCapability().getMemory();\n+            \t\t\" maxContainerCapability:\" + supportedMaxContainerCapability;\n             LOG.info(diagMsg);\n             eventHandler.handle(new JobDiagnosticsUpdateEvent(\n-                getJob().getID(), diagMsg));\n-            eventHandler.handle(new JobEvent(getJob().getID(), JobEventType.JOB_KILL));\n+                jobId, diagMsg));\n+            eventHandler.handle(new JobEvent(jobId, JobEventType.JOB_KILL));\n           }\n         }\n         //set the rounded off memory\n         reqEvent.getCapability().setMemory(reduceResourceReqt);\n         if (reqEvent.getEarlierAttemptFailed()) {\n           //add to the front of queue for fail fast\n           pendingReduces.addFirst(new ContainerRequest(reqEvent, PRIORITY_REDUCE));\n         } else {\n           pendingReduces.add(new ContainerRequest(reqEvent, PRIORITY_REDUCE));\n           //reduces are added to pending and are slowly ramped up\n         }\n       }\n       \n     } else if (\n         event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_DEALLOCATE) {\n+  \n+      LOG.info(\"Processing the event \" + event.toString());\n+\n       TaskAttemptId aId \u003d event.getAttemptID();\n       \n       boolean removed \u003d scheduledRequests.remove(aId);\n       if (!removed) {\n         ContainerId containerId \u003d assignedRequests.get(aId);\n         if (containerId !\u003d null) {\n           removed \u003d true;\n           assignedRequests.remove(aId);\n           containersReleased++;\n           release(containerId);\n         }\n       }\n       if (!removed) {\n         LOG.error(\"Could not deallocate container for task attemptId \" + \n             aId);\n       }\n     } else if (\n         event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_FAILED) {\n       ContainerFailedEvent fEv \u003d (ContainerFailedEvent) event;\n       String host \u003d getHost(fEv.getContMgrAddress());\n       containerFailedOnHost(host);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected synchronized void handleEvent(ContainerAllocatorEvent event) {\n    recalculateReduceSchedule \u003d true;\n    if (event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_REQ) {\n      ContainerRequestEvent reqEvent \u003d (ContainerRequestEvent) event;\n      JobId jobId \u003d getJob().getID();\n      int supportedMaxContainerCapability \u003d\n          getMaxContainerCapability().getMemory();\n      if (reqEvent.getAttemptID().getTaskId().getTaskType().equals(TaskType.MAP)) {\n        if (mapResourceReqt \u003d\u003d 0) {\n          mapResourceReqt \u003d reqEvent.getCapability().getMemory();\n          int minSlotMemSize \u003d getMinContainerCapability().getMemory();\n          mapResourceReqt \u003d (int) Math.ceil((float) mapResourceReqt/minSlotMemSize)\n              * minSlotMemSize;\n          eventHandler.handle(new JobHistoryEvent(jobId, \n              new NormalizedResourceEvent(org.apache.hadoop.mapreduce.TaskType.MAP,\n              mapResourceReqt)));\n          LOG.info(\"mapResourceReqt:\"+mapResourceReqt);\n          if (mapResourceReqt \u003e supportedMaxContainerCapability) {\n            String diagMsg \u003d \"MAP capability required is more than the supported \" +\n            \"max container capability in the cluster. Killing the Job. mapResourceReqt: \" + \n            mapResourceReqt + \" maxContainerCapability:\" + supportedMaxContainerCapability;\n            LOG.info(diagMsg);\n            eventHandler.handle(new JobDiagnosticsUpdateEvent(\n                jobId, diagMsg));\n            eventHandler.handle(new JobEvent(jobId, JobEventType.JOB_KILL));\n          }\n        }\n        //set the rounded off memory\n        reqEvent.getCapability().setMemory(mapResourceReqt);\n        scheduledRequests.addMap(reqEvent);//maps are immediately scheduled\n      } else {\n        if (reduceResourceReqt \u003d\u003d 0) {\n          reduceResourceReqt \u003d reqEvent.getCapability().getMemory();\n          int minSlotMemSize \u003d getMinContainerCapability().getMemory();\n          //round off on slotsize\n          reduceResourceReqt \u003d (int) Math.ceil((float) \n              reduceResourceReqt/minSlotMemSize) * minSlotMemSize;\n          eventHandler.handle(new JobHistoryEvent(jobId, \n              new NormalizedResourceEvent(\n                  org.apache.hadoop.mapreduce.TaskType.REDUCE,\n              reduceResourceReqt)));\n          LOG.info(\"reduceResourceReqt:\"+reduceResourceReqt);\n          if (reduceResourceReqt \u003e supportedMaxContainerCapability) {\n            String diagMsg \u003d \"REDUCE capability required is more than the \" +\n            \t\t\"supported max container capability in the cluster. Killing the \" +\n            \t\t\"Job. reduceResourceReqt: \" + reduceResourceReqt +\n            \t\t\" maxContainerCapability:\" + supportedMaxContainerCapability;\n            LOG.info(diagMsg);\n            eventHandler.handle(new JobDiagnosticsUpdateEvent(\n                jobId, diagMsg));\n            eventHandler.handle(new JobEvent(jobId, JobEventType.JOB_KILL));\n          }\n        }\n        //set the rounded off memory\n        reqEvent.getCapability().setMemory(reduceResourceReqt);\n        if (reqEvent.getEarlierAttemptFailed()) {\n          //add to the front of queue for fail fast\n          pendingReduces.addFirst(new ContainerRequest(reqEvent, PRIORITY_REDUCE));\n        } else {\n          pendingReduces.add(new ContainerRequest(reqEvent, PRIORITY_REDUCE));\n          //reduces are added to pending and are slowly ramped up\n        }\n      }\n      \n    } else if (\n        event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_DEALLOCATE) {\n  \n      LOG.info(\"Processing the event \" + event.toString());\n\n      TaskAttemptId aId \u003d event.getAttemptID();\n      \n      boolean removed \u003d scheduledRequests.remove(aId);\n      if (!removed) {\n        ContainerId containerId \u003d assignedRequests.get(aId);\n        if (containerId !\u003d null) {\n          removed \u003d true;\n          assignedRequests.remove(aId);\n          containersReleased++;\n          release(containerId);\n        }\n      }\n      if (!removed) {\n        LOG.error(\"Could not deallocate container for task attemptId \" + \n            aId);\n      }\n    } else if (\n        event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_FAILED) {\n      ContainerFailedEvent fEv \u003d (ContainerFailedEvent) event;\n      String host \u003d getHost(fEv.getContMgrAddress());\n      containerFailedOnHost(host);\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java",
      "extendedDetails": {}
    },
    "08f8abf5639d39167952dc5120b44fe35c63ff7a": {
      "type": "Ymultichange(Yrename,Ymodifierchange,Ybodychange)",
      "commitMessage": "MAPREDUCE-3572. Moved AM event dispatcher to a separate thread for performance reasons. Contributed by Vinod K V. \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1227426 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "04/01/12 5:37 PM",
      "commitName": "08f8abf5639d39167952dc5120b44fe35c63ff7a",
      "commitAuthor": "Arun Murthy",
      "subchanges": [
        {
          "type": "Yrename",
          "commitMessage": "MAPREDUCE-3572. Moved AM event dispatcher to a separate thread for performance reasons. Contributed by Vinod K V. \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1227426 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "04/01/12 5:37 PM",
          "commitName": "08f8abf5639d39167952dc5120b44fe35c63ff7a",
          "commitAuthor": "Arun Murthy",
          "commitDateOld": "04/01/12 9:10 AM",
          "commitNameOld": "55e94dc5ef4171c4e7b57942f22ead9a01dd9012",
          "commitAuthorOld": "Vinod Kumar Vavilapalli",
          "daysBetweenCommits": 0.35,
          "commitsBetweenForRepo": 7,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,91 +1,87 @@\n-  public synchronized void handle(ContainerAllocatorEvent event) {\n+  protected synchronized void handleEvent(ContainerAllocatorEvent event) {\n     LOG.info(\"Processing the event \" + event.toString());\n     recalculateReduceSchedule \u003d true;\n     if (event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_REQ) {\n       ContainerRequestEvent reqEvent \u003d (ContainerRequestEvent) event;\n       if (reqEvent.getAttemptID().getTaskId().getTaskType().equals(TaskType.MAP)) {\n         if (mapResourceReqt \u003d\u003d 0) {\n           mapResourceReqt \u003d reqEvent.getCapability().getMemory();\n           int minSlotMemSize \u003d getMinContainerCapability().getMemory();\n           mapResourceReqt \u003d (int) Math.ceil((float) mapResourceReqt/minSlotMemSize)\n               * minSlotMemSize;\n-          JobID id \u003d TypeConverter.fromYarn(applicationId);\n-          JobId jobId \u003d TypeConverter.toYarn(id);\n-          eventHandler.handle(new JobHistoryEvent(jobId, \n+          eventHandler.handle(new JobHistoryEvent(getJob().getID(), \n               new NormalizedResourceEvent(org.apache.hadoop.mapreduce.TaskType.MAP,\n               mapResourceReqt)));\n           LOG.info(\"mapResourceReqt:\"+mapResourceReqt);\n           if (mapResourceReqt \u003e getMaxContainerCapability().getMemory()) {\n             String diagMsg \u003d \"MAP capability required is more than the supported \" +\n             \"max container capability in the cluster. Killing the Job. mapResourceReqt: \" + \n             mapResourceReqt + \" maxContainerCapability:\" + getMaxContainerCapability().getMemory();\n             LOG.info(diagMsg);\n             eventHandler.handle(new JobDiagnosticsUpdateEvent(\n                 getJob().getID(), diagMsg));\n             eventHandler.handle(new JobEvent(getJob().getID(), JobEventType.JOB_KILL));\n           }\n         }\n         //set the rounded off memory\n         reqEvent.getCapability().setMemory(mapResourceReqt);\n         scheduledRequests.addMap(reqEvent);//maps are immediately scheduled\n       } else {\n         if (reduceResourceReqt \u003d\u003d 0) {\n           reduceResourceReqt \u003d reqEvent.getCapability().getMemory();\n           int minSlotMemSize \u003d getMinContainerCapability().getMemory();\n           //round off on slotsize\n           reduceResourceReqt \u003d (int) Math.ceil((float) \n               reduceResourceReqt/minSlotMemSize) * minSlotMemSize;\n-          JobID id \u003d TypeConverter.fromYarn(applicationId);\n-          JobId jobId \u003d TypeConverter.toYarn(id);\n-          eventHandler.handle(new JobHistoryEvent(jobId, \n+          eventHandler.handle(new JobHistoryEvent(getJob().getID(), \n               new NormalizedResourceEvent(\n                   org.apache.hadoop.mapreduce.TaskType.REDUCE,\n               reduceResourceReqt)));\n           LOG.info(\"reduceResourceReqt:\"+reduceResourceReqt);\n           if (reduceResourceReqt \u003e getMaxContainerCapability().getMemory()) {\n             String diagMsg \u003d \"REDUCE capability required is more than the \" +\n             \t\t\"supported max container capability in the cluster. Killing the \" +\n             \t\t\"Job. reduceResourceReqt: \" + reduceResourceReqt +\n             \t\t\" maxContainerCapability:\" + getMaxContainerCapability().getMemory();\n             LOG.info(diagMsg);\n             eventHandler.handle(new JobDiagnosticsUpdateEvent(\n                 getJob().getID(), diagMsg));\n             eventHandler.handle(new JobEvent(getJob().getID(), JobEventType.JOB_KILL));\n           }\n         }\n         //set the rounded off memory\n         reqEvent.getCapability().setMemory(reduceResourceReqt);\n         if (reqEvent.getEarlierAttemptFailed()) {\n           //add to the front of queue for fail fast\n           pendingReduces.addFirst(new ContainerRequest(reqEvent, PRIORITY_REDUCE));\n         } else {\n           pendingReduces.add(new ContainerRequest(reqEvent, PRIORITY_REDUCE));\n           //reduces are added to pending and are slowly ramped up\n         }\n       }\n       \n     } else if (\n         event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_DEALLOCATE) {\n       TaskAttemptId aId \u003d event.getAttemptID();\n       \n       boolean removed \u003d scheduledRequests.remove(aId);\n       if (!removed) {\n         ContainerId containerId \u003d assignedRequests.get(aId);\n         if (containerId !\u003d null) {\n           removed \u003d true;\n           assignedRequests.remove(aId);\n           containersReleased++;\n           release(containerId);\n         }\n       }\n       if (!removed) {\n         LOG.error(\"Could not deallocate container for task attemptId \" + \n             aId);\n       }\n     } else if (\n         event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_FAILED) {\n       ContainerFailedEvent fEv \u003d (ContainerFailedEvent) event;\n       String host \u003d getHost(fEv.getContMgrAddress());\n       containerFailedOnHost(host);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected synchronized void handleEvent(ContainerAllocatorEvent event) {\n    LOG.info(\"Processing the event \" + event.toString());\n    recalculateReduceSchedule \u003d true;\n    if (event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_REQ) {\n      ContainerRequestEvent reqEvent \u003d (ContainerRequestEvent) event;\n      if (reqEvent.getAttemptID().getTaskId().getTaskType().equals(TaskType.MAP)) {\n        if (mapResourceReqt \u003d\u003d 0) {\n          mapResourceReqt \u003d reqEvent.getCapability().getMemory();\n          int minSlotMemSize \u003d getMinContainerCapability().getMemory();\n          mapResourceReqt \u003d (int) Math.ceil((float) mapResourceReqt/minSlotMemSize)\n              * minSlotMemSize;\n          eventHandler.handle(new JobHistoryEvent(getJob().getID(), \n              new NormalizedResourceEvent(org.apache.hadoop.mapreduce.TaskType.MAP,\n              mapResourceReqt)));\n          LOG.info(\"mapResourceReqt:\"+mapResourceReqt);\n          if (mapResourceReqt \u003e getMaxContainerCapability().getMemory()) {\n            String diagMsg \u003d \"MAP capability required is more than the supported \" +\n            \"max container capability in the cluster. Killing the Job. mapResourceReqt: \" + \n            mapResourceReqt + \" maxContainerCapability:\" + getMaxContainerCapability().getMemory();\n            LOG.info(diagMsg);\n            eventHandler.handle(new JobDiagnosticsUpdateEvent(\n                getJob().getID(), diagMsg));\n            eventHandler.handle(new JobEvent(getJob().getID(), JobEventType.JOB_KILL));\n          }\n        }\n        //set the rounded off memory\n        reqEvent.getCapability().setMemory(mapResourceReqt);\n        scheduledRequests.addMap(reqEvent);//maps are immediately scheduled\n      } else {\n        if (reduceResourceReqt \u003d\u003d 0) {\n          reduceResourceReqt \u003d reqEvent.getCapability().getMemory();\n          int minSlotMemSize \u003d getMinContainerCapability().getMemory();\n          //round off on slotsize\n          reduceResourceReqt \u003d (int) Math.ceil((float) \n              reduceResourceReqt/minSlotMemSize) * minSlotMemSize;\n          eventHandler.handle(new JobHistoryEvent(getJob().getID(), \n              new NormalizedResourceEvent(\n                  org.apache.hadoop.mapreduce.TaskType.REDUCE,\n              reduceResourceReqt)));\n          LOG.info(\"reduceResourceReqt:\"+reduceResourceReqt);\n          if (reduceResourceReqt \u003e getMaxContainerCapability().getMemory()) {\n            String diagMsg \u003d \"REDUCE capability required is more than the \" +\n            \t\t\"supported max container capability in the cluster. Killing the \" +\n            \t\t\"Job. reduceResourceReqt: \" + reduceResourceReqt +\n            \t\t\" maxContainerCapability:\" + getMaxContainerCapability().getMemory();\n            LOG.info(diagMsg);\n            eventHandler.handle(new JobDiagnosticsUpdateEvent(\n                getJob().getID(), diagMsg));\n            eventHandler.handle(new JobEvent(getJob().getID(), JobEventType.JOB_KILL));\n          }\n        }\n        //set the rounded off memory\n        reqEvent.getCapability().setMemory(reduceResourceReqt);\n        if (reqEvent.getEarlierAttemptFailed()) {\n          //add to the front of queue for fail fast\n          pendingReduces.addFirst(new ContainerRequest(reqEvent, PRIORITY_REDUCE));\n        } else {\n          pendingReduces.add(new ContainerRequest(reqEvent, PRIORITY_REDUCE));\n          //reduces are added to pending and are slowly ramped up\n        }\n      }\n      \n    } else if (\n        event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_DEALLOCATE) {\n      TaskAttemptId aId \u003d event.getAttemptID();\n      \n      boolean removed \u003d scheduledRequests.remove(aId);\n      if (!removed) {\n        ContainerId containerId \u003d assignedRequests.get(aId);\n        if (containerId !\u003d null) {\n          removed \u003d true;\n          assignedRequests.remove(aId);\n          containersReleased++;\n          release(containerId);\n        }\n      }\n      if (!removed) {\n        LOG.error(\"Could not deallocate container for task attemptId \" + \n            aId);\n      }\n    } else if (\n        event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_FAILED) {\n      ContainerFailedEvent fEv \u003d (ContainerFailedEvent) event;\n      String host \u003d getHost(fEv.getContMgrAddress());\n      containerFailedOnHost(host);\n    }\n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java",
          "extendedDetails": {
            "oldValue": "handle",
            "newValue": "handleEvent"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "MAPREDUCE-3572. Moved AM event dispatcher to a separate thread for performance reasons. Contributed by Vinod K V. \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1227426 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "04/01/12 5:37 PM",
          "commitName": "08f8abf5639d39167952dc5120b44fe35c63ff7a",
          "commitAuthor": "Arun Murthy",
          "commitDateOld": "04/01/12 9:10 AM",
          "commitNameOld": "55e94dc5ef4171c4e7b57942f22ead9a01dd9012",
          "commitAuthorOld": "Vinod Kumar Vavilapalli",
          "daysBetweenCommits": 0.35,
          "commitsBetweenForRepo": 7,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,91 +1,87 @@\n-  public synchronized void handle(ContainerAllocatorEvent event) {\n+  protected synchronized void handleEvent(ContainerAllocatorEvent event) {\n     LOG.info(\"Processing the event \" + event.toString());\n     recalculateReduceSchedule \u003d true;\n     if (event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_REQ) {\n       ContainerRequestEvent reqEvent \u003d (ContainerRequestEvent) event;\n       if (reqEvent.getAttemptID().getTaskId().getTaskType().equals(TaskType.MAP)) {\n         if (mapResourceReqt \u003d\u003d 0) {\n           mapResourceReqt \u003d reqEvent.getCapability().getMemory();\n           int minSlotMemSize \u003d getMinContainerCapability().getMemory();\n           mapResourceReqt \u003d (int) Math.ceil((float) mapResourceReqt/minSlotMemSize)\n               * minSlotMemSize;\n-          JobID id \u003d TypeConverter.fromYarn(applicationId);\n-          JobId jobId \u003d TypeConverter.toYarn(id);\n-          eventHandler.handle(new JobHistoryEvent(jobId, \n+          eventHandler.handle(new JobHistoryEvent(getJob().getID(), \n               new NormalizedResourceEvent(org.apache.hadoop.mapreduce.TaskType.MAP,\n               mapResourceReqt)));\n           LOG.info(\"mapResourceReqt:\"+mapResourceReqt);\n           if (mapResourceReqt \u003e getMaxContainerCapability().getMemory()) {\n             String diagMsg \u003d \"MAP capability required is more than the supported \" +\n             \"max container capability in the cluster. Killing the Job. mapResourceReqt: \" + \n             mapResourceReqt + \" maxContainerCapability:\" + getMaxContainerCapability().getMemory();\n             LOG.info(diagMsg);\n             eventHandler.handle(new JobDiagnosticsUpdateEvent(\n                 getJob().getID(), diagMsg));\n             eventHandler.handle(new JobEvent(getJob().getID(), JobEventType.JOB_KILL));\n           }\n         }\n         //set the rounded off memory\n         reqEvent.getCapability().setMemory(mapResourceReqt);\n         scheduledRequests.addMap(reqEvent);//maps are immediately scheduled\n       } else {\n         if (reduceResourceReqt \u003d\u003d 0) {\n           reduceResourceReqt \u003d reqEvent.getCapability().getMemory();\n           int minSlotMemSize \u003d getMinContainerCapability().getMemory();\n           //round off on slotsize\n           reduceResourceReqt \u003d (int) Math.ceil((float) \n               reduceResourceReqt/minSlotMemSize) * minSlotMemSize;\n-          JobID id \u003d TypeConverter.fromYarn(applicationId);\n-          JobId jobId \u003d TypeConverter.toYarn(id);\n-          eventHandler.handle(new JobHistoryEvent(jobId, \n+          eventHandler.handle(new JobHistoryEvent(getJob().getID(), \n               new NormalizedResourceEvent(\n                   org.apache.hadoop.mapreduce.TaskType.REDUCE,\n               reduceResourceReqt)));\n           LOG.info(\"reduceResourceReqt:\"+reduceResourceReqt);\n           if (reduceResourceReqt \u003e getMaxContainerCapability().getMemory()) {\n             String diagMsg \u003d \"REDUCE capability required is more than the \" +\n             \t\t\"supported max container capability in the cluster. Killing the \" +\n             \t\t\"Job. reduceResourceReqt: \" + reduceResourceReqt +\n             \t\t\" maxContainerCapability:\" + getMaxContainerCapability().getMemory();\n             LOG.info(diagMsg);\n             eventHandler.handle(new JobDiagnosticsUpdateEvent(\n                 getJob().getID(), diagMsg));\n             eventHandler.handle(new JobEvent(getJob().getID(), JobEventType.JOB_KILL));\n           }\n         }\n         //set the rounded off memory\n         reqEvent.getCapability().setMemory(reduceResourceReqt);\n         if (reqEvent.getEarlierAttemptFailed()) {\n           //add to the front of queue for fail fast\n           pendingReduces.addFirst(new ContainerRequest(reqEvent, PRIORITY_REDUCE));\n         } else {\n           pendingReduces.add(new ContainerRequest(reqEvent, PRIORITY_REDUCE));\n           //reduces are added to pending and are slowly ramped up\n         }\n       }\n       \n     } else if (\n         event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_DEALLOCATE) {\n       TaskAttemptId aId \u003d event.getAttemptID();\n       \n       boolean removed \u003d scheduledRequests.remove(aId);\n       if (!removed) {\n         ContainerId containerId \u003d assignedRequests.get(aId);\n         if (containerId !\u003d null) {\n           removed \u003d true;\n           assignedRequests.remove(aId);\n           containersReleased++;\n           release(containerId);\n         }\n       }\n       if (!removed) {\n         LOG.error(\"Could not deallocate container for task attemptId \" + \n             aId);\n       }\n     } else if (\n         event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_FAILED) {\n       ContainerFailedEvent fEv \u003d (ContainerFailedEvent) event;\n       String host \u003d getHost(fEv.getContMgrAddress());\n       containerFailedOnHost(host);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected synchronized void handleEvent(ContainerAllocatorEvent event) {\n    LOG.info(\"Processing the event \" + event.toString());\n    recalculateReduceSchedule \u003d true;\n    if (event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_REQ) {\n      ContainerRequestEvent reqEvent \u003d (ContainerRequestEvent) event;\n      if (reqEvent.getAttemptID().getTaskId().getTaskType().equals(TaskType.MAP)) {\n        if (mapResourceReqt \u003d\u003d 0) {\n          mapResourceReqt \u003d reqEvent.getCapability().getMemory();\n          int minSlotMemSize \u003d getMinContainerCapability().getMemory();\n          mapResourceReqt \u003d (int) Math.ceil((float) mapResourceReqt/minSlotMemSize)\n              * minSlotMemSize;\n          eventHandler.handle(new JobHistoryEvent(getJob().getID(), \n              new NormalizedResourceEvent(org.apache.hadoop.mapreduce.TaskType.MAP,\n              mapResourceReqt)));\n          LOG.info(\"mapResourceReqt:\"+mapResourceReqt);\n          if (mapResourceReqt \u003e getMaxContainerCapability().getMemory()) {\n            String diagMsg \u003d \"MAP capability required is more than the supported \" +\n            \"max container capability in the cluster. Killing the Job. mapResourceReqt: \" + \n            mapResourceReqt + \" maxContainerCapability:\" + getMaxContainerCapability().getMemory();\n            LOG.info(diagMsg);\n            eventHandler.handle(new JobDiagnosticsUpdateEvent(\n                getJob().getID(), diagMsg));\n            eventHandler.handle(new JobEvent(getJob().getID(), JobEventType.JOB_KILL));\n          }\n        }\n        //set the rounded off memory\n        reqEvent.getCapability().setMemory(mapResourceReqt);\n        scheduledRequests.addMap(reqEvent);//maps are immediately scheduled\n      } else {\n        if (reduceResourceReqt \u003d\u003d 0) {\n          reduceResourceReqt \u003d reqEvent.getCapability().getMemory();\n          int minSlotMemSize \u003d getMinContainerCapability().getMemory();\n          //round off on slotsize\n          reduceResourceReqt \u003d (int) Math.ceil((float) \n              reduceResourceReqt/minSlotMemSize) * minSlotMemSize;\n          eventHandler.handle(new JobHistoryEvent(getJob().getID(), \n              new NormalizedResourceEvent(\n                  org.apache.hadoop.mapreduce.TaskType.REDUCE,\n              reduceResourceReqt)));\n          LOG.info(\"reduceResourceReqt:\"+reduceResourceReqt);\n          if (reduceResourceReqt \u003e getMaxContainerCapability().getMemory()) {\n            String diagMsg \u003d \"REDUCE capability required is more than the \" +\n            \t\t\"supported max container capability in the cluster. Killing the \" +\n            \t\t\"Job. reduceResourceReqt: \" + reduceResourceReqt +\n            \t\t\" maxContainerCapability:\" + getMaxContainerCapability().getMemory();\n            LOG.info(diagMsg);\n            eventHandler.handle(new JobDiagnosticsUpdateEvent(\n                getJob().getID(), diagMsg));\n            eventHandler.handle(new JobEvent(getJob().getID(), JobEventType.JOB_KILL));\n          }\n        }\n        //set the rounded off memory\n        reqEvent.getCapability().setMemory(reduceResourceReqt);\n        if (reqEvent.getEarlierAttemptFailed()) {\n          //add to the front of queue for fail fast\n          pendingReduces.addFirst(new ContainerRequest(reqEvent, PRIORITY_REDUCE));\n        } else {\n          pendingReduces.add(new ContainerRequest(reqEvent, PRIORITY_REDUCE));\n          //reduces are added to pending and are slowly ramped up\n        }\n      }\n      \n    } else if (\n        event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_DEALLOCATE) {\n      TaskAttemptId aId \u003d event.getAttemptID();\n      \n      boolean removed \u003d scheduledRequests.remove(aId);\n      if (!removed) {\n        ContainerId containerId \u003d assignedRequests.get(aId);\n        if (containerId !\u003d null) {\n          removed \u003d true;\n          assignedRequests.remove(aId);\n          containersReleased++;\n          release(containerId);\n        }\n      }\n      if (!removed) {\n        LOG.error(\"Could not deallocate container for task attemptId \" + \n            aId);\n      }\n    } else if (\n        event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_FAILED) {\n      ContainerFailedEvent fEv \u003d (ContainerFailedEvent) event;\n      String host \u003d getHost(fEv.getContMgrAddress());\n      containerFailedOnHost(host);\n    }\n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java",
          "extendedDetails": {
            "oldValue": "[public, synchronized]",
            "newValue": "[protected, synchronized]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "MAPREDUCE-3572. Moved AM event dispatcher to a separate thread for performance reasons. Contributed by Vinod K V. \n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1227426 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "04/01/12 5:37 PM",
          "commitName": "08f8abf5639d39167952dc5120b44fe35c63ff7a",
          "commitAuthor": "Arun Murthy",
          "commitDateOld": "04/01/12 9:10 AM",
          "commitNameOld": "55e94dc5ef4171c4e7b57942f22ead9a01dd9012",
          "commitAuthorOld": "Vinod Kumar Vavilapalli",
          "daysBetweenCommits": 0.35,
          "commitsBetweenForRepo": 7,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,91 +1,87 @@\n-  public synchronized void handle(ContainerAllocatorEvent event) {\n+  protected synchronized void handleEvent(ContainerAllocatorEvent event) {\n     LOG.info(\"Processing the event \" + event.toString());\n     recalculateReduceSchedule \u003d true;\n     if (event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_REQ) {\n       ContainerRequestEvent reqEvent \u003d (ContainerRequestEvent) event;\n       if (reqEvent.getAttemptID().getTaskId().getTaskType().equals(TaskType.MAP)) {\n         if (mapResourceReqt \u003d\u003d 0) {\n           mapResourceReqt \u003d reqEvent.getCapability().getMemory();\n           int minSlotMemSize \u003d getMinContainerCapability().getMemory();\n           mapResourceReqt \u003d (int) Math.ceil((float) mapResourceReqt/minSlotMemSize)\n               * minSlotMemSize;\n-          JobID id \u003d TypeConverter.fromYarn(applicationId);\n-          JobId jobId \u003d TypeConverter.toYarn(id);\n-          eventHandler.handle(new JobHistoryEvent(jobId, \n+          eventHandler.handle(new JobHistoryEvent(getJob().getID(), \n               new NormalizedResourceEvent(org.apache.hadoop.mapreduce.TaskType.MAP,\n               mapResourceReqt)));\n           LOG.info(\"mapResourceReqt:\"+mapResourceReqt);\n           if (mapResourceReqt \u003e getMaxContainerCapability().getMemory()) {\n             String diagMsg \u003d \"MAP capability required is more than the supported \" +\n             \"max container capability in the cluster. Killing the Job. mapResourceReqt: \" + \n             mapResourceReqt + \" maxContainerCapability:\" + getMaxContainerCapability().getMemory();\n             LOG.info(diagMsg);\n             eventHandler.handle(new JobDiagnosticsUpdateEvent(\n                 getJob().getID(), diagMsg));\n             eventHandler.handle(new JobEvent(getJob().getID(), JobEventType.JOB_KILL));\n           }\n         }\n         //set the rounded off memory\n         reqEvent.getCapability().setMemory(mapResourceReqt);\n         scheduledRequests.addMap(reqEvent);//maps are immediately scheduled\n       } else {\n         if (reduceResourceReqt \u003d\u003d 0) {\n           reduceResourceReqt \u003d reqEvent.getCapability().getMemory();\n           int minSlotMemSize \u003d getMinContainerCapability().getMemory();\n           //round off on slotsize\n           reduceResourceReqt \u003d (int) Math.ceil((float) \n               reduceResourceReqt/minSlotMemSize) * minSlotMemSize;\n-          JobID id \u003d TypeConverter.fromYarn(applicationId);\n-          JobId jobId \u003d TypeConverter.toYarn(id);\n-          eventHandler.handle(new JobHistoryEvent(jobId, \n+          eventHandler.handle(new JobHistoryEvent(getJob().getID(), \n               new NormalizedResourceEvent(\n                   org.apache.hadoop.mapreduce.TaskType.REDUCE,\n               reduceResourceReqt)));\n           LOG.info(\"reduceResourceReqt:\"+reduceResourceReqt);\n           if (reduceResourceReqt \u003e getMaxContainerCapability().getMemory()) {\n             String diagMsg \u003d \"REDUCE capability required is more than the \" +\n             \t\t\"supported max container capability in the cluster. Killing the \" +\n             \t\t\"Job. reduceResourceReqt: \" + reduceResourceReqt +\n             \t\t\" maxContainerCapability:\" + getMaxContainerCapability().getMemory();\n             LOG.info(diagMsg);\n             eventHandler.handle(new JobDiagnosticsUpdateEvent(\n                 getJob().getID(), diagMsg));\n             eventHandler.handle(new JobEvent(getJob().getID(), JobEventType.JOB_KILL));\n           }\n         }\n         //set the rounded off memory\n         reqEvent.getCapability().setMemory(reduceResourceReqt);\n         if (reqEvent.getEarlierAttemptFailed()) {\n           //add to the front of queue for fail fast\n           pendingReduces.addFirst(new ContainerRequest(reqEvent, PRIORITY_REDUCE));\n         } else {\n           pendingReduces.add(new ContainerRequest(reqEvent, PRIORITY_REDUCE));\n           //reduces are added to pending and are slowly ramped up\n         }\n       }\n       \n     } else if (\n         event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_DEALLOCATE) {\n       TaskAttemptId aId \u003d event.getAttemptID();\n       \n       boolean removed \u003d scheduledRequests.remove(aId);\n       if (!removed) {\n         ContainerId containerId \u003d assignedRequests.get(aId);\n         if (containerId !\u003d null) {\n           removed \u003d true;\n           assignedRequests.remove(aId);\n           containersReleased++;\n           release(containerId);\n         }\n       }\n       if (!removed) {\n         LOG.error(\"Could not deallocate container for task attemptId \" + \n             aId);\n       }\n     } else if (\n         event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_FAILED) {\n       ContainerFailedEvent fEv \u003d (ContainerFailedEvent) event;\n       String host \u003d getHost(fEv.getContMgrAddress());\n       containerFailedOnHost(host);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected synchronized void handleEvent(ContainerAllocatorEvent event) {\n    LOG.info(\"Processing the event \" + event.toString());\n    recalculateReduceSchedule \u003d true;\n    if (event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_REQ) {\n      ContainerRequestEvent reqEvent \u003d (ContainerRequestEvent) event;\n      if (reqEvent.getAttemptID().getTaskId().getTaskType().equals(TaskType.MAP)) {\n        if (mapResourceReqt \u003d\u003d 0) {\n          mapResourceReqt \u003d reqEvent.getCapability().getMemory();\n          int minSlotMemSize \u003d getMinContainerCapability().getMemory();\n          mapResourceReqt \u003d (int) Math.ceil((float) mapResourceReqt/minSlotMemSize)\n              * minSlotMemSize;\n          eventHandler.handle(new JobHistoryEvent(getJob().getID(), \n              new NormalizedResourceEvent(org.apache.hadoop.mapreduce.TaskType.MAP,\n              mapResourceReqt)));\n          LOG.info(\"mapResourceReqt:\"+mapResourceReqt);\n          if (mapResourceReqt \u003e getMaxContainerCapability().getMemory()) {\n            String diagMsg \u003d \"MAP capability required is more than the supported \" +\n            \"max container capability in the cluster. Killing the Job. mapResourceReqt: \" + \n            mapResourceReqt + \" maxContainerCapability:\" + getMaxContainerCapability().getMemory();\n            LOG.info(diagMsg);\n            eventHandler.handle(new JobDiagnosticsUpdateEvent(\n                getJob().getID(), diagMsg));\n            eventHandler.handle(new JobEvent(getJob().getID(), JobEventType.JOB_KILL));\n          }\n        }\n        //set the rounded off memory\n        reqEvent.getCapability().setMemory(mapResourceReqt);\n        scheduledRequests.addMap(reqEvent);//maps are immediately scheduled\n      } else {\n        if (reduceResourceReqt \u003d\u003d 0) {\n          reduceResourceReqt \u003d reqEvent.getCapability().getMemory();\n          int minSlotMemSize \u003d getMinContainerCapability().getMemory();\n          //round off on slotsize\n          reduceResourceReqt \u003d (int) Math.ceil((float) \n              reduceResourceReqt/minSlotMemSize) * minSlotMemSize;\n          eventHandler.handle(new JobHistoryEvent(getJob().getID(), \n              new NormalizedResourceEvent(\n                  org.apache.hadoop.mapreduce.TaskType.REDUCE,\n              reduceResourceReqt)));\n          LOG.info(\"reduceResourceReqt:\"+reduceResourceReqt);\n          if (reduceResourceReqt \u003e getMaxContainerCapability().getMemory()) {\n            String diagMsg \u003d \"REDUCE capability required is more than the \" +\n            \t\t\"supported max container capability in the cluster. Killing the \" +\n            \t\t\"Job. reduceResourceReqt: \" + reduceResourceReqt +\n            \t\t\" maxContainerCapability:\" + getMaxContainerCapability().getMemory();\n            LOG.info(diagMsg);\n            eventHandler.handle(new JobDiagnosticsUpdateEvent(\n                getJob().getID(), diagMsg));\n            eventHandler.handle(new JobEvent(getJob().getID(), JobEventType.JOB_KILL));\n          }\n        }\n        //set the rounded off memory\n        reqEvent.getCapability().setMemory(reduceResourceReqt);\n        if (reqEvent.getEarlierAttemptFailed()) {\n          //add to the front of queue for fail fast\n          pendingReduces.addFirst(new ContainerRequest(reqEvent, PRIORITY_REDUCE));\n        } else {\n          pendingReduces.add(new ContainerRequest(reqEvent, PRIORITY_REDUCE));\n          //reduces are added to pending and are slowly ramped up\n        }\n      }\n      \n    } else if (\n        event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_DEALLOCATE) {\n      TaskAttemptId aId \u003d event.getAttemptID();\n      \n      boolean removed \u003d scheduledRequests.remove(aId);\n      if (!removed) {\n        ContainerId containerId \u003d assignedRequests.get(aId);\n        if (containerId !\u003d null) {\n          removed \u003d true;\n          assignedRequests.remove(aId);\n          containersReleased++;\n          release(containerId);\n        }\n      }\n      if (!removed) {\n        LOG.error(\"Could not deallocate container for task attemptId \" + \n            aId);\n      }\n    } else if (\n        event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_FAILED) {\n      ContainerFailedEvent fEv \u003d (ContainerFailedEvent) event;\n      String host \u003d getHost(fEv.getContMgrAddress());\n      containerFailedOnHost(host);\n    }\n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java",
          "extendedDetails": {}
        }
      ]
    },
    "fffdf661e30afd10331d2153ff052c141b7ebe4b": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-2821. Added missing fields (resourcePerMap \u0026 resourcePerReduce) to JobSummary logs. Contributed by Mahadev Konar.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1188528 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/10/11 11:26 PM",
      "commitName": "fffdf661e30afd10331d2153ff052c141b7ebe4b",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "20/10/11 4:45 AM",
      "commitNameOld": "df2991c0cbc3f35c2640b93680667507c4f810dd",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 4.78,
      "commitsBetweenForRepo": 35,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,76 +1,91 @@\n   public synchronized void handle(ContainerAllocatorEvent event) {\n     LOG.info(\"Processing the event \" + event.toString());\n     recalculateReduceSchedule \u003d true;\n     if (event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_REQ) {\n       ContainerRequestEvent reqEvent \u003d (ContainerRequestEvent) event;\n       if (reqEvent.getAttemptID().getTaskId().getTaskType().equals(TaskType.MAP)) {\n         if (mapResourceReqt \u003d\u003d 0) {\n           mapResourceReqt \u003d reqEvent.getCapability().getMemory();\n           int minSlotMemSize \u003d getMinContainerCapability().getMemory();\n-          mapResourceReqt \u003d (int) Math.ceil((float) mapResourceReqt/minSlotMemSize) * minSlotMemSize;\n+          mapResourceReqt \u003d (int) Math.ceil((float) mapResourceReqt/minSlotMemSize)\n+              * minSlotMemSize;\n+          JobID id \u003d TypeConverter.fromYarn(applicationId);\n+          JobId jobId \u003d TypeConverter.toYarn(id);\n+          eventHandler.handle(new JobHistoryEvent(jobId, \n+              new NormalizedResourceEvent(org.apache.hadoop.mapreduce.TaskType.MAP,\n+              mapResourceReqt)));\n           LOG.info(\"mapResourceReqt:\"+mapResourceReqt);\n           if (mapResourceReqt \u003e getMaxContainerCapability().getMemory()) {\n             String diagMsg \u003d \"MAP capability required is more than the supported \" +\n             \"max container capability in the cluster. Killing the Job. mapResourceReqt: \" + \n             mapResourceReqt + \" maxContainerCapability:\" + getMaxContainerCapability().getMemory();\n             LOG.info(diagMsg);\n             eventHandler.handle(new JobDiagnosticsUpdateEvent(\n                 getJob().getID(), diagMsg));\n             eventHandler.handle(new JobEvent(getJob().getID(), JobEventType.JOB_KILL));\n           }\n         }\n         //set the rounded off memory\n         reqEvent.getCapability().setMemory(mapResourceReqt);\n         scheduledRequests.addMap(reqEvent);//maps are immediately scheduled\n       } else {\n         if (reduceResourceReqt \u003d\u003d 0) {\n           reduceResourceReqt \u003d reqEvent.getCapability().getMemory();\n           int minSlotMemSize \u003d getMinContainerCapability().getMemory();\n           //round off on slotsize\n-          reduceResourceReqt \u003d (int) Math.ceil((float) reduceResourceReqt/minSlotMemSize) * minSlotMemSize;\n+          reduceResourceReqt \u003d (int) Math.ceil((float) \n+              reduceResourceReqt/minSlotMemSize) * minSlotMemSize;\n+          JobID id \u003d TypeConverter.fromYarn(applicationId);\n+          JobId jobId \u003d TypeConverter.toYarn(id);\n+          eventHandler.handle(new JobHistoryEvent(jobId, \n+              new NormalizedResourceEvent(\n+                  org.apache.hadoop.mapreduce.TaskType.REDUCE,\n+              reduceResourceReqt)));\n           LOG.info(\"reduceResourceReqt:\"+reduceResourceReqt);\n           if (reduceResourceReqt \u003e getMaxContainerCapability().getMemory()) {\n-            String diagMsg \u003d \"REDUCE capability required is more than the supported \" +\n-            \"max container capability in the cluster. Killing the Job. reduceResourceReqt: \" + \n-            reduceResourceReqt + \" maxContainerCapability:\" + getMaxContainerCapability().getMemory();\n+            String diagMsg \u003d \"REDUCE capability required is more than the \" +\n+            \t\t\"supported max container capability in the cluster. Killing the \" +\n+            \t\t\"Job. reduceResourceReqt: \" + reduceResourceReqt +\n+            \t\t\" maxContainerCapability:\" + getMaxContainerCapability().getMemory();\n             LOG.info(diagMsg);\n             eventHandler.handle(new JobDiagnosticsUpdateEvent(\n                 getJob().getID(), diagMsg));\n             eventHandler.handle(new JobEvent(getJob().getID(), JobEventType.JOB_KILL));\n           }\n         }\n         //set the rounded off memory\n         reqEvent.getCapability().setMemory(reduceResourceReqt);\n         if (reqEvent.getEarlierAttemptFailed()) {\n           //add to the front of queue for fail fast\n           pendingReduces.addFirst(new ContainerRequest(reqEvent, PRIORITY_REDUCE));\n         } else {\n-          pendingReduces.add(new ContainerRequest(reqEvent, PRIORITY_REDUCE));//reduces are added to pending and are slowly ramped up\n+          pendingReduces.add(new ContainerRequest(reqEvent, PRIORITY_REDUCE));\n+          //reduces are added to pending and are slowly ramped up\n         }\n       }\n       \n     } else if (\n         event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_DEALLOCATE) {\n       TaskAttemptId aId \u003d event.getAttemptID();\n       \n       boolean removed \u003d scheduledRequests.remove(aId);\n       if (!removed) {\n         ContainerId containerId \u003d assignedRequests.get(aId);\n         if (containerId !\u003d null) {\n           removed \u003d true;\n           assignedRequests.remove(aId);\n           containersReleased++;\n           release(containerId);\n         }\n       }\n       if (!removed) {\n         LOG.error(\"Could not deallocate container for task attemptId \" + \n             aId);\n       }\n     } else if (\n         event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_FAILED) {\n       ContainerFailedEvent fEv \u003d (ContainerFailedEvent) event;\n       String host \u003d getHost(fEv.getContMgrAddress());\n       containerFailedOnHost(host);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void handle(ContainerAllocatorEvent event) {\n    LOG.info(\"Processing the event \" + event.toString());\n    recalculateReduceSchedule \u003d true;\n    if (event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_REQ) {\n      ContainerRequestEvent reqEvent \u003d (ContainerRequestEvent) event;\n      if (reqEvent.getAttemptID().getTaskId().getTaskType().equals(TaskType.MAP)) {\n        if (mapResourceReqt \u003d\u003d 0) {\n          mapResourceReqt \u003d reqEvent.getCapability().getMemory();\n          int minSlotMemSize \u003d getMinContainerCapability().getMemory();\n          mapResourceReqt \u003d (int) Math.ceil((float) mapResourceReqt/minSlotMemSize)\n              * minSlotMemSize;\n          JobID id \u003d TypeConverter.fromYarn(applicationId);\n          JobId jobId \u003d TypeConverter.toYarn(id);\n          eventHandler.handle(new JobHistoryEvent(jobId, \n              new NormalizedResourceEvent(org.apache.hadoop.mapreduce.TaskType.MAP,\n              mapResourceReqt)));\n          LOG.info(\"mapResourceReqt:\"+mapResourceReqt);\n          if (mapResourceReqt \u003e getMaxContainerCapability().getMemory()) {\n            String diagMsg \u003d \"MAP capability required is more than the supported \" +\n            \"max container capability in the cluster. Killing the Job. mapResourceReqt: \" + \n            mapResourceReqt + \" maxContainerCapability:\" + getMaxContainerCapability().getMemory();\n            LOG.info(diagMsg);\n            eventHandler.handle(new JobDiagnosticsUpdateEvent(\n                getJob().getID(), diagMsg));\n            eventHandler.handle(new JobEvent(getJob().getID(), JobEventType.JOB_KILL));\n          }\n        }\n        //set the rounded off memory\n        reqEvent.getCapability().setMemory(mapResourceReqt);\n        scheduledRequests.addMap(reqEvent);//maps are immediately scheduled\n      } else {\n        if (reduceResourceReqt \u003d\u003d 0) {\n          reduceResourceReqt \u003d reqEvent.getCapability().getMemory();\n          int minSlotMemSize \u003d getMinContainerCapability().getMemory();\n          //round off on slotsize\n          reduceResourceReqt \u003d (int) Math.ceil((float) \n              reduceResourceReqt/minSlotMemSize) * minSlotMemSize;\n          JobID id \u003d TypeConverter.fromYarn(applicationId);\n          JobId jobId \u003d TypeConverter.toYarn(id);\n          eventHandler.handle(new JobHistoryEvent(jobId, \n              new NormalizedResourceEvent(\n                  org.apache.hadoop.mapreduce.TaskType.REDUCE,\n              reduceResourceReqt)));\n          LOG.info(\"reduceResourceReqt:\"+reduceResourceReqt);\n          if (reduceResourceReqt \u003e getMaxContainerCapability().getMemory()) {\n            String diagMsg \u003d \"REDUCE capability required is more than the \" +\n            \t\t\"supported max container capability in the cluster. Killing the \" +\n            \t\t\"Job. reduceResourceReqt: \" + reduceResourceReqt +\n            \t\t\" maxContainerCapability:\" + getMaxContainerCapability().getMemory();\n            LOG.info(diagMsg);\n            eventHandler.handle(new JobDiagnosticsUpdateEvent(\n                getJob().getID(), diagMsg));\n            eventHandler.handle(new JobEvent(getJob().getID(), JobEventType.JOB_KILL));\n          }\n        }\n        //set the rounded off memory\n        reqEvent.getCapability().setMemory(reduceResourceReqt);\n        if (reqEvent.getEarlierAttemptFailed()) {\n          //add to the front of queue for fail fast\n          pendingReduces.addFirst(new ContainerRequest(reqEvent, PRIORITY_REDUCE));\n        } else {\n          pendingReduces.add(new ContainerRequest(reqEvent, PRIORITY_REDUCE));\n          //reduces are added to pending and are slowly ramped up\n        }\n      }\n      \n    } else if (\n        event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_DEALLOCATE) {\n      TaskAttemptId aId \u003d event.getAttemptID();\n      \n      boolean removed \u003d scheduledRequests.remove(aId);\n      if (!removed) {\n        ContainerId containerId \u003d assignedRequests.get(aId);\n        if (containerId !\u003d null) {\n          removed \u003d true;\n          assignedRequests.remove(aId);\n          containersReleased++;\n          release(containerId);\n        }\n      }\n      if (!removed) {\n        LOG.error(\"Could not deallocate container for task attemptId \" + \n            aId);\n      }\n    } else if (\n        event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_FAILED) {\n      ContainerFailedEvent fEv \u003d (ContainerFailedEvent) event;\n      String host \u003d getHost(fEv.getContMgrAddress());\n      containerFailedOnHost(host);\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public synchronized void handle(ContainerAllocatorEvent event) {\n    LOG.info(\"Processing the event \" + event.toString());\n    recalculateReduceSchedule \u003d true;\n    if (event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_REQ) {\n      ContainerRequestEvent reqEvent \u003d (ContainerRequestEvent) event;\n      if (reqEvent.getAttemptID().getTaskId().getTaskType().equals(TaskType.MAP)) {\n        if (mapResourceReqt \u003d\u003d 0) {\n          mapResourceReqt \u003d reqEvent.getCapability().getMemory();\n          int minSlotMemSize \u003d getMinContainerCapability().getMemory();\n          mapResourceReqt \u003d (int) Math.ceil((float) mapResourceReqt/minSlotMemSize) * minSlotMemSize;\n          LOG.info(\"mapResourceReqt:\"+mapResourceReqt);\n          if (mapResourceReqt \u003e getMaxContainerCapability().getMemory()) {\n            String diagMsg \u003d \"MAP capability required is more than the supported \" +\n            \"max container capability in the cluster. Killing the Job. mapResourceReqt: \" + \n            mapResourceReqt + \" maxContainerCapability:\" + getMaxContainerCapability().getMemory();\n            LOG.info(diagMsg);\n            eventHandler.handle(new JobDiagnosticsUpdateEvent(\n                getJob().getID(), diagMsg));\n            eventHandler.handle(new JobEvent(getJob().getID(), JobEventType.JOB_KILL));\n          }\n        }\n        //set the rounded off memory\n        reqEvent.getCapability().setMemory(mapResourceReqt);\n        scheduledRequests.addMap(reqEvent);//maps are immediately scheduled\n      } else {\n        if (reduceResourceReqt \u003d\u003d 0) {\n          reduceResourceReqt \u003d reqEvent.getCapability().getMemory();\n          int minSlotMemSize \u003d getMinContainerCapability().getMemory();\n          //round off on slotsize\n          reduceResourceReqt \u003d (int) Math.ceil((float) reduceResourceReqt/minSlotMemSize) * minSlotMemSize;\n          LOG.info(\"reduceResourceReqt:\"+reduceResourceReqt);\n          if (reduceResourceReqt \u003e getMaxContainerCapability().getMemory()) {\n            String diagMsg \u003d \"REDUCE capability required is more than the supported \" +\n            \"max container capability in the cluster. Killing the Job. reduceResourceReqt: \" + \n            reduceResourceReqt + \" maxContainerCapability:\" + getMaxContainerCapability().getMemory();\n            LOG.info(diagMsg);\n            eventHandler.handle(new JobDiagnosticsUpdateEvent(\n                getJob().getID(), diagMsg));\n            eventHandler.handle(new JobEvent(getJob().getID(), JobEventType.JOB_KILL));\n          }\n        }\n        //set the rounded off memory\n        reqEvent.getCapability().setMemory(reduceResourceReqt);\n        if (reqEvent.getEarlierAttemptFailed()) {\n          //add to the front of queue for fail fast\n          pendingReduces.addFirst(new ContainerRequest(reqEvent, PRIORITY_REDUCE));\n        } else {\n          pendingReduces.add(new ContainerRequest(reqEvent, PRIORITY_REDUCE));//reduces are added to pending and are slowly ramped up\n        }\n      }\n      \n    } else if (\n        event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_DEALLOCATE) {\n      TaskAttemptId aId \u003d event.getAttemptID();\n      \n      boolean removed \u003d scheduledRequests.remove(aId);\n      if (!removed) {\n        ContainerId containerId \u003d assignedRequests.get(aId);\n        if (containerId !\u003d null) {\n          removed \u003d true;\n          assignedRequests.remove(aId);\n          containersReleased++;\n          release(containerId);\n        }\n      }\n      if (!removed) {\n        LOG.error(\"Could not deallocate container for task attemptId \" + \n            aId);\n      }\n    } else if (\n        event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_FAILED) {\n      ContainerFailedEvent fEv \u003d (ContainerFailedEvent) event;\n      String host \u003d getHost(fEv.getContMgrAddress());\n      containerFailedOnHost(host);\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java",
        "newPath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Yintroduced",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "diff": "@@ -0,0 +1,76 @@\n+  public synchronized void handle(ContainerAllocatorEvent event) {\n+    LOG.info(\"Processing the event \" + event.toString());\n+    recalculateReduceSchedule \u003d true;\n+    if (event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_REQ) {\n+      ContainerRequestEvent reqEvent \u003d (ContainerRequestEvent) event;\n+      if (reqEvent.getAttemptID().getTaskId().getTaskType().equals(TaskType.MAP)) {\n+        if (mapResourceReqt \u003d\u003d 0) {\n+          mapResourceReqt \u003d reqEvent.getCapability().getMemory();\n+          int minSlotMemSize \u003d getMinContainerCapability().getMemory();\n+          mapResourceReqt \u003d (int) Math.ceil((float) mapResourceReqt/minSlotMemSize) * minSlotMemSize;\n+          LOG.info(\"mapResourceReqt:\"+mapResourceReqt);\n+          if (mapResourceReqt \u003e getMaxContainerCapability().getMemory()) {\n+            String diagMsg \u003d \"MAP capability required is more than the supported \" +\n+            \"max container capability in the cluster. Killing the Job. mapResourceReqt: \" + \n+            mapResourceReqt + \" maxContainerCapability:\" + getMaxContainerCapability().getMemory();\n+            LOG.info(diagMsg);\n+            eventHandler.handle(new JobDiagnosticsUpdateEvent(\n+                getJob().getID(), diagMsg));\n+            eventHandler.handle(new JobEvent(getJob().getID(), JobEventType.JOB_KILL));\n+          }\n+        }\n+        //set the rounded off memory\n+        reqEvent.getCapability().setMemory(mapResourceReqt);\n+        scheduledRequests.addMap(reqEvent);//maps are immediately scheduled\n+      } else {\n+        if (reduceResourceReqt \u003d\u003d 0) {\n+          reduceResourceReqt \u003d reqEvent.getCapability().getMemory();\n+          int minSlotMemSize \u003d getMinContainerCapability().getMemory();\n+          //round off on slotsize\n+          reduceResourceReqt \u003d (int) Math.ceil((float) reduceResourceReqt/minSlotMemSize) * minSlotMemSize;\n+          LOG.info(\"reduceResourceReqt:\"+reduceResourceReqt);\n+          if (reduceResourceReqt \u003e getMaxContainerCapability().getMemory()) {\n+            String diagMsg \u003d \"REDUCE capability required is more than the supported \" +\n+            \"max container capability in the cluster. Killing the Job. reduceResourceReqt: \" + \n+            reduceResourceReqt + \" maxContainerCapability:\" + getMaxContainerCapability().getMemory();\n+            LOG.info(diagMsg);\n+            eventHandler.handle(new JobDiagnosticsUpdateEvent(\n+                getJob().getID(), diagMsg));\n+            eventHandler.handle(new JobEvent(getJob().getID(), JobEventType.JOB_KILL));\n+          }\n+        }\n+        //set the rounded off memory\n+        reqEvent.getCapability().setMemory(reduceResourceReqt);\n+        if (reqEvent.getEarlierAttemptFailed()) {\n+          //add to the front of queue for fail fast\n+          pendingReduces.addFirst(new ContainerRequest(reqEvent, PRIORITY_REDUCE));\n+        } else {\n+          pendingReduces.add(new ContainerRequest(reqEvent, PRIORITY_REDUCE));//reduces are added to pending and are slowly ramped up\n+        }\n+      }\n+      \n+    } else if (\n+        event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_DEALLOCATE) {\n+      TaskAttemptId aId \u003d event.getAttemptID();\n+      \n+      boolean removed \u003d scheduledRequests.remove(aId);\n+      if (!removed) {\n+        ContainerId containerId \u003d assignedRequests.get(aId);\n+        if (containerId !\u003d null) {\n+          removed \u003d true;\n+          assignedRequests.remove(aId);\n+          containersReleased++;\n+          release(containerId);\n+        }\n+      }\n+      if (!removed) {\n+        LOG.error(\"Could not deallocate container for task attemptId \" + \n+            aId);\n+      }\n+    } else if (\n+        event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_FAILED) {\n+      ContainerFailedEvent fEv \u003d (ContainerFailedEvent) event;\n+      String host \u003d getHost(fEv.getContMgrAddress());\n+      containerFailedOnHost(host);\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public synchronized void handle(ContainerAllocatorEvent event) {\n    LOG.info(\"Processing the event \" + event.toString());\n    recalculateReduceSchedule \u003d true;\n    if (event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_REQ) {\n      ContainerRequestEvent reqEvent \u003d (ContainerRequestEvent) event;\n      if (reqEvent.getAttemptID().getTaskId().getTaskType().equals(TaskType.MAP)) {\n        if (mapResourceReqt \u003d\u003d 0) {\n          mapResourceReqt \u003d reqEvent.getCapability().getMemory();\n          int minSlotMemSize \u003d getMinContainerCapability().getMemory();\n          mapResourceReqt \u003d (int) Math.ceil((float) mapResourceReqt/minSlotMemSize) * minSlotMemSize;\n          LOG.info(\"mapResourceReqt:\"+mapResourceReqt);\n          if (mapResourceReqt \u003e getMaxContainerCapability().getMemory()) {\n            String diagMsg \u003d \"MAP capability required is more than the supported \" +\n            \"max container capability in the cluster. Killing the Job. mapResourceReqt: \" + \n            mapResourceReqt + \" maxContainerCapability:\" + getMaxContainerCapability().getMemory();\n            LOG.info(diagMsg);\n            eventHandler.handle(new JobDiagnosticsUpdateEvent(\n                getJob().getID(), diagMsg));\n            eventHandler.handle(new JobEvent(getJob().getID(), JobEventType.JOB_KILL));\n          }\n        }\n        //set the rounded off memory\n        reqEvent.getCapability().setMemory(mapResourceReqt);\n        scheduledRequests.addMap(reqEvent);//maps are immediately scheduled\n      } else {\n        if (reduceResourceReqt \u003d\u003d 0) {\n          reduceResourceReqt \u003d reqEvent.getCapability().getMemory();\n          int minSlotMemSize \u003d getMinContainerCapability().getMemory();\n          //round off on slotsize\n          reduceResourceReqt \u003d (int) Math.ceil((float) reduceResourceReqt/minSlotMemSize) * minSlotMemSize;\n          LOG.info(\"reduceResourceReqt:\"+reduceResourceReqt);\n          if (reduceResourceReqt \u003e getMaxContainerCapability().getMemory()) {\n            String diagMsg \u003d \"REDUCE capability required is more than the supported \" +\n            \"max container capability in the cluster. Killing the Job. reduceResourceReqt: \" + \n            reduceResourceReqt + \" maxContainerCapability:\" + getMaxContainerCapability().getMemory();\n            LOG.info(diagMsg);\n            eventHandler.handle(new JobDiagnosticsUpdateEvent(\n                getJob().getID(), diagMsg));\n            eventHandler.handle(new JobEvent(getJob().getID(), JobEventType.JOB_KILL));\n          }\n        }\n        //set the rounded off memory\n        reqEvent.getCapability().setMemory(reduceResourceReqt);\n        if (reqEvent.getEarlierAttemptFailed()) {\n          //add to the front of queue for fail fast\n          pendingReduces.addFirst(new ContainerRequest(reqEvent, PRIORITY_REDUCE));\n        } else {\n          pendingReduces.add(new ContainerRequest(reqEvent, PRIORITY_REDUCE));//reduces are added to pending and are slowly ramped up\n        }\n      }\n      \n    } else if (\n        event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_DEALLOCATE) {\n      TaskAttemptId aId \u003d event.getAttemptID();\n      \n      boolean removed \u003d scheduledRequests.remove(aId);\n      if (!removed) {\n        ContainerId containerId \u003d assignedRequests.get(aId);\n        if (containerId !\u003d null) {\n          removed \u003d true;\n          assignedRequests.remove(aId);\n          containersReleased++;\n          release(containerId);\n        }\n      }\n      if (!removed) {\n        LOG.error(\"Could not deallocate container for task attemptId \" + \n            aId);\n      }\n    } else if (\n        event.getType() \u003d\u003d ContainerAllocator.EventType.CONTAINER_FAILED) {\n      ContainerFailedEvent fEv \u003d (ContainerFailedEvent) event;\n      String host \u003d getHost(fEv.getContMgrAddress());\n      containerFailedOnHost(host);\n    }\n  }",
      "path": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java"
    }
  }
}