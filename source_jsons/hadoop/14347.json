{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "JournaledEditsCache.java",
  "functionName": "storeEdits",
  "functionId": "storeEdits___inputData-byte[]__newStartTxn-long__newEndTxn-long__newLayoutVersion-int",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournaledEditsCache.java",
  "functionStartLine": 240,
  "functionEndLine": 297,
  "numCommitsSeen": 3,
  "timeTaken": 993,
  "changeHistory": [
    "1e22f2bfbb1d9a29f5d4fa641b7a0dabd5b1dbf5",
    "c81ac2ff0220b180cd6cbbf18221290c3783bfd5"
  ],
  "changeHistoryShort": {
    "1e22f2bfbb1d9a29f5d4fa641b7a0dabd5b1dbf5": "Ybodychange",
    "c81ac2ff0220b180cd6cbbf18221290c3783bfd5": "Yintroduced"
  },
  "changeHistoryDetails": {
    "1e22f2bfbb1d9a29f5d4fa641b7a0dabd5b1dbf5": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13610. [SBN read] Edit Tail Fast Path Part 4: Cleanup. Integration test, documentation, remove unnecessary dummy sync, minors fixups. Contributed by Erik Krogen.\n",
      "commitDate": "24/12/18 9:34 AM",
      "commitName": "1e22f2bfbb1d9a29f5d4fa641b7a0dabd5b1dbf5",
      "commitAuthor": "Erik Krogen",
      "commitDateOld": "24/12/18 9:33 AM",
      "commitNameOld": "c81ac2ff0220b180cd6cbbf18221290c3783bfd5",
      "commitAuthorOld": "Erik Krogen",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 6,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,56 +1,58 @@\n   void storeEdits(byte[] inputData, long newStartTxn, long newEndTxn,\n       int newLayoutVersion) {\n     if (newStartTxn \u003c 0 || newEndTxn \u003c newStartTxn) {\n       Journal.LOG.error(String.format(\"Attempted to cache data of length %d \" +\n           \"with newStartTxn %d and newEndTxn %d\",\n           inputData.length, newStartTxn, newEndTxn));\n       return;\n     }\n     try (AutoCloseableLock l \u003d writeLock.acquire()) {\n       if (newLayoutVersion !\u003d layoutVersion) {\n         try {\n           updateLayoutVersion(newLayoutVersion, newStartTxn);\n         } catch (IOException ioe) {\n           Journal.LOG.error(String.format(\"Unable to save new edits [%d, %d] \" +\n               \"due to exception when updating to new layout version %d\",\n               newStartTxn, newEndTxn, newLayoutVersion), ioe);\n           return;\n         }\n-      }\n-      if (lowestTxnId \u003c 0 || (highestTxnId + 1) !\u003d newStartTxn) {\n-        // Cache initialization step\n-        if (lowestTxnId \u003e\u003d 0) {\n-          // Cache is out of sync; clear to avoid storing noncontiguous regions\n-          Journal.LOG.error(String.format(\"Edits cache is out of sync; \" +\n-                  \"looked for next txn id at %d but got start txn id for \" +\n-                  \"cache put request at %d\", highestTxnId + 1, newStartTxn));\n-        }\n+      } else if (lowestTxnId \u003d\u003d INVALID_TXN_ID) {\n+        Journal.LOG.info(\"Initializing edits cache starting from txn ID \" +\n+            newStartTxn);\n+        initialize(newStartTxn);\n+      } else if (highestTxnId + 1 !\u003d newStartTxn) {\n+        // Cache is out of sync; clear to avoid storing noncontiguous regions\n+        Journal.LOG.error(String.format(\"Edits cache is out of sync; \" +\n+            \"looked for next txn id at %d but got start txn id for \" +\n+            \"cache put request at %d. Reinitializing at new request.\",\n+            highestTxnId + 1, newStartTxn));\n         initialize(newStartTxn);\n       }\n \n       while ((totalSize + inputData.length) \u003e capacity \u0026\u0026 !dataMap.isEmpty()) {\n         Map.Entry\u003cLong, byte[]\u003e lowest \u003d dataMap.firstEntry();\n         dataMap.remove(lowest.getKey());\n         totalSize -\u003d lowest.getValue().length;\n       }\n       if (inputData.length \u003e capacity) {\n-        initialize(-1);\n+        initialize(INVALID_TXN_ID);\n         Journal.LOG.warn(String.format(\"A single batch of edits was too \" +\n                 \"large to fit into the cache: startTxn \u003d %d, endTxn \u003d %d, \" +\n                 \"input length \u003d %d. The capacity of the cache (%s) must be \" +\n-                \"increased for it to work properly (current capacity %d)\",\n+                \"increased for it to work properly (current capacity %d).\" +\n+                \"Cache is now empty.\",\n             newStartTxn, newEndTxn, inputData.length,\n             DFSConfigKeys.DFS_JOURNALNODE_EDIT_CACHE_SIZE_KEY, capacity));\n         return;\n       }\n       if (dataMap.isEmpty()) {\n         lowestTxnId \u003d newStartTxn;\n       } else {\n         lowestTxnId \u003d dataMap.firstKey();\n       }\n \n       dataMap.put(newStartTxn, inputData);\n       highestTxnId \u003d newEndTxn;\n       totalSize +\u003d inputData.length;\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void storeEdits(byte[] inputData, long newStartTxn, long newEndTxn,\n      int newLayoutVersion) {\n    if (newStartTxn \u003c 0 || newEndTxn \u003c newStartTxn) {\n      Journal.LOG.error(String.format(\"Attempted to cache data of length %d \" +\n          \"with newStartTxn %d and newEndTxn %d\",\n          inputData.length, newStartTxn, newEndTxn));\n      return;\n    }\n    try (AutoCloseableLock l \u003d writeLock.acquire()) {\n      if (newLayoutVersion !\u003d layoutVersion) {\n        try {\n          updateLayoutVersion(newLayoutVersion, newStartTxn);\n        } catch (IOException ioe) {\n          Journal.LOG.error(String.format(\"Unable to save new edits [%d, %d] \" +\n              \"due to exception when updating to new layout version %d\",\n              newStartTxn, newEndTxn, newLayoutVersion), ioe);\n          return;\n        }\n      } else if (lowestTxnId \u003d\u003d INVALID_TXN_ID) {\n        Journal.LOG.info(\"Initializing edits cache starting from txn ID \" +\n            newStartTxn);\n        initialize(newStartTxn);\n      } else if (highestTxnId + 1 !\u003d newStartTxn) {\n        // Cache is out of sync; clear to avoid storing noncontiguous regions\n        Journal.LOG.error(String.format(\"Edits cache is out of sync; \" +\n            \"looked for next txn id at %d but got start txn id for \" +\n            \"cache put request at %d. Reinitializing at new request.\",\n            highestTxnId + 1, newStartTxn));\n        initialize(newStartTxn);\n      }\n\n      while ((totalSize + inputData.length) \u003e capacity \u0026\u0026 !dataMap.isEmpty()) {\n        Map.Entry\u003cLong, byte[]\u003e lowest \u003d dataMap.firstEntry();\n        dataMap.remove(lowest.getKey());\n        totalSize -\u003d lowest.getValue().length;\n      }\n      if (inputData.length \u003e capacity) {\n        initialize(INVALID_TXN_ID);\n        Journal.LOG.warn(String.format(\"A single batch of edits was too \" +\n                \"large to fit into the cache: startTxn \u003d %d, endTxn \u003d %d, \" +\n                \"input length \u003d %d. The capacity of the cache (%s) must be \" +\n                \"increased for it to work properly (current capacity %d).\" +\n                \"Cache is now empty.\",\n            newStartTxn, newEndTxn, inputData.length,\n            DFSConfigKeys.DFS_JOURNALNODE_EDIT_CACHE_SIZE_KEY, capacity));\n        return;\n      }\n      if (dataMap.isEmpty()) {\n        lowestTxnId \u003d newStartTxn;\n      } else {\n        lowestTxnId \u003d dataMap.firstKey();\n      }\n\n      dataMap.put(newStartTxn, inputData);\n      highestTxnId \u003d newEndTxn;\n      totalSize +\u003d inputData.length;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournaledEditsCache.java",
      "extendedDetails": {}
    },
    "c81ac2ff0220b180cd6cbbf18221290c3783bfd5": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-13607. [SBN read] Edit Tail Fast Path Part 1: Enhance JournalNode with an in-memory cache of recent edit transactions. Contributed by Erik Krogen.\n",
      "commitDate": "24/12/18 9:33 AM",
      "commitName": "c81ac2ff0220b180cd6cbbf18221290c3783bfd5",
      "commitAuthor": "Erik Krogen",
      "diff": "@@ -0,0 +1,56 @@\n+  void storeEdits(byte[] inputData, long newStartTxn, long newEndTxn,\n+      int newLayoutVersion) {\n+    if (newStartTxn \u003c 0 || newEndTxn \u003c newStartTxn) {\n+      Journal.LOG.error(String.format(\"Attempted to cache data of length %d \" +\n+          \"with newStartTxn %d and newEndTxn %d\",\n+          inputData.length, newStartTxn, newEndTxn));\n+      return;\n+    }\n+    try (AutoCloseableLock l \u003d writeLock.acquire()) {\n+      if (newLayoutVersion !\u003d layoutVersion) {\n+        try {\n+          updateLayoutVersion(newLayoutVersion, newStartTxn);\n+        } catch (IOException ioe) {\n+          Journal.LOG.error(String.format(\"Unable to save new edits [%d, %d] \" +\n+              \"due to exception when updating to new layout version %d\",\n+              newStartTxn, newEndTxn, newLayoutVersion), ioe);\n+          return;\n+        }\n+      }\n+      if (lowestTxnId \u003c 0 || (highestTxnId + 1) !\u003d newStartTxn) {\n+        // Cache initialization step\n+        if (lowestTxnId \u003e\u003d 0) {\n+          // Cache is out of sync; clear to avoid storing noncontiguous regions\n+          Journal.LOG.error(String.format(\"Edits cache is out of sync; \" +\n+                  \"looked for next txn id at %d but got start txn id for \" +\n+                  \"cache put request at %d\", highestTxnId + 1, newStartTxn));\n+        }\n+        initialize(newStartTxn);\n+      }\n+\n+      while ((totalSize + inputData.length) \u003e capacity \u0026\u0026 !dataMap.isEmpty()) {\n+        Map.Entry\u003cLong, byte[]\u003e lowest \u003d dataMap.firstEntry();\n+        dataMap.remove(lowest.getKey());\n+        totalSize -\u003d lowest.getValue().length;\n+      }\n+      if (inputData.length \u003e capacity) {\n+        initialize(-1);\n+        Journal.LOG.warn(String.format(\"A single batch of edits was too \" +\n+                \"large to fit into the cache: startTxn \u003d %d, endTxn \u003d %d, \" +\n+                \"input length \u003d %d. The capacity of the cache (%s) must be \" +\n+                \"increased for it to work properly (current capacity %d)\",\n+            newStartTxn, newEndTxn, inputData.length,\n+            DFSConfigKeys.DFS_JOURNALNODE_EDIT_CACHE_SIZE_KEY, capacity));\n+        return;\n+      }\n+      if (dataMap.isEmpty()) {\n+        lowestTxnId \u003d newStartTxn;\n+      } else {\n+        lowestTxnId \u003d dataMap.firstKey();\n+      }\n+\n+      dataMap.put(newStartTxn, inputData);\n+      highestTxnId \u003d newEndTxn;\n+      totalSize +\u003d inputData.length;\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  void storeEdits(byte[] inputData, long newStartTxn, long newEndTxn,\n      int newLayoutVersion) {\n    if (newStartTxn \u003c 0 || newEndTxn \u003c newStartTxn) {\n      Journal.LOG.error(String.format(\"Attempted to cache data of length %d \" +\n          \"with newStartTxn %d and newEndTxn %d\",\n          inputData.length, newStartTxn, newEndTxn));\n      return;\n    }\n    try (AutoCloseableLock l \u003d writeLock.acquire()) {\n      if (newLayoutVersion !\u003d layoutVersion) {\n        try {\n          updateLayoutVersion(newLayoutVersion, newStartTxn);\n        } catch (IOException ioe) {\n          Journal.LOG.error(String.format(\"Unable to save new edits [%d, %d] \" +\n              \"due to exception when updating to new layout version %d\",\n              newStartTxn, newEndTxn, newLayoutVersion), ioe);\n          return;\n        }\n      }\n      if (lowestTxnId \u003c 0 || (highestTxnId + 1) !\u003d newStartTxn) {\n        // Cache initialization step\n        if (lowestTxnId \u003e\u003d 0) {\n          // Cache is out of sync; clear to avoid storing noncontiguous regions\n          Journal.LOG.error(String.format(\"Edits cache is out of sync; \" +\n                  \"looked for next txn id at %d but got start txn id for \" +\n                  \"cache put request at %d\", highestTxnId + 1, newStartTxn));\n        }\n        initialize(newStartTxn);\n      }\n\n      while ((totalSize + inputData.length) \u003e capacity \u0026\u0026 !dataMap.isEmpty()) {\n        Map.Entry\u003cLong, byte[]\u003e lowest \u003d dataMap.firstEntry();\n        dataMap.remove(lowest.getKey());\n        totalSize -\u003d lowest.getValue().length;\n      }\n      if (inputData.length \u003e capacity) {\n        initialize(-1);\n        Journal.LOG.warn(String.format(\"A single batch of edits was too \" +\n                \"large to fit into the cache: startTxn \u003d %d, endTxn \u003d %d, \" +\n                \"input length \u003d %d. The capacity of the cache (%s) must be \" +\n                \"increased for it to work properly (current capacity %d)\",\n            newStartTxn, newEndTxn, inputData.length,\n            DFSConfigKeys.DFS_JOURNALNODE_EDIT_CACHE_SIZE_KEY, capacity));\n        return;\n      }\n      if (dataMap.isEmpty()) {\n        lowestTxnId \u003d newStartTxn;\n      } else {\n        lowestTxnId \u003d dataMap.firstKey();\n      }\n\n      dataMap.put(newStartTxn, inputData);\n      highestTxnId \u003d newEndTxn;\n      totalSize +\u003d inputData.length;\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournaledEditsCache.java"
    }
  }
}