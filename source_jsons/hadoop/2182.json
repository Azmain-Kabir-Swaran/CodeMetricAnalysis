{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "DFSClient.java",
  "functionName": "callAppend",
  "functionId": "callAppend___src-String__flag-EnumSet__CreateFlag____progress-Progressable__favoredNodes-String[]",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
  "functionStartLine": 1392,
  "functionEndLine": 1417,
  "numCommitsSeen": 877,
  "timeTaken": 14508,
  "changeHistory": [
    "bd909ed9f2d853f614f04a50e2230a7932732776",
    "b9e0417bdf2b9655dc4256bdb43683eca1ab46be",
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
    "c8d72907ff5a4cb9ce1effca8ad9b69689d11d1d",
    "2cc9514ad643ae49d30524743420ee9744e571bd",
    "61df1b27a797efd094328c7d9141b9e157e01bf4",
    "89a544928083501625bc69f96b530040228f0a5f",
    "2848db814a98b83e7546f65a2751e56fb5b2dbe0",
    "1556f86a31a54733d6550363aa0e027acca7823b",
    "853cb704edf54207313c0e70c9c375212d288b60",
    "2116d0520e528c44fa280f2a5b28594c6d6fc28a",
    "3b773da0361bdec594acd6814c49b81f867bd673",
    "f84552ac35bb5221290be68fece9c779ebeaf4bc",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "d86f3183d93714ba078416af4f609d26376eadb0",
    "fd9997989c1f1c6f806c57a806e7225ca599fc0c",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "bd909ed9f2d853f614f04a50e2230a7932732776": "Ybodychange",
    "b9e0417bdf2b9655dc4256bdb43683eca1ab46be": "Ybodychange",
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93": "Ymultichange(Yparameterchange,Ybodychange)",
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": "Yfilerename",
    "c8d72907ff5a4cb9ce1effca8ad9b69689d11d1d": "Ybodychange",
    "2cc9514ad643ae49d30524743420ee9744e571bd": "Ybodychange",
    "61df1b27a797efd094328c7d9141b9e157e01bf4": "Ybodychange",
    "89a544928083501625bc69f96b530040228f0a5f": "Ymultichange(Yparameterchange,Ybodychange)",
    "2848db814a98b83e7546f65a2751e56fb5b2dbe0": "Ymultichange(Yparameterchange,Ybodychange)",
    "1556f86a31a54733d6550363aa0e027acca7823b": "Ybodychange",
    "853cb704edf54207313c0e70c9c375212d288b60": "Ymultichange(Yparameterchange,Ybodychange)",
    "2116d0520e528c44fa280f2a5b28594c6d6fc28a": "Ybodychange",
    "3b773da0361bdec594acd6814c49b81f867bd673": "Ybodychange",
    "f84552ac35bb5221290be68fece9c779ebeaf4bc": "Ybodychange",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "d86f3183d93714ba078416af4f609d26376eadb0": "Yfilerename",
    "fd9997989c1f1c6f806c57a806e7225ca599fc0c": "Ybodychange",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "bd909ed9f2d853f614f04a50e2230a7932732776": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8999. Allow a file to be closed with COMMITTED but not yet COMPLETE blocks.\n",
      "commitDate": "25/01/16 6:32 PM",
      "commitName": "bd909ed9f2d853f614f04a50e2230a7932732776",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "24/01/16 2:19 PM",
      "commitNameOld": "10a2bc0dffaece216eb9a6bac3236a086b9ece31",
      "commitAuthorOld": "Eric Yang",
      "daysBetweenCommits": 1.18,
      "commitsBetweenForRepo": 11,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,26 +1,26 @@\n   private DFSOutputStream callAppend(String src, EnumSet\u003cCreateFlag\u003e flag,\n       Progressable progress, String[] favoredNodes) throws IOException {\n     CreateFlag.validateForAppend(flag);\n     try {\n-      LastBlockWithStatus blkWithStatus \u003d namenode.append(src, clientName,\n+      final LastBlockWithStatus blkWithStatus \u003d callAppend(src,\n           new EnumSetWritable\u003c\u003e(flag, CreateFlag.class));\n       HdfsFileStatus status \u003d blkWithStatus.getFileStatus();\n       if (status \u003d\u003d null) {\n-        DFSClient.LOG.debug(\"NameNode is on an older version, request file \" +\n-            \"info with additional RPC call for file: \" + src);\n+        LOG.debug(\"NameNode is on an older version, request file \" +\n+            \"info with additional RPC call for file: {}\", src);\n         status \u003d getFileInfo(src);\n       }\n       return DFSOutputStream.newStreamForAppend(this, src, flag, progress,\n           blkWithStatus.getLastBlock(), status,\n           dfsClientConf.createChecksum(null), favoredNodes);\n     } catch(RemoteException re) {\n       throw re.unwrapRemoteException(AccessControlException.class,\n           FileNotFoundException.class,\n           SafeModeException.class,\n           DSQuotaExceededException.class,\n           QuotaByStorageTypeExceededException.class,\n           UnsupportedOperationException.class,\n           UnresolvedPathException.class,\n           SnapshotAccessControlException.class);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private DFSOutputStream callAppend(String src, EnumSet\u003cCreateFlag\u003e flag,\n      Progressable progress, String[] favoredNodes) throws IOException {\n    CreateFlag.validateForAppend(flag);\n    try {\n      final LastBlockWithStatus blkWithStatus \u003d callAppend(src,\n          new EnumSetWritable\u003c\u003e(flag, CreateFlag.class));\n      HdfsFileStatus status \u003d blkWithStatus.getFileStatus();\n      if (status \u003d\u003d null) {\n        LOG.debug(\"NameNode is on an older version, request file \" +\n            \"info with additional RPC call for file: {}\", src);\n        status \u003d getFileInfo(src);\n      }\n      return DFSOutputStream.newStreamForAppend(this, src, flag, progress,\n          blkWithStatus.getLastBlock(), status,\n          dfsClientConf.createChecksum(null), favoredNodes);\n    } catch(RemoteException re) {\n      throw re.unwrapRemoteException(AccessControlException.class,\n          FileNotFoundException.class,\n          SafeModeException.class,\n          DSQuotaExceededException.class,\n          QuotaByStorageTypeExceededException.class,\n          UnsupportedOperationException.class,\n          UnresolvedPathException.class,\n          SnapshotAccessControlException.class);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
      "extendedDetails": {}
    },
    "b9e0417bdf2b9655dc4256bdb43683eca1ab46be": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9290. DFSClient#callAppend() is not backward compatible for slightly older NameNodes. Contributed by Tony Wu.\n",
      "commitDate": "23/10/15 2:37 PM",
      "commitName": "b9e0417bdf2b9655dc4256bdb43683eca1ab46be",
      "commitAuthor": "Kihwal Lee",
      "commitDateOld": "03/10/15 11:38 AM",
      "commitNameOld": "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 20.12,
      "commitsBetweenForRepo": 163,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,20 +1,26 @@\n   private DFSOutputStream callAppend(String src, EnumSet\u003cCreateFlag\u003e flag,\n       Progressable progress, String[] favoredNodes) throws IOException {\n     CreateFlag.validateForAppend(flag);\n     try {\n       LastBlockWithStatus blkWithStatus \u003d namenode.append(src, clientName,\n           new EnumSetWritable\u003c\u003e(flag, CreateFlag.class));\n+      HdfsFileStatus status \u003d blkWithStatus.getFileStatus();\n+      if (status \u003d\u003d null) {\n+        DFSClient.LOG.debug(\"NameNode is on an older version, request file \" +\n+            \"info with additional RPC call for file: \" + src);\n+        status \u003d getFileInfo(src);\n+      }\n       return DFSOutputStream.newStreamForAppend(this, src, flag, progress,\n-          blkWithStatus.getLastBlock(), blkWithStatus.getFileStatus(),\n+          blkWithStatus.getLastBlock(), status,\n           dfsClientConf.createChecksum(null), favoredNodes);\n     } catch(RemoteException re) {\n       throw re.unwrapRemoteException(AccessControlException.class,\n           FileNotFoundException.class,\n           SafeModeException.class,\n           DSQuotaExceededException.class,\n           QuotaByStorageTypeExceededException.class,\n           UnsupportedOperationException.class,\n           UnresolvedPathException.class,\n           SnapshotAccessControlException.class);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private DFSOutputStream callAppend(String src, EnumSet\u003cCreateFlag\u003e flag,\n      Progressable progress, String[] favoredNodes) throws IOException {\n    CreateFlag.validateForAppend(flag);\n    try {\n      LastBlockWithStatus blkWithStatus \u003d namenode.append(src, clientName,\n          new EnumSetWritable\u003c\u003e(flag, CreateFlag.class));\n      HdfsFileStatus status \u003d blkWithStatus.getFileStatus();\n      if (status \u003d\u003d null) {\n        DFSClient.LOG.debug(\"NameNode is on an older version, request file \" +\n            \"info with additional RPC call for file: \" + src);\n        status \u003d getFileInfo(src);\n      }\n      return DFSOutputStream.newStreamForAppend(this, src, flag, progress,\n          blkWithStatus.getLastBlock(), status,\n          dfsClientConf.createChecksum(null), favoredNodes);\n    } catch(RemoteException re) {\n      throw re.unwrapRemoteException(AccessControlException.class,\n          FileNotFoundException.class,\n          SafeModeException.class,\n          DSQuotaExceededException.class,\n          QuotaByStorageTypeExceededException.class,\n          UnsupportedOperationException.class,\n          UnresolvedPathException.class,\n          SnapshotAccessControlException.class);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
      "extendedDetails": {}
    },
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-8979. Clean up checkstyle warnings in hadoop-hdfs-client module. Contributed by Mingliang Liu.\n",
      "commitDate": "03/10/15 11:38 AM",
      "commitName": "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
      "commitAuthor": "Haohui Mai",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-8979. Clean up checkstyle warnings in hadoop-hdfs-client module. Contributed by Mingliang Liu.\n",
          "commitDate": "03/10/15 11:38 AM",
          "commitName": "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "30/09/15 8:39 AM",
          "commitNameOld": "6c17d315287020368689fa078a40a1eaedf89d5b",
          "commitAuthorOld": "",
          "daysBetweenCommits": 3.12,
          "commitsBetweenForRepo": 16,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,22 +1,20 @@\n-  private DFSOutputStream callAppend(String src, int buffersize,\n-      EnumSet\u003cCreateFlag\u003e flag, Progressable progress, String[] favoredNodes)\n-      throws IOException {\n+  private DFSOutputStream callAppend(String src, EnumSet\u003cCreateFlag\u003e flag,\n+      Progressable progress, String[] favoredNodes) throws IOException {\n     CreateFlag.validateForAppend(flag);\n     try {\n       LastBlockWithStatus blkWithStatus \u003d namenode.append(src, clientName,\n           new EnumSetWritable\u003c\u003e(flag, CreateFlag.class));\n-      return DFSOutputStream.newStreamForAppend(this, src, flag, buffersize,\n-          progress, blkWithStatus.getLastBlock(),\n-          blkWithStatus.getFileStatus(), dfsClientConf.createChecksum(null),\n-          favoredNodes);\n+      return DFSOutputStream.newStreamForAppend(this, src, flag, progress,\n+          blkWithStatus.getLastBlock(), blkWithStatus.getFileStatus(),\n+          dfsClientConf.createChecksum(null), favoredNodes);\n     } catch(RemoteException re) {\n       throw re.unwrapRemoteException(AccessControlException.class,\n-                                     FileNotFoundException.class,\n-                                     SafeModeException.class,\n-                                     DSQuotaExceededException.class,\n-                                     QuotaByStorageTypeExceededException.class,\n-                                     UnsupportedOperationException.class,\n-                                     UnresolvedPathException.class,\n-                                     SnapshotAccessControlException.class);\n+          FileNotFoundException.class,\n+          SafeModeException.class,\n+          DSQuotaExceededException.class,\n+          QuotaByStorageTypeExceededException.class,\n+          UnsupportedOperationException.class,\n+          UnresolvedPathException.class,\n+          SnapshotAccessControlException.class);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private DFSOutputStream callAppend(String src, EnumSet\u003cCreateFlag\u003e flag,\n      Progressable progress, String[] favoredNodes) throws IOException {\n    CreateFlag.validateForAppend(flag);\n    try {\n      LastBlockWithStatus blkWithStatus \u003d namenode.append(src, clientName,\n          new EnumSetWritable\u003c\u003e(flag, CreateFlag.class));\n      return DFSOutputStream.newStreamForAppend(this, src, flag, progress,\n          blkWithStatus.getLastBlock(), blkWithStatus.getFileStatus(),\n          dfsClientConf.createChecksum(null), favoredNodes);\n    } catch(RemoteException re) {\n      throw re.unwrapRemoteException(AccessControlException.class,\n          FileNotFoundException.class,\n          SafeModeException.class,\n          DSQuotaExceededException.class,\n          QuotaByStorageTypeExceededException.class,\n          UnsupportedOperationException.class,\n          UnresolvedPathException.class,\n          SnapshotAccessControlException.class);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
          "extendedDetails": {
            "oldValue": "[src-String, buffersize-int, flag-EnumSet\u003cCreateFlag\u003e, progress-Progressable, favoredNodes-String[]]",
            "newValue": "[src-String, flag-EnumSet\u003cCreateFlag\u003e, progress-Progressable, favoredNodes-String[]]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-8979. Clean up checkstyle warnings in hadoop-hdfs-client module. Contributed by Mingliang Liu.\n",
          "commitDate": "03/10/15 11:38 AM",
          "commitName": "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "30/09/15 8:39 AM",
          "commitNameOld": "6c17d315287020368689fa078a40a1eaedf89d5b",
          "commitAuthorOld": "",
          "daysBetweenCommits": 3.12,
          "commitsBetweenForRepo": 16,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,22 +1,20 @@\n-  private DFSOutputStream callAppend(String src, int buffersize,\n-      EnumSet\u003cCreateFlag\u003e flag, Progressable progress, String[] favoredNodes)\n-      throws IOException {\n+  private DFSOutputStream callAppend(String src, EnumSet\u003cCreateFlag\u003e flag,\n+      Progressable progress, String[] favoredNodes) throws IOException {\n     CreateFlag.validateForAppend(flag);\n     try {\n       LastBlockWithStatus blkWithStatus \u003d namenode.append(src, clientName,\n           new EnumSetWritable\u003c\u003e(flag, CreateFlag.class));\n-      return DFSOutputStream.newStreamForAppend(this, src, flag, buffersize,\n-          progress, blkWithStatus.getLastBlock(),\n-          blkWithStatus.getFileStatus(), dfsClientConf.createChecksum(null),\n-          favoredNodes);\n+      return DFSOutputStream.newStreamForAppend(this, src, flag, progress,\n+          blkWithStatus.getLastBlock(), blkWithStatus.getFileStatus(),\n+          dfsClientConf.createChecksum(null), favoredNodes);\n     } catch(RemoteException re) {\n       throw re.unwrapRemoteException(AccessControlException.class,\n-                                     FileNotFoundException.class,\n-                                     SafeModeException.class,\n-                                     DSQuotaExceededException.class,\n-                                     QuotaByStorageTypeExceededException.class,\n-                                     UnsupportedOperationException.class,\n-                                     UnresolvedPathException.class,\n-                                     SnapshotAccessControlException.class);\n+          FileNotFoundException.class,\n+          SafeModeException.class,\n+          DSQuotaExceededException.class,\n+          QuotaByStorageTypeExceededException.class,\n+          UnsupportedOperationException.class,\n+          UnresolvedPathException.class,\n+          SnapshotAccessControlException.class);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private DFSOutputStream callAppend(String src, EnumSet\u003cCreateFlag\u003e flag,\n      Progressable progress, String[] favoredNodes) throws IOException {\n    CreateFlag.validateForAppend(flag);\n    try {\n      LastBlockWithStatus blkWithStatus \u003d namenode.append(src, clientName,\n          new EnumSetWritable\u003c\u003e(flag, CreateFlag.class));\n      return DFSOutputStream.newStreamForAppend(this, src, flag, progress,\n          blkWithStatus.getLastBlock(), blkWithStatus.getFileStatus(),\n          dfsClientConf.createChecksum(null), favoredNodes);\n    } catch(RemoteException re) {\n      throw re.unwrapRemoteException(AccessControlException.class,\n          FileNotFoundException.class,\n          SafeModeException.class,\n          DSQuotaExceededException.class,\n          QuotaByStorageTypeExceededException.class,\n          UnsupportedOperationException.class,\n          UnresolvedPathException.class,\n          SnapshotAccessControlException.class);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
          "extendedDetails": {}
        }
      ]
    },
    "bf37d3d80e5179dea27e5bd5aea804a38aa9934c": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8053. Move DFSIn/OutputStream and related classes to hadoop-hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "26/09/15 11:08 AM",
      "commitName": "bf37d3d80e5179dea27e5bd5aea804a38aa9934c",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "26/09/15 9:06 AM",
      "commitNameOld": "861b52db242f238d7e36ad75c158025be959a696",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 0.08,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private DFSOutputStream callAppend(String src, int buffersize,\n      EnumSet\u003cCreateFlag\u003e flag, Progressable progress, String[] favoredNodes)\n      throws IOException {\n    CreateFlag.validateForAppend(flag);\n    try {\n      LastBlockWithStatus blkWithStatus \u003d namenode.append(src, clientName,\n          new EnumSetWritable\u003c\u003e(flag, CreateFlag.class));\n      return DFSOutputStream.newStreamForAppend(this, src, flag, buffersize,\n          progress, blkWithStatus.getLastBlock(),\n          blkWithStatus.getFileStatus(), dfsClientConf.createChecksum(null),\n          favoredNodes);\n    } catch(RemoteException re) {\n      throw re.unwrapRemoteException(AccessControlException.class,\n                                     FileNotFoundException.class,\n                                     SafeModeException.class,\n                                     DSQuotaExceededException.class,\n                                     QuotaByStorageTypeExceededException.class,\n                                     UnsupportedOperationException.class,\n                                     UnresolvedPathException.class,\n                                     SnapshotAccessControlException.class);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSClient.java"
      }
    },
    "c8d72907ff5a4cb9ce1effca8ad9b69689d11d1d": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8231. StackTrace displayed at client while QuotaByStorageType exceeds (Contributed by J.Andreina and Xiaoyu Yao)\n",
      "commitDate": "24/04/15 12:21 AM",
      "commitName": "c8d72907ff5a4cb9ce1effca8ad9b69689d11d1d",
      "commitAuthor": "Vinayakumar B",
      "commitDateOld": "23/04/15 5:33 PM",
      "commitNameOld": "bcf89ddc7d52e04725caf104f5958e33d9f51b35",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 0.28,
      "commitsBetweenForRepo": 4,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,22 @@\n   private DFSOutputStream callAppend(String src, int buffersize,\n       EnumSet\u003cCreateFlag\u003e flag, Progressable progress, String[] favoredNodes)\n       throws IOException {\n     CreateFlag.validateForAppend(flag);\n     try {\n       LastBlockWithStatus blkWithStatus \u003d namenode.append(src, clientName,\n           new EnumSetWritable\u003c\u003e(flag, CreateFlag.class));\n       return DFSOutputStream.newStreamForAppend(this, src, flag, buffersize,\n           progress, blkWithStatus.getLastBlock(),\n           blkWithStatus.getFileStatus(), dfsClientConf.createChecksum(null),\n           favoredNodes);\n     } catch(RemoteException re) {\n       throw re.unwrapRemoteException(AccessControlException.class,\n                                      FileNotFoundException.class,\n                                      SafeModeException.class,\n                                      DSQuotaExceededException.class,\n+                                     QuotaByStorageTypeExceededException.class,\n                                      UnsupportedOperationException.class,\n                                      UnresolvedPathException.class,\n                                      SnapshotAccessControlException.class);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private DFSOutputStream callAppend(String src, int buffersize,\n      EnumSet\u003cCreateFlag\u003e flag, Progressable progress, String[] favoredNodes)\n      throws IOException {\n    CreateFlag.validateForAppend(flag);\n    try {\n      LastBlockWithStatus blkWithStatus \u003d namenode.append(src, clientName,\n          new EnumSetWritable\u003c\u003e(flag, CreateFlag.class));\n      return DFSOutputStream.newStreamForAppend(this, src, flag, buffersize,\n          progress, blkWithStatus.getLastBlock(),\n          blkWithStatus.getFileStatus(), dfsClientConf.createChecksum(null),\n          favoredNodes);\n    } catch(RemoteException re) {\n      throw re.unwrapRemoteException(AccessControlException.class,\n                                     FileNotFoundException.class,\n                                     SafeModeException.class,\n                                     DSQuotaExceededException.class,\n                                     QuotaByStorageTypeExceededException.class,\n                                     UnsupportedOperationException.class,\n                                     UnresolvedPathException.class,\n                                     SnapshotAccessControlException.class);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
      "extendedDetails": {}
    },
    "2cc9514ad643ae49d30524743420ee9744e571bd": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8100. Refactor DFSClient.Conf to a standalone class and separates short-circuit related conf to ShortCircuitConf.\n",
      "commitDate": "10/04/15 2:48 PM",
      "commitName": "2cc9514ad643ae49d30524743420ee9744e571bd",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "10/04/15 11:40 AM",
      "commitNameOld": "7660da95cb67cbfe034aa8fa2a5bf0f8c9fdf41a",
      "commitAuthorOld": "Arun Suresh",
      "daysBetweenCommits": 0.13,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,21 @@\n   private DFSOutputStream callAppend(String src, int buffersize,\n       EnumSet\u003cCreateFlag\u003e flag, Progressable progress, String[] favoredNodes)\n       throws IOException {\n     CreateFlag.validateForAppend(flag);\n     try {\n       LastBlockWithStatus blkWithStatus \u003d namenode.append(src, clientName,\n           new EnumSetWritable\u003c\u003e(flag, CreateFlag.class));\n       return DFSOutputStream.newStreamForAppend(this, src, flag, buffersize,\n           progress, blkWithStatus.getLastBlock(),\n-          blkWithStatus.getFileStatus(), dfsClientConf.createChecksum(),\n+          blkWithStatus.getFileStatus(), dfsClientConf.createChecksum(null),\n           favoredNodes);\n     } catch(RemoteException re) {\n       throw re.unwrapRemoteException(AccessControlException.class,\n                                      FileNotFoundException.class,\n                                      SafeModeException.class,\n                                      DSQuotaExceededException.class,\n                                      UnsupportedOperationException.class,\n                                      UnresolvedPathException.class,\n                                      SnapshotAccessControlException.class);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private DFSOutputStream callAppend(String src, int buffersize,\n      EnumSet\u003cCreateFlag\u003e flag, Progressable progress, String[] favoredNodes)\n      throws IOException {\n    CreateFlag.validateForAppend(flag);\n    try {\n      LastBlockWithStatus blkWithStatus \u003d namenode.append(src, clientName,\n          new EnumSetWritable\u003c\u003e(flag, CreateFlag.class));\n      return DFSOutputStream.newStreamForAppend(this, src, flag, buffersize,\n          progress, blkWithStatus.getLastBlock(),\n          blkWithStatus.getFileStatus(), dfsClientConf.createChecksum(null),\n          favoredNodes);\n    } catch(RemoteException re) {\n      throw re.unwrapRemoteException(AccessControlException.class,\n                                     FileNotFoundException.class,\n                                     SafeModeException.class,\n                                     DSQuotaExceededException.class,\n                                     UnsupportedOperationException.class,\n                                     UnresolvedPathException.class,\n                                     SnapshotAccessControlException.class);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
      "extendedDetails": {}
    },
    "61df1b27a797efd094328c7d9141b9e157e01bf4": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7410. Support CreateFlags with append() to support hsync() for appending streams (Vinayakumar B via Colin P. McCabe)\n",
      "commitDate": "26/03/15 1:21 PM",
      "commitName": "61df1b27a797efd094328c7d9141b9e157e01bf4",
      "commitAuthor": "Colin Patrick Mccabe",
      "commitDateOld": "25/03/15 10:38 AM",
      "commitNameOld": "5e21e4ca377f68e030f8f3436cd93fd7a74dc5e0",
      "commitAuthorOld": "Jing Zhao",
      "daysBetweenCommits": 1.11,
      "commitsBetweenForRepo": 11,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,21 +1,21 @@\n   private DFSOutputStream callAppend(String src, int buffersize,\n       EnumSet\u003cCreateFlag\u003e flag, Progressable progress, String[] favoredNodes)\n       throws IOException {\n     CreateFlag.validateForAppend(flag);\n     try {\n       LastBlockWithStatus blkWithStatus \u003d namenode.append(src, clientName,\n           new EnumSetWritable\u003c\u003e(flag, CreateFlag.class));\n-      return DFSOutputStream.newStreamForAppend(this, src,\n-          flag.contains(CreateFlag.NEW_BLOCK),\n-          buffersize, progress, blkWithStatus.getLastBlock(),\n-          blkWithStatus.getFileStatus(), dfsClientConf.createChecksum(), favoredNodes);\n+      return DFSOutputStream.newStreamForAppend(this, src, flag, buffersize,\n+          progress, blkWithStatus.getLastBlock(),\n+          blkWithStatus.getFileStatus(), dfsClientConf.createChecksum(),\n+          favoredNodes);\n     } catch(RemoteException re) {\n       throw re.unwrapRemoteException(AccessControlException.class,\n                                      FileNotFoundException.class,\n                                      SafeModeException.class,\n                                      DSQuotaExceededException.class,\n                                      UnsupportedOperationException.class,\n                                      UnresolvedPathException.class,\n                                      SnapshotAccessControlException.class);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private DFSOutputStream callAppend(String src, int buffersize,\n      EnumSet\u003cCreateFlag\u003e flag, Progressable progress, String[] favoredNodes)\n      throws IOException {\n    CreateFlag.validateForAppend(flag);\n    try {\n      LastBlockWithStatus blkWithStatus \u003d namenode.append(src, clientName,\n          new EnumSetWritable\u003c\u003e(flag, CreateFlag.class));\n      return DFSOutputStream.newStreamForAppend(this, src, flag, buffersize,\n          progress, blkWithStatus.getLastBlock(),\n          blkWithStatus.getFileStatus(), dfsClientConf.createChecksum(),\n          favoredNodes);\n    } catch(RemoteException re) {\n      throw re.unwrapRemoteException(AccessControlException.class,\n                                     FileNotFoundException.class,\n                                     SafeModeException.class,\n                                     DSQuotaExceededException.class,\n                                     UnsupportedOperationException.class,\n                                     UnresolvedPathException.class,\n                                     SnapshotAccessControlException.class);\n    }\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
      "extendedDetails": {}
    },
    "89a544928083501625bc69f96b530040228f0a5f": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-7703. Support favouredNodes for the append for new blocks ( Contributed by Vinayakumar B)\n",
      "commitDate": "11/02/15 11:08 PM",
      "commitName": "89a544928083501625bc69f96b530040228f0a5f",
      "commitAuthor": "Vinayakumar B",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-7703. Support favouredNodes for the append for new blocks ( Contributed by Vinayakumar B)\n",
          "commitDate": "11/02/15 11:08 PM",
          "commitName": "89a544928083501625bc69f96b530040228f0a5f",
          "commitAuthor": "Vinayakumar B",
          "commitDateOld": "09/02/15 8:23 PM",
          "commitNameOld": "02340a24f211212b91dc7380c1e5b54ddb5e82eb",
          "commitAuthorOld": "Colin Patrick Mccabe",
          "daysBetweenCommits": 2.11,
          "commitsBetweenForRepo": 35,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,20 +1,21 @@\n   private DFSOutputStream callAppend(String src, int buffersize,\n-      EnumSet\u003cCreateFlag\u003e flag, Progressable progress) throws IOException {\n+      EnumSet\u003cCreateFlag\u003e flag, Progressable progress, String[] favoredNodes)\n+      throws IOException {\n     CreateFlag.validateForAppend(flag);\n     try {\n       LastBlockWithStatus blkWithStatus \u003d namenode.append(src, clientName,\n           new EnumSetWritable\u003c\u003e(flag, CreateFlag.class));\n       return DFSOutputStream.newStreamForAppend(this, src,\n           flag.contains(CreateFlag.NEW_BLOCK),\n           buffersize, progress, blkWithStatus.getLastBlock(),\n-          blkWithStatus.getFileStatus(), dfsClientConf.createChecksum());\n+          blkWithStatus.getFileStatus(), dfsClientConf.createChecksum(), favoredNodes);\n     } catch(RemoteException re) {\n       throw re.unwrapRemoteException(AccessControlException.class,\n                                      FileNotFoundException.class,\n                                      SafeModeException.class,\n                                      DSQuotaExceededException.class,\n                                      UnsupportedOperationException.class,\n                                      UnresolvedPathException.class,\n                                      SnapshotAccessControlException.class);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private DFSOutputStream callAppend(String src, int buffersize,\n      EnumSet\u003cCreateFlag\u003e flag, Progressable progress, String[] favoredNodes)\n      throws IOException {\n    CreateFlag.validateForAppend(flag);\n    try {\n      LastBlockWithStatus blkWithStatus \u003d namenode.append(src, clientName,\n          new EnumSetWritable\u003c\u003e(flag, CreateFlag.class));\n      return DFSOutputStream.newStreamForAppend(this, src,\n          flag.contains(CreateFlag.NEW_BLOCK),\n          buffersize, progress, blkWithStatus.getLastBlock(),\n          blkWithStatus.getFileStatus(), dfsClientConf.createChecksum(), favoredNodes);\n    } catch(RemoteException re) {\n      throw re.unwrapRemoteException(AccessControlException.class,\n                                     FileNotFoundException.class,\n                                     SafeModeException.class,\n                                     DSQuotaExceededException.class,\n                                     UnsupportedOperationException.class,\n                                     UnresolvedPathException.class,\n                                     SnapshotAccessControlException.class);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
          "extendedDetails": {
            "oldValue": "[src-String, buffersize-int, flag-EnumSet\u003cCreateFlag\u003e, progress-Progressable]",
            "newValue": "[src-String, buffersize-int, flag-EnumSet\u003cCreateFlag\u003e, progress-Progressable, favoredNodes-String[]]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-7703. Support favouredNodes for the append for new blocks ( Contributed by Vinayakumar B)\n",
          "commitDate": "11/02/15 11:08 PM",
          "commitName": "89a544928083501625bc69f96b530040228f0a5f",
          "commitAuthor": "Vinayakumar B",
          "commitDateOld": "09/02/15 8:23 PM",
          "commitNameOld": "02340a24f211212b91dc7380c1e5b54ddb5e82eb",
          "commitAuthorOld": "Colin Patrick Mccabe",
          "daysBetweenCommits": 2.11,
          "commitsBetweenForRepo": 35,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,20 +1,21 @@\n   private DFSOutputStream callAppend(String src, int buffersize,\n-      EnumSet\u003cCreateFlag\u003e flag, Progressable progress) throws IOException {\n+      EnumSet\u003cCreateFlag\u003e flag, Progressable progress, String[] favoredNodes)\n+      throws IOException {\n     CreateFlag.validateForAppend(flag);\n     try {\n       LastBlockWithStatus blkWithStatus \u003d namenode.append(src, clientName,\n           new EnumSetWritable\u003c\u003e(flag, CreateFlag.class));\n       return DFSOutputStream.newStreamForAppend(this, src,\n           flag.contains(CreateFlag.NEW_BLOCK),\n           buffersize, progress, blkWithStatus.getLastBlock(),\n-          blkWithStatus.getFileStatus(), dfsClientConf.createChecksum());\n+          blkWithStatus.getFileStatus(), dfsClientConf.createChecksum(), favoredNodes);\n     } catch(RemoteException re) {\n       throw re.unwrapRemoteException(AccessControlException.class,\n                                      FileNotFoundException.class,\n                                      SafeModeException.class,\n                                      DSQuotaExceededException.class,\n                                      UnsupportedOperationException.class,\n                                      UnresolvedPathException.class,\n                                      SnapshotAccessControlException.class);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private DFSOutputStream callAppend(String src, int buffersize,\n      EnumSet\u003cCreateFlag\u003e flag, Progressable progress, String[] favoredNodes)\n      throws IOException {\n    CreateFlag.validateForAppend(flag);\n    try {\n      LastBlockWithStatus blkWithStatus \u003d namenode.append(src, clientName,\n          new EnumSetWritable\u003c\u003e(flag, CreateFlag.class));\n      return DFSOutputStream.newStreamForAppend(this, src,\n          flag.contains(CreateFlag.NEW_BLOCK),\n          buffersize, progress, blkWithStatus.getLastBlock(),\n          blkWithStatus.getFileStatus(), dfsClientConf.createChecksum(), favoredNodes);\n    } catch(RemoteException re) {\n      throw re.unwrapRemoteException(AccessControlException.class,\n                                     FileNotFoundException.class,\n                                     SafeModeException.class,\n                                     DSQuotaExceededException.class,\n                                     UnsupportedOperationException.class,\n                                     UnresolvedPathException.class,\n                                     SnapshotAccessControlException.class);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
          "extendedDetails": {}
        }
      ]
    },
    "2848db814a98b83e7546f65a2751e56fb5b2dbe0": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-3689. Add support for variable length block. Contributed by Jing Zhao.\n",
      "commitDate": "27/01/15 12:58 PM",
      "commitName": "2848db814a98b83e7546f65a2751e56fb5b2dbe0",
      "commitAuthor": "Jing Zhao",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-3689. Add support for variable length block. Contributed by Jing Zhao.\n",
          "commitDate": "27/01/15 12:58 PM",
          "commitName": "2848db814a98b83e7546f65a2751e56fb5b2dbe0",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "24/01/15 3:44 PM",
          "commitNameOld": "e9fd46ddbf46954cfda4bb9c33f1789775be9d18",
          "commitAuthorOld": "yliu",
          "daysBetweenCommits": 2.88,
          "commitsBetweenForRepo": 18,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,19 +1,20 @@\n-  private DFSOutputStream callAppend(String src,\n-      int buffersize, Progressable progress) throws IOException {\n-    LastBlockWithStatus lastBlockWithStatus \u003d null;\n+  private DFSOutputStream callAppend(String src, int buffersize,\n+      EnumSet\u003cCreateFlag\u003e flag, Progressable progress) throws IOException {\n+    CreateFlag.validateForAppend(flag);\n     try {\n-      lastBlockWithStatus \u003d namenode.append(src, clientName);\n+      LastBlockWithStatus blkWithStatus \u003d namenode.append(src, clientName,\n+          new EnumSetWritable\u003c\u003e(flag, CreateFlag.class));\n+      return DFSOutputStream.newStreamForAppend(this, src,\n+          flag.contains(CreateFlag.NEW_BLOCK),\n+          buffersize, progress, blkWithStatus.getLastBlock(),\n+          blkWithStatus.getFileStatus(), dfsClientConf.createChecksum());\n     } catch(RemoteException re) {\n       throw re.unwrapRemoteException(AccessControlException.class,\n                                      FileNotFoundException.class,\n                                      SafeModeException.class,\n                                      DSQuotaExceededException.class,\n                                      UnsupportedOperationException.class,\n                                      UnresolvedPathException.class,\n                                      SnapshotAccessControlException.class);\n     }\n-    HdfsFileStatus newStat \u003d lastBlockWithStatus.getFileStatus();\n-    return DFSOutputStream.newStreamForAppend(this, src, buffersize, progress,\n-        lastBlockWithStatus.getLastBlock(), newStat,\n-        dfsClientConf.createChecksum());\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private DFSOutputStream callAppend(String src, int buffersize,\n      EnumSet\u003cCreateFlag\u003e flag, Progressable progress) throws IOException {\n    CreateFlag.validateForAppend(flag);\n    try {\n      LastBlockWithStatus blkWithStatus \u003d namenode.append(src, clientName,\n          new EnumSetWritable\u003c\u003e(flag, CreateFlag.class));\n      return DFSOutputStream.newStreamForAppend(this, src,\n          flag.contains(CreateFlag.NEW_BLOCK),\n          buffersize, progress, blkWithStatus.getLastBlock(),\n          blkWithStatus.getFileStatus(), dfsClientConf.createChecksum());\n    } catch(RemoteException re) {\n      throw re.unwrapRemoteException(AccessControlException.class,\n                                     FileNotFoundException.class,\n                                     SafeModeException.class,\n                                     DSQuotaExceededException.class,\n                                     UnsupportedOperationException.class,\n                                     UnresolvedPathException.class,\n                                     SnapshotAccessControlException.class);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
          "extendedDetails": {
            "oldValue": "[src-String, buffersize-int, progress-Progressable]",
            "newValue": "[src-String, buffersize-int, flag-EnumSet\u003cCreateFlag\u003e, progress-Progressable]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-3689. Add support for variable length block. Contributed by Jing Zhao.\n",
          "commitDate": "27/01/15 12:58 PM",
          "commitName": "2848db814a98b83e7546f65a2751e56fb5b2dbe0",
          "commitAuthor": "Jing Zhao",
          "commitDateOld": "24/01/15 3:44 PM",
          "commitNameOld": "e9fd46ddbf46954cfda4bb9c33f1789775be9d18",
          "commitAuthorOld": "yliu",
          "daysBetweenCommits": 2.88,
          "commitsBetweenForRepo": 18,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,19 +1,20 @@\n-  private DFSOutputStream callAppend(String src,\n-      int buffersize, Progressable progress) throws IOException {\n-    LastBlockWithStatus lastBlockWithStatus \u003d null;\n+  private DFSOutputStream callAppend(String src, int buffersize,\n+      EnumSet\u003cCreateFlag\u003e flag, Progressable progress) throws IOException {\n+    CreateFlag.validateForAppend(flag);\n     try {\n-      lastBlockWithStatus \u003d namenode.append(src, clientName);\n+      LastBlockWithStatus blkWithStatus \u003d namenode.append(src, clientName,\n+          new EnumSetWritable\u003c\u003e(flag, CreateFlag.class));\n+      return DFSOutputStream.newStreamForAppend(this, src,\n+          flag.contains(CreateFlag.NEW_BLOCK),\n+          buffersize, progress, blkWithStatus.getLastBlock(),\n+          blkWithStatus.getFileStatus(), dfsClientConf.createChecksum());\n     } catch(RemoteException re) {\n       throw re.unwrapRemoteException(AccessControlException.class,\n                                      FileNotFoundException.class,\n                                      SafeModeException.class,\n                                      DSQuotaExceededException.class,\n                                      UnsupportedOperationException.class,\n                                      UnresolvedPathException.class,\n                                      SnapshotAccessControlException.class);\n     }\n-    HdfsFileStatus newStat \u003d lastBlockWithStatus.getFileStatus();\n-    return DFSOutputStream.newStreamForAppend(this, src, buffersize, progress,\n-        lastBlockWithStatus.getLastBlock(), newStat,\n-        dfsClientConf.createChecksum());\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private DFSOutputStream callAppend(String src, int buffersize,\n      EnumSet\u003cCreateFlag\u003e flag, Progressable progress) throws IOException {\n    CreateFlag.validateForAppend(flag);\n    try {\n      LastBlockWithStatus blkWithStatus \u003d namenode.append(src, clientName,\n          new EnumSetWritable\u003c\u003e(flag, CreateFlag.class));\n      return DFSOutputStream.newStreamForAppend(this, src,\n          flag.contains(CreateFlag.NEW_BLOCK),\n          buffersize, progress, blkWithStatus.getLastBlock(),\n          blkWithStatus.getFileStatus(), dfsClientConf.createChecksum());\n    } catch(RemoteException re) {\n      throw re.unwrapRemoteException(AccessControlException.class,\n                                     FileNotFoundException.class,\n                                     SafeModeException.class,\n                                     DSQuotaExceededException.class,\n                                     UnsupportedOperationException.class,\n                                     UnresolvedPathException.class,\n                                     SnapshotAccessControlException.class);\n    }\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
          "extendedDetails": {}
        }
      ]
    },
    "1556f86a31a54733d6550363aa0e027acca7823b": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-7210. Avoid two separate RPC\u0027s namenode.append() and namenode.getFileInfo() for an append call from DFSClient. (Vinayakumar B via umamahesh)\n",
      "commitDate": "28/11/14 7:39 AM",
      "commitName": "1556f86a31a54733d6550363aa0e027acca7823b",
      "commitAuthor": "Uma Maheswara Rao G",
      "commitDateOld": "05/11/14 9:00 PM",
      "commitNameOld": "80d7d183cd4052d6e6d412ff6588d26471c85d6d",
      "commitAuthorOld": "Milan Desai",
      "daysBetweenCommits": 22.44,
      "commitsBetweenForRepo": 163,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,18 +1,19 @@\n   private DFSOutputStream callAppend(String src,\n       int buffersize, Progressable progress) throws IOException {\n-    LocatedBlock lastBlock \u003d null;\n+    LastBlockWithStatus lastBlockWithStatus \u003d null;\n     try {\n-      lastBlock \u003d namenode.append(src, clientName);\n+      lastBlockWithStatus \u003d namenode.append(src, clientName);\n     } catch(RemoteException re) {\n       throw re.unwrapRemoteException(AccessControlException.class,\n                                      FileNotFoundException.class,\n                                      SafeModeException.class,\n                                      DSQuotaExceededException.class,\n                                      UnsupportedOperationException.class,\n                                      UnresolvedPathException.class,\n                                      SnapshotAccessControlException.class);\n     }\n-    HdfsFileStatus newStat \u003d getFileInfo(src);\n+    HdfsFileStatus newStat \u003d lastBlockWithStatus.getFileStatus();\n     return DFSOutputStream.newStreamForAppend(this, src, buffersize, progress,\n-        lastBlock, newStat, dfsClientConf.createChecksum());\n+        lastBlockWithStatus.getLastBlock(), newStat,\n+        dfsClientConf.createChecksum());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private DFSOutputStream callAppend(String src,\n      int buffersize, Progressable progress) throws IOException {\n    LastBlockWithStatus lastBlockWithStatus \u003d null;\n    try {\n      lastBlockWithStatus \u003d namenode.append(src, clientName);\n    } catch(RemoteException re) {\n      throw re.unwrapRemoteException(AccessControlException.class,\n                                     FileNotFoundException.class,\n                                     SafeModeException.class,\n                                     DSQuotaExceededException.class,\n                                     UnsupportedOperationException.class,\n                                     UnresolvedPathException.class,\n                                     SnapshotAccessControlException.class);\n    }\n    HdfsFileStatus newStat \u003d lastBlockWithStatus.getFileStatus();\n    return DFSOutputStream.newStreamForAppend(this, src, buffersize, progress,\n        lastBlockWithStatus.getLastBlock(), newStat,\n        dfsClientConf.createChecksum());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
      "extendedDetails": {}
    },
    "853cb704edf54207313c0e70c9c375212d288b60": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-7203. Concurrent appending to the same file can cause data\ncorruption. Contributed by Kihwal Lee.\n",
      "commitDate": "08/10/14 1:05 PM",
      "commitName": "853cb704edf54207313c0e70c9c375212d288b60",
      "commitAuthor": "Kihwal Lee",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-7203. Concurrent appending to the same file can cause data\ncorruption. Contributed by Kihwal Lee.\n",
          "commitDate": "08/10/14 1:05 PM",
          "commitName": "853cb704edf54207313c0e70c9c375212d288b60",
          "commitAuthor": "Kihwal Lee",
          "commitDateOld": "06/10/14 3:29 PM",
          "commitNameOld": "8dc6abf2f4218b2d84b2c2dc7d18623d897c362d",
          "commitAuthorOld": "Colin Patrick Mccabe",
          "daysBetweenCommits": 1.9,
          "commitsBetweenForRepo": 16,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,17 +1,18 @@\n-  private DFSOutputStream callAppend(HdfsFileStatus stat, String src,\n+  private DFSOutputStream callAppend(String src,\n       int buffersize, Progressable progress) throws IOException {\n     LocatedBlock lastBlock \u003d null;\n     try {\n       lastBlock \u003d namenode.append(src, clientName);\n     } catch(RemoteException re) {\n       throw re.unwrapRemoteException(AccessControlException.class,\n                                      FileNotFoundException.class,\n                                      SafeModeException.class,\n                                      DSQuotaExceededException.class,\n                                      UnsupportedOperationException.class,\n                                      UnresolvedPathException.class,\n                                      SnapshotAccessControlException.class);\n     }\n+    HdfsFileStatus newStat \u003d getFileInfo(src);\n     return DFSOutputStream.newStreamForAppend(this, src, buffersize, progress,\n-        lastBlock, stat, dfsClientConf.createChecksum());\n+        lastBlock, newStat, dfsClientConf.createChecksum());\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private DFSOutputStream callAppend(String src,\n      int buffersize, Progressable progress) throws IOException {\n    LocatedBlock lastBlock \u003d null;\n    try {\n      lastBlock \u003d namenode.append(src, clientName);\n    } catch(RemoteException re) {\n      throw re.unwrapRemoteException(AccessControlException.class,\n                                     FileNotFoundException.class,\n                                     SafeModeException.class,\n                                     DSQuotaExceededException.class,\n                                     UnsupportedOperationException.class,\n                                     UnresolvedPathException.class,\n                                     SnapshotAccessControlException.class);\n    }\n    HdfsFileStatus newStat \u003d getFileInfo(src);\n    return DFSOutputStream.newStreamForAppend(this, src, buffersize, progress,\n        lastBlock, newStat, dfsClientConf.createChecksum());\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
          "extendedDetails": {
            "oldValue": "[stat-HdfsFileStatus, src-String, buffersize-int, progress-Progressable]",
            "newValue": "[src-String, buffersize-int, progress-Progressable]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-7203. Concurrent appending to the same file can cause data\ncorruption. Contributed by Kihwal Lee.\n",
          "commitDate": "08/10/14 1:05 PM",
          "commitName": "853cb704edf54207313c0e70c9c375212d288b60",
          "commitAuthor": "Kihwal Lee",
          "commitDateOld": "06/10/14 3:29 PM",
          "commitNameOld": "8dc6abf2f4218b2d84b2c2dc7d18623d897c362d",
          "commitAuthorOld": "Colin Patrick Mccabe",
          "daysBetweenCommits": 1.9,
          "commitsBetweenForRepo": 16,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,17 +1,18 @@\n-  private DFSOutputStream callAppend(HdfsFileStatus stat, String src,\n+  private DFSOutputStream callAppend(String src,\n       int buffersize, Progressable progress) throws IOException {\n     LocatedBlock lastBlock \u003d null;\n     try {\n       lastBlock \u003d namenode.append(src, clientName);\n     } catch(RemoteException re) {\n       throw re.unwrapRemoteException(AccessControlException.class,\n                                      FileNotFoundException.class,\n                                      SafeModeException.class,\n                                      DSQuotaExceededException.class,\n                                      UnsupportedOperationException.class,\n                                      UnresolvedPathException.class,\n                                      SnapshotAccessControlException.class);\n     }\n+    HdfsFileStatus newStat \u003d getFileInfo(src);\n     return DFSOutputStream.newStreamForAppend(this, src, buffersize, progress,\n-        lastBlock, stat, dfsClientConf.createChecksum());\n+        lastBlock, newStat, dfsClientConf.createChecksum());\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private DFSOutputStream callAppend(String src,\n      int buffersize, Progressable progress) throws IOException {\n    LocatedBlock lastBlock \u003d null;\n    try {\n      lastBlock \u003d namenode.append(src, clientName);\n    } catch(RemoteException re) {\n      throw re.unwrapRemoteException(AccessControlException.class,\n                                     FileNotFoundException.class,\n                                     SafeModeException.class,\n                                     DSQuotaExceededException.class,\n                                     UnsupportedOperationException.class,\n                                     UnresolvedPathException.class,\n                                     SnapshotAccessControlException.class);\n    }\n    HdfsFileStatus newStat \u003d getFileInfo(src);\n    return DFSOutputStream.newStreamForAppend(this, src, buffersize, progress,\n        lastBlock, newStat, dfsClientConf.createChecksum());\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
          "extendedDetails": {}
        }
      ]
    },
    "2116d0520e528c44fa280f2a5b28594c6d6fc28a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-4148. Disallow write/modify operations on files and directories in a snapshot. Contributed by Brandon Li.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2802@1409023 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/11/12 3:26 PM",
      "commitName": "2116d0520e528c44fa280f2a5b28594c6d6fc28a",
      "commitAuthor": "Suresh Srinivas",
      "commitDateOld": "29/10/12 7:09 AM",
      "commitNameOld": "564adec5b5d5264897572f68d9e19a916f887c0d",
      "commitAuthorOld": "",
      "daysBetweenCommits": 15.39,
      "commitsBetweenForRepo": 102,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,16 +1,17 @@\n   private DFSOutputStream callAppend(HdfsFileStatus stat, String src,\n       int buffersize, Progressable progress) throws IOException {\n     LocatedBlock lastBlock \u003d null;\n     try {\n       lastBlock \u003d namenode.append(src, clientName);\n     } catch(RemoteException re) {\n       throw re.unwrapRemoteException(AccessControlException.class,\n                                      FileNotFoundException.class,\n                                      SafeModeException.class,\n                                      DSQuotaExceededException.class,\n                                      UnsupportedOperationException.class,\n-                                     UnresolvedPathException.class);\n+                                     UnresolvedPathException.class,\n+                                     SnapshotAccessControlException.class);\n     }\n     return DFSOutputStream.newStreamForAppend(this, src, buffersize, progress,\n         lastBlock, stat, dfsClientConf.createChecksum());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private DFSOutputStream callAppend(HdfsFileStatus stat, String src,\n      int buffersize, Progressable progress) throws IOException {\n    LocatedBlock lastBlock \u003d null;\n    try {\n      lastBlock \u003d namenode.append(src, clientName);\n    } catch(RemoteException re) {\n      throw re.unwrapRemoteException(AccessControlException.class,\n                                     FileNotFoundException.class,\n                                     SafeModeException.class,\n                                     DSQuotaExceededException.class,\n                                     UnsupportedOperationException.class,\n                                     UnresolvedPathException.class,\n                                     SnapshotAccessControlException.class);\n    }\n    return DFSOutputStream.newStreamForAppend(this, src, buffersize, progress,\n        lastBlock, stat, dfsClientConf.createChecksum());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
      "extendedDetails": {}
    },
    "3b773da0361bdec594acd6814c49b81f867bd673": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-3319. Change DFSOutputStream to not to start a thread in constructors.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1330535 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "25/04/12 1:17 PM",
      "commitName": "3b773da0361bdec594acd6814c49b81f867bd673",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "24/04/12 4:39 PM",
      "commitNameOld": "ea32198db4e783f0c0b93a3f74120fe41ded98e8",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 0.86,
      "commitsBetweenForRepo": 7,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,16 +1,16 @@\n   private DFSOutputStream callAppend(HdfsFileStatus stat, String src,\n       int buffersize, Progressable progress) throws IOException {\n     LocatedBlock lastBlock \u003d null;\n     try {\n       lastBlock \u003d namenode.append(src, clientName);\n     } catch(RemoteException re) {\n       throw re.unwrapRemoteException(AccessControlException.class,\n                                      FileNotFoundException.class,\n                                      SafeModeException.class,\n                                      DSQuotaExceededException.class,\n                                      UnsupportedOperationException.class,\n                                      UnresolvedPathException.class);\n     }\n-    return new DFSOutputStream(this, src, buffersize, progress,\n+    return DFSOutputStream.newStreamForAppend(this, src, buffersize, progress,\n         lastBlock, stat, dfsClientConf.createChecksum());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private DFSOutputStream callAppend(HdfsFileStatus stat, String src,\n      int buffersize, Progressable progress) throws IOException {\n    LocatedBlock lastBlock \u003d null;\n    try {\n      lastBlock \u003d namenode.append(src, clientName);\n    } catch(RemoteException re) {\n      throw re.unwrapRemoteException(AccessControlException.class,\n                                     FileNotFoundException.class,\n                                     SafeModeException.class,\n                                     DSQuotaExceededException.class,\n                                     UnsupportedOperationException.class,\n                                     UnresolvedPathException.class);\n    }\n    return DFSOutputStream.newStreamForAppend(this, src, buffersize, progress,\n        lastBlock, stat, dfsClientConf.createChecksum());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
      "extendedDetails": {}
    },
    "f84552ac35bb5221290be68fece9c779ebeaf4bc": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2130. Switch default checksum to CRC32C. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1196889 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "02/11/11 5:35 PM",
      "commitName": "f84552ac35bb5221290be68fece9c779ebeaf4bc",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "31/10/11 5:38 PM",
      "commitNameOld": "8d1a09b7418d5cb512affaede0bb1ae494502967",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 2.0,
      "commitsBetweenForRepo": 31,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,16 +1,16 @@\n   private DFSOutputStream callAppend(HdfsFileStatus stat, String src,\n       int buffersize, Progressable progress) throws IOException {\n     LocatedBlock lastBlock \u003d null;\n     try {\n       lastBlock \u003d namenode.append(src, clientName);\n     } catch(RemoteException re) {\n       throw re.unwrapRemoteException(AccessControlException.class,\n                                      FileNotFoundException.class,\n                                      SafeModeException.class,\n                                      DSQuotaExceededException.class,\n                                      UnsupportedOperationException.class,\n                                      UnresolvedPathException.class);\n     }\n     return new DFSOutputStream(this, src, buffersize, progress,\n-        lastBlock, stat, dfsClientConf.bytesPerChecksum);\n+        lastBlock, stat, dfsClientConf.createChecksum());\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private DFSOutputStream callAppend(HdfsFileStatus stat, String src,\n      int buffersize, Progressable progress) throws IOException {\n    LocatedBlock lastBlock \u003d null;\n    try {\n      lastBlock \u003d namenode.append(src, clientName);\n    } catch(RemoteException re) {\n      throw re.unwrapRemoteException(AccessControlException.class,\n                                     FileNotFoundException.class,\n                                     SafeModeException.class,\n                                     DSQuotaExceededException.class,\n                                     UnsupportedOperationException.class,\n                                     UnresolvedPathException.class);\n    }\n    return new DFSOutputStream(this, src, buffersize, progress,\n        lastBlock, stat, dfsClientConf.createChecksum());\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
      "extendedDetails": {}
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private DFSOutputStream callAppend(HdfsFileStatus stat, String src,\n      int buffersize, Progressable progress) throws IOException {\n    LocatedBlock lastBlock \u003d null;\n    try {\n      lastBlock \u003d namenode.append(src, clientName);\n    } catch(RemoteException re) {\n      throw re.unwrapRemoteException(AccessControlException.class,\n                                     FileNotFoundException.class,\n                                     SafeModeException.class,\n                                     DSQuotaExceededException.class,\n                                     UnsupportedOperationException.class,\n                                     UnresolvedPathException.class);\n    }\n    return new DFSOutputStream(this, src, buffersize, progress,\n        lastBlock, stat, dfsClientConf.bytesPerChecksum);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java"
      }
    },
    "d86f3183d93714ba078416af4f609d26376eadb0": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-2096. Mavenization of hadoop-hdfs. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159702 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/08/11 10:36 AM",
      "commitName": "d86f3183d93714ba078416af4f609d26376eadb0",
      "commitAuthor": "Thomas White",
      "commitDateOld": "19/08/11 10:26 AM",
      "commitNameOld": "6ee5a73e0e91a2ef27753a32c576835e951d8119",
      "commitAuthorOld": "Thomas White",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  private DFSOutputStream callAppend(HdfsFileStatus stat, String src,\n      int buffersize, Progressable progress) throws IOException {\n    LocatedBlock lastBlock \u003d null;\n    try {\n      lastBlock \u003d namenode.append(src, clientName);\n    } catch(RemoteException re) {\n      throw re.unwrapRemoteException(AccessControlException.class,\n                                     FileNotFoundException.class,\n                                     SafeModeException.class,\n                                     DSQuotaExceededException.class,\n                                     UnsupportedOperationException.class,\n                                     UnresolvedPathException.class);\n    }\n    return new DFSOutputStream(this, src, buffersize, progress,\n        lastBlock, stat, dfsClientConf.bytesPerChecksum);\n  }",
      "path": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
      "extendedDetails": {
        "oldPath": "hdfs/src/java/org/apache/hadoop/hdfs/DFSClient.java",
        "newPath": "hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java"
      }
    },
    "fd9997989c1f1c6f806c57a806e7225ca599fc0c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-2092. Remove some object references to Configuration in DFSClient.  Contributed by Bharath Mundlapudi\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1139097 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "23/06/11 3:24 PM",
      "commitName": "fd9997989c1f1c6f806c57a806e7225ca599fc0c",
      "commitAuthor": "Tsz-wo Sze",
      "commitDateOld": "12/06/11 3:00 PM",
      "commitNameOld": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 11.02,
      "commitsBetweenForRepo": 36,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,17 +1,16 @@\n   private DFSOutputStream callAppend(HdfsFileStatus stat, String src,\n       int buffersize, Progressable progress) throws IOException {\n     LocatedBlock lastBlock \u003d null;\n     try {\n       lastBlock \u003d namenode.append(src, clientName);\n     } catch(RemoteException re) {\n       throw re.unwrapRemoteException(AccessControlException.class,\n                                      FileNotFoundException.class,\n                                      SafeModeException.class,\n                                      DSQuotaExceededException.class,\n                                      UnsupportedOperationException.class,\n                                      UnresolvedPathException.class);\n     }\n     return new DFSOutputStream(this, src, buffersize, progress,\n-        lastBlock, stat, conf.getInt(DFSConfigKeys.DFS_BYTES_PER_CHECKSUM_KEY, \n-                                     DFSConfigKeys.DFS_BYTES_PER_CHECKSUM_DEFAULT));\n+        lastBlock, stat, dfsClientConf.bytesPerChecksum);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private DFSOutputStream callAppend(HdfsFileStatus stat, String src,\n      int buffersize, Progressable progress) throws IOException {\n    LocatedBlock lastBlock \u003d null;\n    try {\n      lastBlock \u003d namenode.append(src, clientName);\n    } catch(RemoteException re) {\n      throw re.unwrapRemoteException(AccessControlException.class,\n                                     FileNotFoundException.class,\n                                     SafeModeException.class,\n                                     DSQuotaExceededException.class,\n                                     UnsupportedOperationException.class,\n                                     UnresolvedPathException.class);\n    }\n    return new DFSOutputStream(this, src, buffersize, progress,\n        lastBlock, stat, dfsClientConf.bytesPerChecksum);\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/DFSClient.java",
      "extendedDetails": {}
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,17 @@\n+  private DFSOutputStream callAppend(HdfsFileStatus stat, String src,\n+      int buffersize, Progressable progress) throws IOException {\n+    LocatedBlock lastBlock \u003d null;\n+    try {\n+      lastBlock \u003d namenode.append(src, clientName);\n+    } catch(RemoteException re) {\n+      throw re.unwrapRemoteException(AccessControlException.class,\n+                                     FileNotFoundException.class,\n+                                     SafeModeException.class,\n+                                     DSQuotaExceededException.class,\n+                                     UnsupportedOperationException.class,\n+                                     UnresolvedPathException.class);\n+    }\n+    return new DFSOutputStream(this, src, buffersize, progress,\n+        lastBlock, stat, conf.getInt(DFSConfigKeys.DFS_BYTES_PER_CHECKSUM_KEY, \n+                                     DFSConfigKeys.DFS_BYTES_PER_CHECKSUM_DEFAULT));\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private DFSOutputStream callAppend(HdfsFileStatus stat, String src,\n      int buffersize, Progressable progress) throws IOException {\n    LocatedBlock lastBlock \u003d null;\n    try {\n      lastBlock \u003d namenode.append(src, clientName);\n    } catch(RemoteException re) {\n      throw re.unwrapRemoteException(AccessControlException.class,\n                                     FileNotFoundException.class,\n                                     SafeModeException.class,\n                                     DSQuotaExceededException.class,\n                                     UnsupportedOperationException.class,\n                                     UnresolvedPathException.class);\n    }\n    return new DFSOutputStream(this, src, buffersize, progress,\n        lastBlock, stat, conf.getInt(DFSConfigKeys.DFS_BYTES_PER_CHECKSUM_KEY, \n                                     DFSConfigKeys.DFS_BYTES_PER_CHECKSUM_DEFAULT));\n  }",
      "path": "hdfs/src/java/org/apache/hadoop/hdfs/DFSClient.java"
    }
  }
}