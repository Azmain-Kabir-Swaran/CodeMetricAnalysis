{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "StripedBlockChecksumReconstructor.java",
  "functionName": "init",
  "functionId": "init",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedBlockChecksumReconstructor.java",
  "functionStartLine": 56,
  "functionEndLine": 70,
  "numCommitsSeen": 8,
  "timeTaken": 1445,
  "changeHistory": [
    "b5af9be72c72734d668f817c99d889031922a951",
    "d749cf65e1ab0e0daf5be86931507183f189e855"
  ],
  "changeHistoryShort": {
    "b5af9be72c72734d668f817c99d889031922a951": "Ybodychange",
    "d749cf65e1ab0e0daf5be86931507183f189e855": "Yintroduced"
  },
  "changeHistoryDetails": {
    "b5af9be72c72734d668f817c99d889031922a951": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8668. Erasure Coding: revisit buffer used for encoding and decoding. Contributed by Sammi Chen\n",
      "commitDate": "12/08/16 10:52 PM",
      "commitName": "b5af9be72c72734d668f817c99d889031922a951",
      "commitAuthor": "Kai Zheng",
      "commitDateOld": "24/06/16 2:39 AM",
      "commitNameOld": "e6cb07520f935efde3e881de8f84ee7f6e0a746f",
      "commitAuthorOld": "Kai Zheng",
      "daysBetweenCommits": 49.84,
      "commitsBetweenForRepo": 424,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,14 +1,15 @@\n   private void init() throws IOException {\n+    initDecoderIfNecessary();\n     getStripedReader().init();\n     // allocate buffer to keep the reconstructed block data\n     targetBuffer \u003d allocateBuffer(getBufferSize());\n     long maxTargetLen \u003d 0L;\n     for (int targetIndex : targetIndices) {\n       maxTargetLen \u003d Math.max(maxTargetLen, getBlockLen(targetIndex));\n     }\n     setMaxTargetLength(maxTargetLen);\n     int checksumSize \u003d getChecksum().getChecksumSize();\n     int bytesPerChecksum \u003d getChecksum().getBytesPerChecksum();\n     int tmpLen \u003d checksumSize * (getBufferSize() / bytesPerChecksum);\n     checksumBuf \u003d new byte[tmpLen];\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void init() throws IOException {\n    initDecoderIfNecessary();\n    getStripedReader().init();\n    // allocate buffer to keep the reconstructed block data\n    targetBuffer \u003d allocateBuffer(getBufferSize());\n    long maxTargetLen \u003d 0L;\n    for (int targetIndex : targetIndices) {\n      maxTargetLen \u003d Math.max(maxTargetLen, getBlockLen(targetIndex));\n    }\n    setMaxTargetLength(maxTargetLen);\n    int checksumSize \u003d getChecksum().getChecksumSize();\n    int bytesPerChecksum \u003d getChecksum().getBytesPerChecksum();\n    int tmpLen \u003d checksumSize * (getBufferSize() / bytesPerChecksum);\n    checksumBuf \u003d new byte[tmpLen];\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedBlockChecksumReconstructor.java",
      "extendedDetails": {}
    },
    "d749cf65e1ab0e0daf5be86931507183f189e855": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-9833. Erasure coding: recomputing block checksum on the fly by reconstructing the missed/corrupt block data. Contributed by Rakesh R.\n",
      "commitDate": "01/06/16 9:56 PM",
      "commitName": "d749cf65e1ab0e0daf5be86931507183f189e855",
      "commitAuthor": "Kai Zheng",
      "diff": "@@ -0,0 +1,14 @@\n+  private void init() throws IOException {\n+    getStripedReader().init();\n+    // allocate buffer to keep the reconstructed block data\n+    targetBuffer \u003d allocateBuffer(getBufferSize());\n+    long maxTargetLen \u003d 0L;\n+    for (int targetIndex : targetIndices) {\n+      maxTargetLen \u003d Math.max(maxTargetLen, getBlockLen(targetIndex));\n+    }\n+    setMaxTargetLength(maxTargetLen);\n+    int checksumSize \u003d getChecksum().getChecksumSize();\n+    int bytesPerChecksum \u003d getChecksum().getBytesPerChecksum();\n+    int tmpLen \u003d checksumSize * (getBufferSize() / bytesPerChecksum);\n+    checksumBuf \u003d new byte[tmpLen];\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void init() throws IOException {\n    getStripedReader().init();\n    // allocate buffer to keep the reconstructed block data\n    targetBuffer \u003d allocateBuffer(getBufferSize());\n    long maxTargetLen \u003d 0L;\n    for (int targetIndex : targetIndices) {\n      maxTargetLen \u003d Math.max(maxTargetLen, getBlockLen(targetIndex));\n    }\n    setMaxTargetLength(maxTargetLen);\n    int checksumSize \u003d getChecksum().getChecksumSize();\n    int bytesPerChecksum \u003d getChecksum().getBytesPerChecksum();\n    int tmpLen \u003d checksumSize * (getBufferSize() / bytesPerChecksum);\n    checksumBuf \u003d new byte[tmpLen];\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/StripedBlockChecksumReconstructor.java"
    }
  }
}