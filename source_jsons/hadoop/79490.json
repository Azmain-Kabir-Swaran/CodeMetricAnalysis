{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "ChecksumFs.java",
  "functionName": "readChunk",
  "functionId": "readChunk___pos-long__buf-byte[]__offset-int__len-int__checksum-byte[]",
  "sourceFilePath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/ChecksumFs.java",
  "functionStartLine": 207,
  "functionEndLine": 244,
  "numCommitsSeen": 28,
  "timeTaken": 1079,
  "changeHistory": [
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "0f6dfeeacbab65a31a33927a4eb84871d371fe52",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
    "ae91b5d0f4b18b939025ef374d971d7ada179473",
    "3f371a0a644181b204111ee4e12c995fc7b5e5f5"
  ],
  "changeHistoryShort": {
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "0f6dfeeacbab65a31a33927a4eb84871d371fe52": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yfilerename",
    "ae91b5d0f4b18b939025ef374d971d7ada179473": "Ybodychange",
    "3f371a0a644181b204111ee4e12c995fc7b5e5f5": "Yintroduced"
  },
  "changeHistoryDetails": {
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    protected int readChunk(long pos, byte[] buf, int offset, int len,\n        byte[] checksum) throws IOException {\n      boolean eof \u003d false;\n      if (needChecksum()) {\n        assert checksum !\u003d null; // we have a checksum buffer\n        assert checksum.length % CHECKSUM_SIZE \u003d\u003d 0; // it is sane length\n        assert len \u003e\u003d bytesPerSum; // we must read at least one chunk\n\n        final int checksumsToRead \u003d Math.min(\n          len/bytesPerSum, // number of checksums based on len to read\n          checksum.length / CHECKSUM_SIZE); // size of checksum buffer\n        long checksumPos \u003d getChecksumFilePos(pos); \n        if(checksumPos !\u003d sums.getPos()) {\n          sums.seek(checksumPos);\n        }\n\n        int sumLenRead \u003d sums.read(checksum, 0, CHECKSUM_SIZE * checksumsToRead);\n        if (sumLenRead \u003e\u003d 0 \u0026\u0026 sumLenRead % CHECKSUM_SIZE !\u003d 0) {\n          throw new EOFException(\"Checksum file not a length multiple of checksum size \" +\n                                 \"in \" + file + \" at \" + pos + \" checksumpos: \" + checksumPos +\n                                 \" sumLenread: \" + sumLenRead );\n        }\n        if (sumLenRead \u003c\u003d 0) { // we\u0027re at the end of the file\n          eof \u003d true;\n        } else {\n          // Adjust amount of data to read based on how many checksum chunks we read\n          len \u003d Math.min(len, bytesPerSum * (sumLenRead / CHECKSUM_SIZE));\n        }\n      }\n      if (pos !\u003d datas.getPos()) {\n        datas.seek(pos);\n      }\n      int nread \u003d readFully(datas, buf, offset, len);\n      if (eof \u0026\u0026 nread \u003e 0) {\n        throw new ChecksumException(\"Checksum error: \"+file+\" at \"+pos, pos);\n      }\n      return nread;\n    }",
      "path": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/ChecksumFs.java",
      "extendedDetails": {
        "oldPath": "hadoop-common/src/main/java/org/apache/hadoop/fs/ChecksumFs.java",
        "newPath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/ChecksumFs.java"
      }
    },
    "0f6dfeeacbab65a31a33927a4eb84871d371fe52": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-6671. Use maven for hadoop common builds. Contributed by Alejandro Abdelnur.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1153184 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "02/08/11 9:37 AM",
      "commitName": "0f6dfeeacbab65a31a33927a4eb84871d371fe52",
      "commitAuthor": "Thomas White",
      "commitDateOld": "01/08/11 3:53 PM",
      "commitNameOld": "9bac807cedbcff34e1a144fb475eff267e5ed86d",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.74,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    protected int readChunk(long pos, byte[] buf, int offset, int len,\n        byte[] checksum) throws IOException {\n      boolean eof \u003d false;\n      if (needChecksum()) {\n        assert checksum !\u003d null; // we have a checksum buffer\n        assert checksum.length % CHECKSUM_SIZE \u003d\u003d 0; // it is sane length\n        assert len \u003e\u003d bytesPerSum; // we must read at least one chunk\n\n        final int checksumsToRead \u003d Math.min(\n          len/bytesPerSum, // number of checksums based on len to read\n          checksum.length / CHECKSUM_SIZE); // size of checksum buffer\n        long checksumPos \u003d getChecksumFilePos(pos); \n        if(checksumPos !\u003d sums.getPos()) {\n          sums.seek(checksumPos);\n        }\n\n        int sumLenRead \u003d sums.read(checksum, 0, CHECKSUM_SIZE * checksumsToRead);\n        if (sumLenRead \u003e\u003d 0 \u0026\u0026 sumLenRead % CHECKSUM_SIZE !\u003d 0) {\n          throw new EOFException(\"Checksum file not a length multiple of checksum size \" +\n                                 \"in \" + file + \" at \" + pos + \" checksumpos: \" + checksumPos +\n                                 \" sumLenread: \" + sumLenRead );\n        }\n        if (sumLenRead \u003c\u003d 0) { // we\u0027re at the end of the file\n          eof \u003d true;\n        } else {\n          // Adjust amount of data to read based on how many checksum chunks we read\n          len \u003d Math.min(len, bytesPerSum * (sumLenRead / CHECKSUM_SIZE));\n        }\n      }\n      if (pos !\u003d datas.getPos()) {\n        datas.seek(pos);\n      }\n      int nread \u003d readFully(datas, buf, offset, len);\n      if (eof \u0026\u0026 nread \u003e 0) {\n        throw new ChecksumException(\"Checksum error: \"+file+\" at \"+pos, pos);\n      }\n      return nread;\n    }",
      "path": "hadoop-common/src/main/java/org/apache/hadoop/fs/ChecksumFs.java",
      "extendedDetails": {
        "oldPath": "common/src/java/org/apache/hadoop/fs/ChecksumFs.java",
        "newPath": "hadoop-common/src/main/java/org/apache/hadoop/fs/ChecksumFs.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "commitDateOld": "11/06/11 9:13 PM",
      "commitNameOld": "a285fb5effe9ba3be4ec5f942afaf5ddd1186151",
      "commitAuthorOld": "Eli Collins",
      "daysBetweenCommits": 0.74,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "    protected int readChunk(long pos, byte[] buf, int offset, int len,\n        byte[] checksum) throws IOException {\n      boolean eof \u003d false;\n      if (needChecksum()) {\n        assert checksum !\u003d null; // we have a checksum buffer\n        assert checksum.length % CHECKSUM_SIZE \u003d\u003d 0; // it is sane length\n        assert len \u003e\u003d bytesPerSum; // we must read at least one chunk\n\n        final int checksumsToRead \u003d Math.min(\n          len/bytesPerSum, // number of checksums based on len to read\n          checksum.length / CHECKSUM_SIZE); // size of checksum buffer\n        long checksumPos \u003d getChecksumFilePos(pos); \n        if(checksumPos !\u003d sums.getPos()) {\n          sums.seek(checksumPos);\n        }\n\n        int sumLenRead \u003d sums.read(checksum, 0, CHECKSUM_SIZE * checksumsToRead);\n        if (sumLenRead \u003e\u003d 0 \u0026\u0026 sumLenRead % CHECKSUM_SIZE !\u003d 0) {\n          throw new EOFException(\"Checksum file not a length multiple of checksum size \" +\n                                 \"in \" + file + \" at \" + pos + \" checksumpos: \" + checksumPos +\n                                 \" sumLenread: \" + sumLenRead );\n        }\n        if (sumLenRead \u003c\u003d 0) { // we\u0027re at the end of the file\n          eof \u003d true;\n        } else {\n          // Adjust amount of data to read based on how many checksum chunks we read\n          len \u003d Math.min(len, bytesPerSum * (sumLenRead / CHECKSUM_SIZE));\n        }\n      }\n      if (pos !\u003d datas.getPos()) {\n        datas.seek(pos);\n      }\n      int nread \u003d readFully(datas, buf, offset, len);\n      if (eof \u0026\u0026 nread \u003e 0) {\n        throw new ChecksumException(\"Checksum error: \"+file+\" at \"+pos, pos);\n      }\n      return nread;\n    }",
      "path": "common/src/java/org/apache/hadoop/fs/ChecksumFs.java",
      "extendedDetails": {
        "oldPath": "src/java/org/apache/hadoop/fs/ChecksumFs.java",
        "newPath": "common/src/java/org/apache/hadoop/fs/ChecksumFs.java"
      }
    },
    "ae91b5d0f4b18b939025ef374d971d7ada179473": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-3205. Read multiple chunks directly from FSInputChecker subclass into user buffers. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@896243 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "05/01/10 2:14 PM",
      "commitName": "ae91b5d0f4b18b939025ef374d971d7ada179473",
      "commitAuthor": "Thomas White",
      "commitDateOld": "30/10/09 3:24 PM",
      "commitNameOld": "3f371a0a644181b204111ee4e12c995fc7b5e5f5",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 66.99,
      "commitsBetweenForRepo": 54,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,24 +1,38 @@\n     protected int readChunk(long pos, byte[] buf, int offset, int len,\n         byte[] checksum) throws IOException {\n       boolean eof \u003d false;\n       if (needChecksum()) {\n-        try {\n-          final long checksumPos \u003d getChecksumFilePos(pos); \n-          if (checksumPos !\u003d sums.getPos()) {\n-            sums.seek(checksumPos);\n-          }\n-          sums.readFully(checksum);\n-        } catch (EOFException e) {\n-          eof \u003d true;\n+        assert checksum !\u003d null; // we have a checksum buffer\n+        assert checksum.length % CHECKSUM_SIZE \u003d\u003d 0; // it is sane length\n+        assert len \u003e\u003d bytesPerSum; // we must read at least one chunk\n+\n+        final int checksumsToRead \u003d Math.min(\n+          len/bytesPerSum, // number of checksums based on len to read\n+          checksum.length / CHECKSUM_SIZE); // size of checksum buffer\n+        long checksumPos \u003d getChecksumFilePos(pos); \n+        if(checksumPos !\u003d sums.getPos()) {\n+          sums.seek(checksumPos);\n         }\n-        len \u003d bytesPerSum;\n+\n+        int sumLenRead \u003d sums.read(checksum, 0, CHECKSUM_SIZE * checksumsToRead);\n+        if (sumLenRead \u003e\u003d 0 \u0026\u0026 sumLenRead % CHECKSUM_SIZE !\u003d 0) {\n+          throw new EOFException(\"Checksum file not a length multiple of checksum size \" +\n+                                 \"in \" + file + \" at \" + pos + \" checksumpos: \" + checksumPos +\n+                                 \" sumLenread: \" + sumLenRead );\n+        }\n+        if (sumLenRead \u003c\u003d 0) { // we\u0027re at the end of the file\n+          eof \u003d true;\n+        } else {\n+          // Adjust amount of data to read based on how many checksum chunks we read\n+          len \u003d Math.min(len, bytesPerSum * (sumLenRead / CHECKSUM_SIZE));\n+        }\n       }\n       if (pos !\u003d datas.getPos()) {\n         datas.seek(pos);\n       }\n-      final int nread \u003d readFully(datas, buf, offset, len);\n+      int nread \u003d readFully(datas, buf, offset, len);\n       if (eof \u0026\u0026 nread \u003e 0) {\n         throw new ChecksumException(\"Checksum error: \"+file+\" at \"+pos, pos);\n       }\n       return nread;\n     }\n\\ No newline at end of file\n",
      "actualSource": "    protected int readChunk(long pos, byte[] buf, int offset, int len,\n        byte[] checksum) throws IOException {\n      boolean eof \u003d false;\n      if (needChecksum()) {\n        assert checksum !\u003d null; // we have a checksum buffer\n        assert checksum.length % CHECKSUM_SIZE \u003d\u003d 0; // it is sane length\n        assert len \u003e\u003d bytesPerSum; // we must read at least one chunk\n\n        final int checksumsToRead \u003d Math.min(\n          len/bytesPerSum, // number of checksums based on len to read\n          checksum.length / CHECKSUM_SIZE); // size of checksum buffer\n        long checksumPos \u003d getChecksumFilePos(pos); \n        if(checksumPos !\u003d sums.getPos()) {\n          sums.seek(checksumPos);\n        }\n\n        int sumLenRead \u003d sums.read(checksum, 0, CHECKSUM_SIZE * checksumsToRead);\n        if (sumLenRead \u003e\u003d 0 \u0026\u0026 sumLenRead % CHECKSUM_SIZE !\u003d 0) {\n          throw new EOFException(\"Checksum file not a length multiple of checksum size \" +\n                                 \"in \" + file + \" at \" + pos + \" checksumpos: \" + checksumPos +\n                                 \" sumLenread: \" + sumLenRead );\n        }\n        if (sumLenRead \u003c\u003d 0) { // we\u0027re at the end of the file\n          eof \u003d true;\n        } else {\n          // Adjust amount of data to read based on how many checksum chunks we read\n          len \u003d Math.min(len, bytesPerSum * (sumLenRead / CHECKSUM_SIZE));\n        }\n      }\n      if (pos !\u003d datas.getPos()) {\n        datas.seek(pos);\n      }\n      int nread \u003d readFully(datas, buf, offset, len);\n      if (eof \u0026\u0026 nread \u003e 0) {\n        throw new ChecksumException(\"Checksum error: \"+file+\" at \"+pos, pos);\n      }\n      return nread;\n    }",
      "path": "src/java/org/apache/hadoop/fs/ChecksumFs.java",
      "extendedDetails": {}
    },
    "3f371a0a644181b204111ee4e12c995fc7b5e5f5": {
      "type": "Yintroduced",
      "commitMessage": "Hadoop-6223. Add new file system interface AbstractFileSystem with implementation of some file systems that delegate to old FileSystem. Contributed by Sanjay Radia.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@831475 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "30/10/09 3:24 PM",
      "commitName": "3f371a0a644181b204111ee4e12c995fc7b5e5f5",
      "commitAuthor": "Suresh Srinivas",
      "diff": "@@ -0,0 +1,24 @@\n+    protected int readChunk(long pos, byte[] buf, int offset, int len,\n+        byte[] checksum) throws IOException {\n+      boolean eof \u003d false;\n+      if (needChecksum()) {\n+        try {\n+          final long checksumPos \u003d getChecksumFilePos(pos); \n+          if (checksumPos !\u003d sums.getPos()) {\n+            sums.seek(checksumPos);\n+          }\n+          sums.readFully(checksum);\n+        } catch (EOFException e) {\n+          eof \u003d true;\n+        }\n+        len \u003d bytesPerSum;\n+      }\n+      if (pos !\u003d datas.getPos()) {\n+        datas.seek(pos);\n+      }\n+      final int nread \u003d readFully(datas, buf, offset, len);\n+      if (eof \u0026\u0026 nread \u003e 0) {\n+        throw new ChecksumException(\"Checksum error: \"+file+\" at \"+pos, pos);\n+      }\n+      return nread;\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    protected int readChunk(long pos, byte[] buf, int offset, int len,\n        byte[] checksum) throws IOException {\n      boolean eof \u003d false;\n      if (needChecksum()) {\n        try {\n          final long checksumPos \u003d getChecksumFilePos(pos); \n          if (checksumPos !\u003d sums.getPos()) {\n            sums.seek(checksumPos);\n          }\n          sums.readFully(checksum);\n        } catch (EOFException e) {\n          eof \u003d true;\n        }\n        len \u003d bytesPerSum;\n      }\n      if (pos !\u003d datas.getPos()) {\n        datas.seek(pos);\n      }\n      final int nread \u003d readFully(datas, buf, offset, len);\n      if (eof \u0026\u0026 nread \u003e 0) {\n        throw new ChecksumException(\"Checksum error: \"+file+\" at \"+pos, pos);\n      }\n      return nread;\n    }",
      "path": "src/java/org/apache/hadoop/fs/ChecksumFs.java"
    }
  }
}