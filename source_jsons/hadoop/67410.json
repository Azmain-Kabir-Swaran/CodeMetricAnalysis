{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "HadoopArchives.java",
  "functionName": "archive",
  "functionId": "archive___parentPath-Path__srcPaths-List__Path____archiveName-String__dest-Path",
  "sourceFilePath": "hadoop-tools/hadoop-archives/src/main/java/org/apache/hadoop/tools/HadoopArchives.java",
  "functionStartLine": 465,
  "functionEndLine": 574,
  "numCommitsSeen": 18,
  "timeTaken": 5671,
  "changeHistory": [
    "4222c971080f2b150713727092c7197df58c88e5",
    "94c6a4aa85e7d98e9b532b330f30783315f4334b",
    "92c38e41e1fffb9d60d4fa5d4d2212777af9e9a5",
    "ea1c6f31c2d2ea5b38ed57e2aa241d122103a721",
    "0201be46c298e94176ec6297e9d9cdba3afc2bbd",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc"
  ],
  "changeHistoryShort": {
    "4222c971080f2b150713727092c7197df58c88e5": "Ybodychange",
    "94c6a4aa85e7d98e9b532b330f30783315f4334b": "Ybodychange",
    "92c38e41e1fffb9d60d4fa5d4d2212777af9e9a5": "Ybodychange",
    "ea1c6f31c2d2ea5b38ed57e2aa241d122103a721": "Ybodychange",
    "0201be46c298e94176ec6297e9d9cdba3afc2bbd": "Yfilerename",
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": "Yfilerename",
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": "Yfilerename",
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": "Yintroduced"
  },
  "changeHistoryDetails": {
    "4222c971080f2b150713727092c7197df58c88e5": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-10392. Use FileSystem#makeQualified(Path) instead of Path#makeQualified(FileSystem) (ajisakaa via aw)\n",
      "commitDate": "11/08/17 9:25 AM",
      "commitName": "4222c971080f2b150713727092c7197df58c88e5",
      "commitAuthor": "Allen Wittenauer",
      "commitDateOld": "15/11/16 10:57 AM",
      "commitNameOld": "5af572b6443715b7a741296c1bd520a1840f9a7c",
      "commitAuthorOld": "Mingliang Liu",
      "daysBetweenCommits": 268.9,
      "commitsBetweenForRepo": 1484,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,110 +1,110 @@\n   void archive(Path parentPath, List\u003cPath\u003e srcPaths, \n       String archiveName, Path dest) throws IOException {\n     checkPaths(conf, srcPaths);\n     int numFiles \u003d 0;\n     long totalSize \u003d 0;\n     FileSystem fs \u003d parentPath.getFileSystem(conf);\n     this.blockSize \u003d conf.getLong(HAR_BLOCKSIZE_LABEL, blockSize);\n     this.partSize \u003d conf.getLong(HAR_PARTSIZE_LABEL, partSize);\n     conf.setLong(HAR_BLOCKSIZE_LABEL, blockSize);\n     conf.setLong(HAR_PARTSIZE_LABEL, partSize);\n     conf.set(DST_HAR_LABEL, archiveName);\n-    conf.set(SRC_PARENT_LABEL, parentPath.makeQualified(fs).toString());\n+    conf.set(SRC_PARENT_LABEL, fs.makeQualified(parentPath).toString());\n     conf.setInt(HAR_REPLICATION_LABEL, repl);\n     Path outputPath \u003d new Path(dest, archiveName);\n     FileOutputFormat.setOutputPath(conf, outputPath);\n     FileSystem outFs \u003d outputPath.getFileSystem(conf);\n     if (outFs.exists(outputPath)) {\n       throw new IOException(\"Archive path: \"\n           + outputPath.toString() + \" already exists\");\n     }\n     if (outFs.isFile(dest)) {\n       throw new IOException(\"Destination \" + dest.toString()\n           + \" should be a directory but is a file\");\n     }\n     conf.set(DST_DIR_LABEL, outputPath.toString());\n     Path stagingArea;\n     try {\n       stagingArea \u003d JobSubmissionFiles.getStagingDir(new Cluster(conf), \n           conf);\n     } catch (InterruptedException ie) {\n       throw new IOException(ie);\n     }\n     Path jobDirectory \u003d new Path(stagingArea,\n         NAME+\"_\"+Integer.toString(new Random().nextInt(Integer.MAX_VALUE), 36));\n     FsPermission mapredSysPerms \u003d \n       new FsPermission(JobSubmissionFiles.JOB_DIR_PERMISSION);\n     FileSystem.mkdirs(jobDirectory.getFileSystem(conf), jobDirectory,\n                       mapredSysPerms);\n     conf.set(JOB_DIR_LABEL, jobDirectory.toString());\n     //get a tmp directory for input splits\n     FileSystem jobfs \u003d jobDirectory.getFileSystem(conf);\n     Path srcFiles \u003d new Path(jobDirectory, \"_har_src_files\");\n     conf.set(SRC_LIST_LABEL, srcFiles.toString());\n     SequenceFile.Writer srcWriter \u003d SequenceFile.createWriter(jobfs, conf,\n         srcFiles, LongWritable.class, HarEntry.class, \n         SequenceFile.CompressionType.NONE);\n     // get the list of files \n     // create single list of files and dirs\n     try {\n       // write the top level dirs in first \n       writeTopLevelDirs(srcWriter, srcPaths, parentPath);\n       srcWriter.sync();\n       // these are the input paths passed \n       // from the command line\n       // we do a recursive ls on these paths \n       // and then write them to the input file \n       // one at a time\n       for (Path src: srcPaths) {\n         ArrayList\u003cFileStatusDir\u003e allFiles \u003d new ArrayList\u003cFileStatusDir\u003e();\n         FileStatus fstatus \u003d fs.getFileStatus(src);\n         FileStatusDir fdir \u003d new FileStatusDir(fstatus, null);\n         recursivels(fs, fdir, allFiles);\n         for (FileStatusDir statDir: allFiles) {\n           FileStatus stat \u003d statDir.getFileStatus();\n           long len \u003d stat.isDirectory()? 0:stat.getLen();\n           final Path path \u003d relPathToRoot(stat.getPath(), parentPath);\n           final String[] children;\n           if (stat.isDirectory()) {\n             //get the children \n             FileStatus[] list \u003d statDir.getChildren();\n             children \u003d new String[list.length];\n             for (int i \u003d 0; i \u003c list.length; i++) {\n               children[i] \u003d list[i].getPath().getName();\n             }\n           }\n           else {\n             children \u003d null;\n           }\n           append(srcWriter, len, path.toString(), children);\n           srcWriter.sync();\n           numFiles++;\n           totalSize +\u003d len;\n         }\n       }\n     } finally {\n       srcWriter.close();\n     }\n     conf.setInt(SRC_COUNT_LABEL, numFiles);\n     conf.setLong(TOTAL_SIZE_LABEL, totalSize);\n     int numMaps \u003d (int)(totalSize/partSize);\n     //run atleast one map.\n     conf.setNumMapTasks(numMaps \u003d\u003d 0? 1:numMaps);\n     conf.setNumReduceTasks(1);\n     conf.setInputFormat(HArchiveInputFormat.class);\n     conf.setOutputFormat(NullOutputFormat.class);\n     conf.setMapperClass(HArchivesMapper.class);\n     conf.setReducerClass(HArchivesReducer.class);\n     conf.setMapOutputKeyClass(IntWritable.class);\n     conf.setMapOutputValueClass(Text.class);\n     FileInputFormat.addInputPath(conf, jobDirectory);\n     //make sure no speculative execution is done\n     conf.setSpeculativeExecution(false);\n     JobClient.runJob(conf);\n     //delete the tmp job directory\n     try {\n       jobfs.delete(jobDirectory, true);\n     } catch(IOException ie) {\n       LOG.info(\"Unable to clean tmp directory \" + jobDirectory);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void archive(Path parentPath, List\u003cPath\u003e srcPaths, \n      String archiveName, Path dest) throws IOException {\n    checkPaths(conf, srcPaths);\n    int numFiles \u003d 0;\n    long totalSize \u003d 0;\n    FileSystem fs \u003d parentPath.getFileSystem(conf);\n    this.blockSize \u003d conf.getLong(HAR_BLOCKSIZE_LABEL, blockSize);\n    this.partSize \u003d conf.getLong(HAR_PARTSIZE_LABEL, partSize);\n    conf.setLong(HAR_BLOCKSIZE_LABEL, blockSize);\n    conf.setLong(HAR_PARTSIZE_LABEL, partSize);\n    conf.set(DST_HAR_LABEL, archiveName);\n    conf.set(SRC_PARENT_LABEL, fs.makeQualified(parentPath).toString());\n    conf.setInt(HAR_REPLICATION_LABEL, repl);\n    Path outputPath \u003d new Path(dest, archiveName);\n    FileOutputFormat.setOutputPath(conf, outputPath);\n    FileSystem outFs \u003d outputPath.getFileSystem(conf);\n    if (outFs.exists(outputPath)) {\n      throw new IOException(\"Archive path: \"\n          + outputPath.toString() + \" already exists\");\n    }\n    if (outFs.isFile(dest)) {\n      throw new IOException(\"Destination \" + dest.toString()\n          + \" should be a directory but is a file\");\n    }\n    conf.set(DST_DIR_LABEL, outputPath.toString());\n    Path stagingArea;\n    try {\n      stagingArea \u003d JobSubmissionFiles.getStagingDir(new Cluster(conf), \n          conf);\n    } catch (InterruptedException ie) {\n      throw new IOException(ie);\n    }\n    Path jobDirectory \u003d new Path(stagingArea,\n        NAME+\"_\"+Integer.toString(new Random().nextInt(Integer.MAX_VALUE), 36));\n    FsPermission mapredSysPerms \u003d \n      new FsPermission(JobSubmissionFiles.JOB_DIR_PERMISSION);\n    FileSystem.mkdirs(jobDirectory.getFileSystem(conf), jobDirectory,\n                      mapredSysPerms);\n    conf.set(JOB_DIR_LABEL, jobDirectory.toString());\n    //get a tmp directory for input splits\n    FileSystem jobfs \u003d jobDirectory.getFileSystem(conf);\n    Path srcFiles \u003d new Path(jobDirectory, \"_har_src_files\");\n    conf.set(SRC_LIST_LABEL, srcFiles.toString());\n    SequenceFile.Writer srcWriter \u003d SequenceFile.createWriter(jobfs, conf,\n        srcFiles, LongWritable.class, HarEntry.class, \n        SequenceFile.CompressionType.NONE);\n    // get the list of files \n    // create single list of files and dirs\n    try {\n      // write the top level dirs in first \n      writeTopLevelDirs(srcWriter, srcPaths, parentPath);\n      srcWriter.sync();\n      // these are the input paths passed \n      // from the command line\n      // we do a recursive ls on these paths \n      // and then write them to the input file \n      // one at a time\n      for (Path src: srcPaths) {\n        ArrayList\u003cFileStatusDir\u003e allFiles \u003d new ArrayList\u003cFileStatusDir\u003e();\n        FileStatus fstatus \u003d fs.getFileStatus(src);\n        FileStatusDir fdir \u003d new FileStatusDir(fstatus, null);\n        recursivels(fs, fdir, allFiles);\n        for (FileStatusDir statDir: allFiles) {\n          FileStatus stat \u003d statDir.getFileStatus();\n          long len \u003d stat.isDirectory()? 0:stat.getLen();\n          final Path path \u003d relPathToRoot(stat.getPath(), parentPath);\n          final String[] children;\n          if (stat.isDirectory()) {\n            //get the children \n            FileStatus[] list \u003d statDir.getChildren();\n            children \u003d new String[list.length];\n            for (int i \u003d 0; i \u003c list.length; i++) {\n              children[i] \u003d list[i].getPath().getName();\n            }\n          }\n          else {\n            children \u003d null;\n          }\n          append(srcWriter, len, path.toString(), children);\n          srcWriter.sync();\n          numFiles++;\n          totalSize +\u003d len;\n        }\n      }\n    } finally {\n      srcWriter.close();\n    }\n    conf.setInt(SRC_COUNT_LABEL, numFiles);\n    conf.setLong(TOTAL_SIZE_LABEL, totalSize);\n    int numMaps \u003d (int)(totalSize/partSize);\n    //run atleast one map.\n    conf.setNumMapTasks(numMaps \u003d\u003d 0? 1:numMaps);\n    conf.setNumReduceTasks(1);\n    conf.setInputFormat(HArchiveInputFormat.class);\n    conf.setOutputFormat(NullOutputFormat.class);\n    conf.setMapperClass(HArchivesMapper.class);\n    conf.setReducerClass(HArchivesReducer.class);\n    conf.setMapOutputKeyClass(IntWritable.class);\n    conf.setMapOutputValueClass(Text.class);\n    FileInputFormat.addInputPath(conf, jobDirectory);\n    //make sure no speculative execution is done\n    conf.setSpeculativeExecution(false);\n    JobClient.runJob(conf);\n    //delete the tmp job directory\n    try {\n      jobfs.delete(jobDirectory, true);\n    } catch(IOException ie) {\n      LOG.info(\"Unable to clean tmp directory \" + jobDirectory);\n    }\n  }",
      "path": "hadoop-tools/hadoop-archives/src/main/java/org/apache/hadoop/tools/HadoopArchives.java",
      "extendedDetails": {}
    },
    "94c6a4aa85e7d98e9b532b330f30783315f4334b": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-12017. Hadoop archives command should use configurable replication factor when closing (Contributed by Bibin A Chundatt)\n",
      "commitDate": "21/07/15 9:55 PM",
      "commitName": "94c6a4aa85e7d98e9b532b330f30783315f4334b",
      "commitAuthor": "Vinayakumar B",
      "commitDateOld": "21/07/15 12:42 AM",
      "commitNameOld": "87f29c6b8acc07cc011713a79554d51945e265ac",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 0.88,
      "commitsBetweenForRepo": 13,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,111 +1,110 @@\n   void archive(Path parentPath, List\u003cPath\u003e srcPaths, \n       String archiveName, Path dest) throws IOException {\n     checkPaths(conf, srcPaths);\n     int numFiles \u003d 0;\n     long totalSize \u003d 0;\n     FileSystem fs \u003d parentPath.getFileSystem(conf);\n     this.blockSize \u003d conf.getLong(HAR_BLOCKSIZE_LABEL, blockSize);\n     this.partSize \u003d conf.getLong(HAR_PARTSIZE_LABEL, partSize);\n     conf.setLong(HAR_BLOCKSIZE_LABEL, blockSize);\n     conf.setLong(HAR_PARTSIZE_LABEL, partSize);\n     conf.set(DST_HAR_LABEL, archiveName);\n     conf.set(SRC_PARENT_LABEL, parentPath.makeQualified(fs).toString());\n+    conf.setInt(HAR_REPLICATION_LABEL, repl);\n     Path outputPath \u003d new Path(dest, archiveName);\n     FileOutputFormat.setOutputPath(conf, outputPath);\n     FileSystem outFs \u003d outputPath.getFileSystem(conf);\n     if (outFs.exists(outputPath)) {\n       throw new IOException(\"Archive path: \"\n           + outputPath.toString() + \" already exists\");\n     }\n     if (outFs.isFile(dest)) {\n       throw new IOException(\"Destination \" + dest.toString()\n           + \" should be a directory but is a file\");\n     }\n     conf.set(DST_DIR_LABEL, outputPath.toString());\n     Path stagingArea;\n     try {\n       stagingArea \u003d JobSubmissionFiles.getStagingDir(new Cluster(conf), \n           conf);\n     } catch (InterruptedException ie) {\n       throw new IOException(ie);\n     }\n     Path jobDirectory \u003d new Path(stagingArea,\n         NAME+\"_\"+Integer.toString(new Random().nextInt(Integer.MAX_VALUE), 36));\n     FsPermission mapredSysPerms \u003d \n       new FsPermission(JobSubmissionFiles.JOB_DIR_PERMISSION);\n     FileSystem.mkdirs(jobDirectory.getFileSystem(conf), jobDirectory,\n                       mapredSysPerms);\n     conf.set(JOB_DIR_LABEL, jobDirectory.toString());\n     //get a tmp directory for input splits\n     FileSystem jobfs \u003d jobDirectory.getFileSystem(conf);\n     Path srcFiles \u003d new Path(jobDirectory, \"_har_src_files\");\n     conf.set(SRC_LIST_LABEL, srcFiles.toString());\n     SequenceFile.Writer srcWriter \u003d SequenceFile.createWriter(jobfs, conf,\n         srcFiles, LongWritable.class, HarEntry.class, \n         SequenceFile.CompressionType.NONE);\n     // get the list of files \n     // create single list of files and dirs\n     try {\n       // write the top level dirs in first \n       writeTopLevelDirs(srcWriter, srcPaths, parentPath);\n       srcWriter.sync();\n       // these are the input paths passed \n       // from the command line\n       // we do a recursive ls on these paths \n       // and then write them to the input file \n       // one at a time\n       for (Path src: srcPaths) {\n         ArrayList\u003cFileStatusDir\u003e allFiles \u003d new ArrayList\u003cFileStatusDir\u003e();\n         FileStatus fstatus \u003d fs.getFileStatus(src);\n         FileStatusDir fdir \u003d new FileStatusDir(fstatus, null);\n         recursivels(fs, fdir, allFiles);\n         for (FileStatusDir statDir: allFiles) {\n           FileStatus stat \u003d statDir.getFileStatus();\n           long len \u003d stat.isDirectory()? 0:stat.getLen();\n           final Path path \u003d relPathToRoot(stat.getPath(), parentPath);\n           final String[] children;\n           if (stat.isDirectory()) {\n             //get the children \n             FileStatus[] list \u003d statDir.getChildren();\n             children \u003d new String[list.length];\n             for (int i \u003d 0; i \u003c list.length; i++) {\n               children[i] \u003d list[i].getPath().getName();\n             }\n           }\n           else {\n             children \u003d null;\n           }\n           append(srcWriter, len, path.toString(), children);\n           srcWriter.sync();\n           numFiles++;\n           totalSize +\u003d len;\n         }\n       }\n     } finally {\n       srcWriter.close();\n     }\n-    //increase the replication of src files\n-    jobfs.setReplication(srcFiles, repl);\n     conf.setInt(SRC_COUNT_LABEL, numFiles);\n     conf.setLong(TOTAL_SIZE_LABEL, totalSize);\n     int numMaps \u003d (int)(totalSize/partSize);\n     //run atleast one map.\n     conf.setNumMapTasks(numMaps \u003d\u003d 0? 1:numMaps);\n     conf.setNumReduceTasks(1);\n     conf.setInputFormat(HArchiveInputFormat.class);\n     conf.setOutputFormat(NullOutputFormat.class);\n     conf.setMapperClass(HArchivesMapper.class);\n     conf.setReducerClass(HArchivesReducer.class);\n     conf.setMapOutputKeyClass(IntWritable.class);\n     conf.setMapOutputValueClass(Text.class);\n     FileInputFormat.addInputPath(conf, jobDirectory);\n     //make sure no speculative execution is done\n     conf.setSpeculativeExecution(false);\n     JobClient.runJob(conf);\n     //delete the tmp job directory\n     try {\n       jobfs.delete(jobDirectory, true);\n     } catch(IOException ie) {\n       LOG.info(\"Unable to clean tmp directory \" + jobDirectory);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void archive(Path parentPath, List\u003cPath\u003e srcPaths, \n      String archiveName, Path dest) throws IOException {\n    checkPaths(conf, srcPaths);\n    int numFiles \u003d 0;\n    long totalSize \u003d 0;\n    FileSystem fs \u003d parentPath.getFileSystem(conf);\n    this.blockSize \u003d conf.getLong(HAR_BLOCKSIZE_LABEL, blockSize);\n    this.partSize \u003d conf.getLong(HAR_PARTSIZE_LABEL, partSize);\n    conf.setLong(HAR_BLOCKSIZE_LABEL, blockSize);\n    conf.setLong(HAR_PARTSIZE_LABEL, partSize);\n    conf.set(DST_HAR_LABEL, archiveName);\n    conf.set(SRC_PARENT_LABEL, parentPath.makeQualified(fs).toString());\n    conf.setInt(HAR_REPLICATION_LABEL, repl);\n    Path outputPath \u003d new Path(dest, archiveName);\n    FileOutputFormat.setOutputPath(conf, outputPath);\n    FileSystem outFs \u003d outputPath.getFileSystem(conf);\n    if (outFs.exists(outputPath)) {\n      throw new IOException(\"Archive path: \"\n          + outputPath.toString() + \" already exists\");\n    }\n    if (outFs.isFile(dest)) {\n      throw new IOException(\"Destination \" + dest.toString()\n          + \" should be a directory but is a file\");\n    }\n    conf.set(DST_DIR_LABEL, outputPath.toString());\n    Path stagingArea;\n    try {\n      stagingArea \u003d JobSubmissionFiles.getStagingDir(new Cluster(conf), \n          conf);\n    } catch (InterruptedException ie) {\n      throw new IOException(ie);\n    }\n    Path jobDirectory \u003d new Path(stagingArea,\n        NAME+\"_\"+Integer.toString(new Random().nextInt(Integer.MAX_VALUE), 36));\n    FsPermission mapredSysPerms \u003d \n      new FsPermission(JobSubmissionFiles.JOB_DIR_PERMISSION);\n    FileSystem.mkdirs(jobDirectory.getFileSystem(conf), jobDirectory,\n                      mapredSysPerms);\n    conf.set(JOB_DIR_LABEL, jobDirectory.toString());\n    //get a tmp directory for input splits\n    FileSystem jobfs \u003d jobDirectory.getFileSystem(conf);\n    Path srcFiles \u003d new Path(jobDirectory, \"_har_src_files\");\n    conf.set(SRC_LIST_LABEL, srcFiles.toString());\n    SequenceFile.Writer srcWriter \u003d SequenceFile.createWriter(jobfs, conf,\n        srcFiles, LongWritable.class, HarEntry.class, \n        SequenceFile.CompressionType.NONE);\n    // get the list of files \n    // create single list of files and dirs\n    try {\n      // write the top level dirs in first \n      writeTopLevelDirs(srcWriter, srcPaths, parentPath);\n      srcWriter.sync();\n      // these are the input paths passed \n      // from the command line\n      // we do a recursive ls on these paths \n      // and then write them to the input file \n      // one at a time\n      for (Path src: srcPaths) {\n        ArrayList\u003cFileStatusDir\u003e allFiles \u003d new ArrayList\u003cFileStatusDir\u003e();\n        FileStatus fstatus \u003d fs.getFileStatus(src);\n        FileStatusDir fdir \u003d new FileStatusDir(fstatus, null);\n        recursivels(fs, fdir, allFiles);\n        for (FileStatusDir statDir: allFiles) {\n          FileStatus stat \u003d statDir.getFileStatus();\n          long len \u003d stat.isDirectory()? 0:stat.getLen();\n          final Path path \u003d relPathToRoot(stat.getPath(), parentPath);\n          final String[] children;\n          if (stat.isDirectory()) {\n            //get the children \n            FileStatus[] list \u003d statDir.getChildren();\n            children \u003d new String[list.length];\n            for (int i \u003d 0; i \u003c list.length; i++) {\n              children[i] \u003d list[i].getPath().getName();\n            }\n          }\n          else {\n            children \u003d null;\n          }\n          append(srcWriter, len, path.toString(), children);\n          srcWriter.sync();\n          numFiles++;\n          totalSize +\u003d len;\n        }\n      }\n    } finally {\n      srcWriter.close();\n    }\n    conf.setInt(SRC_COUNT_LABEL, numFiles);\n    conf.setLong(TOTAL_SIZE_LABEL, totalSize);\n    int numMaps \u003d (int)(totalSize/partSize);\n    //run atleast one map.\n    conf.setNumMapTasks(numMaps \u003d\u003d 0? 1:numMaps);\n    conf.setNumReduceTasks(1);\n    conf.setInputFormat(HArchiveInputFormat.class);\n    conf.setOutputFormat(NullOutputFormat.class);\n    conf.setMapperClass(HArchivesMapper.class);\n    conf.setReducerClass(HArchivesReducer.class);\n    conf.setMapOutputKeyClass(IntWritable.class);\n    conf.setMapOutputValueClass(Text.class);\n    FileInputFormat.addInputPath(conf, jobDirectory);\n    //make sure no speculative execution is done\n    conf.setSpeculativeExecution(false);\n    JobClient.runJob(conf);\n    //delete the tmp job directory\n    try {\n      jobfs.delete(jobDirectory, true);\n    } catch(IOException ie) {\n      LOG.info(\"Unable to clean tmp directory \" + jobDirectory);\n    }\n  }",
      "path": "hadoop-tools/hadoop-archives/src/main/java/org/apache/hadoop/tools/HadoopArchives.java",
      "extendedDetails": {}
    },
    "92c38e41e1fffb9d60d4fa5d4d2212777af9e9a5": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-9723. Improve error message when hadoop archive output path already exists. Contributed by Jean-Baptiste Onofré and Yongjun Zhang.\n",
      "commitDate": "13/05/15 1:28 AM",
      "commitName": "92c38e41e1fffb9d60d4fa5d4d2212777af9e9a5",
      "commitAuthor": "Akira Ajisaka",
      "commitDateOld": "27/03/15 11:28 PM",
      "commitNameOld": "27d49e6714ad7fc6038bc001e70ff5be3755f1ef",
      "commitAuthorOld": "Harsh J",
      "daysBetweenCommits": 46.08,
      "commitsBetweenForRepo": 467,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,106 +1,111 @@\n   void archive(Path parentPath, List\u003cPath\u003e srcPaths, \n       String archiveName, Path dest) throws IOException {\n     checkPaths(conf, srcPaths);\n     int numFiles \u003d 0;\n     long totalSize \u003d 0;\n     FileSystem fs \u003d parentPath.getFileSystem(conf);\n     this.blockSize \u003d conf.getLong(HAR_BLOCKSIZE_LABEL, blockSize);\n     this.partSize \u003d conf.getLong(HAR_PARTSIZE_LABEL, partSize);\n     conf.setLong(HAR_BLOCKSIZE_LABEL, blockSize);\n     conf.setLong(HAR_PARTSIZE_LABEL, partSize);\n     conf.set(DST_HAR_LABEL, archiveName);\n     conf.set(SRC_PARENT_LABEL, parentPath.makeQualified(fs).toString());\n     Path outputPath \u003d new Path(dest, archiveName);\n     FileOutputFormat.setOutputPath(conf, outputPath);\n     FileSystem outFs \u003d outputPath.getFileSystem(conf);\n-    if (outFs.exists(outputPath) || outFs.isFile(dest)) {\n-      throw new IOException(\"Invalid Output: \" + outputPath);\n+    if (outFs.exists(outputPath)) {\n+      throw new IOException(\"Archive path: \"\n+          + outputPath.toString() + \" already exists\");\n+    }\n+    if (outFs.isFile(dest)) {\n+      throw new IOException(\"Destination \" + dest.toString()\n+          + \" should be a directory but is a file\");\n     }\n     conf.set(DST_DIR_LABEL, outputPath.toString());\n     Path stagingArea;\n     try {\n       stagingArea \u003d JobSubmissionFiles.getStagingDir(new Cluster(conf), \n           conf);\n     } catch (InterruptedException ie) {\n       throw new IOException(ie);\n     }\n     Path jobDirectory \u003d new Path(stagingArea,\n         NAME+\"_\"+Integer.toString(new Random().nextInt(Integer.MAX_VALUE), 36));\n     FsPermission mapredSysPerms \u003d \n       new FsPermission(JobSubmissionFiles.JOB_DIR_PERMISSION);\n     FileSystem.mkdirs(jobDirectory.getFileSystem(conf), jobDirectory,\n                       mapredSysPerms);\n     conf.set(JOB_DIR_LABEL, jobDirectory.toString());\n     //get a tmp directory for input splits\n     FileSystem jobfs \u003d jobDirectory.getFileSystem(conf);\n     Path srcFiles \u003d new Path(jobDirectory, \"_har_src_files\");\n     conf.set(SRC_LIST_LABEL, srcFiles.toString());\n     SequenceFile.Writer srcWriter \u003d SequenceFile.createWriter(jobfs, conf,\n         srcFiles, LongWritable.class, HarEntry.class, \n         SequenceFile.CompressionType.NONE);\n     // get the list of files \n     // create single list of files and dirs\n     try {\n       // write the top level dirs in first \n       writeTopLevelDirs(srcWriter, srcPaths, parentPath);\n       srcWriter.sync();\n       // these are the input paths passed \n       // from the command line\n       // we do a recursive ls on these paths \n       // and then write them to the input file \n       // one at a time\n       for (Path src: srcPaths) {\n         ArrayList\u003cFileStatusDir\u003e allFiles \u003d new ArrayList\u003cFileStatusDir\u003e();\n         FileStatus fstatus \u003d fs.getFileStatus(src);\n         FileStatusDir fdir \u003d new FileStatusDir(fstatus, null);\n         recursivels(fs, fdir, allFiles);\n         for (FileStatusDir statDir: allFiles) {\n           FileStatus stat \u003d statDir.getFileStatus();\n           long len \u003d stat.isDirectory()? 0:stat.getLen();\n           final Path path \u003d relPathToRoot(stat.getPath(), parentPath);\n           final String[] children;\n           if (stat.isDirectory()) {\n             //get the children \n             FileStatus[] list \u003d statDir.getChildren();\n             children \u003d new String[list.length];\n             for (int i \u003d 0; i \u003c list.length; i++) {\n               children[i] \u003d list[i].getPath().getName();\n             }\n           }\n           else {\n             children \u003d null;\n           }\n           append(srcWriter, len, path.toString(), children);\n           srcWriter.sync();\n           numFiles++;\n           totalSize +\u003d len;\n         }\n       }\n     } finally {\n       srcWriter.close();\n     }\n     //increase the replication of src files\n     jobfs.setReplication(srcFiles, repl);\n     conf.setInt(SRC_COUNT_LABEL, numFiles);\n     conf.setLong(TOTAL_SIZE_LABEL, totalSize);\n     int numMaps \u003d (int)(totalSize/partSize);\n     //run atleast one map.\n     conf.setNumMapTasks(numMaps \u003d\u003d 0? 1:numMaps);\n     conf.setNumReduceTasks(1);\n     conf.setInputFormat(HArchiveInputFormat.class);\n     conf.setOutputFormat(NullOutputFormat.class);\n     conf.setMapperClass(HArchivesMapper.class);\n     conf.setReducerClass(HArchivesReducer.class);\n     conf.setMapOutputKeyClass(IntWritable.class);\n     conf.setMapOutputValueClass(Text.class);\n     FileInputFormat.addInputPath(conf, jobDirectory);\n     //make sure no speculative execution is done\n     conf.setSpeculativeExecution(false);\n     JobClient.runJob(conf);\n     //delete the tmp job directory\n     try {\n       jobfs.delete(jobDirectory, true);\n     } catch(IOException ie) {\n       LOG.info(\"Unable to clean tmp directory \" + jobDirectory);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void archive(Path parentPath, List\u003cPath\u003e srcPaths, \n      String archiveName, Path dest) throws IOException {\n    checkPaths(conf, srcPaths);\n    int numFiles \u003d 0;\n    long totalSize \u003d 0;\n    FileSystem fs \u003d parentPath.getFileSystem(conf);\n    this.blockSize \u003d conf.getLong(HAR_BLOCKSIZE_LABEL, blockSize);\n    this.partSize \u003d conf.getLong(HAR_PARTSIZE_LABEL, partSize);\n    conf.setLong(HAR_BLOCKSIZE_LABEL, blockSize);\n    conf.setLong(HAR_PARTSIZE_LABEL, partSize);\n    conf.set(DST_HAR_LABEL, archiveName);\n    conf.set(SRC_PARENT_LABEL, parentPath.makeQualified(fs).toString());\n    Path outputPath \u003d new Path(dest, archiveName);\n    FileOutputFormat.setOutputPath(conf, outputPath);\n    FileSystem outFs \u003d outputPath.getFileSystem(conf);\n    if (outFs.exists(outputPath)) {\n      throw new IOException(\"Archive path: \"\n          + outputPath.toString() + \" already exists\");\n    }\n    if (outFs.isFile(dest)) {\n      throw new IOException(\"Destination \" + dest.toString()\n          + \" should be a directory but is a file\");\n    }\n    conf.set(DST_DIR_LABEL, outputPath.toString());\n    Path stagingArea;\n    try {\n      stagingArea \u003d JobSubmissionFiles.getStagingDir(new Cluster(conf), \n          conf);\n    } catch (InterruptedException ie) {\n      throw new IOException(ie);\n    }\n    Path jobDirectory \u003d new Path(stagingArea,\n        NAME+\"_\"+Integer.toString(new Random().nextInt(Integer.MAX_VALUE), 36));\n    FsPermission mapredSysPerms \u003d \n      new FsPermission(JobSubmissionFiles.JOB_DIR_PERMISSION);\n    FileSystem.mkdirs(jobDirectory.getFileSystem(conf), jobDirectory,\n                      mapredSysPerms);\n    conf.set(JOB_DIR_LABEL, jobDirectory.toString());\n    //get a tmp directory for input splits\n    FileSystem jobfs \u003d jobDirectory.getFileSystem(conf);\n    Path srcFiles \u003d new Path(jobDirectory, \"_har_src_files\");\n    conf.set(SRC_LIST_LABEL, srcFiles.toString());\n    SequenceFile.Writer srcWriter \u003d SequenceFile.createWriter(jobfs, conf,\n        srcFiles, LongWritable.class, HarEntry.class, \n        SequenceFile.CompressionType.NONE);\n    // get the list of files \n    // create single list of files and dirs\n    try {\n      // write the top level dirs in first \n      writeTopLevelDirs(srcWriter, srcPaths, parentPath);\n      srcWriter.sync();\n      // these are the input paths passed \n      // from the command line\n      // we do a recursive ls on these paths \n      // and then write them to the input file \n      // one at a time\n      for (Path src: srcPaths) {\n        ArrayList\u003cFileStatusDir\u003e allFiles \u003d new ArrayList\u003cFileStatusDir\u003e();\n        FileStatus fstatus \u003d fs.getFileStatus(src);\n        FileStatusDir fdir \u003d new FileStatusDir(fstatus, null);\n        recursivels(fs, fdir, allFiles);\n        for (FileStatusDir statDir: allFiles) {\n          FileStatus stat \u003d statDir.getFileStatus();\n          long len \u003d stat.isDirectory()? 0:stat.getLen();\n          final Path path \u003d relPathToRoot(stat.getPath(), parentPath);\n          final String[] children;\n          if (stat.isDirectory()) {\n            //get the children \n            FileStatus[] list \u003d statDir.getChildren();\n            children \u003d new String[list.length];\n            for (int i \u003d 0; i \u003c list.length; i++) {\n              children[i] \u003d list[i].getPath().getName();\n            }\n          }\n          else {\n            children \u003d null;\n          }\n          append(srcWriter, len, path.toString(), children);\n          srcWriter.sync();\n          numFiles++;\n          totalSize +\u003d len;\n        }\n      }\n    } finally {\n      srcWriter.close();\n    }\n    //increase the replication of src files\n    jobfs.setReplication(srcFiles, repl);\n    conf.setInt(SRC_COUNT_LABEL, numFiles);\n    conf.setLong(TOTAL_SIZE_LABEL, totalSize);\n    int numMaps \u003d (int)(totalSize/partSize);\n    //run atleast one map.\n    conf.setNumMapTasks(numMaps \u003d\u003d 0? 1:numMaps);\n    conf.setNumReduceTasks(1);\n    conf.setInputFormat(HArchiveInputFormat.class);\n    conf.setOutputFormat(NullOutputFormat.class);\n    conf.setMapperClass(HArchivesMapper.class);\n    conf.setReducerClass(HArchivesReducer.class);\n    conf.setMapOutputKeyClass(IntWritable.class);\n    conf.setMapOutputValueClass(Text.class);\n    FileInputFormat.addInputPath(conf, jobDirectory);\n    //make sure no speculative execution is done\n    conf.setSpeculativeExecution(false);\n    JobClient.runJob(conf);\n    //delete the tmp job directory\n    try {\n      jobfs.delete(jobDirectory, true);\n    } catch(IOException ie) {\n      LOG.info(\"Unable to clean tmp directory \" + jobDirectory);\n    }\n  }",
      "path": "hadoop-tools/hadoop-archives/src/main/java/org/apache/hadoop/tools/HadoopArchives.java",
      "extendedDetails": {}
    },
    "ea1c6f31c2d2ea5b38ed57e2aa241d122103a721": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-11021. Configurable replication factor in the hadoop archive command. Contributed by Zhe Zhang.\n",
      "commitDate": "29/08/14 2:44 PM",
      "commitName": "ea1c6f31c2d2ea5b38ed57e2aa241d122103a721",
      "commitAuthor": "Andrew Wang",
      "commitDateOld": "14/08/14 1:28 PM",
      "commitNameOld": "399e428deb6a4aafff20bfff6a2fd0acfd21366a",
      "commitAuthorOld": "Allen Wittenauer",
      "daysBetweenCommits": 15.05,
      "commitsBetweenForRepo": 111,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,106 +1,106 @@\n   void archive(Path parentPath, List\u003cPath\u003e srcPaths, \n       String archiveName, Path dest) throws IOException {\n     checkPaths(conf, srcPaths);\n     int numFiles \u003d 0;\n     long totalSize \u003d 0;\n     FileSystem fs \u003d parentPath.getFileSystem(conf);\n     this.blockSize \u003d conf.getLong(HAR_BLOCKSIZE_LABEL, blockSize);\n     this.partSize \u003d conf.getLong(HAR_PARTSIZE_LABEL, partSize);\n     conf.setLong(HAR_BLOCKSIZE_LABEL, blockSize);\n     conf.setLong(HAR_PARTSIZE_LABEL, partSize);\n     conf.set(DST_HAR_LABEL, archiveName);\n     conf.set(SRC_PARENT_LABEL, parentPath.makeQualified(fs).toString());\n     Path outputPath \u003d new Path(dest, archiveName);\n     FileOutputFormat.setOutputPath(conf, outputPath);\n     FileSystem outFs \u003d outputPath.getFileSystem(conf);\n     if (outFs.exists(outputPath) || outFs.isFile(dest)) {\n       throw new IOException(\"Invalid Output: \" + outputPath);\n     }\n     conf.set(DST_DIR_LABEL, outputPath.toString());\n     Path stagingArea;\n     try {\n       stagingArea \u003d JobSubmissionFiles.getStagingDir(new Cluster(conf), \n           conf);\n     } catch (InterruptedException ie) {\n       throw new IOException(ie);\n     }\n     Path jobDirectory \u003d new Path(stagingArea,\n         NAME+\"_\"+Integer.toString(new Random().nextInt(Integer.MAX_VALUE), 36));\n     FsPermission mapredSysPerms \u003d \n       new FsPermission(JobSubmissionFiles.JOB_DIR_PERMISSION);\n     FileSystem.mkdirs(jobDirectory.getFileSystem(conf), jobDirectory,\n                       mapredSysPerms);\n     conf.set(JOB_DIR_LABEL, jobDirectory.toString());\n     //get a tmp directory for input splits\n     FileSystem jobfs \u003d jobDirectory.getFileSystem(conf);\n     Path srcFiles \u003d new Path(jobDirectory, \"_har_src_files\");\n     conf.set(SRC_LIST_LABEL, srcFiles.toString());\n     SequenceFile.Writer srcWriter \u003d SequenceFile.createWriter(jobfs, conf,\n         srcFiles, LongWritable.class, HarEntry.class, \n         SequenceFile.CompressionType.NONE);\n     // get the list of files \n     // create single list of files and dirs\n     try {\n       // write the top level dirs in first \n       writeTopLevelDirs(srcWriter, srcPaths, parentPath);\n       srcWriter.sync();\n       // these are the input paths passed \n       // from the command line\n       // we do a recursive ls on these paths \n       // and then write them to the input file \n       // one at a time\n       for (Path src: srcPaths) {\n         ArrayList\u003cFileStatusDir\u003e allFiles \u003d new ArrayList\u003cFileStatusDir\u003e();\n         FileStatus fstatus \u003d fs.getFileStatus(src);\n         FileStatusDir fdir \u003d new FileStatusDir(fstatus, null);\n         recursivels(fs, fdir, allFiles);\n         for (FileStatusDir statDir: allFiles) {\n           FileStatus stat \u003d statDir.getFileStatus();\n           long len \u003d stat.isDirectory()? 0:stat.getLen();\n           final Path path \u003d relPathToRoot(stat.getPath(), parentPath);\n           final String[] children;\n           if (stat.isDirectory()) {\n             //get the children \n             FileStatus[] list \u003d statDir.getChildren();\n             children \u003d new String[list.length];\n             for (int i \u003d 0; i \u003c list.length; i++) {\n               children[i] \u003d list[i].getPath().getName();\n             }\n           }\n           else {\n             children \u003d null;\n           }\n           append(srcWriter, len, path.toString(), children);\n           srcWriter.sync();\n           numFiles++;\n           totalSize +\u003d len;\n         }\n       }\n     } finally {\n       srcWriter.close();\n     }\n     //increase the replication of src files\n-    jobfs.setReplication(srcFiles, (short) 10);\n+    jobfs.setReplication(srcFiles, repl);\n     conf.setInt(SRC_COUNT_LABEL, numFiles);\n     conf.setLong(TOTAL_SIZE_LABEL, totalSize);\n     int numMaps \u003d (int)(totalSize/partSize);\n     //run atleast one map.\n     conf.setNumMapTasks(numMaps \u003d\u003d 0? 1:numMaps);\n     conf.setNumReduceTasks(1);\n     conf.setInputFormat(HArchiveInputFormat.class);\n     conf.setOutputFormat(NullOutputFormat.class);\n     conf.setMapperClass(HArchivesMapper.class);\n     conf.setReducerClass(HArchivesReducer.class);\n     conf.setMapOutputKeyClass(IntWritable.class);\n     conf.setMapOutputValueClass(Text.class);\n     FileInputFormat.addInputPath(conf, jobDirectory);\n     //make sure no speculative execution is done\n     conf.setSpeculativeExecution(false);\n     JobClient.runJob(conf);\n     //delete the tmp job directory\n     try {\n       jobfs.delete(jobDirectory, true);\n     } catch(IOException ie) {\n       LOG.info(\"Unable to clean tmp directory \" + jobDirectory);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void archive(Path parentPath, List\u003cPath\u003e srcPaths, \n      String archiveName, Path dest) throws IOException {\n    checkPaths(conf, srcPaths);\n    int numFiles \u003d 0;\n    long totalSize \u003d 0;\n    FileSystem fs \u003d parentPath.getFileSystem(conf);\n    this.blockSize \u003d conf.getLong(HAR_BLOCKSIZE_LABEL, blockSize);\n    this.partSize \u003d conf.getLong(HAR_PARTSIZE_LABEL, partSize);\n    conf.setLong(HAR_BLOCKSIZE_LABEL, blockSize);\n    conf.setLong(HAR_PARTSIZE_LABEL, partSize);\n    conf.set(DST_HAR_LABEL, archiveName);\n    conf.set(SRC_PARENT_LABEL, parentPath.makeQualified(fs).toString());\n    Path outputPath \u003d new Path(dest, archiveName);\n    FileOutputFormat.setOutputPath(conf, outputPath);\n    FileSystem outFs \u003d outputPath.getFileSystem(conf);\n    if (outFs.exists(outputPath) || outFs.isFile(dest)) {\n      throw new IOException(\"Invalid Output: \" + outputPath);\n    }\n    conf.set(DST_DIR_LABEL, outputPath.toString());\n    Path stagingArea;\n    try {\n      stagingArea \u003d JobSubmissionFiles.getStagingDir(new Cluster(conf), \n          conf);\n    } catch (InterruptedException ie) {\n      throw new IOException(ie);\n    }\n    Path jobDirectory \u003d new Path(stagingArea,\n        NAME+\"_\"+Integer.toString(new Random().nextInt(Integer.MAX_VALUE), 36));\n    FsPermission mapredSysPerms \u003d \n      new FsPermission(JobSubmissionFiles.JOB_DIR_PERMISSION);\n    FileSystem.mkdirs(jobDirectory.getFileSystem(conf), jobDirectory,\n                      mapredSysPerms);\n    conf.set(JOB_DIR_LABEL, jobDirectory.toString());\n    //get a tmp directory for input splits\n    FileSystem jobfs \u003d jobDirectory.getFileSystem(conf);\n    Path srcFiles \u003d new Path(jobDirectory, \"_har_src_files\");\n    conf.set(SRC_LIST_LABEL, srcFiles.toString());\n    SequenceFile.Writer srcWriter \u003d SequenceFile.createWriter(jobfs, conf,\n        srcFiles, LongWritable.class, HarEntry.class, \n        SequenceFile.CompressionType.NONE);\n    // get the list of files \n    // create single list of files and dirs\n    try {\n      // write the top level dirs in first \n      writeTopLevelDirs(srcWriter, srcPaths, parentPath);\n      srcWriter.sync();\n      // these are the input paths passed \n      // from the command line\n      // we do a recursive ls on these paths \n      // and then write them to the input file \n      // one at a time\n      for (Path src: srcPaths) {\n        ArrayList\u003cFileStatusDir\u003e allFiles \u003d new ArrayList\u003cFileStatusDir\u003e();\n        FileStatus fstatus \u003d fs.getFileStatus(src);\n        FileStatusDir fdir \u003d new FileStatusDir(fstatus, null);\n        recursivels(fs, fdir, allFiles);\n        for (FileStatusDir statDir: allFiles) {\n          FileStatus stat \u003d statDir.getFileStatus();\n          long len \u003d stat.isDirectory()? 0:stat.getLen();\n          final Path path \u003d relPathToRoot(stat.getPath(), parentPath);\n          final String[] children;\n          if (stat.isDirectory()) {\n            //get the children \n            FileStatus[] list \u003d statDir.getChildren();\n            children \u003d new String[list.length];\n            for (int i \u003d 0; i \u003c list.length; i++) {\n              children[i] \u003d list[i].getPath().getName();\n            }\n          }\n          else {\n            children \u003d null;\n          }\n          append(srcWriter, len, path.toString(), children);\n          srcWriter.sync();\n          numFiles++;\n          totalSize +\u003d len;\n        }\n      }\n    } finally {\n      srcWriter.close();\n    }\n    //increase the replication of src files\n    jobfs.setReplication(srcFiles, repl);\n    conf.setInt(SRC_COUNT_LABEL, numFiles);\n    conf.setLong(TOTAL_SIZE_LABEL, totalSize);\n    int numMaps \u003d (int)(totalSize/partSize);\n    //run atleast one map.\n    conf.setNumMapTasks(numMaps \u003d\u003d 0? 1:numMaps);\n    conf.setNumReduceTasks(1);\n    conf.setInputFormat(HArchiveInputFormat.class);\n    conf.setOutputFormat(NullOutputFormat.class);\n    conf.setMapperClass(HArchivesMapper.class);\n    conf.setReducerClass(HArchivesReducer.class);\n    conf.setMapOutputKeyClass(IntWritable.class);\n    conf.setMapOutputValueClass(Text.class);\n    FileInputFormat.addInputPath(conf, jobDirectory);\n    //make sure no speculative execution is done\n    conf.setSpeculativeExecution(false);\n    JobClient.runJob(conf);\n    //delete the tmp job directory\n    try {\n      jobfs.delete(jobDirectory, true);\n    } catch(IOException ie) {\n      LOG.info(\"Unable to clean tmp directory \" + jobDirectory);\n    }\n  }",
      "path": "hadoop-tools/hadoop-archives/src/main/java/org/apache/hadoop/tools/HadoopArchives.java",
      "extendedDetails": {}
    },
    "0201be46c298e94176ec6297e9d9cdba3afc2bbd": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7810. move hadoop archive to core from tools. (tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213907 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/12/11 12:17 PM",
      "commitName": "0201be46c298e94176ec6297e9d9cdba3afc2bbd",
      "commitAuthor": "Alejandro Abdelnur",
      "commitDateOld": "13/12/11 10:07 AM",
      "commitNameOld": "f2f4e9341387199e04679ebc8de5e05c0fdbd437",
      "commitAuthorOld": "Suresh Srinivas",
      "daysBetweenCommits": 0.09,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  void archive(Path parentPath, List\u003cPath\u003e srcPaths, \n      String archiveName, Path dest) throws IOException {\n    checkPaths(conf, srcPaths);\n    int numFiles \u003d 0;\n    long totalSize \u003d 0;\n    FileSystem fs \u003d parentPath.getFileSystem(conf);\n    this.blockSize \u003d conf.getLong(HAR_BLOCKSIZE_LABEL, blockSize);\n    this.partSize \u003d conf.getLong(HAR_PARTSIZE_LABEL, partSize);\n    conf.setLong(HAR_BLOCKSIZE_LABEL, blockSize);\n    conf.setLong(HAR_PARTSIZE_LABEL, partSize);\n    conf.set(DST_HAR_LABEL, archiveName);\n    conf.set(SRC_PARENT_LABEL, parentPath.makeQualified(fs).toString());\n    Path outputPath \u003d new Path(dest, archiveName);\n    FileOutputFormat.setOutputPath(conf, outputPath);\n    FileSystem outFs \u003d outputPath.getFileSystem(conf);\n    if (outFs.exists(outputPath) || outFs.isFile(dest)) {\n      throw new IOException(\"Invalid Output: \" + outputPath);\n    }\n    conf.set(DST_DIR_LABEL, outputPath.toString());\n    Path stagingArea;\n    try {\n      stagingArea \u003d JobSubmissionFiles.getStagingDir(new Cluster(conf), \n          conf);\n    } catch (InterruptedException ie) {\n      throw new IOException(ie);\n    }\n    Path jobDirectory \u003d new Path(stagingArea,\n        NAME+\"_\"+Integer.toString(new Random().nextInt(Integer.MAX_VALUE), 36));\n    FsPermission mapredSysPerms \u003d \n      new FsPermission(JobSubmissionFiles.JOB_DIR_PERMISSION);\n    FileSystem.mkdirs(jobDirectory.getFileSystem(conf), jobDirectory,\n                      mapredSysPerms);\n    conf.set(JOB_DIR_LABEL, jobDirectory.toString());\n    //get a tmp directory for input splits\n    FileSystem jobfs \u003d jobDirectory.getFileSystem(conf);\n    Path srcFiles \u003d new Path(jobDirectory, \"_har_src_files\");\n    conf.set(SRC_LIST_LABEL, srcFiles.toString());\n    SequenceFile.Writer srcWriter \u003d SequenceFile.createWriter(jobfs, conf,\n        srcFiles, LongWritable.class, HarEntry.class, \n        SequenceFile.CompressionType.NONE);\n    // get the list of files \n    // create single list of files and dirs\n    try {\n      // write the top level dirs in first \n      writeTopLevelDirs(srcWriter, srcPaths, parentPath);\n      srcWriter.sync();\n      // these are the input paths passed \n      // from the command line\n      // we do a recursive ls on these paths \n      // and then write them to the input file \n      // one at a time\n      for (Path src: srcPaths) {\n        ArrayList\u003cFileStatusDir\u003e allFiles \u003d new ArrayList\u003cFileStatusDir\u003e();\n        FileStatus fstatus \u003d fs.getFileStatus(src);\n        FileStatusDir fdir \u003d new FileStatusDir(fstatus, null);\n        recursivels(fs, fdir, allFiles);\n        for (FileStatusDir statDir: allFiles) {\n          FileStatus stat \u003d statDir.getFileStatus();\n          long len \u003d stat.isDirectory()? 0:stat.getLen();\n          final Path path \u003d relPathToRoot(stat.getPath(), parentPath);\n          final String[] children;\n          if (stat.isDirectory()) {\n            //get the children \n            FileStatus[] list \u003d statDir.getChildren();\n            children \u003d new String[list.length];\n            for (int i \u003d 0; i \u003c list.length; i++) {\n              children[i] \u003d list[i].getPath().getName();\n            }\n          }\n          else {\n            children \u003d null;\n          }\n          append(srcWriter, len, path.toString(), children);\n          srcWriter.sync();\n          numFiles++;\n          totalSize +\u003d len;\n        }\n      }\n    } finally {\n      srcWriter.close();\n    }\n    //increase the replication of src files\n    jobfs.setReplication(srcFiles, (short) 10);\n    conf.setInt(SRC_COUNT_LABEL, numFiles);\n    conf.setLong(TOTAL_SIZE_LABEL, totalSize);\n    int numMaps \u003d (int)(totalSize/partSize);\n    //run atleast one map.\n    conf.setNumMapTasks(numMaps \u003d\u003d 0? 1:numMaps);\n    conf.setNumReduceTasks(1);\n    conf.setInputFormat(HArchiveInputFormat.class);\n    conf.setOutputFormat(NullOutputFormat.class);\n    conf.setMapperClass(HArchivesMapper.class);\n    conf.setReducerClass(HArchivesReducer.class);\n    conf.setMapOutputKeyClass(IntWritable.class);\n    conf.setMapOutputValueClass(Text.class);\n    FileInputFormat.addInputPath(conf, jobDirectory);\n    //make sure no speculative execution is done\n    conf.setSpeculativeExecution(false);\n    JobClient.runJob(conf);\n    //delete the tmp job directory\n    try {\n      jobfs.delete(jobDirectory, true);\n    } catch(IOException ie) {\n      LOG.info(\"Unable to clean tmp directory \" + jobDirectory);\n    }\n  }",
      "path": "hadoop-tools/hadoop-archives/src/main/java/org/apache/hadoop/tools/HadoopArchives.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/HadoopArchives.java",
        "newPath": "hadoop-tools/hadoop-archives/src/main/java/org/apache/hadoop/tools/HadoopArchives.java"
      }
    },
    "cd7157784e5e5ddc4e77144d042e54dd0d04bac1": {
      "type": "Yfilerename",
      "commitMessage": "HADOOP-7560. Change src layout to be heirarchical. Contributed by Alejandro Abdelnur.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1161332 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "24/08/11 5:14 PM",
      "commitName": "cd7157784e5e5ddc4e77144d042e54dd0d04bac1",
      "commitAuthor": "Arun Murthy",
      "commitDateOld": "24/08/11 5:06 PM",
      "commitNameOld": "bb0005cfec5fd2861600ff5babd259b48ba18b63",
      "commitAuthorOld": "Arun Murthy",
      "daysBetweenCommits": 0.01,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  void archive(Path parentPath, List\u003cPath\u003e srcPaths, \n      String archiveName, Path dest) throws IOException {\n    checkPaths(conf, srcPaths);\n    int numFiles \u003d 0;\n    long totalSize \u003d 0;\n    FileSystem fs \u003d parentPath.getFileSystem(conf);\n    this.blockSize \u003d conf.getLong(HAR_BLOCKSIZE_LABEL, blockSize);\n    this.partSize \u003d conf.getLong(HAR_PARTSIZE_LABEL, partSize);\n    conf.setLong(HAR_BLOCKSIZE_LABEL, blockSize);\n    conf.setLong(HAR_PARTSIZE_LABEL, partSize);\n    conf.set(DST_HAR_LABEL, archiveName);\n    conf.set(SRC_PARENT_LABEL, parentPath.makeQualified(fs).toString());\n    Path outputPath \u003d new Path(dest, archiveName);\n    FileOutputFormat.setOutputPath(conf, outputPath);\n    FileSystem outFs \u003d outputPath.getFileSystem(conf);\n    if (outFs.exists(outputPath) || outFs.isFile(dest)) {\n      throw new IOException(\"Invalid Output: \" + outputPath);\n    }\n    conf.set(DST_DIR_LABEL, outputPath.toString());\n    Path stagingArea;\n    try {\n      stagingArea \u003d JobSubmissionFiles.getStagingDir(new Cluster(conf), \n          conf);\n    } catch (InterruptedException ie) {\n      throw new IOException(ie);\n    }\n    Path jobDirectory \u003d new Path(stagingArea,\n        NAME+\"_\"+Integer.toString(new Random().nextInt(Integer.MAX_VALUE), 36));\n    FsPermission mapredSysPerms \u003d \n      new FsPermission(JobSubmissionFiles.JOB_DIR_PERMISSION);\n    FileSystem.mkdirs(jobDirectory.getFileSystem(conf), jobDirectory,\n                      mapredSysPerms);\n    conf.set(JOB_DIR_LABEL, jobDirectory.toString());\n    //get a tmp directory for input splits\n    FileSystem jobfs \u003d jobDirectory.getFileSystem(conf);\n    Path srcFiles \u003d new Path(jobDirectory, \"_har_src_files\");\n    conf.set(SRC_LIST_LABEL, srcFiles.toString());\n    SequenceFile.Writer srcWriter \u003d SequenceFile.createWriter(jobfs, conf,\n        srcFiles, LongWritable.class, HarEntry.class, \n        SequenceFile.CompressionType.NONE);\n    // get the list of files \n    // create single list of files and dirs\n    try {\n      // write the top level dirs in first \n      writeTopLevelDirs(srcWriter, srcPaths, parentPath);\n      srcWriter.sync();\n      // these are the input paths passed \n      // from the command line\n      // we do a recursive ls on these paths \n      // and then write them to the input file \n      // one at a time\n      for (Path src: srcPaths) {\n        ArrayList\u003cFileStatusDir\u003e allFiles \u003d new ArrayList\u003cFileStatusDir\u003e();\n        FileStatus fstatus \u003d fs.getFileStatus(src);\n        FileStatusDir fdir \u003d new FileStatusDir(fstatus, null);\n        recursivels(fs, fdir, allFiles);\n        for (FileStatusDir statDir: allFiles) {\n          FileStatus stat \u003d statDir.getFileStatus();\n          long len \u003d stat.isDirectory()? 0:stat.getLen();\n          final Path path \u003d relPathToRoot(stat.getPath(), parentPath);\n          final String[] children;\n          if (stat.isDirectory()) {\n            //get the children \n            FileStatus[] list \u003d statDir.getChildren();\n            children \u003d new String[list.length];\n            for (int i \u003d 0; i \u003c list.length; i++) {\n              children[i] \u003d list[i].getPath().getName();\n            }\n          }\n          else {\n            children \u003d null;\n          }\n          append(srcWriter, len, path.toString(), children);\n          srcWriter.sync();\n          numFiles++;\n          totalSize +\u003d len;\n        }\n      }\n    } finally {\n      srcWriter.close();\n    }\n    //increase the replication of src files\n    jobfs.setReplication(srcFiles, (short) 10);\n    conf.setInt(SRC_COUNT_LABEL, numFiles);\n    conf.setLong(TOTAL_SIZE_LABEL, totalSize);\n    int numMaps \u003d (int)(totalSize/partSize);\n    //run atleast one map.\n    conf.setNumMapTasks(numMaps \u003d\u003d 0? 1:numMaps);\n    conf.setNumReduceTasks(1);\n    conf.setInputFormat(HArchiveInputFormat.class);\n    conf.setOutputFormat(NullOutputFormat.class);\n    conf.setMapperClass(HArchivesMapper.class);\n    conf.setReducerClass(HArchivesReducer.class);\n    conf.setMapOutputKeyClass(IntWritable.class);\n    conf.setMapOutputValueClass(Text.class);\n    FileInputFormat.addInputPath(conf, jobDirectory);\n    //make sure no speculative execution is done\n    conf.setSpeculativeExecution(false);\n    JobClient.runJob(conf);\n    //delete the tmp job directory\n    try {\n      jobfs.delete(jobDirectory, true);\n    } catch(IOException ie) {\n      LOG.info(\"Unable to clean tmp directory \" + jobDirectory);\n    }\n  }",
      "path": "hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/HadoopArchives.java",
      "extendedDetails": {
        "oldPath": "hadoop-mapreduce/src/tools/org/apache/hadoop/tools/HadoopArchives.java",
        "newPath": "hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/HadoopArchives.java"
      }
    },
    "dbecbe5dfe50f834fc3b8401709079e9470cc517": {
      "type": "Yfilerename",
      "commitMessage": "MAPREDUCE-279. MapReduce 2.0. Merging MR-279 branch into trunk. Contributed by Arun C Murthy, Christopher Douglas, Devaraj Das, Greg Roelofs, Jeffrey Naisbitt, Josh Wills, Jonathan Eagles, Krishna Ramachandran, Luke Lu, Mahadev Konar, Robert Evans, Sharad Agarwal, Siddharth Seth, Thomas Graves, and Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1159166 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "18/08/11 4:07 AM",
      "commitName": "dbecbe5dfe50f834fc3b8401709079e9470cc517",
      "commitAuthor": "Vinod Kumar Vavilapalli",
      "commitDateOld": "17/08/11 8:02 PM",
      "commitNameOld": "dd86860633d2ed64705b669a75bf318442ed6225",
      "commitAuthorOld": "Todd Lipcon",
      "daysBetweenCommits": 0.34,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  void archive(Path parentPath, List\u003cPath\u003e srcPaths, \n      String archiveName, Path dest) throws IOException {\n    checkPaths(conf, srcPaths);\n    int numFiles \u003d 0;\n    long totalSize \u003d 0;\n    FileSystem fs \u003d parentPath.getFileSystem(conf);\n    this.blockSize \u003d conf.getLong(HAR_BLOCKSIZE_LABEL, blockSize);\n    this.partSize \u003d conf.getLong(HAR_PARTSIZE_LABEL, partSize);\n    conf.setLong(HAR_BLOCKSIZE_LABEL, blockSize);\n    conf.setLong(HAR_PARTSIZE_LABEL, partSize);\n    conf.set(DST_HAR_LABEL, archiveName);\n    conf.set(SRC_PARENT_LABEL, parentPath.makeQualified(fs).toString());\n    Path outputPath \u003d new Path(dest, archiveName);\n    FileOutputFormat.setOutputPath(conf, outputPath);\n    FileSystem outFs \u003d outputPath.getFileSystem(conf);\n    if (outFs.exists(outputPath) || outFs.isFile(dest)) {\n      throw new IOException(\"Invalid Output: \" + outputPath);\n    }\n    conf.set(DST_DIR_LABEL, outputPath.toString());\n    Path stagingArea;\n    try {\n      stagingArea \u003d JobSubmissionFiles.getStagingDir(new Cluster(conf), \n          conf);\n    } catch (InterruptedException ie) {\n      throw new IOException(ie);\n    }\n    Path jobDirectory \u003d new Path(stagingArea,\n        NAME+\"_\"+Integer.toString(new Random().nextInt(Integer.MAX_VALUE), 36));\n    FsPermission mapredSysPerms \u003d \n      new FsPermission(JobSubmissionFiles.JOB_DIR_PERMISSION);\n    FileSystem.mkdirs(jobDirectory.getFileSystem(conf), jobDirectory,\n                      mapredSysPerms);\n    conf.set(JOB_DIR_LABEL, jobDirectory.toString());\n    //get a tmp directory for input splits\n    FileSystem jobfs \u003d jobDirectory.getFileSystem(conf);\n    Path srcFiles \u003d new Path(jobDirectory, \"_har_src_files\");\n    conf.set(SRC_LIST_LABEL, srcFiles.toString());\n    SequenceFile.Writer srcWriter \u003d SequenceFile.createWriter(jobfs, conf,\n        srcFiles, LongWritable.class, HarEntry.class, \n        SequenceFile.CompressionType.NONE);\n    // get the list of files \n    // create single list of files and dirs\n    try {\n      // write the top level dirs in first \n      writeTopLevelDirs(srcWriter, srcPaths, parentPath);\n      srcWriter.sync();\n      // these are the input paths passed \n      // from the command line\n      // we do a recursive ls on these paths \n      // and then write them to the input file \n      // one at a time\n      for (Path src: srcPaths) {\n        ArrayList\u003cFileStatusDir\u003e allFiles \u003d new ArrayList\u003cFileStatusDir\u003e();\n        FileStatus fstatus \u003d fs.getFileStatus(src);\n        FileStatusDir fdir \u003d new FileStatusDir(fstatus, null);\n        recursivels(fs, fdir, allFiles);\n        for (FileStatusDir statDir: allFiles) {\n          FileStatus stat \u003d statDir.getFileStatus();\n          long len \u003d stat.isDirectory()? 0:stat.getLen();\n          final Path path \u003d relPathToRoot(stat.getPath(), parentPath);\n          final String[] children;\n          if (stat.isDirectory()) {\n            //get the children \n            FileStatus[] list \u003d statDir.getChildren();\n            children \u003d new String[list.length];\n            for (int i \u003d 0; i \u003c list.length; i++) {\n              children[i] \u003d list[i].getPath().getName();\n            }\n          }\n          else {\n            children \u003d null;\n          }\n          append(srcWriter, len, path.toString(), children);\n          srcWriter.sync();\n          numFiles++;\n          totalSize +\u003d len;\n        }\n      }\n    } finally {\n      srcWriter.close();\n    }\n    //increase the replication of src files\n    jobfs.setReplication(srcFiles, (short) 10);\n    conf.setInt(SRC_COUNT_LABEL, numFiles);\n    conf.setLong(TOTAL_SIZE_LABEL, totalSize);\n    int numMaps \u003d (int)(totalSize/partSize);\n    //run atleast one map.\n    conf.setNumMapTasks(numMaps \u003d\u003d 0? 1:numMaps);\n    conf.setNumReduceTasks(1);\n    conf.setInputFormat(HArchiveInputFormat.class);\n    conf.setOutputFormat(NullOutputFormat.class);\n    conf.setMapperClass(HArchivesMapper.class);\n    conf.setReducerClass(HArchivesReducer.class);\n    conf.setMapOutputKeyClass(IntWritable.class);\n    conf.setMapOutputValueClass(Text.class);\n    FileInputFormat.addInputPath(conf, jobDirectory);\n    //make sure no speculative execution is done\n    conf.setSpeculativeExecution(false);\n    JobClient.runJob(conf);\n    //delete the tmp job directory\n    try {\n      jobfs.delete(jobDirectory, true);\n    } catch(IOException ie) {\n      LOG.info(\"Unable to clean tmp directory \" + jobDirectory);\n    }\n  }",
      "path": "hadoop-mapreduce/src/tools/org/apache/hadoop/tools/HadoopArchives.java",
      "extendedDetails": {
        "oldPath": "mapreduce/src/tools/org/apache/hadoop/tools/HadoopArchives.java",
        "newPath": "hadoop-mapreduce/src/tools/org/apache/hadoop/tools/HadoopArchives.java"
      }
    },
    "a196766ea07775f18ded69bd9e8d239f8cfd3ccc": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-7106. Reorganize SVN layout to combine HDFS, Common, and MR in a single tree (project unsplit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1134994 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/06/11 3:00 PM",
      "commitName": "a196766ea07775f18ded69bd9e8d239f8cfd3ccc",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,106 @@\n+  void archive(Path parentPath, List\u003cPath\u003e srcPaths, \n+      String archiveName, Path dest) throws IOException {\n+    checkPaths(conf, srcPaths);\n+    int numFiles \u003d 0;\n+    long totalSize \u003d 0;\n+    FileSystem fs \u003d parentPath.getFileSystem(conf);\n+    this.blockSize \u003d conf.getLong(HAR_BLOCKSIZE_LABEL, blockSize);\n+    this.partSize \u003d conf.getLong(HAR_PARTSIZE_LABEL, partSize);\n+    conf.setLong(HAR_BLOCKSIZE_LABEL, blockSize);\n+    conf.setLong(HAR_PARTSIZE_LABEL, partSize);\n+    conf.set(DST_HAR_LABEL, archiveName);\n+    conf.set(SRC_PARENT_LABEL, parentPath.makeQualified(fs).toString());\n+    Path outputPath \u003d new Path(dest, archiveName);\n+    FileOutputFormat.setOutputPath(conf, outputPath);\n+    FileSystem outFs \u003d outputPath.getFileSystem(conf);\n+    if (outFs.exists(outputPath) || outFs.isFile(dest)) {\n+      throw new IOException(\"Invalid Output: \" + outputPath);\n+    }\n+    conf.set(DST_DIR_LABEL, outputPath.toString());\n+    Path stagingArea;\n+    try {\n+      stagingArea \u003d JobSubmissionFiles.getStagingDir(new Cluster(conf), \n+          conf);\n+    } catch (InterruptedException ie) {\n+      throw new IOException(ie);\n+    }\n+    Path jobDirectory \u003d new Path(stagingArea,\n+        NAME+\"_\"+Integer.toString(new Random().nextInt(Integer.MAX_VALUE), 36));\n+    FsPermission mapredSysPerms \u003d \n+      new FsPermission(JobSubmissionFiles.JOB_DIR_PERMISSION);\n+    FileSystem.mkdirs(jobDirectory.getFileSystem(conf), jobDirectory,\n+                      mapredSysPerms);\n+    conf.set(JOB_DIR_LABEL, jobDirectory.toString());\n+    //get a tmp directory for input splits\n+    FileSystem jobfs \u003d jobDirectory.getFileSystem(conf);\n+    Path srcFiles \u003d new Path(jobDirectory, \"_har_src_files\");\n+    conf.set(SRC_LIST_LABEL, srcFiles.toString());\n+    SequenceFile.Writer srcWriter \u003d SequenceFile.createWriter(jobfs, conf,\n+        srcFiles, LongWritable.class, HarEntry.class, \n+        SequenceFile.CompressionType.NONE);\n+    // get the list of files \n+    // create single list of files and dirs\n+    try {\n+      // write the top level dirs in first \n+      writeTopLevelDirs(srcWriter, srcPaths, parentPath);\n+      srcWriter.sync();\n+      // these are the input paths passed \n+      // from the command line\n+      // we do a recursive ls on these paths \n+      // and then write them to the input file \n+      // one at a time\n+      for (Path src: srcPaths) {\n+        ArrayList\u003cFileStatusDir\u003e allFiles \u003d new ArrayList\u003cFileStatusDir\u003e();\n+        FileStatus fstatus \u003d fs.getFileStatus(src);\n+        FileStatusDir fdir \u003d new FileStatusDir(fstatus, null);\n+        recursivels(fs, fdir, allFiles);\n+        for (FileStatusDir statDir: allFiles) {\n+          FileStatus stat \u003d statDir.getFileStatus();\n+          long len \u003d stat.isDirectory()? 0:stat.getLen();\n+          final Path path \u003d relPathToRoot(stat.getPath(), parentPath);\n+          final String[] children;\n+          if (stat.isDirectory()) {\n+            //get the children \n+            FileStatus[] list \u003d statDir.getChildren();\n+            children \u003d new String[list.length];\n+            for (int i \u003d 0; i \u003c list.length; i++) {\n+              children[i] \u003d list[i].getPath().getName();\n+            }\n+          }\n+          else {\n+            children \u003d null;\n+          }\n+          append(srcWriter, len, path.toString(), children);\n+          srcWriter.sync();\n+          numFiles++;\n+          totalSize +\u003d len;\n+        }\n+      }\n+    } finally {\n+      srcWriter.close();\n+    }\n+    //increase the replication of src files\n+    jobfs.setReplication(srcFiles, (short) 10);\n+    conf.setInt(SRC_COUNT_LABEL, numFiles);\n+    conf.setLong(TOTAL_SIZE_LABEL, totalSize);\n+    int numMaps \u003d (int)(totalSize/partSize);\n+    //run atleast one map.\n+    conf.setNumMapTasks(numMaps \u003d\u003d 0? 1:numMaps);\n+    conf.setNumReduceTasks(1);\n+    conf.setInputFormat(HArchiveInputFormat.class);\n+    conf.setOutputFormat(NullOutputFormat.class);\n+    conf.setMapperClass(HArchivesMapper.class);\n+    conf.setReducerClass(HArchivesReducer.class);\n+    conf.setMapOutputKeyClass(IntWritable.class);\n+    conf.setMapOutputValueClass(Text.class);\n+    FileInputFormat.addInputPath(conf, jobDirectory);\n+    //make sure no speculative execution is done\n+    conf.setSpeculativeExecution(false);\n+    JobClient.runJob(conf);\n+    //delete the tmp job directory\n+    try {\n+      jobfs.delete(jobDirectory, true);\n+    } catch(IOException ie) {\n+      LOG.info(\"Unable to clean tmp directory \" + jobDirectory);\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  void archive(Path parentPath, List\u003cPath\u003e srcPaths, \n      String archiveName, Path dest) throws IOException {\n    checkPaths(conf, srcPaths);\n    int numFiles \u003d 0;\n    long totalSize \u003d 0;\n    FileSystem fs \u003d parentPath.getFileSystem(conf);\n    this.blockSize \u003d conf.getLong(HAR_BLOCKSIZE_LABEL, blockSize);\n    this.partSize \u003d conf.getLong(HAR_PARTSIZE_LABEL, partSize);\n    conf.setLong(HAR_BLOCKSIZE_LABEL, blockSize);\n    conf.setLong(HAR_PARTSIZE_LABEL, partSize);\n    conf.set(DST_HAR_LABEL, archiveName);\n    conf.set(SRC_PARENT_LABEL, parentPath.makeQualified(fs).toString());\n    Path outputPath \u003d new Path(dest, archiveName);\n    FileOutputFormat.setOutputPath(conf, outputPath);\n    FileSystem outFs \u003d outputPath.getFileSystem(conf);\n    if (outFs.exists(outputPath) || outFs.isFile(dest)) {\n      throw new IOException(\"Invalid Output: \" + outputPath);\n    }\n    conf.set(DST_DIR_LABEL, outputPath.toString());\n    Path stagingArea;\n    try {\n      stagingArea \u003d JobSubmissionFiles.getStagingDir(new Cluster(conf), \n          conf);\n    } catch (InterruptedException ie) {\n      throw new IOException(ie);\n    }\n    Path jobDirectory \u003d new Path(stagingArea,\n        NAME+\"_\"+Integer.toString(new Random().nextInt(Integer.MAX_VALUE), 36));\n    FsPermission mapredSysPerms \u003d \n      new FsPermission(JobSubmissionFiles.JOB_DIR_PERMISSION);\n    FileSystem.mkdirs(jobDirectory.getFileSystem(conf), jobDirectory,\n                      mapredSysPerms);\n    conf.set(JOB_DIR_LABEL, jobDirectory.toString());\n    //get a tmp directory for input splits\n    FileSystem jobfs \u003d jobDirectory.getFileSystem(conf);\n    Path srcFiles \u003d new Path(jobDirectory, \"_har_src_files\");\n    conf.set(SRC_LIST_LABEL, srcFiles.toString());\n    SequenceFile.Writer srcWriter \u003d SequenceFile.createWriter(jobfs, conf,\n        srcFiles, LongWritable.class, HarEntry.class, \n        SequenceFile.CompressionType.NONE);\n    // get the list of files \n    // create single list of files and dirs\n    try {\n      // write the top level dirs in first \n      writeTopLevelDirs(srcWriter, srcPaths, parentPath);\n      srcWriter.sync();\n      // these are the input paths passed \n      // from the command line\n      // we do a recursive ls on these paths \n      // and then write them to the input file \n      // one at a time\n      for (Path src: srcPaths) {\n        ArrayList\u003cFileStatusDir\u003e allFiles \u003d new ArrayList\u003cFileStatusDir\u003e();\n        FileStatus fstatus \u003d fs.getFileStatus(src);\n        FileStatusDir fdir \u003d new FileStatusDir(fstatus, null);\n        recursivels(fs, fdir, allFiles);\n        for (FileStatusDir statDir: allFiles) {\n          FileStatus stat \u003d statDir.getFileStatus();\n          long len \u003d stat.isDirectory()? 0:stat.getLen();\n          final Path path \u003d relPathToRoot(stat.getPath(), parentPath);\n          final String[] children;\n          if (stat.isDirectory()) {\n            //get the children \n            FileStatus[] list \u003d statDir.getChildren();\n            children \u003d new String[list.length];\n            for (int i \u003d 0; i \u003c list.length; i++) {\n              children[i] \u003d list[i].getPath().getName();\n            }\n          }\n          else {\n            children \u003d null;\n          }\n          append(srcWriter, len, path.toString(), children);\n          srcWriter.sync();\n          numFiles++;\n          totalSize +\u003d len;\n        }\n      }\n    } finally {\n      srcWriter.close();\n    }\n    //increase the replication of src files\n    jobfs.setReplication(srcFiles, (short) 10);\n    conf.setInt(SRC_COUNT_LABEL, numFiles);\n    conf.setLong(TOTAL_SIZE_LABEL, totalSize);\n    int numMaps \u003d (int)(totalSize/partSize);\n    //run atleast one map.\n    conf.setNumMapTasks(numMaps \u003d\u003d 0? 1:numMaps);\n    conf.setNumReduceTasks(1);\n    conf.setInputFormat(HArchiveInputFormat.class);\n    conf.setOutputFormat(NullOutputFormat.class);\n    conf.setMapperClass(HArchivesMapper.class);\n    conf.setReducerClass(HArchivesReducer.class);\n    conf.setMapOutputKeyClass(IntWritable.class);\n    conf.setMapOutputValueClass(Text.class);\n    FileInputFormat.addInputPath(conf, jobDirectory);\n    //make sure no speculative execution is done\n    conf.setSpeculativeExecution(false);\n    JobClient.runJob(conf);\n    //delete the tmp job directory\n    try {\n      jobfs.delete(jobDirectory, true);\n    } catch(IOException ie) {\n      LOG.info(\"Unable to clean tmp directory \" + jobDirectory);\n    }\n  }",
      "path": "mapreduce/src/tools/org/apache/hadoop/tools/HadoopArchives.java"
    }
  }
}