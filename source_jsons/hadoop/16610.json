{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "RouterRpcServer.java",
  "functionName": "getDatanodeStorageReport",
  "functionId": "getDatanodeStorageReport___type-DatanodeReportType",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java",
  "functionStartLine": 916,
  "functionEndLine": 919,
  "numCommitsSeen": 75,
  "timeTaken": 5474,
  "changeHistory": [
    "6425ed27ea638da75f656204d6df4adad1d91fe1",
    "fb5b3dce6192265bce9b9d93ab663bdc5be8048e",
    "fa121eb66bc42e9cb5586f8c2e268cfdc2ed187a",
    "6e2b5fa493ff8e8c2bb28e6f6f4c19347bc9b99d",
    "e71bc00a471422ddb26dd54e706f09f0fe09925c",
    "d5d6a0353bb85b882cc4ef60e3a12d63243d34ba",
    "ca4f209b49e3aad6a80306f7342c9b6b560a79a7"
  ],
  "changeHistoryShort": {
    "6425ed27ea638da75f656204d6df4adad1d91fe1": "Ybodychange",
    "fb5b3dce6192265bce9b9d93ab663bdc5be8048e": "Ybodychange",
    "fa121eb66bc42e9cb5586f8c2e268cfdc2ed187a": "Ybodychange",
    "6e2b5fa493ff8e8c2bb28e6f6f4c19347bc9b99d": "Yfilerename",
    "e71bc00a471422ddb26dd54e706f09f0fe09925c": "Ybodychange",
    "d5d6a0353bb85b882cc4ef60e3a12d63243d34ba": "Ybodychange",
    "ca4f209b49e3aad6a80306f7342c9b6b560a79a7": "Yintroduced"
  },
  "changeHistoryDetails": {
    "6425ed27ea638da75f656204d6df4adad1d91fe1": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13790. RBF: Move ClientProtocol APIs to its own module. Contributed by Chao Sun.\n",
      "commitDate": "19/08/18 11:50 PM",
      "commitName": "6425ed27ea638da75f656204d6df4adad1d91fe1",
      "commitAuthor": "Brahma Reddy Battula",
      "commitDateOld": "17/08/18 8:01 AM",
      "commitNameOld": "fb5b3dce6192265bce9b9d93ab663bdc5be8048e",
      "commitAuthorOld": "Xiaoyu Yao",
      "daysBetweenCommits": 2.66,
      "commitsBetweenForRepo": 8,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,26 +1,4 @@\n   public DatanodeStorageReport[] getDatanodeStorageReport(\n       DatanodeReportType type) throws IOException {\n-    checkOperation(OperationCategory.UNCHECKED);\n-\n-    Map\u003cString, DatanodeStorageReport[]\u003e dnSubcluster \u003d\n-        getDatanodeStorageReportMap(type);\n-\n-    // Avoid repeating machines in multiple subclusters\n-    Map\u003cString, DatanodeStorageReport\u003e datanodesMap \u003d new LinkedHashMap\u003c\u003e();\n-    for (DatanodeStorageReport[] dns : dnSubcluster.values()) {\n-      for (DatanodeStorageReport dn : dns) {\n-        DatanodeInfo dnInfo \u003d dn.getDatanodeInfo();\n-        String nodeId \u003d dnInfo.getXferAddr();\n-        if (!datanodesMap.containsKey(nodeId)) {\n-          datanodesMap.put(nodeId, dn);\n-        }\n-        // TODO merge somehow, right now it just takes the first one\n-      }\n-    }\n-\n-    Collection\u003cDatanodeStorageReport\u003e datanodes \u003d datanodesMap.values();\n-    DatanodeStorageReport[] combinedData \u003d\n-        new DatanodeStorageReport[datanodes.size()];\n-    combinedData \u003d datanodes.toArray(combinedData);\n-    return combinedData;\n+    return clientProto.getDatanodeStorageReport(type);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public DatanodeStorageReport[] getDatanodeStorageReport(\n      DatanodeReportType type) throws IOException {\n    return clientProto.getDatanodeStorageReport(type);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java",
      "extendedDetails": {}
    },
    "fb5b3dce6192265bce9b9d93ab663bdc5be8048e": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"HDFS-13790. RBF: Move ClientProtocol APIs to its own module. Contributed by Chao Sun.\"\n\nThis reverts commit fa121eb66bc42e9cb5586f8c2e268cfdc2ed187a.\n",
      "commitDate": "17/08/18 8:01 AM",
      "commitName": "fb5b3dce6192265bce9b9d93ab663bdc5be8048e",
      "commitAuthor": "Xiaoyu Yao",
      "commitDateOld": "17/08/18 2:52 AM",
      "commitNameOld": "fa121eb66bc42e9cb5586f8c2e268cfdc2ed187a",
      "commitAuthorOld": "Brahma Reddy Battula",
      "daysBetweenCommits": 0.21,
      "commitsBetweenForRepo": 2,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,4 +1,26 @@\n   public DatanodeStorageReport[] getDatanodeStorageReport(\n       DatanodeReportType type) throws IOException {\n-    return clientProto.getDatanodeStorageReport(type);\n+    checkOperation(OperationCategory.UNCHECKED);\n+\n+    Map\u003cString, DatanodeStorageReport[]\u003e dnSubcluster \u003d\n+        getDatanodeStorageReportMap(type);\n+\n+    // Avoid repeating machines in multiple subclusters\n+    Map\u003cString, DatanodeStorageReport\u003e datanodesMap \u003d new LinkedHashMap\u003c\u003e();\n+    for (DatanodeStorageReport[] dns : dnSubcluster.values()) {\n+      for (DatanodeStorageReport dn : dns) {\n+        DatanodeInfo dnInfo \u003d dn.getDatanodeInfo();\n+        String nodeId \u003d dnInfo.getXferAddr();\n+        if (!datanodesMap.containsKey(nodeId)) {\n+          datanodesMap.put(nodeId, dn);\n+        }\n+        // TODO merge somehow, right now it just takes the first one\n+      }\n+    }\n+\n+    Collection\u003cDatanodeStorageReport\u003e datanodes \u003d datanodesMap.values();\n+    DatanodeStorageReport[] combinedData \u003d\n+        new DatanodeStorageReport[datanodes.size()];\n+    combinedData \u003d datanodes.toArray(combinedData);\n+    return combinedData;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public DatanodeStorageReport[] getDatanodeStorageReport(\n      DatanodeReportType type) throws IOException {\n    checkOperation(OperationCategory.UNCHECKED);\n\n    Map\u003cString, DatanodeStorageReport[]\u003e dnSubcluster \u003d\n        getDatanodeStorageReportMap(type);\n\n    // Avoid repeating machines in multiple subclusters\n    Map\u003cString, DatanodeStorageReport\u003e datanodesMap \u003d new LinkedHashMap\u003c\u003e();\n    for (DatanodeStorageReport[] dns : dnSubcluster.values()) {\n      for (DatanodeStorageReport dn : dns) {\n        DatanodeInfo dnInfo \u003d dn.getDatanodeInfo();\n        String nodeId \u003d dnInfo.getXferAddr();\n        if (!datanodesMap.containsKey(nodeId)) {\n          datanodesMap.put(nodeId, dn);\n        }\n        // TODO merge somehow, right now it just takes the first one\n      }\n    }\n\n    Collection\u003cDatanodeStorageReport\u003e datanodes \u003d datanodesMap.values();\n    DatanodeStorageReport[] combinedData \u003d\n        new DatanodeStorageReport[datanodes.size()];\n    combinedData \u003d datanodes.toArray(combinedData);\n    return combinedData;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java",
      "extendedDetails": {}
    },
    "fa121eb66bc42e9cb5586f8c2e268cfdc2ed187a": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13790. RBF: Move ClientProtocol APIs to its own module. Contributed by Chao Sun.\n",
      "commitDate": "17/08/18 2:52 AM",
      "commitName": "fa121eb66bc42e9cb5586f8c2e268cfdc2ed187a",
      "commitAuthor": "Brahma Reddy Battula",
      "commitDateOld": "12/08/18 3:06 AM",
      "commitNameOld": "39ed3a66dbb01383ed16b141183fc48bfd2e613d",
      "commitAuthorOld": "Uma Maheswara Rao G",
      "daysBetweenCommits": 4.99,
      "commitsBetweenForRepo": 43,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,26 +1,4 @@\n   public DatanodeStorageReport[] getDatanodeStorageReport(\n       DatanodeReportType type) throws IOException {\n-    checkOperation(OperationCategory.UNCHECKED);\n-\n-    Map\u003cString, DatanodeStorageReport[]\u003e dnSubcluster \u003d\n-        getDatanodeStorageReportMap(type);\n-\n-    // Avoid repeating machines in multiple subclusters\n-    Map\u003cString, DatanodeStorageReport\u003e datanodesMap \u003d new LinkedHashMap\u003c\u003e();\n-    for (DatanodeStorageReport[] dns : dnSubcluster.values()) {\n-      for (DatanodeStorageReport dn : dns) {\n-        DatanodeInfo dnInfo \u003d dn.getDatanodeInfo();\n-        String nodeId \u003d dnInfo.getXferAddr();\n-        if (!datanodesMap.containsKey(nodeId)) {\n-          datanodesMap.put(nodeId, dn);\n-        }\n-        // TODO merge somehow, right now it just takes the first one\n-      }\n-    }\n-\n-    Collection\u003cDatanodeStorageReport\u003e datanodes \u003d datanodesMap.values();\n-    DatanodeStorageReport[] combinedData \u003d\n-        new DatanodeStorageReport[datanodes.size()];\n-    combinedData \u003d datanodes.toArray(combinedData);\n-    return combinedData;\n+    return clientProto.getDatanodeStorageReport(type);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public DatanodeStorageReport[] getDatanodeStorageReport(\n      DatanodeReportType type) throws IOException {\n    return clientProto.getDatanodeStorageReport(type);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java",
      "extendedDetails": {}
    },
    "6e2b5fa493ff8e8c2bb28e6f6f4c19347bc9b99d": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-13215. RBF: Move Router to its own module. Contributed by Wei Yan\n",
      "commitDate": "19/03/18 10:13 PM",
      "commitName": "6e2b5fa493ff8e8c2bb28e6f6f4c19347bc9b99d",
      "commitAuthor": "weiy",
      "commitDateOld": "19/03/18 5:19 PM",
      "commitNameOld": "e65ff1c8be48ef4f04ed96f96ac4caef4974944d",
      "commitAuthorOld": "Inigo Goiri",
      "daysBetweenCommits": 0.2,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  public DatanodeStorageReport[] getDatanodeStorageReport(\n      DatanodeReportType type) throws IOException {\n    checkOperation(OperationCategory.UNCHECKED);\n\n    Map\u003cString, DatanodeStorageReport[]\u003e dnSubcluster \u003d\n        getDatanodeStorageReportMap(type);\n\n    // Avoid repeating machines in multiple subclusters\n    Map\u003cString, DatanodeStorageReport\u003e datanodesMap \u003d new LinkedHashMap\u003c\u003e();\n    for (DatanodeStorageReport[] dns : dnSubcluster.values()) {\n      for (DatanodeStorageReport dn : dns) {\n        DatanodeInfo dnInfo \u003d dn.getDatanodeInfo();\n        String nodeId \u003d dnInfo.getXferAddr();\n        if (!datanodesMap.containsKey(nodeId)) {\n          datanodesMap.put(nodeId, dn);\n        }\n        // TODO merge somehow, right now it just takes the first one\n      }\n    }\n\n    Collection\u003cDatanodeStorageReport\u003e datanodes \u003d datanodesMap.values();\n    DatanodeStorageReport[] combinedData \u003d\n        new DatanodeStorageReport[datanodes.size()];\n    combinedData \u003d datanodes.toArray(combinedData);\n    return combinedData;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java"
      }
    },
    "e71bc00a471422ddb26dd54e706f09f0fe09925c": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-13224. RBF: Resolvers to support mount points across multiple subclusters. Contributed by Inigo Goiri.\n",
      "commitDate": "15/03/18 10:32 AM",
      "commitName": "e71bc00a471422ddb26dd54e706f09f0fe09925c",
      "commitAuthor": "Inigo Goiri",
      "commitDateOld": "19/02/18 5:37 PM",
      "commitNameOld": "8896d20b91520053a6bbfb680adb345cd24f4142",
      "commitAuthorOld": "Yiqun Lin",
      "daysBetweenCommits": 23.66,
      "commitsBetweenForRepo": 181,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,26 @@\n   public DatanodeStorageReport[] getDatanodeStorageReport(\n       DatanodeReportType type) throws IOException {\n     checkOperation(OperationCategory.UNCHECKED);\n \n-    Map\u003cString, DatanodeStorageReport\u003e datanodesMap \u003d new HashMap\u003c\u003e();\n-    RemoteMethod method \u003d new RemoteMethod(\"getDatanodeStorageReport\",\n-        new Class\u003c?\u003e[] {DatanodeReportType.class}, type);\n-    Set\u003cFederationNamespaceInfo\u003e nss \u003d namenodeResolver.getNamespaces();\n-    Map\u003cFederationNamespaceInfo, DatanodeStorageReport[]\u003e results \u003d\n-        rpcClient.invokeConcurrent(\n-            nss, method, true, false, DatanodeStorageReport[].class);\n-    for (DatanodeStorageReport[] result : results.values()) {\n-      for (DatanodeStorageReport node : result) {\n-        String nodeId \u003d node.getDatanodeInfo().getXferAddr();\n+    Map\u003cString, DatanodeStorageReport[]\u003e dnSubcluster \u003d\n+        getDatanodeStorageReportMap(type);\n+\n+    // Avoid repeating machines in multiple subclusters\n+    Map\u003cString, DatanodeStorageReport\u003e datanodesMap \u003d new LinkedHashMap\u003c\u003e();\n+    for (DatanodeStorageReport[] dns : dnSubcluster.values()) {\n+      for (DatanodeStorageReport dn : dns) {\n+        DatanodeInfo dnInfo \u003d dn.getDatanodeInfo();\n+        String nodeId \u003d dnInfo.getXferAddr();\n         if (!datanodesMap.containsKey(nodeId)) {\n-          datanodesMap.put(nodeId, node);\n+          datanodesMap.put(nodeId, dn);\n         }\n         // TODO merge somehow, right now it just takes the first one\n       }\n     }\n \n     Collection\u003cDatanodeStorageReport\u003e datanodes \u003d datanodesMap.values();\n-    // TODO sort somehow\n     DatanodeStorageReport[] combinedData \u003d\n         new DatanodeStorageReport[datanodes.size()];\n     combinedData \u003d datanodes.toArray(combinedData);\n     return combinedData;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public DatanodeStorageReport[] getDatanodeStorageReport(\n      DatanodeReportType type) throws IOException {\n    checkOperation(OperationCategory.UNCHECKED);\n\n    Map\u003cString, DatanodeStorageReport[]\u003e dnSubcluster \u003d\n        getDatanodeStorageReportMap(type);\n\n    // Avoid repeating machines in multiple subclusters\n    Map\u003cString, DatanodeStorageReport\u003e datanodesMap \u003d new LinkedHashMap\u003c\u003e();\n    for (DatanodeStorageReport[] dns : dnSubcluster.values()) {\n      for (DatanodeStorageReport dn : dns) {\n        DatanodeInfo dnInfo \u003d dn.getDatanodeInfo();\n        String nodeId \u003d dnInfo.getXferAddr();\n        if (!datanodesMap.containsKey(nodeId)) {\n          datanodesMap.put(nodeId, dn);\n        }\n        // TODO merge somehow, right now it just takes the first one\n      }\n    }\n\n    Collection\u003cDatanodeStorageReport\u003e datanodes \u003d datanodesMap.values();\n    DatanodeStorageReport[] combinedData \u003d\n        new DatanodeStorageReport[datanodes.size()];\n    combinedData \u003d datanodes.toArray(combinedData);\n    return combinedData;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java",
      "extendedDetails": {}
    },
    "d5d6a0353bb85b882cc4ef60e3a12d63243d34ba": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-12919. RBF: Support erasure coding methods in RouterRpcServer. Contributed by Inigo Goiri.\n",
      "commitDate": "12/01/18 4:25 PM",
      "commitName": "d5d6a0353bb85b882cc4ef60e3a12d63243d34ba",
      "commitAuthor": "Inigo Goiri",
      "commitDateOld": "09/01/18 9:59 PM",
      "commitNameOld": "d98a2e6e2383f8b66def346409b0517aa32d298d",
      "commitAuthorOld": "Yiqun Lin",
      "daysBetweenCommits": 2.77,
      "commitsBetweenForRepo": 16,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,28 +1,28 @@\n   public DatanodeStorageReport[] getDatanodeStorageReport(\n       DatanodeReportType type) throws IOException {\n     checkOperation(OperationCategory.UNCHECKED);\n \n     Map\u003cString, DatanodeStorageReport\u003e datanodesMap \u003d new HashMap\u003c\u003e();\n     RemoteMethod method \u003d new RemoteMethod(\"getDatanodeStorageReport\",\n         new Class\u003c?\u003e[] {DatanodeReportType.class}, type);\n     Set\u003cFederationNamespaceInfo\u003e nss \u003d namenodeResolver.getNamespaces();\n-    Map\u003cFederationNamespaceInfo, Object\u003e results \u003d\n-        rpcClient.invokeConcurrent(nss, method, true, false);\n-    for (Object r : results.values()) {\n-      DatanodeStorageReport[] result \u003d (DatanodeStorageReport[]) r;\n+    Map\u003cFederationNamespaceInfo, DatanodeStorageReport[]\u003e results \u003d\n+        rpcClient.invokeConcurrent(\n+            nss, method, true, false, DatanodeStorageReport[].class);\n+    for (DatanodeStorageReport[] result : results.values()) {\n       for (DatanodeStorageReport node : result) {\n         String nodeId \u003d node.getDatanodeInfo().getXferAddr();\n         if (!datanodesMap.containsKey(nodeId)) {\n           datanodesMap.put(nodeId, node);\n         }\n         // TODO merge somehow, right now it just takes the first one\n       }\n     }\n \n     Collection\u003cDatanodeStorageReport\u003e datanodes \u003d datanodesMap.values();\n     // TODO sort somehow\n     DatanodeStorageReport[] combinedData \u003d\n         new DatanodeStorageReport[datanodes.size()];\n     combinedData \u003d datanodes.toArray(combinedData);\n     return combinedData;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  public DatanodeStorageReport[] getDatanodeStorageReport(\n      DatanodeReportType type) throws IOException {\n    checkOperation(OperationCategory.UNCHECKED);\n\n    Map\u003cString, DatanodeStorageReport\u003e datanodesMap \u003d new HashMap\u003c\u003e();\n    RemoteMethod method \u003d new RemoteMethod(\"getDatanodeStorageReport\",\n        new Class\u003c?\u003e[] {DatanodeReportType.class}, type);\n    Set\u003cFederationNamespaceInfo\u003e nss \u003d namenodeResolver.getNamespaces();\n    Map\u003cFederationNamespaceInfo, DatanodeStorageReport[]\u003e results \u003d\n        rpcClient.invokeConcurrent(\n            nss, method, true, false, DatanodeStorageReport[].class);\n    for (DatanodeStorageReport[] result : results.values()) {\n      for (DatanodeStorageReport node : result) {\n        String nodeId \u003d node.getDatanodeInfo().getXferAddr();\n        if (!datanodesMap.containsKey(nodeId)) {\n          datanodesMap.put(nodeId, node);\n        }\n        // TODO merge somehow, right now it just takes the first one\n      }\n    }\n\n    Collection\u003cDatanodeStorageReport\u003e datanodes \u003d datanodesMap.values();\n    // TODO sort somehow\n    DatanodeStorageReport[] combinedData \u003d\n        new DatanodeStorageReport[datanodes.size()];\n    combinedData \u003d datanodes.toArray(combinedData);\n    return combinedData;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java",
      "extendedDetails": {}
    },
    "ca4f209b49e3aad6a80306f7342c9b6b560a79a7": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-11546. Federation Router RPC server. Contributed by Jason Kace and Inigo Goiri.\n\n(cherry picked from commit 8a9cdebebf26841a0f1e99fb08135f4597f2eba2)\n",
      "commitDate": "06/10/17 6:50 PM",
      "commitName": "ca4f209b49e3aad6a80306f7342c9b6b560a79a7",
      "commitAuthor": "Inigo Goiri",
      "diff": "@@ -0,0 +1,28 @@\n+  public DatanodeStorageReport[] getDatanodeStorageReport(\n+      DatanodeReportType type) throws IOException {\n+    checkOperation(OperationCategory.UNCHECKED);\n+\n+    Map\u003cString, DatanodeStorageReport\u003e datanodesMap \u003d new HashMap\u003c\u003e();\n+    RemoteMethod method \u003d new RemoteMethod(\"getDatanodeStorageReport\",\n+        new Class\u003c?\u003e[] {DatanodeReportType.class}, type);\n+    Set\u003cFederationNamespaceInfo\u003e nss \u003d namenodeResolver.getNamespaces();\n+    Map\u003cFederationNamespaceInfo, Object\u003e results \u003d\n+        rpcClient.invokeConcurrent(nss, method, true, false);\n+    for (Object r : results.values()) {\n+      DatanodeStorageReport[] result \u003d (DatanodeStorageReport[]) r;\n+      for (DatanodeStorageReport node : result) {\n+        String nodeId \u003d node.getDatanodeInfo().getXferAddr();\n+        if (!datanodesMap.containsKey(nodeId)) {\n+          datanodesMap.put(nodeId, node);\n+        }\n+        // TODO merge somehow, right now it just takes the first one\n+      }\n+    }\n+\n+    Collection\u003cDatanodeStorageReport\u003e datanodes \u003d datanodesMap.values();\n+    // TODO sort somehow\n+    DatanodeStorageReport[] combinedData \u003d\n+        new DatanodeStorageReport[datanodes.size()];\n+    combinedData \u003d datanodes.toArray(combinedData);\n+    return combinedData;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  public DatanodeStorageReport[] getDatanodeStorageReport(\n      DatanodeReportType type) throws IOException {\n    checkOperation(OperationCategory.UNCHECKED);\n\n    Map\u003cString, DatanodeStorageReport\u003e datanodesMap \u003d new HashMap\u003c\u003e();\n    RemoteMethod method \u003d new RemoteMethod(\"getDatanodeStorageReport\",\n        new Class\u003c?\u003e[] {DatanodeReportType.class}, type);\n    Set\u003cFederationNamespaceInfo\u003e nss \u003d namenodeResolver.getNamespaces();\n    Map\u003cFederationNamespaceInfo, Object\u003e results \u003d\n        rpcClient.invokeConcurrent(nss, method, true, false);\n    for (Object r : results.values()) {\n      DatanodeStorageReport[] result \u003d (DatanodeStorageReport[]) r;\n      for (DatanodeStorageReport node : result) {\n        String nodeId \u003d node.getDatanodeInfo().getXferAddr();\n        if (!datanodesMap.containsKey(nodeId)) {\n          datanodesMap.put(nodeId, node);\n        }\n        // TODO merge somehow, right now it just takes the first one\n      }\n    }\n\n    Collection\u003cDatanodeStorageReport\u003e datanodes \u003d datanodesMap.values();\n    // TODO sort somehow\n    DatanodeStorageReport[] combinedData \u003d\n        new DatanodeStorageReport[datanodes.size()];\n    combinedData \u003d datanodes.toArray(combinedData);\n    return combinedData;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/federation/router/RouterRpcServer.java"
    }
  }
}