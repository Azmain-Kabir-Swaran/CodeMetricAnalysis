{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "S3ABlockOutputStream.java",
  "functionName": "uploadBlockAsync",
  "functionId": "uploadBlockAsync___block-S3ADataBlocks.DataBlock(modifiers-final)",
  "sourceFilePath": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ABlockOutputStream.java",
  "functionStartLine": 613,
  "functionEndLine": 669,
  "numCommitsSeen": 10,
  "timeTaken": 2310,
  "changeHistory": [
    "29b19cd59245c8809b697b3d7d7445813a685aad",
    "990063d2af0a37e9474949f33128805e34c3f016",
    "de8b6ca5ef8614de6d6277b7617e27c788b0555c",
    "dab00da19f25619ccc71c7f803a235b21766bf1e",
    "6c348c56918973fd988b110e79231324a8befe12"
  ],
  "changeHistoryShort": {
    "29b19cd59245c8809b697b3d7d7445813a685aad": "Ybodychange",
    "990063d2af0a37e9474949f33128805e34c3f016": "Ybodychange",
    "de8b6ca5ef8614de6d6277b7617e27c788b0555c": "Ybodychange",
    "dab00da19f25619ccc71c7f803a235b21766bf1e": "Ybodychange",
    "6c348c56918973fd988b110e79231324a8befe12": "Yintroduced"
  },
  "changeHistoryDetails": {
    "29b19cd59245c8809b697b3d7d7445813a685aad": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-16900. Very large files can be truncated when written through the S3A FileSystem.\n\nContributed by Mukund Thakur and Steve Loughran.\n\nThis patch ensures that writes to S3A fail when more than 10,000 blocks are\nwritten. That upper bound still exists. To write massive files, make sure\nthat the value of fs.s3a.multipart.size is set to a size which is large\nenough to upload the files in fewer than 10,000 blocks.\n\nChange-Id: Icec604e2a357ffd38d7ae7bc3f887ff55f2d721a\n",
      "commitDate": "20/05/20 5:42 AM",
      "commitName": "29b19cd59245c8809b697b3d7d7445813a685aad",
      "commitAuthor": "Mukund Thakur",
      "commitDateOld": "12/11/19 10:17 AM",
      "commitNameOld": "990063d2af0a37e9474949f33128805e34c3f016",
      "commitAuthorOld": "Steve Loughran",
      "daysBetweenCommits": 189.77,
      "commitsBetweenForRepo": 658,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,46 +1,57 @@\n     private void uploadBlockAsync(final S3ADataBlocks.DataBlock block)\n         throws IOException {\n       LOG.debug(\"Queueing upload of {} for upload {}\", block, uploadId);\n       Preconditions.checkNotNull(uploadId, \"Null uploadId\");\n+      maybeRethrowUploadFailure();\n       partsSubmitted++;\n       final int size \u003d block.dataSize();\n       bytesSubmitted +\u003d size;\n-      final S3ADataBlocks.BlockUploadData uploadData \u003d block.startUpload();\n       final int currentPartNumber \u003d partETagsFutures.size() + 1;\n-      final UploadPartRequest request \u003d\n-          writeOperationHelper.newUploadPartRequest(\n-              key,\n-              uploadId,\n-              currentPartNumber,\n-              size,\n-              uploadData.getUploadStream(),\n-              uploadData.getFile(),\n-              0L);\n-\n+      final UploadPartRequest request;\n+      final S3ADataBlocks.BlockUploadData uploadData;\n+      try {\n+        uploadData \u003d block.startUpload();\n+        request \u003d writeOperationHelper.newUploadPartRequest(\n+            key,\n+            uploadId,\n+            currentPartNumber,\n+            size,\n+            uploadData.getUploadStream(),\n+            uploadData.getFile(),\n+            0L);\n+      } catch (IOException e) {\n+        // failure to start the upload.\n+        noteUploadFailure(e);\n+        throw e;\n+      }\n       long transferQueueTime \u003d now();\n       BlockUploadProgress callback \u003d\n           new BlockUploadProgress(\n               block, progressListener, transferQueueTime);\n       request.setGeneralProgressListener(callback);\n       statistics.blockUploadQueued(block.dataSize());\n       ListenableFuture\u003cPartETag\u003e partETagFuture \u003d\n           executorService.submit(() -\u003e {\n             // this is the queued upload operation\n             // do the upload\n             try {\n               LOG.debug(\"Uploading part {} for id \u0027{}\u0027\",\n                   currentPartNumber, uploadId);\n               PartETag partETag \u003d writeOperationHelper.uploadPart(request)\n                   .getPartETag();\n               LOG.debug(\"Completed upload of {} to part {}\",\n                   block, partETag.getETag());\n               LOG.debug(\"Stream statistics of {}\", statistics);\n               partsUploaded++;\n               return partETag;\n+            } catch (IOException e) {\n+              // save immediately.\n+              noteUploadFailure(e);\n+              throw e;\n             } finally {\n               // close the stream and block\n               cleanupWithLogger(LOG, uploadData, block);\n             }\n           });\n       partETagsFutures.add(partETagFuture);\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void uploadBlockAsync(final S3ADataBlocks.DataBlock block)\n        throws IOException {\n      LOG.debug(\"Queueing upload of {} for upload {}\", block, uploadId);\n      Preconditions.checkNotNull(uploadId, \"Null uploadId\");\n      maybeRethrowUploadFailure();\n      partsSubmitted++;\n      final int size \u003d block.dataSize();\n      bytesSubmitted +\u003d size;\n      final int currentPartNumber \u003d partETagsFutures.size() + 1;\n      final UploadPartRequest request;\n      final S3ADataBlocks.BlockUploadData uploadData;\n      try {\n        uploadData \u003d block.startUpload();\n        request \u003d writeOperationHelper.newUploadPartRequest(\n            key,\n            uploadId,\n            currentPartNumber,\n            size,\n            uploadData.getUploadStream(),\n            uploadData.getFile(),\n            0L);\n      } catch (IOException e) {\n        // failure to start the upload.\n        noteUploadFailure(e);\n        throw e;\n      }\n      long transferQueueTime \u003d now();\n      BlockUploadProgress callback \u003d\n          new BlockUploadProgress(\n              block, progressListener, transferQueueTime);\n      request.setGeneralProgressListener(callback);\n      statistics.blockUploadQueued(block.dataSize());\n      ListenableFuture\u003cPartETag\u003e partETagFuture \u003d\n          executorService.submit(() -\u003e {\n            // this is the queued upload operation\n            // do the upload\n            try {\n              LOG.debug(\"Uploading part {} for id \u0027{}\u0027\",\n                  currentPartNumber, uploadId);\n              PartETag partETag \u003d writeOperationHelper.uploadPart(request)\n                  .getPartETag();\n              LOG.debug(\"Completed upload of {} to part {}\",\n                  block, partETag.getETag());\n              LOG.debug(\"Stream statistics of {}\", statistics);\n              partsUploaded++;\n              return partETag;\n            } catch (IOException e) {\n              // save immediately.\n              noteUploadFailure(e);\n              throw e;\n            } finally {\n              // close the stream and block\n              cleanupWithLogger(LOG, uploadData, block);\n            }\n          });\n      partETagsFutures.add(partETagFuture);\n    }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ABlockOutputStream.java",
      "extendedDetails": {}
    },
    "990063d2af0a37e9474949f33128805e34c3f016": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-16665. Filesystems to be closed if they failed during initialize().\n\nContributed by Steve Loughran.\n\nThis FileSystem instantiation so if an IOException or RuntimeException is\nraised in the invocation of FileSystem.initialize() then a best-effort\nattempt is made to close the FS instance; exceptions raised that there\nare swallowed.\n\nThe S3AFileSystem is also modified to do its own cleanup if an\nIOException is raised during its initialize() process, it being the\nFS we know has the \"potential\" to leak threads, especially in\nextension points (e.g AWS Authenticators) which spawn threads.\n\nChange-Id: Ib84073a606c9d53bf53cbfca4629876a03894f04\n",
      "commitDate": "12/11/19 10:17 AM",
      "commitName": "990063d2af0a37e9474949f33128805e34c3f016",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "25/09/19 4:16 AM",
      "commitNameOld": "e346e3638c595a512cd582739ff51fb64c3b4950",
      "commitAuthorOld": "Steve Loughran",
      "daysBetweenCommits": 48.29,
      "commitsBetweenForRepo": 247,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,46 +1,46 @@\n     private void uploadBlockAsync(final S3ADataBlocks.DataBlock block)\n         throws IOException {\n       LOG.debug(\"Queueing upload of {} for upload {}\", block, uploadId);\n       Preconditions.checkNotNull(uploadId, \"Null uploadId\");\n       partsSubmitted++;\n       final int size \u003d block.dataSize();\n       bytesSubmitted +\u003d size;\n       final S3ADataBlocks.BlockUploadData uploadData \u003d block.startUpload();\n       final int currentPartNumber \u003d partETagsFutures.size() + 1;\n       final UploadPartRequest request \u003d\n           writeOperationHelper.newUploadPartRequest(\n               key,\n               uploadId,\n               currentPartNumber,\n               size,\n               uploadData.getUploadStream(),\n               uploadData.getFile(),\n               0L);\n \n       long transferQueueTime \u003d now();\n       BlockUploadProgress callback \u003d\n           new BlockUploadProgress(\n               block, progressListener, transferQueueTime);\n       request.setGeneralProgressListener(callback);\n       statistics.blockUploadQueued(block.dataSize());\n       ListenableFuture\u003cPartETag\u003e partETagFuture \u003d\n           executorService.submit(() -\u003e {\n             // this is the queued upload operation\n             // do the upload\n             try {\n               LOG.debug(\"Uploading part {} for id \u0027{}\u0027\",\n                   currentPartNumber, uploadId);\n               PartETag partETag \u003d writeOperationHelper.uploadPart(request)\n                   .getPartETag();\n               LOG.debug(\"Completed upload of {} to part {}\",\n                   block, partETag.getETag());\n               LOG.debug(\"Stream statistics of {}\", statistics);\n               partsUploaded++;\n               return partETag;\n             } finally {\n               // close the stream and block\n-              closeAll(LOG, uploadData, block);\n+              cleanupWithLogger(LOG, uploadData, block);\n             }\n           });\n       partETagsFutures.add(partETagFuture);\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void uploadBlockAsync(final S3ADataBlocks.DataBlock block)\n        throws IOException {\n      LOG.debug(\"Queueing upload of {} for upload {}\", block, uploadId);\n      Preconditions.checkNotNull(uploadId, \"Null uploadId\");\n      partsSubmitted++;\n      final int size \u003d block.dataSize();\n      bytesSubmitted +\u003d size;\n      final S3ADataBlocks.BlockUploadData uploadData \u003d block.startUpload();\n      final int currentPartNumber \u003d partETagsFutures.size() + 1;\n      final UploadPartRequest request \u003d\n          writeOperationHelper.newUploadPartRequest(\n              key,\n              uploadId,\n              currentPartNumber,\n              size,\n              uploadData.getUploadStream(),\n              uploadData.getFile(),\n              0L);\n\n      long transferQueueTime \u003d now();\n      BlockUploadProgress callback \u003d\n          new BlockUploadProgress(\n              block, progressListener, transferQueueTime);\n      request.setGeneralProgressListener(callback);\n      statistics.blockUploadQueued(block.dataSize());\n      ListenableFuture\u003cPartETag\u003e partETagFuture \u003d\n          executorService.submit(() -\u003e {\n            // this is the queued upload operation\n            // do the upload\n            try {\n              LOG.debug(\"Uploading part {} for id \u0027{}\u0027\",\n                  currentPartNumber, uploadId);\n              PartETag partETag \u003d writeOperationHelper.uploadPart(request)\n                  .getPartETag();\n              LOG.debug(\"Completed upload of {} to part {}\",\n                  block, partETag.getETag());\n              LOG.debug(\"Stream statistics of {}\", statistics);\n              partsUploaded++;\n              return partETag;\n            } finally {\n              // close the stream and block\n              cleanupWithLogger(LOG, uploadData, block);\n            }\n          });\n      partETagsFutures.add(partETagFuture);\n    }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ABlockOutputStream.java",
      "extendedDetails": {}
    },
    "de8b6ca5ef8614de6d6277b7617e27c788b0555c": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-13786 Add S3A committer for zero-rename commits to S3 endpoints.\nContributed by Steve Loughran and Ryan Blue.\n",
      "commitDate": "22/11/17 7:28 AM",
      "commitName": "de8b6ca5ef8614de6d6277b7617e27c788b0555c",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "01/09/17 6:13 AM",
      "commitNameOld": "621b43e254afaff708cd6fc4698b29628f6abc33",
      "commitAuthorOld": "Steve Loughran",
      "daysBetweenCommits": 82.09,
      "commitsBetweenForRepo": 710,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,43 +1,46 @@\n     private void uploadBlockAsync(final S3ADataBlocks.DataBlock block)\n         throws IOException {\n-      LOG.debug(\"Queueing upload of {}\", block);\n+      LOG.debug(\"Queueing upload of {} for upload {}\", block, uploadId);\n+      Preconditions.checkNotNull(uploadId, \"Null uploadId\");\n+      partsSubmitted++;\n       final int size \u003d block.dataSize();\n+      bytesSubmitted +\u003d size;\n       final S3ADataBlocks.BlockUploadData uploadData \u003d block.startUpload();\n       final int currentPartNumber \u003d partETagsFutures.size() + 1;\n       final UploadPartRequest request \u003d\n           writeOperationHelper.newUploadPartRequest(\n+              key,\n               uploadId,\n               currentPartNumber,\n               size,\n               uploadData.getUploadStream(),\n-              uploadData.getFile());\n+              uploadData.getFile(),\n+              0L);\n \n       long transferQueueTime \u003d now();\n       BlockUploadProgress callback \u003d\n           new BlockUploadProgress(\n               block, progressListener, transferQueueTime);\n       request.setGeneralProgressListener(callback);\n       statistics.blockUploadQueued(block.dataSize());\n       ListenableFuture\u003cPartETag\u003e partETagFuture \u003d\n-          executorService.submit(new Callable\u003cPartETag\u003e() {\n-            @Override\n-            public PartETag call() throws Exception {\n-              // this is the queued upload operation\n-              LOG.debug(\"Uploading part {} for id \u0027{}\u0027\", currentPartNumber,\n-                  uploadId);\n-              // do the upload\n-              PartETag partETag;\n-              try {\n-                partETag \u003d fs.uploadPart(request).getPartETag();\n-                LOG.debug(\"Completed upload of {} to part {}\", block,\n-                    partETag.getETag());\n-                LOG.debug(\"Stream statistics of {}\", statistics);\n-              } finally {\n-                // close the stream and block\n-                closeAll(LOG, uploadData, block);\n-              }\n+          executorService.submit(() -\u003e {\n+            // this is the queued upload operation\n+            // do the upload\n+            try {\n+              LOG.debug(\"Uploading part {} for id \u0027{}\u0027\",\n+                  currentPartNumber, uploadId);\n+              PartETag partETag \u003d writeOperationHelper.uploadPart(request)\n+                  .getPartETag();\n+              LOG.debug(\"Completed upload of {} to part {}\",\n+                  block, partETag.getETag());\n+              LOG.debug(\"Stream statistics of {}\", statistics);\n+              partsUploaded++;\n               return partETag;\n+            } finally {\n+              // close the stream and block\n+              closeAll(LOG, uploadData, block);\n             }\n           });\n       partETagsFutures.add(partETagFuture);\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void uploadBlockAsync(final S3ADataBlocks.DataBlock block)\n        throws IOException {\n      LOG.debug(\"Queueing upload of {} for upload {}\", block, uploadId);\n      Preconditions.checkNotNull(uploadId, \"Null uploadId\");\n      partsSubmitted++;\n      final int size \u003d block.dataSize();\n      bytesSubmitted +\u003d size;\n      final S3ADataBlocks.BlockUploadData uploadData \u003d block.startUpload();\n      final int currentPartNumber \u003d partETagsFutures.size() + 1;\n      final UploadPartRequest request \u003d\n          writeOperationHelper.newUploadPartRequest(\n              key,\n              uploadId,\n              currentPartNumber,\n              size,\n              uploadData.getUploadStream(),\n              uploadData.getFile(),\n              0L);\n\n      long transferQueueTime \u003d now();\n      BlockUploadProgress callback \u003d\n          new BlockUploadProgress(\n              block, progressListener, transferQueueTime);\n      request.setGeneralProgressListener(callback);\n      statistics.blockUploadQueued(block.dataSize());\n      ListenableFuture\u003cPartETag\u003e partETagFuture \u003d\n          executorService.submit(() -\u003e {\n            // this is the queued upload operation\n            // do the upload\n            try {\n              LOG.debug(\"Uploading part {} for id \u0027{}\u0027\",\n                  currentPartNumber, uploadId);\n              PartETag partETag \u003d writeOperationHelper.uploadPart(request)\n                  .getPartETag();\n              LOG.debug(\"Completed upload of {} to part {}\",\n                  block, partETag.getETag());\n              LOG.debug(\"Stream statistics of {}\", statistics);\n              partsUploaded++;\n              return partETag;\n            } finally {\n              // close the stream and block\n              closeAll(LOG, uploadData, block);\n            }\n          });\n      partETagsFutures.add(partETagFuture);\n    }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ABlockOutputStream.java",
      "extendedDetails": {}
    },
    "dab00da19f25619ccc71c7f803a235b21766bf1e": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-14028. S3A BlockOutputStreams doesn\u0027t delete temporary files in multipart uploads or handle part upload failures.\nContributed by Steve Loughran.\n\n(cherry picked from commit 29fe5af017b945d8750c074ca39031b5b777eddd)\n",
      "commitDate": "25/02/17 7:35 AM",
      "commitName": "dab00da19f25619ccc71c7f803a235b21766bf1e",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "11/02/17 1:59 PM",
      "commitNameOld": "839b690ed5edc2ac4984640d58c005bb63cd8a07",
      "commitAuthorOld": "Lei Xu",
      "daysBetweenCommits": 13.73,
      "commitsBetweenForRepo": 86,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,37 +1,43 @@\n     private void uploadBlockAsync(final S3ADataBlocks.DataBlock block)\n         throws IOException {\n       LOG.debug(\"Queueing upload of {}\", block);\n       final int size \u003d block.dataSize();\n-      final InputStream uploadStream \u003d block.startUpload();\n+      final S3ADataBlocks.BlockUploadData uploadData \u003d block.startUpload();\n       final int currentPartNumber \u003d partETagsFutures.size() + 1;\n       final UploadPartRequest request \u003d\n           writeOperationHelper.newUploadPartRequest(\n               uploadId,\n-              uploadStream,\n               currentPartNumber,\n-              size);\n+              size,\n+              uploadData.getUploadStream(),\n+              uploadData.getFile());\n+\n       long transferQueueTime \u003d now();\n       BlockUploadProgress callback \u003d\n           new BlockUploadProgress(\n               block, progressListener, transferQueueTime);\n       request.setGeneralProgressListener(callback);\n       statistics.blockUploadQueued(block.dataSize());\n       ListenableFuture\u003cPartETag\u003e partETagFuture \u003d\n           executorService.submit(new Callable\u003cPartETag\u003e() {\n             @Override\n             public PartETag call() throws Exception {\n               // this is the queued upload operation\n               LOG.debug(\"Uploading part {} for id \u0027{}\u0027\", currentPartNumber,\n                   uploadId);\n               // do the upload\n-              PartETag partETag \u003d fs.uploadPart(request).getPartETag();\n-              LOG.debug(\"Completed upload of {}\", block);\n-              LOG.debug(\"Stream statistics of {}\", statistics);\n-\n-              // close the block\n-              block.close();\n+              PartETag partETag;\n+              try {\n+                partETag \u003d fs.uploadPart(request).getPartETag();\n+                LOG.debug(\"Completed upload of {} to part {}\", block,\n+                    partETag.getETag());\n+                LOG.debug(\"Stream statistics of {}\", statistics);\n+              } finally {\n+                // close the stream and block\n+                closeAll(LOG, uploadData, block);\n+              }\n               return partETag;\n             }\n           });\n       partETagsFutures.add(partETagFuture);\n     }\n\\ No newline at end of file\n",
      "actualSource": "    private void uploadBlockAsync(final S3ADataBlocks.DataBlock block)\n        throws IOException {\n      LOG.debug(\"Queueing upload of {}\", block);\n      final int size \u003d block.dataSize();\n      final S3ADataBlocks.BlockUploadData uploadData \u003d block.startUpload();\n      final int currentPartNumber \u003d partETagsFutures.size() + 1;\n      final UploadPartRequest request \u003d\n          writeOperationHelper.newUploadPartRequest(\n              uploadId,\n              currentPartNumber,\n              size,\n              uploadData.getUploadStream(),\n              uploadData.getFile());\n\n      long transferQueueTime \u003d now();\n      BlockUploadProgress callback \u003d\n          new BlockUploadProgress(\n              block, progressListener, transferQueueTime);\n      request.setGeneralProgressListener(callback);\n      statistics.blockUploadQueued(block.dataSize());\n      ListenableFuture\u003cPartETag\u003e partETagFuture \u003d\n          executorService.submit(new Callable\u003cPartETag\u003e() {\n            @Override\n            public PartETag call() throws Exception {\n              // this is the queued upload operation\n              LOG.debug(\"Uploading part {} for id \u0027{}\u0027\", currentPartNumber,\n                  uploadId);\n              // do the upload\n              PartETag partETag;\n              try {\n                partETag \u003d fs.uploadPart(request).getPartETag();\n                LOG.debug(\"Completed upload of {} to part {}\", block,\n                    partETag.getETag());\n                LOG.debug(\"Stream statistics of {}\", statistics);\n              } finally {\n                // close the stream and block\n                closeAll(LOG, uploadData, block);\n              }\n              return partETag;\n            }\n          });\n      partETagsFutures.add(partETagFuture);\n    }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ABlockOutputStream.java",
      "extendedDetails": {}
    },
    "6c348c56918973fd988b110e79231324a8befe12": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-13560. S3ABlockOutputStream to support huge (many GB) file writes. Contributed by Steve Loughran\n",
      "commitDate": "18/10/16 1:16 PM",
      "commitName": "6c348c56918973fd988b110e79231324a8befe12",
      "commitAuthor": "Steve Loughran",
      "diff": "@@ -0,0 +1,37 @@\n+    private void uploadBlockAsync(final S3ADataBlocks.DataBlock block)\n+        throws IOException {\n+      LOG.debug(\"Queueing upload of {}\", block);\n+      final int size \u003d block.dataSize();\n+      final InputStream uploadStream \u003d block.startUpload();\n+      final int currentPartNumber \u003d partETagsFutures.size() + 1;\n+      final UploadPartRequest request \u003d\n+          writeOperationHelper.newUploadPartRequest(\n+              uploadId,\n+              uploadStream,\n+              currentPartNumber,\n+              size);\n+      long transferQueueTime \u003d now();\n+      BlockUploadProgress callback \u003d\n+          new BlockUploadProgress(\n+              block, progressListener, transferQueueTime);\n+      request.setGeneralProgressListener(callback);\n+      statistics.blockUploadQueued(block.dataSize());\n+      ListenableFuture\u003cPartETag\u003e partETagFuture \u003d\n+          executorService.submit(new Callable\u003cPartETag\u003e() {\n+            @Override\n+            public PartETag call() throws Exception {\n+              // this is the queued upload operation\n+              LOG.debug(\"Uploading part {} for id \u0027{}\u0027\", currentPartNumber,\n+                  uploadId);\n+              // do the upload\n+              PartETag partETag \u003d fs.uploadPart(request).getPartETag();\n+              LOG.debug(\"Completed upload of {}\", block);\n+              LOG.debug(\"Stream statistics of {}\", statistics);\n+\n+              // close the block\n+              block.close();\n+              return partETag;\n+            }\n+          });\n+      partETagsFutures.add(partETagFuture);\n+    }\n\\ No newline at end of file\n",
      "actualSource": "    private void uploadBlockAsync(final S3ADataBlocks.DataBlock block)\n        throws IOException {\n      LOG.debug(\"Queueing upload of {}\", block);\n      final int size \u003d block.dataSize();\n      final InputStream uploadStream \u003d block.startUpload();\n      final int currentPartNumber \u003d partETagsFutures.size() + 1;\n      final UploadPartRequest request \u003d\n          writeOperationHelper.newUploadPartRequest(\n              uploadId,\n              uploadStream,\n              currentPartNumber,\n              size);\n      long transferQueueTime \u003d now();\n      BlockUploadProgress callback \u003d\n          new BlockUploadProgress(\n              block, progressListener, transferQueueTime);\n      request.setGeneralProgressListener(callback);\n      statistics.blockUploadQueued(block.dataSize());\n      ListenableFuture\u003cPartETag\u003e partETagFuture \u003d\n          executorService.submit(new Callable\u003cPartETag\u003e() {\n            @Override\n            public PartETag call() throws Exception {\n              // this is the queued upload operation\n              LOG.debug(\"Uploading part {} for id \u0027{}\u0027\", currentPartNumber,\n                  uploadId);\n              // do the upload\n              PartETag partETag \u003d fs.uploadPart(request).getPartETag();\n              LOG.debug(\"Completed upload of {}\", block);\n              LOG.debug(\"Stream statistics of {}\", statistics);\n\n              // close the block\n              block.close();\n              return partETag;\n            }\n          });\n      partETagsFutures.add(partETagFuture);\n    }",
      "path": "hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ABlockOutputStream.java"
    }
  }
}