{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "BlockReaderLocalLegacy.java",
  "functionName": "newBlockReader",
  "functionId": "newBlockReader___conf-DfsClientConf__userGroupInformation-UserGroupInformation__configuration-Configuration__file-String__blk-ExtendedBlock__token-Token__BlockTokenIdentifier____node-DatanodeInfo__startOffset-long__length-long__storageType-StorageType",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderLocalLegacy.java",
  "functionStartLine": 189,
  "functionEndLine": 262,
  "numCommitsSeen": 77,
  "timeTaken": 7603,
  "changeHistory": [
    "5d748bd056a32f2c6922514cd0c5b31d866a9919",
    "f308561f1d885491b88db73ac63003202056d661",
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
    "39285e6a1978ea5e53bdc1b0aef62421382124a8",
    "6ee0539ede78b640f01c5eac18ded161182a7835",
    "d5a9a3daa0224249221ffa7b8bd5751ab2feca56",
    "892ade689f9bcce76daae8f66fc00a49bee8548e",
    "e2c9b288b223b9fd82dc12018936e13128413492",
    "2cc9514ad643ae49d30524743420ee9744e571bd",
    "463aec11718e47d4aabb86a7a539cb973460aae6",
    "cfd8647d0f20c08761f908be1f5b718c1c372498",
    "beb0d25d2a7ba5004c6aabd105546ba9a9fec9be",
    "c68b1d1b31e304c27e419e810ded0fc97e435ea6",
    "bbb24fbf5d220fbe137d43651ba3802a9806b1a3",
    "694a6721316aea14c1244447974231abc8dff0cb"
  ],
  "changeHistoryShort": {
    "5d748bd056a32f2c6922514cd0c5b31d866a9919": "Ymultichange(Yparameterchange,Ybodychange)",
    "f308561f1d885491b88db73ac63003202056d661": "Yfilerename",
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93": "Ybodychange",
    "39285e6a1978ea5e53bdc1b0aef62421382124a8": "Ybodychange",
    "6ee0539ede78b640f01c5eac18ded161182a7835": "Ybodychange",
    "d5a9a3daa0224249221ffa7b8bd5751ab2feca56": "Ybodychange",
    "892ade689f9bcce76daae8f66fc00a49bee8548e": "Ymultichange(Yparameterchange,Ybodychange)",
    "e2c9b288b223b9fd82dc12018936e13128413492": "Ymultichange(Yfilerename,Ybodychange)",
    "2cc9514ad643ae49d30524743420ee9744e571bd": "Ymultichange(Yparameterchange,Ybodychange)",
    "463aec11718e47d4aabb86a7a539cb973460aae6": "Ymultichange(Yparameterchange,Ybodychange)",
    "cfd8647d0f20c08761f908be1f5b718c1c372498": "Ybodychange",
    "beb0d25d2a7ba5004c6aabd105546ba9a9fec9be": "Ymultichange(Yparameterchange,Ybodychange)",
    "c68b1d1b31e304c27e419e810ded0fc97e435ea6": "Ymultichange(Yparameterchange,Ybodychange)",
    "bbb24fbf5d220fbe137d43651ba3802a9806b1a3": "Ymultichange(Yparameterchange,Ybodychange)",
    "694a6721316aea14c1244447974231abc8dff0cb": "Yintroduced"
  },
  "changeHistoryDetails": {
    "5d748bd056a32f2c6922514cd0c5b31d866a9919": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-13702. Remove HTrace hooks from DFSClient to reduce CPU usage. Contributed by Todd Lipcon.\n",
      "commitDate": "02/07/18 3:11 AM",
      "commitName": "5d748bd056a32f2c6922514cd0c5b31d866a9919",
      "commitAuthor": "Andrew Wang",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-13702. Remove HTrace hooks from DFSClient to reduce CPU usage. Contributed by Todd Lipcon.\n",
          "commitDate": "02/07/18 3:11 AM",
          "commitName": "5d748bd056a32f2c6922514cd0c5b31d866a9919",
          "commitAuthor": "Andrew Wang",
          "commitDateOld": "25/04/16 12:01 PM",
          "commitNameOld": "f308561f1d885491b88db73ac63003202056d661",
          "commitAuthorOld": "Tsz-Wo Nicholas Sze",
          "daysBetweenCommits": 797.63,
          "commitsBetweenForRepo": 6015,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,75 +1,74 @@\n   static BlockReaderLocalLegacy newBlockReader(DfsClientConf conf,\n       UserGroupInformation userGroupInformation,\n       Configuration configuration, String file, ExtendedBlock blk,\n       Token\u003cBlockTokenIdentifier\u003e token, DatanodeInfo node,\n-      long startOffset, long length, StorageType storageType,\n-      Tracer tracer) throws IOException {\n+      long startOffset, long length, StorageType storageType)\n+      throws IOException {\n     final ShortCircuitConf scConf \u003d conf.getShortCircuitConf();\n     LocalDatanodeInfo localDatanodeInfo \u003d getLocalDatanodeInfo(node\n         .getIpcPort());\n     // check the cache first\n     BlockLocalPathInfo pathinfo \u003d localDatanodeInfo.getBlockLocalPathInfo(blk);\n     if (pathinfo \u003d\u003d null) {\n       if (userGroupInformation \u003d\u003d null) {\n         userGroupInformation \u003d UserGroupInformation.getCurrentUser();\n       }\n       pathinfo \u003d getBlockPathInfo(userGroupInformation, blk, node,\n           configuration, conf.getSocketTimeout(), token,\n           conf.isConnectToDnViaHostname(), storageType);\n     }\n \n     // check to see if the file exists. It may so happen that the\n     // HDFS file has been deleted and this block-lookup is occurring\n     // on behalf of a new HDFS file. This time, the block file could\n     // be residing in a different portion of the fs.data.dir directory.\n     // In this case, we remove this entry from the cache. The next\n     // call to this method will re-populate the cache.\n     FileInputStream dataIn \u003d null;\n     FileInputStream checksumIn \u003d null;\n     BlockReaderLocalLegacy localBlockReader \u003d null;\n     final boolean skipChecksumCheck \u003d scConf.isSkipShortCircuitChecksums()\n         || storageType.isTransient();\n     try {\n       // get a local file system\n       File blkfile \u003d new File(pathinfo.getBlockPath());\n       dataIn \u003d new FileInputStream(blkfile);\n \n       LOG.debug(\"New BlockReaderLocalLegacy for file {} of size {} startOffset \"\n               + \"{} length {} short circuit checksum {}\",\n           blkfile, blkfile.length(), startOffset, length, !skipChecksumCheck);\n \n       if (!skipChecksumCheck) {\n         // get the metadata file\n         File metafile \u003d new File(pathinfo.getMetaPath());\n         checksumIn \u003d new FileInputStream(metafile);\n \n         final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(\n             new DataInputStream(checksumIn), blk);\n         long firstChunkOffset \u003d startOffset\n             - (startOffset % checksum.getBytesPerChecksum());\n         localBlockReader \u003d new BlockReaderLocalLegacy(scConf, file, blk,\n-            startOffset, checksum, true, dataIn, firstChunkOffset, checksumIn,\n-            tracer);\n+            startOffset, checksum, true, dataIn, firstChunkOffset, checksumIn);\n       } else {\n         localBlockReader \u003d new BlockReaderLocalLegacy(scConf, file, blk,\n-            startOffset, dataIn, tracer);\n+            startOffset, dataIn);\n       }\n     } catch (IOException e) {\n       // remove from cache\n       localDatanodeInfo.removeBlockLocalPathInfo(blk);\n       LOG.warn(\"BlockReaderLocalLegacy: Removing \" + blk\n           + \" from cache because local file \" + pathinfo.getBlockPath()\n           + \" could not be opened.\");\n       throw e;\n     } finally {\n       if (localBlockReader \u003d\u003d null) {\n         if (dataIn !\u003d null) {\n           dataIn.close();\n         }\n         if (checksumIn !\u003d null) {\n           checksumIn.close();\n         }\n       }\n     }\n     return localBlockReader;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static BlockReaderLocalLegacy newBlockReader(DfsClientConf conf,\n      UserGroupInformation userGroupInformation,\n      Configuration configuration, String file, ExtendedBlock blk,\n      Token\u003cBlockTokenIdentifier\u003e token, DatanodeInfo node,\n      long startOffset, long length, StorageType storageType)\n      throws IOException {\n    final ShortCircuitConf scConf \u003d conf.getShortCircuitConf();\n    LocalDatanodeInfo localDatanodeInfo \u003d getLocalDatanodeInfo(node\n        .getIpcPort());\n    // check the cache first\n    BlockLocalPathInfo pathinfo \u003d localDatanodeInfo.getBlockLocalPathInfo(blk);\n    if (pathinfo \u003d\u003d null) {\n      if (userGroupInformation \u003d\u003d null) {\n        userGroupInformation \u003d UserGroupInformation.getCurrentUser();\n      }\n      pathinfo \u003d getBlockPathInfo(userGroupInformation, blk, node,\n          configuration, conf.getSocketTimeout(), token,\n          conf.isConnectToDnViaHostname(), storageType);\n    }\n\n    // check to see if the file exists. It may so happen that the\n    // HDFS file has been deleted and this block-lookup is occurring\n    // on behalf of a new HDFS file. This time, the block file could\n    // be residing in a different portion of the fs.data.dir directory.\n    // In this case, we remove this entry from the cache. The next\n    // call to this method will re-populate the cache.\n    FileInputStream dataIn \u003d null;\n    FileInputStream checksumIn \u003d null;\n    BlockReaderLocalLegacy localBlockReader \u003d null;\n    final boolean skipChecksumCheck \u003d scConf.isSkipShortCircuitChecksums()\n        || storageType.isTransient();\n    try {\n      // get a local file system\n      File blkfile \u003d new File(pathinfo.getBlockPath());\n      dataIn \u003d new FileInputStream(blkfile);\n\n      LOG.debug(\"New BlockReaderLocalLegacy for file {} of size {} startOffset \"\n              + \"{} length {} short circuit checksum {}\",\n          blkfile, blkfile.length(), startOffset, length, !skipChecksumCheck);\n\n      if (!skipChecksumCheck) {\n        // get the metadata file\n        File metafile \u003d new File(pathinfo.getMetaPath());\n        checksumIn \u003d new FileInputStream(metafile);\n\n        final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(\n            new DataInputStream(checksumIn), blk);\n        long firstChunkOffset \u003d startOffset\n            - (startOffset % checksum.getBytesPerChecksum());\n        localBlockReader \u003d new BlockReaderLocalLegacy(scConf, file, blk,\n            startOffset, checksum, true, dataIn, firstChunkOffset, checksumIn);\n      } else {\n        localBlockReader \u003d new BlockReaderLocalLegacy(scConf, file, blk,\n            startOffset, dataIn);\n      }\n    } catch (IOException e) {\n      // remove from cache\n      localDatanodeInfo.removeBlockLocalPathInfo(blk);\n      LOG.warn(\"BlockReaderLocalLegacy: Removing \" + blk\n          + \" from cache because local file \" + pathinfo.getBlockPath()\n          + \" could not be opened.\");\n      throw e;\n    } finally {\n      if (localBlockReader \u003d\u003d null) {\n        if (dataIn !\u003d null) {\n          dataIn.close();\n        }\n        if (checksumIn !\u003d null) {\n          checksumIn.close();\n        }\n      }\n    }\n    return localBlockReader;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderLocalLegacy.java",
          "extendedDetails": {
            "oldValue": "[conf-DfsClientConf, userGroupInformation-UserGroupInformation, configuration-Configuration, file-String, blk-ExtendedBlock, token-Token\u003cBlockTokenIdentifier\u003e, node-DatanodeInfo, startOffset-long, length-long, storageType-StorageType, tracer-Tracer]",
            "newValue": "[conf-DfsClientConf, userGroupInformation-UserGroupInformation, configuration-Configuration, file-String, blk-ExtendedBlock, token-Token\u003cBlockTokenIdentifier\u003e, node-DatanodeInfo, startOffset-long, length-long, storageType-StorageType]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-13702. Remove HTrace hooks from DFSClient to reduce CPU usage. Contributed by Todd Lipcon.\n",
          "commitDate": "02/07/18 3:11 AM",
          "commitName": "5d748bd056a32f2c6922514cd0c5b31d866a9919",
          "commitAuthor": "Andrew Wang",
          "commitDateOld": "25/04/16 12:01 PM",
          "commitNameOld": "f308561f1d885491b88db73ac63003202056d661",
          "commitAuthorOld": "Tsz-Wo Nicholas Sze",
          "daysBetweenCommits": 797.63,
          "commitsBetweenForRepo": 6015,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,75 +1,74 @@\n   static BlockReaderLocalLegacy newBlockReader(DfsClientConf conf,\n       UserGroupInformation userGroupInformation,\n       Configuration configuration, String file, ExtendedBlock blk,\n       Token\u003cBlockTokenIdentifier\u003e token, DatanodeInfo node,\n-      long startOffset, long length, StorageType storageType,\n-      Tracer tracer) throws IOException {\n+      long startOffset, long length, StorageType storageType)\n+      throws IOException {\n     final ShortCircuitConf scConf \u003d conf.getShortCircuitConf();\n     LocalDatanodeInfo localDatanodeInfo \u003d getLocalDatanodeInfo(node\n         .getIpcPort());\n     // check the cache first\n     BlockLocalPathInfo pathinfo \u003d localDatanodeInfo.getBlockLocalPathInfo(blk);\n     if (pathinfo \u003d\u003d null) {\n       if (userGroupInformation \u003d\u003d null) {\n         userGroupInformation \u003d UserGroupInformation.getCurrentUser();\n       }\n       pathinfo \u003d getBlockPathInfo(userGroupInformation, blk, node,\n           configuration, conf.getSocketTimeout(), token,\n           conf.isConnectToDnViaHostname(), storageType);\n     }\n \n     // check to see if the file exists. It may so happen that the\n     // HDFS file has been deleted and this block-lookup is occurring\n     // on behalf of a new HDFS file. This time, the block file could\n     // be residing in a different portion of the fs.data.dir directory.\n     // In this case, we remove this entry from the cache. The next\n     // call to this method will re-populate the cache.\n     FileInputStream dataIn \u003d null;\n     FileInputStream checksumIn \u003d null;\n     BlockReaderLocalLegacy localBlockReader \u003d null;\n     final boolean skipChecksumCheck \u003d scConf.isSkipShortCircuitChecksums()\n         || storageType.isTransient();\n     try {\n       // get a local file system\n       File blkfile \u003d new File(pathinfo.getBlockPath());\n       dataIn \u003d new FileInputStream(blkfile);\n \n       LOG.debug(\"New BlockReaderLocalLegacy for file {} of size {} startOffset \"\n               + \"{} length {} short circuit checksum {}\",\n           blkfile, blkfile.length(), startOffset, length, !skipChecksumCheck);\n \n       if (!skipChecksumCheck) {\n         // get the metadata file\n         File metafile \u003d new File(pathinfo.getMetaPath());\n         checksumIn \u003d new FileInputStream(metafile);\n \n         final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(\n             new DataInputStream(checksumIn), blk);\n         long firstChunkOffset \u003d startOffset\n             - (startOffset % checksum.getBytesPerChecksum());\n         localBlockReader \u003d new BlockReaderLocalLegacy(scConf, file, blk,\n-            startOffset, checksum, true, dataIn, firstChunkOffset, checksumIn,\n-            tracer);\n+            startOffset, checksum, true, dataIn, firstChunkOffset, checksumIn);\n       } else {\n         localBlockReader \u003d new BlockReaderLocalLegacy(scConf, file, blk,\n-            startOffset, dataIn, tracer);\n+            startOffset, dataIn);\n       }\n     } catch (IOException e) {\n       // remove from cache\n       localDatanodeInfo.removeBlockLocalPathInfo(blk);\n       LOG.warn(\"BlockReaderLocalLegacy: Removing \" + blk\n           + \" from cache because local file \" + pathinfo.getBlockPath()\n           + \" could not be opened.\");\n       throw e;\n     } finally {\n       if (localBlockReader \u003d\u003d null) {\n         if (dataIn !\u003d null) {\n           dataIn.close();\n         }\n         if (checksumIn !\u003d null) {\n           checksumIn.close();\n         }\n       }\n     }\n     return localBlockReader;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static BlockReaderLocalLegacy newBlockReader(DfsClientConf conf,\n      UserGroupInformation userGroupInformation,\n      Configuration configuration, String file, ExtendedBlock blk,\n      Token\u003cBlockTokenIdentifier\u003e token, DatanodeInfo node,\n      long startOffset, long length, StorageType storageType)\n      throws IOException {\n    final ShortCircuitConf scConf \u003d conf.getShortCircuitConf();\n    LocalDatanodeInfo localDatanodeInfo \u003d getLocalDatanodeInfo(node\n        .getIpcPort());\n    // check the cache first\n    BlockLocalPathInfo pathinfo \u003d localDatanodeInfo.getBlockLocalPathInfo(blk);\n    if (pathinfo \u003d\u003d null) {\n      if (userGroupInformation \u003d\u003d null) {\n        userGroupInformation \u003d UserGroupInformation.getCurrentUser();\n      }\n      pathinfo \u003d getBlockPathInfo(userGroupInformation, blk, node,\n          configuration, conf.getSocketTimeout(), token,\n          conf.isConnectToDnViaHostname(), storageType);\n    }\n\n    // check to see if the file exists. It may so happen that the\n    // HDFS file has been deleted and this block-lookup is occurring\n    // on behalf of a new HDFS file. This time, the block file could\n    // be residing in a different portion of the fs.data.dir directory.\n    // In this case, we remove this entry from the cache. The next\n    // call to this method will re-populate the cache.\n    FileInputStream dataIn \u003d null;\n    FileInputStream checksumIn \u003d null;\n    BlockReaderLocalLegacy localBlockReader \u003d null;\n    final boolean skipChecksumCheck \u003d scConf.isSkipShortCircuitChecksums()\n        || storageType.isTransient();\n    try {\n      // get a local file system\n      File blkfile \u003d new File(pathinfo.getBlockPath());\n      dataIn \u003d new FileInputStream(blkfile);\n\n      LOG.debug(\"New BlockReaderLocalLegacy for file {} of size {} startOffset \"\n              + \"{} length {} short circuit checksum {}\",\n          blkfile, blkfile.length(), startOffset, length, !skipChecksumCheck);\n\n      if (!skipChecksumCheck) {\n        // get the metadata file\n        File metafile \u003d new File(pathinfo.getMetaPath());\n        checksumIn \u003d new FileInputStream(metafile);\n\n        final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(\n            new DataInputStream(checksumIn), blk);\n        long firstChunkOffset \u003d startOffset\n            - (startOffset % checksum.getBytesPerChecksum());\n        localBlockReader \u003d new BlockReaderLocalLegacy(scConf, file, blk,\n            startOffset, checksum, true, dataIn, firstChunkOffset, checksumIn);\n      } else {\n        localBlockReader \u003d new BlockReaderLocalLegacy(scConf, file, blk,\n            startOffset, dataIn);\n      }\n    } catch (IOException e) {\n      // remove from cache\n      localDatanodeInfo.removeBlockLocalPathInfo(blk);\n      LOG.warn(\"BlockReaderLocalLegacy: Removing \" + blk\n          + \" from cache because local file \" + pathinfo.getBlockPath()\n          + \" could not be opened.\");\n      throw e;\n    } finally {\n      if (localBlockReader \u003d\u003d null) {\n        if (dataIn !\u003d null) {\n          dataIn.close();\n        }\n        if (checksumIn !\u003d null) {\n          checksumIn.close();\n        }\n      }\n    }\n    return localBlockReader;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderLocalLegacy.java",
          "extendedDetails": {}
        }
      ]
    },
    "f308561f1d885491b88db73ac63003202056d661": {
      "type": "Yfilerename",
      "commitMessage": "HDFS-8057 Move BlockReader implementation to the client implementation package.  Contributed by Takanobu Asanuma\n",
      "commitDate": "25/04/16 12:01 PM",
      "commitName": "f308561f1d885491b88db73ac63003202056d661",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "commitDateOld": "25/04/16 9:38 AM",
      "commitNameOld": "10f0f7851a3255caab775777e8fb6c2781d97062",
      "commitAuthorOld": "Kihwal Lee",
      "daysBetweenCommits": 0.1,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "",
      "actualSource": "  static BlockReaderLocalLegacy newBlockReader(DfsClientConf conf,\n      UserGroupInformation userGroupInformation,\n      Configuration configuration, String file, ExtendedBlock blk,\n      Token\u003cBlockTokenIdentifier\u003e token, DatanodeInfo node,\n      long startOffset, long length, StorageType storageType,\n      Tracer tracer) throws IOException {\n    final ShortCircuitConf scConf \u003d conf.getShortCircuitConf();\n    LocalDatanodeInfo localDatanodeInfo \u003d getLocalDatanodeInfo(node\n        .getIpcPort());\n    // check the cache first\n    BlockLocalPathInfo pathinfo \u003d localDatanodeInfo.getBlockLocalPathInfo(blk);\n    if (pathinfo \u003d\u003d null) {\n      if (userGroupInformation \u003d\u003d null) {\n        userGroupInformation \u003d UserGroupInformation.getCurrentUser();\n      }\n      pathinfo \u003d getBlockPathInfo(userGroupInformation, blk, node,\n          configuration, conf.getSocketTimeout(), token,\n          conf.isConnectToDnViaHostname(), storageType);\n    }\n\n    // check to see if the file exists. It may so happen that the\n    // HDFS file has been deleted and this block-lookup is occurring\n    // on behalf of a new HDFS file. This time, the block file could\n    // be residing in a different portion of the fs.data.dir directory.\n    // In this case, we remove this entry from the cache. The next\n    // call to this method will re-populate the cache.\n    FileInputStream dataIn \u003d null;\n    FileInputStream checksumIn \u003d null;\n    BlockReaderLocalLegacy localBlockReader \u003d null;\n    final boolean skipChecksumCheck \u003d scConf.isSkipShortCircuitChecksums()\n        || storageType.isTransient();\n    try {\n      // get a local file system\n      File blkfile \u003d new File(pathinfo.getBlockPath());\n      dataIn \u003d new FileInputStream(blkfile);\n\n      LOG.debug(\"New BlockReaderLocalLegacy for file {} of size {} startOffset \"\n              + \"{} length {} short circuit checksum {}\",\n          blkfile, blkfile.length(), startOffset, length, !skipChecksumCheck);\n\n      if (!skipChecksumCheck) {\n        // get the metadata file\n        File metafile \u003d new File(pathinfo.getMetaPath());\n        checksumIn \u003d new FileInputStream(metafile);\n\n        final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(\n            new DataInputStream(checksumIn), blk);\n        long firstChunkOffset \u003d startOffset\n            - (startOffset % checksum.getBytesPerChecksum());\n        localBlockReader \u003d new BlockReaderLocalLegacy(scConf, file, blk,\n            startOffset, checksum, true, dataIn, firstChunkOffset, checksumIn,\n            tracer);\n      } else {\n        localBlockReader \u003d new BlockReaderLocalLegacy(scConf, file, blk,\n            startOffset, dataIn, tracer);\n      }\n    } catch (IOException e) {\n      // remove from cache\n      localDatanodeInfo.removeBlockLocalPathInfo(blk);\n      LOG.warn(\"BlockReaderLocalLegacy: Removing \" + blk\n          + \" from cache because local file \" + pathinfo.getBlockPath()\n          + \" could not be opened.\");\n      throw e;\n    } finally {\n      if (localBlockReader \u003d\u003d null) {\n        if (dataIn !\u003d null) {\n          dataIn.close();\n        }\n        if (checksumIn !\u003d null) {\n          checksumIn.close();\n        }\n      }\n    }\n    return localBlockReader;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderLocalLegacy.java",
      "extendedDetails": {
        "oldPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocalLegacy.java",
        "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderLocalLegacy.java"
      }
    },
    "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8979. Clean up checkstyle warnings in hadoop-hdfs-client module. Contributed by Mingliang Liu.\n",
      "commitDate": "03/10/15 11:38 AM",
      "commitName": "7136e8c5582dc4061b566cb9f11a0d6a6d08bb93",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "30/09/15 8:39 AM",
      "commitNameOld": "6c17d315287020368689fa078a40a1eaedf89d5b",
      "commitAuthorOld": "",
      "daysBetweenCommits": 3.12,
      "commitsBetweenForRepo": 16,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,75 +1,75 @@\n   static BlockReaderLocalLegacy newBlockReader(DfsClientConf conf,\n       UserGroupInformation userGroupInformation,\n       Configuration configuration, String file, ExtendedBlock blk,\n-      Token\u003cBlockTokenIdentifier\u003e token, DatanodeInfo node, \n+      Token\u003cBlockTokenIdentifier\u003e token, DatanodeInfo node,\n       long startOffset, long length, StorageType storageType,\n       Tracer tracer) throws IOException {\n     final ShortCircuitConf scConf \u003d conf.getShortCircuitConf();\n     LocalDatanodeInfo localDatanodeInfo \u003d getLocalDatanodeInfo(node\n         .getIpcPort());\n     // check the cache first\n     BlockLocalPathInfo pathinfo \u003d localDatanodeInfo.getBlockLocalPathInfo(blk);\n     if (pathinfo \u003d\u003d null) {\n       if (userGroupInformation \u003d\u003d null) {\n         userGroupInformation \u003d UserGroupInformation.getCurrentUser();\n       }\n       pathinfo \u003d getBlockPathInfo(userGroupInformation, blk, node,\n           configuration, conf.getSocketTimeout(), token,\n           conf.isConnectToDnViaHostname(), storageType);\n     }\n \n     // check to see if the file exists. It may so happen that the\n     // HDFS file has been deleted and this block-lookup is occurring\n     // on behalf of a new HDFS file. This time, the block file could\n     // be residing in a different portion of the fs.data.dir directory.\n     // In this case, we remove this entry from the cache. The next\n     // call to this method will re-populate the cache.\n     FileInputStream dataIn \u003d null;\n     FileInputStream checksumIn \u003d null;\n     BlockReaderLocalLegacy localBlockReader \u003d null;\n     final boolean skipChecksumCheck \u003d scConf.isSkipShortCircuitChecksums()\n         || storageType.isTransient();\n     try {\n       // get a local file system\n       File blkfile \u003d new File(pathinfo.getBlockPath());\n       dataIn \u003d new FileInputStream(blkfile);\n \n       LOG.debug(\"New BlockReaderLocalLegacy for file {} of size {} startOffset \"\n               + \"{} length {} short circuit checksum {}\",\n           blkfile, blkfile.length(), startOffset, length, !skipChecksumCheck);\n \n       if (!skipChecksumCheck) {\n         // get the metadata file\n         File metafile \u003d new File(pathinfo.getMetaPath());\n         checksumIn \u003d new FileInputStream(metafile);\n \n         final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(\n             new DataInputStream(checksumIn), blk);\n         long firstChunkOffset \u003d startOffset\n             - (startOffset % checksum.getBytesPerChecksum());\n-        localBlockReader \u003d new BlockReaderLocalLegacy(scConf, file, blk, token,\n-            startOffset, length, pathinfo, checksum, true, dataIn,\n-            firstChunkOffset, checksumIn, tracer);\n+        localBlockReader \u003d new BlockReaderLocalLegacy(scConf, file, blk,\n+            startOffset, checksum, true, dataIn, firstChunkOffset, checksumIn,\n+            tracer);\n       } else {\n-        localBlockReader \u003d new BlockReaderLocalLegacy(scConf, file, blk, token,\n-            startOffset, length, pathinfo, dataIn, tracer);\n+        localBlockReader \u003d new BlockReaderLocalLegacy(scConf, file, blk,\n+            startOffset, dataIn, tracer);\n       }\n     } catch (IOException e) {\n       // remove from cache\n       localDatanodeInfo.removeBlockLocalPathInfo(blk);\n       LOG.warn(\"BlockReaderLocalLegacy: Removing \" + blk\n           + \" from cache because local file \" + pathinfo.getBlockPath()\n           + \" could not be opened.\");\n       throw e;\n     } finally {\n       if (localBlockReader \u003d\u003d null) {\n         if (dataIn !\u003d null) {\n           dataIn.close();\n         }\n         if (checksumIn !\u003d null) {\n           checksumIn.close();\n         }\n       }\n     }\n     return localBlockReader;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static BlockReaderLocalLegacy newBlockReader(DfsClientConf conf,\n      UserGroupInformation userGroupInformation,\n      Configuration configuration, String file, ExtendedBlock blk,\n      Token\u003cBlockTokenIdentifier\u003e token, DatanodeInfo node,\n      long startOffset, long length, StorageType storageType,\n      Tracer tracer) throws IOException {\n    final ShortCircuitConf scConf \u003d conf.getShortCircuitConf();\n    LocalDatanodeInfo localDatanodeInfo \u003d getLocalDatanodeInfo(node\n        .getIpcPort());\n    // check the cache first\n    BlockLocalPathInfo pathinfo \u003d localDatanodeInfo.getBlockLocalPathInfo(blk);\n    if (pathinfo \u003d\u003d null) {\n      if (userGroupInformation \u003d\u003d null) {\n        userGroupInformation \u003d UserGroupInformation.getCurrentUser();\n      }\n      pathinfo \u003d getBlockPathInfo(userGroupInformation, blk, node,\n          configuration, conf.getSocketTimeout(), token,\n          conf.isConnectToDnViaHostname(), storageType);\n    }\n\n    // check to see if the file exists. It may so happen that the\n    // HDFS file has been deleted and this block-lookup is occurring\n    // on behalf of a new HDFS file. This time, the block file could\n    // be residing in a different portion of the fs.data.dir directory.\n    // In this case, we remove this entry from the cache. The next\n    // call to this method will re-populate the cache.\n    FileInputStream dataIn \u003d null;\n    FileInputStream checksumIn \u003d null;\n    BlockReaderLocalLegacy localBlockReader \u003d null;\n    final boolean skipChecksumCheck \u003d scConf.isSkipShortCircuitChecksums()\n        || storageType.isTransient();\n    try {\n      // get a local file system\n      File blkfile \u003d new File(pathinfo.getBlockPath());\n      dataIn \u003d new FileInputStream(blkfile);\n\n      LOG.debug(\"New BlockReaderLocalLegacy for file {} of size {} startOffset \"\n              + \"{} length {} short circuit checksum {}\",\n          blkfile, blkfile.length(), startOffset, length, !skipChecksumCheck);\n\n      if (!skipChecksumCheck) {\n        // get the metadata file\n        File metafile \u003d new File(pathinfo.getMetaPath());\n        checksumIn \u003d new FileInputStream(metafile);\n\n        final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(\n            new DataInputStream(checksumIn), blk);\n        long firstChunkOffset \u003d startOffset\n            - (startOffset % checksum.getBytesPerChecksum());\n        localBlockReader \u003d new BlockReaderLocalLegacy(scConf, file, blk,\n            startOffset, checksum, true, dataIn, firstChunkOffset, checksumIn,\n            tracer);\n      } else {\n        localBlockReader \u003d new BlockReaderLocalLegacy(scConf, file, blk,\n            startOffset, dataIn, tracer);\n      }\n    } catch (IOException e) {\n      // remove from cache\n      localDatanodeInfo.removeBlockLocalPathInfo(blk);\n      LOG.warn(\"BlockReaderLocalLegacy: Removing \" + blk\n          + \" from cache because local file \" + pathinfo.getBlockPath()\n          + \" could not be opened.\");\n      throw e;\n    } finally {\n      if (localBlockReader \u003d\u003d null) {\n        if (dataIn !\u003d null) {\n          dataIn.close();\n        }\n        if (checksumIn !\u003d null) {\n          checksumIn.close();\n        }\n      }\n    }\n    return localBlockReader;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocalLegacy.java",
      "extendedDetails": {}
    },
    "39285e6a1978ea5e53bdc1b0aef62421382124a8": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-8971. Remove guards when calling LOG.debug() and LOG.trace() in client package. Contributed by Mingliang Liu.\n",
      "commitDate": "29/09/15 5:52 PM",
      "commitName": "39285e6a1978ea5e53bdc1b0aef62421382124a8",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "29/09/15 5:51 PM",
      "commitNameOld": "6ee0539ede78b640f01c5eac18ded161182a7835",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,77 +1,75 @@\n   static BlockReaderLocalLegacy newBlockReader(DfsClientConf conf,\n       UserGroupInformation userGroupInformation,\n       Configuration configuration, String file, ExtendedBlock blk,\n       Token\u003cBlockTokenIdentifier\u003e token, DatanodeInfo node, \n       long startOffset, long length, StorageType storageType,\n       Tracer tracer) throws IOException {\n     final ShortCircuitConf scConf \u003d conf.getShortCircuitConf();\n     LocalDatanodeInfo localDatanodeInfo \u003d getLocalDatanodeInfo(node\n         .getIpcPort());\n     // check the cache first\n     BlockLocalPathInfo pathinfo \u003d localDatanodeInfo.getBlockLocalPathInfo(blk);\n     if (pathinfo \u003d\u003d null) {\n       if (userGroupInformation \u003d\u003d null) {\n         userGroupInformation \u003d UserGroupInformation.getCurrentUser();\n       }\n       pathinfo \u003d getBlockPathInfo(userGroupInformation, blk, node,\n           configuration, conf.getSocketTimeout(), token,\n           conf.isConnectToDnViaHostname(), storageType);\n     }\n \n     // check to see if the file exists. It may so happen that the\n     // HDFS file has been deleted and this block-lookup is occurring\n     // on behalf of a new HDFS file. This time, the block file could\n     // be residing in a different portion of the fs.data.dir directory.\n     // In this case, we remove this entry from the cache. The next\n     // call to this method will re-populate the cache.\n     FileInputStream dataIn \u003d null;\n     FileInputStream checksumIn \u003d null;\n     BlockReaderLocalLegacy localBlockReader \u003d null;\n     final boolean skipChecksumCheck \u003d scConf.isSkipShortCircuitChecksums()\n         || storageType.isTransient();\n     try {\n       // get a local file system\n       File blkfile \u003d new File(pathinfo.getBlockPath());\n       dataIn \u003d new FileInputStream(blkfile);\n \n-      if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"New BlockReaderLocalLegacy for file \" + blkfile + \" of size \"\n-            + blkfile.length() + \" startOffset \" + startOffset + \" length \"\n-            + length + \" short circuit checksum \" + !skipChecksumCheck);\n-      }\n+      LOG.debug(\"New BlockReaderLocalLegacy for file {} of size {} startOffset \"\n+              + \"{} length {} short circuit checksum {}\",\n+          blkfile, blkfile.length(), startOffset, length, !skipChecksumCheck);\n \n       if (!skipChecksumCheck) {\n         // get the metadata file\n         File metafile \u003d new File(pathinfo.getMetaPath());\n         checksumIn \u003d new FileInputStream(metafile);\n \n         final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(\n             new DataInputStream(checksumIn), blk);\n         long firstChunkOffset \u003d startOffset\n             - (startOffset % checksum.getBytesPerChecksum());\n         localBlockReader \u003d new BlockReaderLocalLegacy(scConf, file, blk, token,\n             startOffset, length, pathinfo, checksum, true, dataIn,\n             firstChunkOffset, checksumIn, tracer);\n       } else {\n         localBlockReader \u003d new BlockReaderLocalLegacy(scConf, file, blk, token,\n             startOffset, length, pathinfo, dataIn, tracer);\n       }\n     } catch (IOException e) {\n       // remove from cache\n       localDatanodeInfo.removeBlockLocalPathInfo(blk);\n       LOG.warn(\"BlockReaderLocalLegacy: Removing \" + blk\n           + \" from cache because local file \" + pathinfo.getBlockPath()\n           + \" could not be opened.\");\n       throw e;\n     } finally {\n       if (localBlockReader \u003d\u003d null) {\n         if (dataIn !\u003d null) {\n           dataIn.close();\n         }\n         if (checksumIn !\u003d null) {\n           checksumIn.close();\n         }\n       }\n     }\n     return localBlockReader;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static BlockReaderLocalLegacy newBlockReader(DfsClientConf conf,\n      UserGroupInformation userGroupInformation,\n      Configuration configuration, String file, ExtendedBlock blk,\n      Token\u003cBlockTokenIdentifier\u003e token, DatanodeInfo node, \n      long startOffset, long length, StorageType storageType,\n      Tracer tracer) throws IOException {\n    final ShortCircuitConf scConf \u003d conf.getShortCircuitConf();\n    LocalDatanodeInfo localDatanodeInfo \u003d getLocalDatanodeInfo(node\n        .getIpcPort());\n    // check the cache first\n    BlockLocalPathInfo pathinfo \u003d localDatanodeInfo.getBlockLocalPathInfo(blk);\n    if (pathinfo \u003d\u003d null) {\n      if (userGroupInformation \u003d\u003d null) {\n        userGroupInformation \u003d UserGroupInformation.getCurrentUser();\n      }\n      pathinfo \u003d getBlockPathInfo(userGroupInformation, blk, node,\n          configuration, conf.getSocketTimeout(), token,\n          conf.isConnectToDnViaHostname(), storageType);\n    }\n\n    // check to see if the file exists. It may so happen that the\n    // HDFS file has been deleted and this block-lookup is occurring\n    // on behalf of a new HDFS file. This time, the block file could\n    // be residing in a different portion of the fs.data.dir directory.\n    // In this case, we remove this entry from the cache. The next\n    // call to this method will re-populate the cache.\n    FileInputStream dataIn \u003d null;\n    FileInputStream checksumIn \u003d null;\n    BlockReaderLocalLegacy localBlockReader \u003d null;\n    final boolean skipChecksumCheck \u003d scConf.isSkipShortCircuitChecksums()\n        || storageType.isTransient();\n    try {\n      // get a local file system\n      File blkfile \u003d new File(pathinfo.getBlockPath());\n      dataIn \u003d new FileInputStream(blkfile);\n\n      LOG.debug(\"New BlockReaderLocalLegacy for file {} of size {} startOffset \"\n              + \"{} length {} short circuit checksum {}\",\n          blkfile, blkfile.length(), startOffset, length, !skipChecksumCheck);\n\n      if (!skipChecksumCheck) {\n        // get the metadata file\n        File metafile \u003d new File(pathinfo.getMetaPath());\n        checksumIn \u003d new FileInputStream(metafile);\n\n        final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(\n            new DataInputStream(checksumIn), blk);\n        long firstChunkOffset \u003d startOffset\n            - (startOffset % checksum.getBytesPerChecksum());\n        localBlockReader \u003d new BlockReaderLocalLegacy(scConf, file, blk, token,\n            startOffset, length, pathinfo, checksum, true, dataIn,\n            firstChunkOffset, checksumIn, tracer);\n      } else {\n        localBlockReader \u003d new BlockReaderLocalLegacy(scConf, file, blk, token,\n            startOffset, length, pathinfo, dataIn, tracer);\n      }\n    } catch (IOException e) {\n      // remove from cache\n      localDatanodeInfo.removeBlockLocalPathInfo(blk);\n      LOG.warn(\"BlockReaderLocalLegacy: Removing \" + blk\n          + \" from cache because local file \" + pathinfo.getBlockPath()\n          + \" could not be opened.\");\n      throw e;\n    } finally {\n      if (localBlockReader \u003d\u003d null) {\n        if (dataIn !\u003d null) {\n          dataIn.close();\n        }\n        if (checksumIn !\u003d null) {\n          checksumIn.close();\n        }\n      }\n    }\n    return localBlockReader;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocalLegacy.java",
      "extendedDetails": {}
    },
    "6ee0539ede78b640f01c5eac18ded161182a7835": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"HDFS-9170. Move libhdfs / fuse-dfs / libwebhdfs to hdfs-client. Contributed by Haohui Mai.\"\n\nThis reverts commit d5a9a3daa0224249221ffa7b8bd5751ab2feca56.\n",
      "commitDate": "29/09/15 5:51 PM",
      "commitName": "6ee0539ede78b640f01c5eac18ded161182a7835",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "29/09/15 5:48 PM",
      "commitNameOld": "d5a9a3daa0224249221ffa7b8bd5751ab2feca56",
      "commitAuthorOld": "Haohui Mai",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,75 +1,77 @@\n   static BlockReaderLocalLegacy newBlockReader(DfsClientConf conf,\n       UserGroupInformation userGroupInformation,\n       Configuration configuration, String file, ExtendedBlock blk,\n       Token\u003cBlockTokenIdentifier\u003e token, DatanodeInfo node, \n       long startOffset, long length, StorageType storageType,\n       Tracer tracer) throws IOException {\n     final ShortCircuitConf scConf \u003d conf.getShortCircuitConf();\n     LocalDatanodeInfo localDatanodeInfo \u003d getLocalDatanodeInfo(node\n         .getIpcPort());\n     // check the cache first\n     BlockLocalPathInfo pathinfo \u003d localDatanodeInfo.getBlockLocalPathInfo(blk);\n     if (pathinfo \u003d\u003d null) {\n       if (userGroupInformation \u003d\u003d null) {\n         userGroupInformation \u003d UserGroupInformation.getCurrentUser();\n       }\n       pathinfo \u003d getBlockPathInfo(userGroupInformation, blk, node,\n           configuration, conf.getSocketTimeout(), token,\n           conf.isConnectToDnViaHostname(), storageType);\n     }\n \n     // check to see if the file exists. It may so happen that the\n     // HDFS file has been deleted and this block-lookup is occurring\n     // on behalf of a new HDFS file. This time, the block file could\n     // be residing in a different portion of the fs.data.dir directory.\n     // In this case, we remove this entry from the cache. The next\n     // call to this method will re-populate the cache.\n     FileInputStream dataIn \u003d null;\n     FileInputStream checksumIn \u003d null;\n     BlockReaderLocalLegacy localBlockReader \u003d null;\n     final boolean skipChecksumCheck \u003d scConf.isSkipShortCircuitChecksums()\n         || storageType.isTransient();\n     try {\n       // get a local file system\n       File blkfile \u003d new File(pathinfo.getBlockPath());\n       dataIn \u003d new FileInputStream(blkfile);\n \n-      LOG.debug(\"New BlockReaderLocalLegacy for file {} of size {} startOffset \"\n-              + \"{} length {} short circuit checksum {}\",\n-          blkfile, blkfile.length(), startOffset, length, !skipChecksumCheck);\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"New BlockReaderLocalLegacy for file \" + blkfile + \" of size \"\n+            + blkfile.length() + \" startOffset \" + startOffset + \" length \"\n+            + length + \" short circuit checksum \" + !skipChecksumCheck);\n+      }\n \n       if (!skipChecksumCheck) {\n         // get the metadata file\n         File metafile \u003d new File(pathinfo.getMetaPath());\n         checksumIn \u003d new FileInputStream(metafile);\n \n         final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(\n             new DataInputStream(checksumIn), blk);\n         long firstChunkOffset \u003d startOffset\n             - (startOffset % checksum.getBytesPerChecksum());\n         localBlockReader \u003d new BlockReaderLocalLegacy(scConf, file, blk, token,\n             startOffset, length, pathinfo, checksum, true, dataIn,\n             firstChunkOffset, checksumIn, tracer);\n       } else {\n         localBlockReader \u003d new BlockReaderLocalLegacy(scConf, file, blk, token,\n             startOffset, length, pathinfo, dataIn, tracer);\n       }\n     } catch (IOException e) {\n       // remove from cache\n       localDatanodeInfo.removeBlockLocalPathInfo(blk);\n       LOG.warn(\"BlockReaderLocalLegacy: Removing \" + blk\n           + \" from cache because local file \" + pathinfo.getBlockPath()\n           + \" could not be opened.\");\n       throw e;\n     } finally {\n       if (localBlockReader \u003d\u003d null) {\n         if (dataIn !\u003d null) {\n           dataIn.close();\n         }\n         if (checksumIn !\u003d null) {\n           checksumIn.close();\n         }\n       }\n     }\n     return localBlockReader;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static BlockReaderLocalLegacy newBlockReader(DfsClientConf conf,\n      UserGroupInformation userGroupInformation,\n      Configuration configuration, String file, ExtendedBlock blk,\n      Token\u003cBlockTokenIdentifier\u003e token, DatanodeInfo node, \n      long startOffset, long length, StorageType storageType,\n      Tracer tracer) throws IOException {\n    final ShortCircuitConf scConf \u003d conf.getShortCircuitConf();\n    LocalDatanodeInfo localDatanodeInfo \u003d getLocalDatanodeInfo(node\n        .getIpcPort());\n    // check the cache first\n    BlockLocalPathInfo pathinfo \u003d localDatanodeInfo.getBlockLocalPathInfo(blk);\n    if (pathinfo \u003d\u003d null) {\n      if (userGroupInformation \u003d\u003d null) {\n        userGroupInformation \u003d UserGroupInformation.getCurrentUser();\n      }\n      pathinfo \u003d getBlockPathInfo(userGroupInformation, blk, node,\n          configuration, conf.getSocketTimeout(), token,\n          conf.isConnectToDnViaHostname(), storageType);\n    }\n\n    // check to see if the file exists. It may so happen that the\n    // HDFS file has been deleted and this block-lookup is occurring\n    // on behalf of a new HDFS file. This time, the block file could\n    // be residing in a different portion of the fs.data.dir directory.\n    // In this case, we remove this entry from the cache. The next\n    // call to this method will re-populate the cache.\n    FileInputStream dataIn \u003d null;\n    FileInputStream checksumIn \u003d null;\n    BlockReaderLocalLegacy localBlockReader \u003d null;\n    final boolean skipChecksumCheck \u003d scConf.isSkipShortCircuitChecksums()\n        || storageType.isTransient();\n    try {\n      // get a local file system\n      File blkfile \u003d new File(pathinfo.getBlockPath());\n      dataIn \u003d new FileInputStream(blkfile);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"New BlockReaderLocalLegacy for file \" + blkfile + \" of size \"\n            + blkfile.length() + \" startOffset \" + startOffset + \" length \"\n            + length + \" short circuit checksum \" + !skipChecksumCheck);\n      }\n\n      if (!skipChecksumCheck) {\n        // get the metadata file\n        File metafile \u003d new File(pathinfo.getMetaPath());\n        checksumIn \u003d new FileInputStream(metafile);\n\n        final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(\n            new DataInputStream(checksumIn), blk);\n        long firstChunkOffset \u003d startOffset\n            - (startOffset % checksum.getBytesPerChecksum());\n        localBlockReader \u003d new BlockReaderLocalLegacy(scConf, file, blk, token,\n            startOffset, length, pathinfo, checksum, true, dataIn,\n            firstChunkOffset, checksumIn, tracer);\n      } else {\n        localBlockReader \u003d new BlockReaderLocalLegacy(scConf, file, blk, token,\n            startOffset, length, pathinfo, dataIn, tracer);\n      }\n    } catch (IOException e) {\n      // remove from cache\n      localDatanodeInfo.removeBlockLocalPathInfo(blk);\n      LOG.warn(\"BlockReaderLocalLegacy: Removing \" + blk\n          + \" from cache because local file \" + pathinfo.getBlockPath()\n          + \" could not be opened.\");\n      throw e;\n    } finally {\n      if (localBlockReader \u003d\u003d null) {\n        if (dataIn !\u003d null) {\n          dataIn.close();\n        }\n        if (checksumIn !\u003d null) {\n          checksumIn.close();\n        }\n      }\n    }\n    return localBlockReader;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocalLegacy.java",
      "extendedDetails": {}
    },
    "d5a9a3daa0224249221ffa7b8bd5751ab2feca56": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-9170. Move libhdfs / fuse-dfs / libwebhdfs to hdfs-client. Contributed by Haohui Mai.\n",
      "commitDate": "29/09/15 5:48 PM",
      "commitName": "d5a9a3daa0224249221ffa7b8bd5751ab2feca56",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "28/09/15 7:42 AM",
      "commitNameOld": "892ade689f9bcce76daae8f66fc00a49bee8548e",
      "commitAuthorOld": "Colin Patrick Mccabe",
      "daysBetweenCommits": 1.42,
      "commitsBetweenForRepo": 19,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,77 +1,75 @@\n   static BlockReaderLocalLegacy newBlockReader(DfsClientConf conf,\n       UserGroupInformation userGroupInformation,\n       Configuration configuration, String file, ExtendedBlock blk,\n       Token\u003cBlockTokenIdentifier\u003e token, DatanodeInfo node, \n       long startOffset, long length, StorageType storageType,\n       Tracer tracer) throws IOException {\n     final ShortCircuitConf scConf \u003d conf.getShortCircuitConf();\n     LocalDatanodeInfo localDatanodeInfo \u003d getLocalDatanodeInfo(node\n         .getIpcPort());\n     // check the cache first\n     BlockLocalPathInfo pathinfo \u003d localDatanodeInfo.getBlockLocalPathInfo(blk);\n     if (pathinfo \u003d\u003d null) {\n       if (userGroupInformation \u003d\u003d null) {\n         userGroupInformation \u003d UserGroupInformation.getCurrentUser();\n       }\n       pathinfo \u003d getBlockPathInfo(userGroupInformation, blk, node,\n           configuration, conf.getSocketTimeout(), token,\n           conf.isConnectToDnViaHostname(), storageType);\n     }\n \n     // check to see if the file exists. It may so happen that the\n     // HDFS file has been deleted and this block-lookup is occurring\n     // on behalf of a new HDFS file. This time, the block file could\n     // be residing in a different portion of the fs.data.dir directory.\n     // In this case, we remove this entry from the cache. The next\n     // call to this method will re-populate the cache.\n     FileInputStream dataIn \u003d null;\n     FileInputStream checksumIn \u003d null;\n     BlockReaderLocalLegacy localBlockReader \u003d null;\n     final boolean skipChecksumCheck \u003d scConf.isSkipShortCircuitChecksums()\n         || storageType.isTransient();\n     try {\n       // get a local file system\n       File blkfile \u003d new File(pathinfo.getBlockPath());\n       dataIn \u003d new FileInputStream(blkfile);\n \n-      if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"New BlockReaderLocalLegacy for file \" + blkfile + \" of size \"\n-            + blkfile.length() + \" startOffset \" + startOffset + \" length \"\n-            + length + \" short circuit checksum \" + !skipChecksumCheck);\n-      }\n+      LOG.debug(\"New BlockReaderLocalLegacy for file {} of size {} startOffset \"\n+              + \"{} length {} short circuit checksum {}\",\n+          blkfile, blkfile.length(), startOffset, length, !skipChecksumCheck);\n \n       if (!skipChecksumCheck) {\n         // get the metadata file\n         File metafile \u003d new File(pathinfo.getMetaPath());\n         checksumIn \u003d new FileInputStream(metafile);\n \n         final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(\n             new DataInputStream(checksumIn), blk);\n         long firstChunkOffset \u003d startOffset\n             - (startOffset % checksum.getBytesPerChecksum());\n         localBlockReader \u003d new BlockReaderLocalLegacy(scConf, file, blk, token,\n             startOffset, length, pathinfo, checksum, true, dataIn,\n             firstChunkOffset, checksumIn, tracer);\n       } else {\n         localBlockReader \u003d new BlockReaderLocalLegacy(scConf, file, blk, token,\n             startOffset, length, pathinfo, dataIn, tracer);\n       }\n     } catch (IOException e) {\n       // remove from cache\n       localDatanodeInfo.removeBlockLocalPathInfo(blk);\n       LOG.warn(\"BlockReaderLocalLegacy: Removing \" + blk\n           + \" from cache because local file \" + pathinfo.getBlockPath()\n           + \" could not be opened.\");\n       throw e;\n     } finally {\n       if (localBlockReader \u003d\u003d null) {\n         if (dataIn !\u003d null) {\n           dataIn.close();\n         }\n         if (checksumIn !\u003d null) {\n           checksumIn.close();\n         }\n       }\n     }\n     return localBlockReader;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static BlockReaderLocalLegacy newBlockReader(DfsClientConf conf,\n      UserGroupInformation userGroupInformation,\n      Configuration configuration, String file, ExtendedBlock blk,\n      Token\u003cBlockTokenIdentifier\u003e token, DatanodeInfo node, \n      long startOffset, long length, StorageType storageType,\n      Tracer tracer) throws IOException {\n    final ShortCircuitConf scConf \u003d conf.getShortCircuitConf();\n    LocalDatanodeInfo localDatanodeInfo \u003d getLocalDatanodeInfo(node\n        .getIpcPort());\n    // check the cache first\n    BlockLocalPathInfo pathinfo \u003d localDatanodeInfo.getBlockLocalPathInfo(blk);\n    if (pathinfo \u003d\u003d null) {\n      if (userGroupInformation \u003d\u003d null) {\n        userGroupInformation \u003d UserGroupInformation.getCurrentUser();\n      }\n      pathinfo \u003d getBlockPathInfo(userGroupInformation, blk, node,\n          configuration, conf.getSocketTimeout(), token,\n          conf.isConnectToDnViaHostname(), storageType);\n    }\n\n    // check to see if the file exists. It may so happen that the\n    // HDFS file has been deleted and this block-lookup is occurring\n    // on behalf of a new HDFS file. This time, the block file could\n    // be residing in a different portion of the fs.data.dir directory.\n    // In this case, we remove this entry from the cache. The next\n    // call to this method will re-populate the cache.\n    FileInputStream dataIn \u003d null;\n    FileInputStream checksumIn \u003d null;\n    BlockReaderLocalLegacy localBlockReader \u003d null;\n    final boolean skipChecksumCheck \u003d scConf.isSkipShortCircuitChecksums()\n        || storageType.isTransient();\n    try {\n      // get a local file system\n      File blkfile \u003d new File(pathinfo.getBlockPath());\n      dataIn \u003d new FileInputStream(blkfile);\n\n      LOG.debug(\"New BlockReaderLocalLegacy for file {} of size {} startOffset \"\n              + \"{} length {} short circuit checksum {}\",\n          blkfile, blkfile.length(), startOffset, length, !skipChecksumCheck);\n\n      if (!skipChecksumCheck) {\n        // get the metadata file\n        File metafile \u003d new File(pathinfo.getMetaPath());\n        checksumIn \u003d new FileInputStream(metafile);\n\n        final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(\n            new DataInputStream(checksumIn), blk);\n        long firstChunkOffset \u003d startOffset\n            - (startOffset % checksum.getBytesPerChecksum());\n        localBlockReader \u003d new BlockReaderLocalLegacy(scConf, file, blk, token,\n            startOffset, length, pathinfo, checksum, true, dataIn,\n            firstChunkOffset, checksumIn, tracer);\n      } else {\n        localBlockReader \u003d new BlockReaderLocalLegacy(scConf, file, blk, token,\n            startOffset, length, pathinfo, dataIn, tracer);\n      }\n    } catch (IOException e) {\n      // remove from cache\n      localDatanodeInfo.removeBlockLocalPathInfo(blk);\n      LOG.warn(\"BlockReaderLocalLegacy: Removing \" + blk\n          + \" from cache because local file \" + pathinfo.getBlockPath()\n          + \" could not be opened.\");\n      throw e;\n    } finally {\n      if (localBlockReader \u003d\u003d null) {\n        if (dataIn !\u003d null) {\n          dataIn.close();\n        }\n        if (checksumIn !\u003d null) {\n          checksumIn.close();\n        }\n      }\n    }\n    return localBlockReader;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocalLegacy.java",
      "extendedDetails": {}
    },
    "892ade689f9bcce76daae8f66fc00a49bee8548e": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-9080. Update htrace version to 4.0.1 (cmccabe)\n",
      "commitDate": "28/09/15 7:42 AM",
      "commitName": "892ade689f9bcce76daae8f66fc00a49bee8548e",
      "commitAuthor": "Colin Patrick Mccabe",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-9080. Update htrace version to 4.0.1 (cmccabe)\n",
          "commitDate": "28/09/15 7:42 AM",
          "commitName": "892ade689f9bcce76daae8f66fc00a49bee8548e",
          "commitAuthor": "Colin Patrick Mccabe",
          "commitDateOld": "28/08/15 2:38 PM",
          "commitNameOld": "e2c9b288b223b9fd82dc12018936e13128413492",
          "commitAuthorOld": "Haohui Mai",
          "daysBetweenCommits": 30.71,
          "commitsBetweenForRepo": 195,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,77 +1,77 @@\n   static BlockReaderLocalLegacy newBlockReader(DfsClientConf conf,\n       UserGroupInformation userGroupInformation,\n       Configuration configuration, String file, ExtendedBlock blk,\n       Token\u003cBlockTokenIdentifier\u003e token, DatanodeInfo node, \n-      long startOffset, long length, StorageType storageType)\n-      throws IOException {\n+      long startOffset, long length, StorageType storageType,\n+      Tracer tracer) throws IOException {\n     final ShortCircuitConf scConf \u003d conf.getShortCircuitConf();\n     LocalDatanodeInfo localDatanodeInfo \u003d getLocalDatanodeInfo(node\n         .getIpcPort());\n     // check the cache first\n     BlockLocalPathInfo pathinfo \u003d localDatanodeInfo.getBlockLocalPathInfo(blk);\n     if (pathinfo \u003d\u003d null) {\n       if (userGroupInformation \u003d\u003d null) {\n         userGroupInformation \u003d UserGroupInformation.getCurrentUser();\n       }\n       pathinfo \u003d getBlockPathInfo(userGroupInformation, blk, node,\n           configuration, conf.getSocketTimeout(), token,\n           conf.isConnectToDnViaHostname(), storageType);\n     }\n \n     // check to see if the file exists. It may so happen that the\n     // HDFS file has been deleted and this block-lookup is occurring\n     // on behalf of a new HDFS file. This time, the block file could\n     // be residing in a different portion of the fs.data.dir directory.\n     // In this case, we remove this entry from the cache. The next\n     // call to this method will re-populate the cache.\n     FileInputStream dataIn \u003d null;\n     FileInputStream checksumIn \u003d null;\n     BlockReaderLocalLegacy localBlockReader \u003d null;\n     final boolean skipChecksumCheck \u003d scConf.isSkipShortCircuitChecksums()\n         || storageType.isTransient();\n     try {\n       // get a local file system\n       File blkfile \u003d new File(pathinfo.getBlockPath());\n       dataIn \u003d new FileInputStream(blkfile);\n \n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"New BlockReaderLocalLegacy for file \" + blkfile + \" of size \"\n             + blkfile.length() + \" startOffset \" + startOffset + \" length \"\n             + length + \" short circuit checksum \" + !skipChecksumCheck);\n       }\n \n       if (!skipChecksumCheck) {\n         // get the metadata file\n         File metafile \u003d new File(pathinfo.getMetaPath());\n         checksumIn \u003d new FileInputStream(metafile);\n \n         final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(\n             new DataInputStream(checksumIn), blk);\n         long firstChunkOffset \u003d startOffset\n             - (startOffset % checksum.getBytesPerChecksum());\n         localBlockReader \u003d new BlockReaderLocalLegacy(scConf, file, blk, token,\n             startOffset, length, pathinfo, checksum, true, dataIn,\n-            firstChunkOffset, checksumIn);\n+            firstChunkOffset, checksumIn, tracer);\n       } else {\n         localBlockReader \u003d new BlockReaderLocalLegacy(scConf, file, blk, token,\n-            startOffset, length, pathinfo, dataIn);\n+            startOffset, length, pathinfo, dataIn, tracer);\n       }\n     } catch (IOException e) {\n       // remove from cache\n       localDatanodeInfo.removeBlockLocalPathInfo(blk);\n       LOG.warn(\"BlockReaderLocalLegacy: Removing \" + blk\n           + \" from cache because local file \" + pathinfo.getBlockPath()\n           + \" could not be opened.\");\n       throw e;\n     } finally {\n       if (localBlockReader \u003d\u003d null) {\n         if (dataIn !\u003d null) {\n           dataIn.close();\n         }\n         if (checksumIn !\u003d null) {\n           checksumIn.close();\n         }\n       }\n     }\n     return localBlockReader;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static BlockReaderLocalLegacy newBlockReader(DfsClientConf conf,\n      UserGroupInformation userGroupInformation,\n      Configuration configuration, String file, ExtendedBlock blk,\n      Token\u003cBlockTokenIdentifier\u003e token, DatanodeInfo node, \n      long startOffset, long length, StorageType storageType,\n      Tracer tracer) throws IOException {\n    final ShortCircuitConf scConf \u003d conf.getShortCircuitConf();\n    LocalDatanodeInfo localDatanodeInfo \u003d getLocalDatanodeInfo(node\n        .getIpcPort());\n    // check the cache first\n    BlockLocalPathInfo pathinfo \u003d localDatanodeInfo.getBlockLocalPathInfo(blk);\n    if (pathinfo \u003d\u003d null) {\n      if (userGroupInformation \u003d\u003d null) {\n        userGroupInformation \u003d UserGroupInformation.getCurrentUser();\n      }\n      pathinfo \u003d getBlockPathInfo(userGroupInformation, blk, node,\n          configuration, conf.getSocketTimeout(), token,\n          conf.isConnectToDnViaHostname(), storageType);\n    }\n\n    // check to see if the file exists. It may so happen that the\n    // HDFS file has been deleted and this block-lookup is occurring\n    // on behalf of a new HDFS file. This time, the block file could\n    // be residing in a different portion of the fs.data.dir directory.\n    // In this case, we remove this entry from the cache. The next\n    // call to this method will re-populate the cache.\n    FileInputStream dataIn \u003d null;\n    FileInputStream checksumIn \u003d null;\n    BlockReaderLocalLegacy localBlockReader \u003d null;\n    final boolean skipChecksumCheck \u003d scConf.isSkipShortCircuitChecksums()\n        || storageType.isTransient();\n    try {\n      // get a local file system\n      File blkfile \u003d new File(pathinfo.getBlockPath());\n      dataIn \u003d new FileInputStream(blkfile);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"New BlockReaderLocalLegacy for file \" + blkfile + \" of size \"\n            + blkfile.length() + \" startOffset \" + startOffset + \" length \"\n            + length + \" short circuit checksum \" + !skipChecksumCheck);\n      }\n\n      if (!skipChecksumCheck) {\n        // get the metadata file\n        File metafile \u003d new File(pathinfo.getMetaPath());\n        checksumIn \u003d new FileInputStream(metafile);\n\n        final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(\n            new DataInputStream(checksumIn), blk);\n        long firstChunkOffset \u003d startOffset\n            - (startOffset % checksum.getBytesPerChecksum());\n        localBlockReader \u003d new BlockReaderLocalLegacy(scConf, file, blk, token,\n            startOffset, length, pathinfo, checksum, true, dataIn,\n            firstChunkOffset, checksumIn, tracer);\n      } else {\n        localBlockReader \u003d new BlockReaderLocalLegacy(scConf, file, blk, token,\n            startOffset, length, pathinfo, dataIn, tracer);\n      }\n    } catch (IOException e) {\n      // remove from cache\n      localDatanodeInfo.removeBlockLocalPathInfo(blk);\n      LOG.warn(\"BlockReaderLocalLegacy: Removing \" + blk\n          + \" from cache because local file \" + pathinfo.getBlockPath()\n          + \" could not be opened.\");\n      throw e;\n    } finally {\n      if (localBlockReader \u003d\u003d null) {\n        if (dataIn !\u003d null) {\n          dataIn.close();\n        }\n        if (checksumIn !\u003d null) {\n          checksumIn.close();\n        }\n      }\n    }\n    return localBlockReader;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocalLegacy.java",
          "extendedDetails": {
            "oldValue": "[conf-DfsClientConf, userGroupInformation-UserGroupInformation, configuration-Configuration, file-String, blk-ExtendedBlock, token-Token\u003cBlockTokenIdentifier\u003e, node-DatanodeInfo, startOffset-long, length-long, storageType-StorageType]",
            "newValue": "[conf-DfsClientConf, userGroupInformation-UserGroupInformation, configuration-Configuration, file-String, blk-ExtendedBlock, token-Token\u003cBlockTokenIdentifier\u003e, node-DatanodeInfo, startOffset-long, length-long, storageType-StorageType, tracer-Tracer]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-9080. Update htrace version to 4.0.1 (cmccabe)\n",
          "commitDate": "28/09/15 7:42 AM",
          "commitName": "892ade689f9bcce76daae8f66fc00a49bee8548e",
          "commitAuthor": "Colin Patrick Mccabe",
          "commitDateOld": "28/08/15 2:38 PM",
          "commitNameOld": "e2c9b288b223b9fd82dc12018936e13128413492",
          "commitAuthorOld": "Haohui Mai",
          "daysBetweenCommits": 30.71,
          "commitsBetweenForRepo": 195,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,77 +1,77 @@\n   static BlockReaderLocalLegacy newBlockReader(DfsClientConf conf,\n       UserGroupInformation userGroupInformation,\n       Configuration configuration, String file, ExtendedBlock blk,\n       Token\u003cBlockTokenIdentifier\u003e token, DatanodeInfo node, \n-      long startOffset, long length, StorageType storageType)\n-      throws IOException {\n+      long startOffset, long length, StorageType storageType,\n+      Tracer tracer) throws IOException {\n     final ShortCircuitConf scConf \u003d conf.getShortCircuitConf();\n     LocalDatanodeInfo localDatanodeInfo \u003d getLocalDatanodeInfo(node\n         .getIpcPort());\n     // check the cache first\n     BlockLocalPathInfo pathinfo \u003d localDatanodeInfo.getBlockLocalPathInfo(blk);\n     if (pathinfo \u003d\u003d null) {\n       if (userGroupInformation \u003d\u003d null) {\n         userGroupInformation \u003d UserGroupInformation.getCurrentUser();\n       }\n       pathinfo \u003d getBlockPathInfo(userGroupInformation, blk, node,\n           configuration, conf.getSocketTimeout(), token,\n           conf.isConnectToDnViaHostname(), storageType);\n     }\n \n     // check to see if the file exists. It may so happen that the\n     // HDFS file has been deleted and this block-lookup is occurring\n     // on behalf of a new HDFS file. This time, the block file could\n     // be residing in a different portion of the fs.data.dir directory.\n     // In this case, we remove this entry from the cache. The next\n     // call to this method will re-populate the cache.\n     FileInputStream dataIn \u003d null;\n     FileInputStream checksumIn \u003d null;\n     BlockReaderLocalLegacy localBlockReader \u003d null;\n     final boolean skipChecksumCheck \u003d scConf.isSkipShortCircuitChecksums()\n         || storageType.isTransient();\n     try {\n       // get a local file system\n       File blkfile \u003d new File(pathinfo.getBlockPath());\n       dataIn \u003d new FileInputStream(blkfile);\n \n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"New BlockReaderLocalLegacy for file \" + blkfile + \" of size \"\n             + blkfile.length() + \" startOffset \" + startOffset + \" length \"\n             + length + \" short circuit checksum \" + !skipChecksumCheck);\n       }\n \n       if (!skipChecksumCheck) {\n         // get the metadata file\n         File metafile \u003d new File(pathinfo.getMetaPath());\n         checksumIn \u003d new FileInputStream(metafile);\n \n         final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(\n             new DataInputStream(checksumIn), blk);\n         long firstChunkOffset \u003d startOffset\n             - (startOffset % checksum.getBytesPerChecksum());\n         localBlockReader \u003d new BlockReaderLocalLegacy(scConf, file, blk, token,\n             startOffset, length, pathinfo, checksum, true, dataIn,\n-            firstChunkOffset, checksumIn);\n+            firstChunkOffset, checksumIn, tracer);\n       } else {\n         localBlockReader \u003d new BlockReaderLocalLegacy(scConf, file, blk, token,\n-            startOffset, length, pathinfo, dataIn);\n+            startOffset, length, pathinfo, dataIn, tracer);\n       }\n     } catch (IOException e) {\n       // remove from cache\n       localDatanodeInfo.removeBlockLocalPathInfo(blk);\n       LOG.warn(\"BlockReaderLocalLegacy: Removing \" + blk\n           + \" from cache because local file \" + pathinfo.getBlockPath()\n           + \" could not be opened.\");\n       throw e;\n     } finally {\n       if (localBlockReader \u003d\u003d null) {\n         if (dataIn !\u003d null) {\n           dataIn.close();\n         }\n         if (checksumIn !\u003d null) {\n           checksumIn.close();\n         }\n       }\n     }\n     return localBlockReader;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static BlockReaderLocalLegacy newBlockReader(DfsClientConf conf,\n      UserGroupInformation userGroupInformation,\n      Configuration configuration, String file, ExtendedBlock blk,\n      Token\u003cBlockTokenIdentifier\u003e token, DatanodeInfo node, \n      long startOffset, long length, StorageType storageType,\n      Tracer tracer) throws IOException {\n    final ShortCircuitConf scConf \u003d conf.getShortCircuitConf();\n    LocalDatanodeInfo localDatanodeInfo \u003d getLocalDatanodeInfo(node\n        .getIpcPort());\n    // check the cache first\n    BlockLocalPathInfo pathinfo \u003d localDatanodeInfo.getBlockLocalPathInfo(blk);\n    if (pathinfo \u003d\u003d null) {\n      if (userGroupInformation \u003d\u003d null) {\n        userGroupInformation \u003d UserGroupInformation.getCurrentUser();\n      }\n      pathinfo \u003d getBlockPathInfo(userGroupInformation, blk, node,\n          configuration, conf.getSocketTimeout(), token,\n          conf.isConnectToDnViaHostname(), storageType);\n    }\n\n    // check to see if the file exists. It may so happen that the\n    // HDFS file has been deleted and this block-lookup is occurring\n    // on behalf of a new HDFS file. This time, the block file could\n    // be residing in a different portion of the fs.data.dir directory.\n    // In this case, we remove this entry from the cache. The next\n    // call to this method will re-populate the cache.\n    FileInputStream dataIn \u003d null;\n    FileInputStream checksumIn \u003d null;\n    BlockReaderLocalLegacy localBlockReader \u003d null;\n    final boolean skipChecksumCheck \u003d scConf.isSkipShortCircuitChecksums()\n        || storageType.isTransient();\n    try {\n      // get a local file system\n      File blkfile \u003d new File(pathinfo.getBlockPath());\n      dataIn \u003d new FileInputStream(blkfile);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"New BlockReaderLocalLegacy for file \" + blkfile + \" of size \"\n            + blkfile.length() + \" startOffset \" + startOffset + \" length \"\n            + length + \" short circuit checksum \" + !skipChecksumCheck);\n      }\n\n      if (!skipChecksumCheck) {\n        // get the metadata file\n        File metafile \u003d new File(pathinfo.getMetaPath());\n        checksumIn \u003d new FileInputStream(metafile);\n\n        final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(\n            new DataInputStream(checksumIn), blk);\n        long firstChunkOffset \u003d startOffset\n            - (startOffset % checksum.getBytesPerChecksum());\n        localBlockReader \u003d new BlockReaderLocalLegacy(scConf, file, blk, token,\n            startOffset, length, pathinfo, checksum, true, dataIn,\n            firstChunkOffset, checksumIn, tracer);\n      } else {\n        localBlockReader \u003d new BlockReaderLocalLegacy(scConf, file, blk, token,\n            startOffset, length, pathinfo, dataIn, tracer);\n      }\n    } catch (IOException e) {\n      // remove from cache\n      localDatanodeInfo.removeBlockLocalPathInfo(blk);\n      LOG.warn(\"BlockReaderLocalLegacy: Removing \" + blk\n          + \" from cache because local file \" + pathinfo.getBlockPath()\n          + \" could not be opened.\");\n      throw e;\n    } finally {\n      if (localBlockReader \u003d\u003d null) {\n        if (dataIn !\u003d null) {\n          dataIn.close();\n        }\n        if (checksumIn !\u003d null) {\n          checksumIn.close();\n        }\n      }\n    }\n    return localBlockReader;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocalLegacy.java",
          "extendedDetails": {}
        }
      ]
    },
    "e2c9b288b223b9fd82dc12018936e13128413492": {
      "type": "Ymultichange(Yfilerename,Ybodychange)",
      "commitMessage": "HDFS-8925. Move BlockReaderLocal to hdfs-client. Contributed by Mingliang Liu.\n",
      "commitDate": "28/08/15 2:38 PM",
      "commitName": "e2c9b288b223b9fd82dc12018936e13128413492",
      "commitAuthor": "Haohui Mai",
      "subchanges": [
        {
          "type": "Yfilerename",
          "commitMessage": "HDFS-8925. Move BlockReaderLocal to hdfs-client. Contributed by Mingliang Liu.\n",
          "commitDate": "28/08/15 2:38 PM",
          "commitName": "e2c9b288b223b9fd82dc12018936e13128413492",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "28/08/15 2:21 PM",
          "commitNameOld": "b94b56806d3d6e04984e229b479f7ac15b62bbfa",
          "commitAuthorOld": "Colin Patrick Mccabe",
          "daysBetweenCommits": 0.01,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,77 +1,77 @@\n   static BlockReaderLocalLegacy newBlockReader(DfsClientConf conf,\n       UserGroupInformation userGroupInformation,\n       Configuration configuration, String file, ExtendedBlock blk,\n       Token\u003cBlockTokenIdentifier\u003e token, DatanodeInfo node, \n       long startOffset, long length, StorageType storageType)\n       throws IOException {\n     final ShortCircuitConf scConf \u003d conf.getShortCircuitConf();\n     LocalDatanodeInfo localDatanodeInfo \u003d getLocalDatanodeInfo(node\n         .getIpcPort());\n     // check the cache first\n     BlockLocalPathInfo pathinfo \u003d localDatanodeInfo.getBlockLocalPathInfo(blk);\n     if (pathinfo \u003d\u003d null) {\n       if (userGroupInformation \u003d\u003d null) {\n         userGroupInformation \u003d UserGroupInformation.getCurrentUser();\n       }\n       pathinfo \u003d getBlockPathInfo(userGroupInformation, blk, node,\n           configuration, conf.getSocketTimeout(), token,\n           conf.isConnectToDnViaHostname(), storageType);\n     }\n \n     // check to see if the file exists. It may so happen that the\n     // HDFS file has been deleted and this block-lookup is occurring\n     // on behalf of a new HDFS file. This time, the block file could\n     // be residing in a different portion of the fs.data.dir directory.\n     // In this case, we remove this entry from the cache. The next\n     // call to this method will re-populate the cache.\n     FileInputStream dataIn \u003d null;\n     FileInputStream checksumIn \u003d null;\n     BlockReaderLocalLegacy localBlockReader \u003d null;\n     final boolean skipChecksumCheck \u003d scConf.isSkipShortCircuitChecksums()\n         || storageType.isTransient();\n     try {\n       // get a local file system\n       File blkfile \u003d new File(pathinfo.getBlockPath());\n       dataIn \u003d new FileInputStream(blkfile);\n \n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"New BlockReaderLocalLegacy for file \" + blkfile + \" of size \"\n             + blkfile.length() + \" startOffset \" + startOffset + \" length \"\n             + length + \" short circuit checksum \" + !skipChecksumCheck);\n       }\n \n       if (!skipChecksumCheck) {\n         // get the metadata file\n         File metafile \u003d new File(pathinfo.getMetaPath());\n         checksumIn \u003d new FileInputStream(metafile);\n \n         final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(\n             new DataInputStream(checksumIn), blk);\n         long firstChunkOffset \u003d startOffset\n             - (startOffset % checksum.getBytesPerChecksum());\n         localBlockReader \u003d new BlockReaderLocalLegacy(scConf, file, blk, token,\n             startOffset, length, pathinfo, checksum, true, dataIn,\n             firstChunkOffset, checksumIn);\n       } else {\n         localBlockReader \u003d new BlockReaderLocalLegacy(scConf, file, blk, token,\n             startOffset, length, pathinfo, dataIn);\n       }\n     } catch (IOException e) {\n       // remove from cache\n       localDatanodeInfo.removeBlockLocalPathInfo(blk);\n-      DFSClient.LOG.warn(\"BlockReaderLocalLegacy: Removing \" + blk\n+      LOG.warn(\"BlockReaderLocalLegacy: Removing \" + blk\n           + \" from cache because local file \" + pathinfo.getBlockPath()\n           + \" could not be opened.\");\n       throw e;\n     } finally {\n       if (localBlockReader \u003d\u003d null) {\n         if (dataIn !\u003d null) {\n           dataIn.close();\n         }\n         if (checksumIn !\u003d null) {\n           checksumIn.close();\n         }\n       }\n     }\n     return localBlockReader;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static BlockReaderLocalLegacy newBlockReader(DfsClientConf conf,\n      UserGroupInformation userGroupInformation,\n      Configuration configuration, String file, ExtendedBlock blk,\n      Token\u003cBlockTokenIdentifier\u003e token, DatanodeInfo node, \n      long startOffset, long length, StorageType storageType)\n      throws IOException {\n    final ShortCircuitConf scConf \u003d conf.getShortCircuitConf();\n    LocalDatanodeInfo localDatanodeInfo \u003d getLocalDatanodeInfo(node\n        .getIpcPort());\n    // check the cache first\n    BlockLocalPathInfo pathinfo \u003d localDatanodeInfo.getBlockLocalPathInfo(blk);\n    if (pathinfo \u003d\u003d null) {\n      if (userGroupInformation \u003d\u003d null) {\n        userGroupInformation \u003d UserGroupInformation.getCurrentUser();\n      }\n      pathinfo \u003d getBlockPathInfo(userGroupInformation, blk, node,\n          configuration, conf.getSocketTimeout(), token,\n          conf.isConnectToDnViaHostname(), storageType);\n    }\n\n    // check to see if the file exists. It may so happen that the\n    // HDFS file has been deleted and this block-lookup is occurring\n    // on behalf of a new HDFS file. This time, the block file could\n    // be residing in a different portion of the fs.data.dir directory.\n    // In this case, we remove this entry from the cache. The next\n    // call to this method will re-populate the cache.\n    FileInputStream dataIn \u003d null;\n    FileInputStream checksumIn \u003d null;\n    BlockReaderLocalLegacy localBlockReader \u003d null;\n    final boolean skipChecksumCheck \u003d scConf.isSkipShortCircuitChecksums()\n        || storageType.isTransient();\n    try {\n      // get a local file system\n      File blkfile \u003d new File(pathinfo.getBlockPath());\n      dataIn \u003d new FileInputStream(blkfile);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"New BlockReaderLocalLegacy for file \" + blkfile + \" of size \"\n            + blkfile.length() + \" startOffset \" + startOffset + \" length \"\n            + length + \" short circuit checksum \" + !skipChecksumCheck);\n      }\n\n      if (!skipChecksumCheck) {\n        // get the metadata file\n        File metafile \u003d new File(pathinfo.getMetaPath());\n        checksumIn \u003d new FileInputStream(metafile);\n\n        final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(\n            new DataInputStream(checksumIn), blk);\n        long firstChunkOffset \u003d startOffset\n            - (startOffset % checksum.getBytesPerChecksum());\n        localBlockReader \u003d new BlockReaderLocalLegacy(scConf, file, blk, token,\n            startOffset, length, pathinfo, checksum, true, dataIn,\n            firstChunkOffset, checksumIn);\n      } else {\n        localBlockReader \u003d new BlockReaderLocalLegacy(scConf, file, blk, token,\n            startOffset, length, pathinfo, dataIn);\n      }\n    } catch (IOException e) {\n      // remove from cache\n      localDatanodeInfo.removeBlockLocalPathInfo(blk);\n      LOG.warn(\"BlockReaderLocalLegacy: Removing \" + blk\n          + \" from cache because local file \" + pathinfo.getBlockPath()\n          + \" could not be opened.\");\n      throw e;\n    } finally {\n      if (localBlockReader \u003d\u003d null) {\n        if (dataIn !\u003d null) {\n          dataIn.close();\n        }\n        if (checksumIn !\u003d null) {\n          checksumIn.close();\n        }\n      }\n    }\n    return localBlockReader;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocalLegacy.java",
          "extendedDetails": {
            "oldPath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocalLegacy.java",
            "newPath": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocalLegacy.java"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-8925. Move BlockReaderLocal to hdfs-client. Contributed by Mingliang Liu.\n",
          "commitDate": "28/08/15 2:38 PM",
          "commitName": "e2c9b288b223b9fd82dc12018936e13128413492",
          "commitAuthor": "Haohui Mai",
          "commitDateOld": "28/08/15 2:21 PM",
          "commitNameOld": "b94b56806d3d6e04984e229b479f7ac15b62bbfa",
          "commitAuthorOld": "Colin Patrick Mccabe",
          "daysBetweenCommits": 0.01,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,77 +1,77 @@\n   static BlockReaderLocalLegacy newBlockReader(DfsClientConf conf,\n       UserGroupInformation userGroupInformation,\n       Configuration configuration, String file, ExtendedBlock blk,\n       Token\u003cBlockTokenIdentifier\u003e token, DatanodeInfo node, \n       long startOffset, long length, StorageType storageType)\n       throws IOException {\n     final ShortCircuitConf scConf \u003d conf.getShortCircuitConf();\n     LocalDatanodeInfo localDatanodeInfo \u003d getLocalDatanodeInfo(node\n         .getIpcPort());\n     // check the cache first\n     BlockLocalPathInfo pathinfo \u003d localDatanodeInfo.getBlockLocalPathInfo(blk);\n     if (pathinfo \u003d\u003d null) {\n       if (userGroupInformation \u003d\u003d null) {\n         userGroupInformation \u003d UserGroupInformation.getCurrentUser();\n       }\n       pathinfo \u003d getBlockPathInfo(userGroupInformation, blk, node,\n           configuration, conf.getSocketTimeout(), token,\n           conf.isConnectToDnViaHostname(), storageType);\n     }\n \n     // check to see if the file exists. It may so happen that the\n     // HDFS file has been deleted and this block-lookup is occurring\n     // on behalf of a new HDFS file. This time, the block file could\n     // be residing in a different portion of the fs.data.dir directory.\n     // In this case, we remove this entry from the cache. The next\n     // call to this method will re-populate the cache.\n     FileInputStream dataIn \u003d null;\n     FileInputStream checksumIn \u003d null;\n     BlockReaderLocalLegacy localBlockReader \u003d null;\n     final boolean skipChecksumCheck \u003d scConf.isSkipShortCircuitChecksums()\n         || storageType.isTransient();\n     try {\n       // get a local file system\n       File blkfile \u003d new File(pathinfo.getBlockPath());\n       dataIn \u003d new FileInputStream(blkfile);\n \n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"New BlockReaderLocalLegacy for file \" + blkfile + \" of size \"\n             + blkfile.length() + \" startOffset \" + startOffset + \" length \"\n             + length + \" short circuit checksum \" + !skipChecksumCheck);\n       }\n \n       if (!skipChecksumCheck) {\n         // get the metadata file\n         File metafile \u003d new File(pathinfo.getMetaPath());\n         checksumIn \u003d new FileInputStream(metafile);\n \n         final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(\n             new DataInputStream(checksumIn), blk);\n         long firstChunkOffset \u003d startOffset\n             - (startOffset % checksum.getBytesPerChecksum());\n         localBlockReader \u003d new BlockReaderLocalLegacy(scConf, file, blk, token,\n             startOffset, length, pathinfo, checksum, true, dataIn,\n             firstChunkOffset, checksumIn);\n       } else {\n         localBlockReader \u003d new BlockReaderLocalLegacy(scConf, file, blk, token,\n             startOffset, length, pathinfo, dataIn);\n       }\n     } catch (IOException e) {\n       // remove from cache\n       localDatanodeInfo.removeBlockLocalPathInfo(blk);\n-      DFSClient.LOG.warn(\"BlockReaderLocalLegacy: Removing \" + blk\n+      LOG.warn(\"BlockReaderLocalLegacy: Removing \" + blk\n           + \" from cache because local file \" + pathinfo.getBlockPath()\n           + \" could not be opened.\");\n       throw e;\n     } finally {\n       if (localBlockReader \u003d\u003d null) {\n         if (dataIn !\u003d null) {\n           dataIn.close();\n         }\n         if (checksumIn !\u003d null) {\n           checksumIn.close();\n         }\n       }\n     }\n     return localBlockReader;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static BlockReaderLocalLegacy newBlockReader(DfsClientConf conf,\n      UserGroupInformation userGroupInformation,\n      Configuration configuration, String file, ExtendedBlock blk,\n      Token\u003cBlockTokenIdentifier\u003e token, DatanodeInfo node, \n      long startOffset, long length, StorageType storageType)\n      throws IOException {\n    final ShortCircuitConf scConf \u003d conf.getShortCircuitConf();\n    LocalDatanodeInfo localDatanodeInfo \u003d getLocalDatanodeInfo(node\n        .getIpcPort());\n    // check the cache first\n    BlockLocalPathInfo pathinfo \u003d localDatanodeInfo.getBlockLocalPathInfo(blk);\n    if (pathinfo \u003d\u003d null) {\n      if (userGroupInformation \u003d\u003d null) {\n        userGroupInformation \u003d UserGroupInformation.getCurrentUser();\n      }\n      pathinfo \u003d getBlockPathInfo(userGroupInformation, blk, node,\n          configuration, conf.getSocketTimeout(), token,\n          conf.isConnectToDnViaHostname(), storageType);\n    }\n\n    // check to see if the file exists. It may so happen that the\n    // HDFS file has been deleted and this block-lookup is occurring\n    // on behalf of a new HDFS file. This time, the block file could\n    // be residing in a different portion of the fs.data.dir directory.\n    // In this case, we remove this entry from the cache. The next\n    // call to this method will re-populate the cache.\n    FileInputStream dataIn \u003d null;\n    FileInputStream checksumIn \u003d null;\n    BlockReaderLocalLegacy localBlockReader \u003d null;\n    final boolean skipChecksumCheck \u003d scConf.isSkipShortCircuitChecksums()\n        || storageType.isTransient();\n    try {\n      // get a local file system\n      File blkfile \u003d new File(pathinfo.getBlockPath());\n      dataIn \u003d new FileInputStream(blkfile);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"New BlockReaderLocalLegacy for file \" + blkfile + \" of size \"\n            + blkfile.length() + \" startOffset \" + startOffset + \" length \"\n            + length + \" short circuit checksum \" + !skipChecksumCheck);\n      }\n\n      if (!skipChecksumCheck) {\n        // get the metadata file\n        File metafile \u003d new File(pathinfo.getMetaPath());\n        checksumIn \u003d new FileInputStream(metafile);\n\n        final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(\n            new DataInputStream(checksumIn), blk);\n        long firstChunkOffset \u003d startOffset\n            - (startOffset % checksum.getBytesPerChecksum());\n        localBlockReader \u003d new BlockReaderLocalLegacy(scConf, file, blk, token,\n            startOffset, length, pathinfo, checksum, true, dataIn,\n            firstChunkOffset, checksumIn);\n      } else {\n        localBlockReader \u003d new BlockReaderLocalLegacy(scConf, file, blk, token,\n            startOffset, length, pathinfo, dataIn);\n      }\n    } catch (IOException e) {\n      // remove from cache\n      localDatanodeInfo.removeBlockLocalPathInfo(blk);\n      LOG.warn(\"BlockReaderLocalLegacy: Removing \" + blk\n          + \" from cache because local file \" + pathinfo.getBlockPath()\n          + \" could not be opened.\");\n      throw e;\n    } finally {\n      if (localBlockReader \u003d\u003d null) {\n        if (dataIn !\u003d null) {\n          dataIn.close();\n        }\n        if (checksumIn !\u003d null) {\n          checksumIn.close();\n        }\n      }\n    }\n    return localBlockReader;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocalLegacy.java",
          "extendedDetails": {}
        }
      ]
    },
    "2cc9514ad643ae49d30524743420ee9744e571bd": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-8100. Refactor DFSClient.Conf to a standalone class and separates short-circuit related conf to ShortCircuitConf.\n",
      "commitDate": "10/04/15 2:48 PM",
      "commitName": "2cc9514ad643ae49d30524743420ee9744e571bd",
      "commitAuthor": "Tsz-Wo Nicholas Sze",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-8100. Refactor DFSClient.Conf to a standalone class and separates short-circuit related conf to ShortCircuitConf.\n",
          "commitDate": "10/04/15 2:48 PM",
          "commitName": "2cc9514ad643ae49d30524743420ee9744e571bd",
          "commitAuthor": "Tsz-Wo Nicholas Sze",
          "commitDateOld": "07/04/15 1:59 PM",
          "commitNameOld": "571a1ce9d037d99e7c9042bcb77ae7a2c4daf6d3",
          "commitAuthorOld": "Tsz-Wo Nicholas Sze",
          "daysBetweenCommits": 3.03,
          "commitsBetweenForRepo": 48,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,76 +1,77 @@\n-  static BlockReaderLocalLegacy newBlockReader(DFSClient.Conf conf,\n+  static BlockReaderLocalLegacy newBlockReader(DfsClientConf conf,\n       UserGroupInformation userGroupInformation,\n       Configuration configuration, String file, ExtendedBlock blk,\n       Token\u003cBlockTokenIdentifier\u003e token, DatanodeInfo node, \n       long startOffset, long length, StorageType storageType)\n       throws IOException {\n+    final ShortCircuitConf scConf \u003d conf.getShortCircuitConf();\n     LocalDatanodeInfo localDatanodeInfo \u003d getLocalDatanodeInfo(node\n         .getIpcPort());\n     // check the cache first\n     BlockLocalPathInfo pathinfo \u003d localDatanodeInfo.getBlockLocalPathInfo(blk);\n     if (pathinfo \u003d\u003d null) {\n       if (userGroupInformation \u003d\u003d null) {\n         userGroupInformation \u003d UserGroupInformation.getCurrentUser();\n       }\n       pathinfo \u003d getBlockPathInfo(userGroupInformation, blk, node,\n-          configuration, conf.socketTimeout, token,\n-          conf.connectToDnViaHostname, storageType);\n+          configuration, conf.getSocketTimeout(), token,\n+          conf.isConnectToDnViaHostname(), storageType);\n     }\n \n     // check to see if the file exists. It may so happen that the\n     // HDFS file has been deleted and this block-lookup is occurring\n     // on behalf of a new HDFS file. This time, the block file could\n     // be residing in a different portion of the fs.data.dir directory.\n     // In this case, we remove this entry from the cache. The next\n     // call to this method will re-populate the cache.\n     FileInputStream dataIn \u003d null;\n     FileInputStream checksumIn \u003d null;\n     BlockReaderLocalLegacy localBlockReader \u003d null;\n-    boolean skipChecksumCheck \u003d conf.skipShortCircuitChecksums ||\n-        storageType.isTransient();\n+    final boolean skipChecksumCheck \u003d scConf.isSkipShortCircuitChecksums()\n+        || storageType.isTransient();\n     try {\n       // get a local file system\n       File blkfile \u003d new File(pathinfo.getBlockPath());\n       dataIn \u003d new FileInputStream(blkfile);\n \n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"New BlockReaderLocalLegacy for file \" + blkfile + \" of size \"\n             + blkfile.length() + \" startOffset \" + startOffset + \" length \"\n             + length + \" short circuit checksum \" + !skipChecksumCheck);\n       }\n \n       if (!skipChecksumCheck) {\n         // get the metadata file\n         File metafile \u003d new File(pathinfo.getMetaPath());\n         checksumIn \u003d new FileInputStream(metafile);\n \n         final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(\n             new DataInputStream(checksumIn), blk);\n         long firstChunkOffset \u003d startOffset\n             - (startOffset % checksum.getBytesPerChecksum());\n-        localBlockReader \u003d new BlockReaderLocalLegacy(conf, file, blk, token,\n+        localBlockReader \u003d new BlockReaderLocalLegacy(scConf, file, blk, token,\n             startOffset, length, pathinfo, checksum, true, dataIn,\n             firstChunkOffset, checksumIn);\n       } else {\n-        localBlockReader \u003d new BlockReaderLocalLegacy(conf, file, blk, token,\n+        localBlockReader \u003d new BlockReaderLocalLegacy(scConf, file, blk, token,\n             startOffset, length, pathinfo, dataIn);\n       }\n     } catch (IOException e) {\n       // remove from cache\n       localDatanodeInfo.removeBlockLocalPathInfo(blk);\n       DFSClient.LOG.warn(\"BlockReaderLocalLegacy: Removing \" + blk\n           + \" from cache because local file \" + pathinfo.getBlockPath()\n           + \" could not be opened.\");\n       throw e;\n     } finally {\n       if (localBlockReader \u003d\u003d null) {\n         if (dataIn !\u003d null) {\n           dataIn.close();\n         }\n         if (checksumIn !\u003d null) {\n           checksumIn.close();\n         }\n       }\n     }\n     return localBlockReader;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static BlockReaderLocalLegacy newBlockReader(DfsClientConf conf,\n      UserGroupInformation userGroupInformation,\n      Configuration configuration, String file, ExtendedBlock blk,\n      Token\u003cBlockTokenIdentifier\u003e token, DatanodeInfo node, \n      long startOffset, long length, StorageType storageType)\n      throws IOException {\n    final ShortCircuitConf scConf \u003d conf.getShortCircuitConf();\n    LocalDatanodeInfo localDatanodeInfo \u003d getLocalDatanodeInfo(node\n        .getIpcPort());\n    // check the cache first\n    BlockLocalPathInfo pathinfo \u003d localDatanodeInfo.getBlockLocalPathInfo(blk);\n    if (pathinfo \u003d\u003d null) {\n      if (userGroupInformation \u003d\u003d null) {\n        userGroupInformation \u003d UserGroupInformation.getCurrentUser();\n      }\n      pathinfo \u003d getBlockPathInfo(userGroupInformation, blk, node,\n          configuration, conf.getSocketTimeout(), token,\n          conf.isConnectToDnViaHostname(), storageType);\n    }\n\n    // check to see if the file exists. It may so happen that the\n    // HDFS file has been deleted and this block-lookup is occurring\n    // on behalf of a new HDFS file. This time, the block file could\n    // be residing in a different portion of the fs.data.dir directory.\n    // In this case, we remove this entry from the cache. The next\n    // call to this method will re-populate the cache.\n    FileInputStream dataIn \u003d null;\n    FileInputStream checksumIn \u003d null;\n    BlockReaderLocalLegacy localBlockReader \u003d null;\n    final boolean skipChecksumCheck \u003d scConf.isSkipShortCircuitChecksums()\n        || storageType.isTransient();\n    try {\n      // get a local file system\n      File blkfile \u003d new File(pathinfo.getBlockPath());\n      dataIn \u003d new FileInputStream(blkfile);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"New BlockReaderLocalLegacy for file \" + blkfile + \" of size \"\n            + blkfile.length() + \" startOffset \" + startOffset + \" length \"\n            + length + \" short circuit checksum \" + !skipChecksumCheck);\n      }\n\n      if (!skipChecksumCheck) {\n        // get the metadata file\n        File metafile \u003d new File(pathinfo.getMetaPath());\n        checksumIn \u003d new FileInputStream(metafile);\n\n        final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(\n            new DataInputStream(checksumIn), blk);\n        long firstChunkOffset \u003d startOffset\n            - (startOffset % checksum.getBytesPerChecksum());\n        localBlockReader \u003d new BlockReaderLocalLegacy(scConf, file, blk, token,\n            startOffset, length, pathinfo, checksum, true, dataIn,\n            firstChunkOffset, checksumIn);\n      } else {\n        localBlockReader \u003d new BlockReaderLocalLegacy(scConf, file, blk, token,\n            startOffset, length, pathinfo, dataIn);\n      }\n    } catch (IOException e) {\n      // remove from cache\n      localDatanodeInfo.removeBlockLocalPathInfo(blk);\n      DFSClient.LOG.warn(\"BlockReaderLocalLegacy: Removing \" + blk\n          + \" from cache because local file \" + pathinfo.getBlockPath()\n          + \" could not be opened.\");\n      throw e;\n    } finally {\n      if (localBlockReader \u003d\u003d null) {\n        if (dataIn !\u003d null) {\n          dataIn.close();\n        }\n        if (checksumIn !\u003d null) {\n          checksumIn.close();\n        }\n      }\n    }\n    return localBlockReader;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocalLegacy.java",
          "extendedDetails": {
            "oldValue": "[conf-DFSClient.Conf, userGroupInformation-UserGroupInformation, configuration-Configuration, file-String, blk-ExtendedBlock, token-Token\u003cBlockTokenIdentifier\u003e, node-DatanodeInfo, startOffset-long, length-long, storageType-StorageType]",
            "newValue": "[conf-DfsClientConf, userGroupInformation-UserGroupInformation, configuration-Configuration, file-String, blk-ExtendedBlock, token-Token\u003cBlockTokenIdentifier\u003e, node-DatanodeInfo, startOffset-long, length-long, storageType-StorageType]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-8100. Refactor DFSClient.Conf to a standalone class and separates short-circuit related conf to ShortCircuitConf.\n",
          "commitDate": "10/04/15 2:48 PM",
          "commitName": "2cc9514ad643ae49d30524743420ee9744e571bd",
          "commitAuthor": "Tsz-Wo Nicholas Sze",
          "commitDateOld": "07/04/15 1:59 PM",
          "commitNameOld": "571a1ce9d037d99e7c9042bcb77ae7a2c4daf6d3",
          "commitAuthorOld": "Tsz-Wo Nicholas Sze",
          "daysBetweenCommits": 3.03,
          "commitsBetweenForRepo": 48,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,76 +1,77 @@\n-  static BlockReaderLocalLegacy newBlockReader(DFSClient.Conf conf,\n+  static BlockReaderLocalLegacy newBlockReader(DfsClientConf conf,\n       UserGroupInformation userGroupInformation,\n       Configuration configuration, String file, ExtendedBlock blk,\n       Token\u003cBlockTokenIdentifier\u003e token, DatanodeInfo node, \n       long startOffset, long length, StorageType storageType)\n       throws IOException {\n+    final ShortCircuitConf scConf \u003d conf.getShortCircuitConf();\n     LocalDatanodeInfo localDatanodeInfo \u003d getLocalDatanodeInfo(node\n         .getIpcPort());\n     // check the cache first\n     BlockLocalPathInfo pathinfo \u003d localDatanodeInfo.getBlockLocalPathInfo(blk);\n     if (pathinfo \u003d\u003d null) {\n       if (userGroupInformation \u003d\u003d null) {\n         userGroupInformation \u003d UserGroupInformation.getCurrentUser();\n       }\n       pathinfo \u003d getBlockPathInfo(userGroupInformation, blk, node,\n-          configuration, conf.socketTimeout, token,\n-          conf.connectToDnViaHostname, storageType);\n+          configuration, conf.getSocketTimeout(), token,\n+          conf.isConnectToDnViaHostname(), storageType);\n     }\n \n     // check to see if the file exists. It may so happen that the\n     // HDFS file has been deleted and this block-lookup is occurring\n     // on behalf of a new HDFS file. This time, the block file could\n     // be residing in a different portion of the fs.data.dir directory.\n     // In this case, we remove this entry from the cache. The next\n     // call to this method will re-populate the cache.\n     FileInputStream dataIn \u003d null;\n     FileInputStream checksumIn \u003d null;\n     BlockReaderLocalLegacy localBlockReader \u003d null;\n-    boolean skipChecksumCheck \u003d conf.skipShortCircuitChecksums ||\n-        storageType.isTransient();\n+    final boolean skipChecksumCheck \u003d scConf.isSkipShortCircuitChecksums()\n+        || storageType.isTransient();\n     try {\n       // get a local file system\n       File blkfile \u003d new File(pathinfo.getBlockPath());\n       dataIn \u003d new FileInputStream(blkfile);\n \n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"New BlockReaderLocalLegacy for file \" + blkfile + \" of size \"\n             + blkfile.length() + \" startOffset \" + startOffset + \" length \"\n             + length + \" short circuit checksum \" + !skipChecksumCheck);\n       }\n \n       if (!skipChecksumCheck) {\n         // get the metadata file\n         File metafile \u003d new File(pathinfo.getMetaPath());\n         checksumIn \u003d new FileInputStream(metafile);\n \n         final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(\n             new DataInputStream(checksumIn), blk);\n         long firstChunkOffset \u003d startOffset\n             - (startOffset % checksum.getBytesPerChecksum());\n-        localBlockReader \u003d new BlockReaderLocalLegacy(conf, file, blk, token,\n+        localBlockReader \u003d new BlockReaderLocalLegacy(scConf, file, blk, token,\n             startOffset, length, pathinfo, checksum, true, dataIn,\n             firstChunkOffset, checksumIn);\n       } else {\n-        localBlockReader \u003d new BlockReaderLocalLegacy(conf, file, blk, token,\n+        localBlockReader \u003d new BlockReaderLocalLegacy(scConf, file, blk, token,\n             startOffset, length, pathinfo, dataIn);\n       }\n     } catch (IOException e) {\n       // remove from cache\n       localDatanodeInfo.removeBlockLocalPathInfo(blk);\n       DFSClient.LOG.warn(\"BlockReaderLocalLegacy: Removing \" + blk\n           + \" from cache because local file \" + pathinfo.getBlockPath()\n           + \" could not be opened.\");\n       throw e;\n     } finally {\n       if (localBlockReader \u003d\u003d null) {\n         if (dataIn !\u003d null) {\n           dataIn.close();\n         }\n         if (checksumIn !\u003d null) {\n           checksumIn.close();\n         }\n       }\n     }\n     return localBlockReader;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static BlockReaderLocalLegacy newBlockReader(DfsClientConf conf,\n      UserGroupInformation userGroupInformation,\n      Configuration configuration, String file, ExtendedBlock blk,\n      Token\u003cBlockTokenIdentifier\u003e token, DatanodeInfo node, \n      long startOffset, long length, StorageType storageType)\n      throws IOException {\n    final ShortCircuitConf scConf \u003d conf.getShortCircuitConf();\n    LocalDatanodeInfo localDatanodeInfo \u003d getLocalDatanodeInfo(node\n        .getIpcPort());\n    // check the cache first\n    BlockLocalPathInfo pathinfo \u003d localDatanodeInfo.getBlockLocalPathInfo(blk);\n    if (pathinfo \u003d\u003d null) {\n      if (userGroupInformation \u003d\u003d null) {\n        userGroupInformation \u003d UserGroupInformation.getCurrentUser();\n      }\n      pathinfo \u003d getBlockPathInfo(userGroupInformation, blk, node,\n          configuration, conf.getSocketTimeout(), token,\n          conf.isConnectToDnViaHostname(), storageType);\n    }\n\n    // check to see if the file exists. It may so happen that the\n    // HDFS file has been deleted and this block-lookup is occurring\n    // on behalf of a new HDFS file. This time, the block file could\n    // be residing in a different portion of the fs.data.dir directory.\n    // In this case, we remove this entry from the cache. The next\n    // call to this method will re-populate the cache.\n    FileInputStream dataIn \u003d null;\n    FileInputStream checksumIn \u003d null;\n    BlockReaderLocalLegacy localBlockReader \u003d null;\n    final boolean skipChecksumCheck \u003d scConf.isSkipShortCircuitChecksums()\n        || storageType.isTransient();\n    try {\n      // get a local file system\n      File blkfile \u003d new File(pathinfo.getBlockPath());\n      dataIn \u003d new FileInputStream(blkfile);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"New BlockReaderLocalLegacy for file \" + blkfile + \" of size \"\n            + blkfile.length() + \" startOffset \" + startOffset + \" length \"\n            + length + \" short circuit checksum \" + !skipChecksumCheck);\n      }\n\n      if (!skipChecksumCheck) {\n        // get the metadata file\n        File metafile \u003d new File(pathinfo.getMetaPath());\n        checksumIn \u003d new FileInputStream(metafile);\n\n        final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(\n            new DataInputStream(checksumIn), blk);\n        long firstChunkOffset \u003d startOffset\n            - (startOffset % checksum.getBytesPerChecksum());\n        localBlockReader \u003d new BlockReaderLocalLegacy(scConf, file, blk, token,\n            startOffset, length, pathinfo, checksum, true, dataIn,\n            firstChunkOffset, checksumIn);\n      } else {\n        localBlockReader \u003d new BlockReaderLocalLegacy(scConf, file, blk, token,\n            startOffset, length, pathinfo, dataIn);\n      }\n    } catch (IOException e) {\n      // remove from cache\n      localDatanodeInfo.removeBlockLocalPathInfo(blk);\n      DFSClient.LOG.warn(\"BlockReaderLocalLegacy: Removing \" + blk\n          + \" from cache because local file \" + pathinfo.getBlockPath()\n          + \" could not be opened.\");\n      throw e;\n    } finally {\n      if (localBlockReader \u003d\u003d null) {\n        if (dataIn !\u003d null) {\n          dataIn.close();\n        }\n        if (checksumIn !\u003d null) {\n          checksumIn.close();\n        }\n      }\n    }\n    return localBlockReader;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocalLegacy.java",
          "extendedDetails": {}
        }
      ]
    },
    "463aec11718e47d4aabb86a7a539cb973460aae6": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-6934. Move checksum computation off the hot path when writing to RAM disk. Contributed by Chris Nauroth.\n",
      "commitDate": "27/10/14 9:38 AM",
      "commitName": "463aec11718e47d4aabb86a7a539cb973460aae6",
      "commitAuthor": "cnauroth",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-6934. Move checksum computation off the hot path when writing to RAM disk. Contributed by Chris Nauroth.\n",
          "commitDate": "27/10/14 9:38 AM",
          "commitName": "463aec11718e47d4aabb86a7a539cb973460aae6",
          "commitAuthor": "cnauroth",
          "commitDateOld": "03/10/14 1:35 PM",
          "commitNameOld": "7f6ed7fe365166e8075359f1d0ad035fa876c70f",
          "commitAuthorOld": "Colin Patrick Mccabe",
          "daysBetweenCommits": 23.84,
          "commitsBetweenForRepo": 188,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,81 +1,76 @@\n   static BlockReaderLocalLegacy newBlockReader(DFSClient.Conf conf,\n       UserGroupInformation userGroupInformation,\n       Configuration configuration, String file, ExtendedBlock blk,\n       Token\u003cBlockTokenIdentifier\u003e token, DatanodeInfo node, \n-      long startOffset, long length) throws IOException {\n+      long startOffset, long length, StorageType storageType)\n+      throws IOException {\n     LocalDatanodeInfo localDatanodeInfo \u003d getLocalDatanodeInfo(node\n         .getIpcPort());\n     // check the cache first\n     BlockLocalPathInfo pathinfo \u003d localDatanodeInfo.getBlockLocalPathInfo(blk);\n     if (pathinfo \u003d\u003d null) {\n       if (userGroupInformation \u003d\u003d null) {\n         userGroupInformation \u003d UserGroupInformation.getCurrentUser();\n       }\n       pathinfo \u003d getBlockPathInfo(userGroupInformation, blk, node,\n           configuration, conf.socketTimeout, token,\n-          conf.connectToDnViaHostname);\n+          conf.connectToDnViaHostname, storageType);\n     }\n \n     // check to see if the file exists. It may so happen that the\n     // HDFS file has been deleted and this block-lookup is occurring\n     // on behalf of a new HDFS file. This time, the block file could\n     // be residing in a different portion of the fs.data.dir directory.\n     // In this case, we remove this entry from the cache. The next\n     // call to this method will re-populate the cache.\n     FileInputStream dataIn \u003d null;\n     FileInputStream checksumIn \u003d null;\n     BlockReaderLocalLegacy localBlockReader \u003d null;\n-    boolean skipChecksumCheck \u003d conf.skipShortCircuitChecksums;\n+    boolean skipChecksumCheck \u003d conf.skipShortCircuitChecksums ||\n+        storageType.isTransient();\n     try {\n       // get a local file system\n       File blkfile \u003d new File(pathinfo.getBlockPath());\n       dataIn \u003d new FileInputStream(blkfile);\n \n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"New BlockReaderLocalLegacy for file \" + blkfile + \" of size \"\n             + blkfile.length() + \" startOffset \" + startOffset + \" length \"\n             + length + \" short circuit checksum \" + !skipChecksumCheck);\n       }\n \n       if (!skipChecksumCheck) {\n         // get the metadata file\n         File metafile \u003d new File(pathinfo.getMetaPath());\n         checksumIn \u003d new FileInputStream(metafile);\n \n-        // read and handle the common header here. For now just a version\n-        BlockMetadataHeader header \u003d BlockMetadataHeader\n-            .readHeader(new DataInputStream(checksumIn));\n-        short version \u003d header.getVersion();\n-        if (version !\u003d BlockMetadataHeader.VERSION) {\n-          LOG.warn(\"Wrong version (\" + version + \") for metadata file for \"\n-              + blk + \" ignoring ...\");\n-        }\n-        DataChecksum checksum \u003d header.getChecksum();\n+        final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(\n+            new DataInputStream(checksumIn), blk);\n         long firstChunkOffset \u003d startOffset\n             - (startOffset % checksum.getBytesPerChecksum());\n         localBlockReader \u003d new BlockReaderLocalLegacy(conf, file, blk, token,\n             startOffset, length, pathinfo, checksum, true, dataIn,\n             firstChunkOffset, checksumIn);\n       } else {\n         localBlockReader \u003d new BlockReaderLocalLegacy(conf, file, blk, token,\n             startOffset, length, pathinfo, dataIn);\n       }\n     } catch (IOException e) {\n       // remove from cache\n       localDatanodeInfo.removeBlockLocalPathInfo(blk);\n       DFSClient.LOG.warn(\"BlockReaderLocalLegacy: Removing \" + blk\n           + \" from cache because local file \" + pathinfo.getBlockPath()\n           + \" could not be opened.\");\n       throw e;\n     } finally {\n       if (localBlockReader \u003d\u003d null) {\n         if (dataIn !\u003d null) {\n           dataIn.close();\n         }\n         if (checksumIn !\u003d null) {\n           checksumIn.close();\n         }\n       }\n     }\n     return localBlockReader;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static BlockReaderLocalLegacy newBlockReader(DFSClient.Conf conf,\n      UserGroupInformation userGroupInformation,\n      Configuration configuration, String file, ExtendedBlock blk,\n      Token\u003cBlockTokenIdentifier\u003e token, DatanodeInfo node, \n      long startOffset, long length, StorageType storageType)\n      throws IOException {\n    LocalDatanodeInfo localDatanodeInfo \u003d getLocalDatanodeInfo(node\n        .getIpcPort());\n    // check the cache first\n    BlockLocalPathInfo pathinfo \u003d localDatanodeInfo.getBlockLocalPathInfo(blk);\n    if (pathinfo \u003d\u003d null) {\n      if (userGroupInformation \u003d\u003d null) {\n        userGroupInformation \u003d UserGroupInformation.getCurrentUser();\n      }\n      pathinfo \u003d getBlockPathInfo(userGroupInformation, blk, node,\n          configuration, conf.socketTimeout, token,\n          conf.connectToDnViaHostname, storageType);\n    }\n\n    // check to see if the file exists. It may so happen that the\n    // HDFS file has been deleted and this block-lookup is occurring\n    // on behalf of a new HDFS file. This time, the block file could\n    // be residing in a different portion of the fs.data.dir directory.\n    // In this case, we remove this entry from the cache. The next\n    // call to this method will re-populate the cache.\n    FileInputStream dataIn \u003d null;\n    FileInputStream checksumIn \u003d null;\n    BlockReaderLocalLegacy localBlockReader \u003d null;\n    boolean skipChecksumCheck \u003d conf.skipShortCircuitChecksums ||\n        storageType.isTransient();\n    try {\n      // get a local file system\n      File blkfile \u003d new File(pathinfo.getBlockPath());\n      dataIn \u003d new FileInputStream(blkfile);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"New BlockReaderLocalLegacy for file \" + blkfile + \" of size \"\n            + blkfile.length() + \" startOffset \" + startOffset + \" length \"\n            + length + \" short circuit checksum \" + !skipChecksumCheck);\n      }\n\n      if (!skipChecksumCheck) {\n        // get the metadata file\n        File metafile \u003d new File(pathinfo.getMetaPath());\n        checksumIn \u003d new FileInputStream(metafile);\n\n        final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(\n            new DataInputStream(checksumIn), blk);\n        long firstChunkOffset \u003d startOffset\n            - (startOffset % checksum.getBytesPerChecksum());\n        localBlockReader \u003d new BlockReaderLocalLegacy(conf, file, blk, token,\n            startOffset, length, pathinfo, checksum, true, dataIn,\n            firstChunkOffset, checksumIn);\n      } else {\n        localBlockReader \u003d new BlockReaderLocalLegacy(conf, file, blk, token,\n            startOffset, length, pathinfo, dataIn);\n      }\n    } catch (IOException e) {\n      // remove from cache\n      localDatanodeInfo.removeBlockLocalPathInfo(blk);\n      DFSClient.LOG.warn(\"BlockReaderLocalLegacy: Removing \" + blk\n          + \" from cache because local file \" + pathinfo.getBlockPath()\n          + \" could not be opened.\");\n      throw e;\n    } finally {\n      if (localBlockReader \u003d\u003d null) {\n        if (dataIn !\u003d null) {\n          dataIn.close();\n        }\n        if (checksumIn !\u003d null) {\n          checksumIn.close();\n        }\n      }\n    }\n    return localBlockReader;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocalLegacy.java",
          "extendedDetails": {
            "oldValue": "[conf-DFSClient.Conf, userGroupInformation-UserGroupInformation, configuration-Configuration, file-String, blk-ExtendedBlock, token-Token\u003cBlockTokenIdentifier\u003e, node-DatanodeInfo, startOffset-long, length-long]",
            "newValue": "[conf-DFSClient.Conf, userGroupInformation-UserGroupInformation, configuration-Configuration, file-String, blk-ExtendedBlock, token-Token\u003cBlockTokenIdentifier\u003e, node-DatanodeInfo, startOffset-long, length-long, storageType-StorageType]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-6934. Move checksum computation off the hot path when writing to RAM disk. Contributed by Chris Nauroth.\n",
          "commitDate": "27/10/14 9:38 AM",
          "commitName": "463aec11718e47d4aabb86a7a539cb973460aae6",
          "commitAuthor": "cnauroth",
          "commitDateOld": "03/10/14 1:35 PM",
          "commitNameOld": "7f6ed7fe365166e8075359f1d0ad035fa876c70f",
          "commitAuthorOld": "Colin Patrick Mccabe",
          "daysBetweenCommits": 23.84,
          "commitsBetweenForRepo": 188,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,81 +1,76 @@\n   static BlockReaderLocalLegacy newBlockReader(DFSClient.Conf conf,\n       UserGroupInformation userGroupInformation,\n       Configuration configuration, String file, ExtendedBlock blk,\n       Token\u003cBlockTokenIdentifier\u003e token, DatanodeInfo node, \n-      long startOffset, long length) throws IOException {\n+      long startOffset, long length, StorageType storageType)\n+      throws IOException {\n     LocalDatanodeInfo localDatanodeInfo \u003d getLocalDatanodeInfo(node\n         .getIpcPort());\n     // check the cache first\n     BlockLocalPathInfo pathinfo \u003d localDatanodeInfo.getBlockLocalPathInfo(blk);\n     if (pathinfo \u003d\u003d null) {\n       if (userGroupInformation \u003d\u003d null) {\n         userGroupInformation \u003d UserGroupInformation.getCurrentUser();\n       }\n       pathinfo \u003d getBlockPathInfo(userGroupInformation, blk, node,\n           configuration, conf.socketTimeout, token,\n-          conf.connectToDnViaHostname);\n+          conf.connectToDnViaHostname, storageType);\n     }\n \n     // check to see if the file exists. It may so happen that the\n     // HDFS file has been deleted and this block-lookup is occurring\n     // on behalf of a new HDFS file. This time, the block file could\n     // be residing in a different portion of the fs.data.dir directory.\n     // In this case, we remove this entry from the cache. The next\n     // call to this method will re-populate the cache.\n     FileInputStream dataIn \u003d null;\n     FileInputStream checksumIn \u003d null;\n     BlockReaderLocalLegacy localBlockReader \u003d null;\n-    boolean skipChecksumCheck \u003d conf.skipShortCircuitChecksums;\n+    boolean skipChecksumCheck \u003d conf.skipShortCircuitChecksums ||\n+        storageType.isTransient();\n     try {\n       // get a local file system\n       File blkfile \u003d new File(pathinfo.getBlockPath());\n       dataIn \u003d new FileInputStream(blkfile);\n \n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"New BlockReaderLocalLegacy for file \" + blkfile + \" of size \"\n             + blkfile.length() + \" startOffset \" + startOffset + \" length \"\n             + length + \" short circuit checksum \" + !skipChecksumCheck);\n       }\n \n       if (!skipChecksumCheck) {\n         // get the metadata file\n         File metafile \u003d new File(pathinfo.getMetaPath());\n         checksumIn \u003d new FileInputStream(metafile);\n \n-        // read and handle the common header here. For now just a version\n-        BlockMetadataHeader header \u003d BlockMetadataHeader\n-            .readHeader(new DataInputStream(checksumIn));\n-        short version \u003d header.getVersion();\n-        if (version !\u003d BlockMetadataHeader.VERSION) {\n-          LOG.warn(\"Wrong version (\" + version + \") for metadata file for \"\n-              + blk + \" ignoring ...\");\n-        }\n-        DataChecksum checksum \u003d header.getChecksum();\n+        final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(\n+            new DataInputStream(checksumIn), blk);\n         long firstChunkOffset \u003d startOffset\n             - (startOffset % checksum.getBytesPerChecksum());\n         localBlockReader \u003d new BlockReaderLocalLegacy(conf, file, blk, token,\n             startOffset, length, pathinfo, checksum, true, dataIn,\n             firstChunkOffset, checksumIn);\n       } else {\n         localBlockReader \u003d new BlockReaderLocalLegacy(conf, file, blk, token,\n             startOffset, length, pathinfo, dataIn);\n       }\n     } catch (IOException e) {\n       // remove from cache\n       localDatanodeInfo.removeBlockLocalPathInfo(blk);\n       DFSClient.LOG.warn(\"BlockReaderLocalLegacy: Removing \" + blk\n           + \" from cache because local file \" + pathinfo.getBlockPath()\n           + \" could not be opened.\");\n       throw e;\n     } finally {\n       if (localBlockReader \u003d\u003d null) {\n         if (dataIn !\u003d null) {\n           dataIn.close();\n         }\n         if (checksumIn !\u003d null) {\n           checksumIn.close();\n         }\n       }\n     }\n     return localBlockReader;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static BlockReaderLocalLegacy newBlockReader(DFSClient.Conf conf,\n      UserGroupInformation userGroupInformation,\n      Configuration configuration, String file, ExtendedBlock blk,\n      Token\u003cBlockTokenIdentifier\u003e token, DatanodeInfo node, \n      long startOffset, long length, StorageType storageType)\n      throws IOException {\n    LocalDatanodeInfo localDatanodeInfo \u003d getLocalDatanodeInfo(node\n        .getIpcPort());\n    // check the cache first\n    BlockLocalPathInfo pathinfo \u003d localDatanodeInfo.getBlockLocalPathInfo(blk);\n    if (pathinfo \u003d\u003d null) {\n      if (userGroupInformation \u003d\u003d null) {\n        userGroupInformation \u003d UserGroupInformation.getCurrentUser();\n      }\n      pathinfo \u003d getBlockPathInfo(userGroupInformation, blk, node,\n          configuration, conf.socketTimeout, token,\n          conf.connectToDnViaHostname, storageType);\n    }\n\n    // check to see if the file exists. It may so happen that the\n    // HDFS file has been deleted and this block-lookup is occurring\n    // on behalf of a new HDFS file. This time, the block file could\n    // be residing in a different portion of the fs.data.dir directory.\n    // In this case, we remove this entry from the cache. The next\n    // call to this method will re-populate the cache.\n    FileInputStream dataIn \u003d null;\n    FileInputStream checksumIn \u003d null;\n    BlockReaderLocalLegacy localBlockReader \u003d null;\n    boolean skipChecksumCheck \u003d conf.skipShortCircuitChecksums ||\n        storageType.isTransient();\n    try {\n      // get a local file system\n      File blkfile \u003d new File(pathinfo.getBlockPath());\n      dataIn \u003d new FileInputStream(blkfile);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"New BlockReaderLocalLegacy for file \" + blkfile + \" of size \"\n            + blkfile.length() + \" startOffset \" + startOffset + \" length \"\n            + length + \" short circuit checksum \" + !skipChecksumCheck);\n      }\n\n      if (!skipChecksumCheck) {\n        // get the metadata file\n        File metafile \u003d new File(pathinfo.getMetaPath());\n        checksumIn \u003d new FileInputStream(metafile);\n\n        final DataChecksum checksum \u003d BlockMetadataHeader.readDataChecksum(\n            new DataInputStream(checksumIn), blk);\n        long firstChunkOffset \u003d startOffset\n            - (startOffset % checksum.getBytesPerChecksum());\n        localBlockReader \u003d new BlockReaderLocalLegacy(conf, file, blk, token,\n            startOffset, length, pathinfo, checksum, true, dataIn,\n            firstChunkOffset, checksumIn);\n      } else {\n        localBlockReader \u003d new BlockReaderLocalLegacy(conf, file, blk, token,\n            startOffset, length, pathinfo, dataIn);\n      }\n    } catch (IOException e) {\n      // remove from cache\n      localDatanodeInfo.removeBlockLocalPathInfo(blk);\n      DFSClient.LOG.warn(\"BlockReaderLocalLegacy: Removing \" + blk\n          + \" from cache because local file \" + pathinfo.getBlockPath()\n          + \" could not be opened.\");\n      throw e;\n    } finally {\n      if (localBlockReader \u003d\u003d null) {\n        if (dataIn !\u003d null) {\n          dataIn.close();\n        }\n        if (checksumIn !\u003d null) {\n          checksumIn.close();\n        }\n      }\n    }\n    return localBlockReader;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocalLegacy.java",
          "extendedDetails": {}
        }
      ]
    },
    "cfd8647d0f20c08761f908be1f5b718c1c372498": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6448. BlockReaderLocalLegacy should set socket timeout based on conf.socketTimeout (liangxie via cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1598079 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "28/05/14 10:44 AM",
      "commitName": "cfd8647d0f20c08761f908be1f5b718c1c372498",
      "commitAuthor": "Colin McCabe",
      "commitDateOld": "01/04/14 10:09 PM",
      "commitNameOld": "f93d99990a9a02ce693cd74466c2e5f127c1f560",
      "commitAuthorOld": "Tsz-wo Sze",
      "daysBetweenCommits": 56.52,
      "commitsBetweenForRepo": 317,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,81 +1,81 @@\n   static BlockReaderLocalLegacy newBlockReader(DFSClient.Conf conf,\n       UserGroupInformation userGroupInformation,\n       Configuration configuration, String file, ExtendedBlock blk,\n       Token\u003cBlockTokenIdentifier\u003e token, DatanodeInfo node, \n       long startOffset, long length) throws IOException {\n     LocalDatanodeInfo localDatanodeInfo \u003d getLocalDatanodeInfo(node\n         .getIpcPort());\n     // check the cache first\n     BlockLocalPathInfo pathinfo \u003d localDatanodeInfo.getBlockLocalPathInfo(blk);\n     if (pathinfo \u003d\u003d null) {\n       if (userGroupInformation \u003d\u003d null) {\n         userGroupInformation \u003d UserGroupInformation.getCurrentUser();\n       }\n       pathinfo \u003d getBlockPathInfo(userGroupInformation, blk, node,\n-          configuration, conf.hdfsTimeout, token,\n+          configuration, conf.socketTimeout, token,\n           conf.connectToDnViaHostname);\n     }\n \n     // check to see if the file exists. It may so happen that the\n     // HDFS file has been deleted and this block-lookup is occurring\n     // on behalf of a new HDFS file. This time, the block file could\n     // be residing in a different portion of the fs.data.dir directory.\n     // In this case, we remove this entry from the cache. The next\n     // call to this method will re-populate the cache.\n     FileInputStream dataIn \u003d null;\n     FileInputStream checksumIn \u003d null;\n     BlockReaderLocalLegacy localBlockReader \u003d null;\n     boolean skipChecksumCheck \u003d conf.skipShortCircuitChecksums;\n     try {\n       // get a local file system\n       File blkfile \u003d new File(pathinfo.getBlockPath());\n       dataIn \u003d new FileInputStream(blkfile);\n \n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"New BlockReaderLocalLegacy for file \" + blkfile + \" of size \"\n             + blkfile.length() + \" startOffset \" + startOffset + \" length \"\n             + length + \" short circuit checksum \" + !skipChecksumCheck);\n       }\n \n       if (!skipChecksumCheck) {\n         // get the metadata file\n         File metafile \u003d new File(pathinfo.getMetaPath());\n         checksumIn \u003d new FileInputStream(metafile);\n \n         // read and handle the common header here. For now just a version\n         BlockMetadataHeader header \u003d BlockMetadataHeader\n             .readHeader(new DataInputStream(checksumIn));\n         short version \u003d header.getVersion();\n         if (version !\u003d BlockMetadataHeader.VERSION) {\n           LOG.warn(\"Wrong version (\" + version + \") for metadata file for \"\n               + blk + \" ignoring ...\");\n         }\n         DataChecksum checksum \u003d header.getChecksum();\n         long firstChunkOffset \u003d startOffset\n             - (startOffset % checksum.getBytesPerChecksum());\n         localBlockReader \u003d new BlockReaderLocalLegacy(conf, file, blk, token,\n             startOffset, length, pathinfo, checksum, true, dataIn,\n             firstChunkOffset, checksumIn);\n       } else {\n         localBlockReader \u003d new BlockReaderLocalLegacy(conf, file, blk, token,\n             startOffset, length, pathinfo, dataIn);\n       }\n     } catch (IOException e) {\n       // remove from cache\n       localDatanodeInfo.removeBlockLocalPathInfo(blk);\n       DFSClient.LOG.warn(\"BlockReaderLocalLegacy: Removing \" + blk\n           + \" from cache because local file \" + pathinfo.getBlockPath()\n           + \" could not be opened.\");\n       throw e;\n     } finally {\n       if (localBlockReader \u003d\u003d null) {\n         if (dataIn !\u003d null) {\n           dataIn.close();\n         }\n         if (checksumIn !\u003d null) {\n           checksumIn.close();\n         }\n       }\n     }\n     return localBlockReader;\n   }\n\\ No newline at end of file\n",
      "actualSource": "  static BlockReaderLocalLegacy newBlockReader(DFSClient.Conf conf,\n      UserGroupInformation userGroupInformation,\n      Configuration configuration, String file, ExtendedBlock blk,\n      Token\u003cBlockTokenIdentifier\u003e token, DatanodeInfo node, \n      long startOffset, long length) throws IOException {\n    LocalDatanodeInfo localDatanodeInfo \u003d getLocalDatanodeInfo(node\n        .getIpcPort());\n    // check the cache first\n    BlockLocalPathInfo pathinfo \u003d localDatanodeInfo.getBlockLocalPathInfo(blk);\n    if (pathinfo \u003d\u003d null) {\n      if (userGroupInformation \u003d\u003d null) {\n        userGroupInformation \u003d UserGroupInformation.getCurrentUser();\n      }\n      pathinfo \u003d getBlockPathInfo(userGroupInformation, blk, node,\n          configuration, conf.socketTimeout, token,\n          conf.connectToDnViaHostname);\n    }\n\n    // check to see if the file exists. It may so happen that the\n    // HDFS file has been deleted and this block-lookup is occurring\n    // on behalf of a new HDFS file. This time, the block file could\n    // be residing in a different portion of the fs.data.dir directory.\n    // In this case, we remove this entry from the cache. The next\n    // call to this method will re-populate the cache.\n    FileInputStream dataIn \u003d null;\n    FileInputStream checksumIn \u003d null;\n    BlockReaderLocalLegacy localBlockReader \u003d null;\n    boolean skipChecksumCheck \u003d conf.skipShortCircuitChecksums;\n    try {\n      // get a local file system\n      File blkfile \u003d new File(pathinfo.getBlockPath());\n      dataIn \u003d new FileInputStream(blkfile);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"New BlockReaderLocalLegacy for file \" + blkfile + \" of size \"\n            + blkfile.length() + \" startOffset \" + startOffset + \" length \"\n            + length + \" short circuit checksum \" + !skipChecksumCheck);\n      }\n\n      if (!skipChecksumCheck) {\n        // get the metadata file\n        File metafile \u003d new File(pathinfo.getMetaPath());\n        checksumIn \u003d new FileInputStream(metafile);\n\n        // read and handle the common header here. For now just a version\n        BlockMetadataHeader header \u003d BlockMetadataHeader\n            .readHeader(new DataInputStream(checksumIn));\n        short version \u003d header.getVersion();\n        if (version !\u003d BlockMetadataHeader.VERSION) {\n          LOG.warn(\"Wrong version (\" + version + \") for metadata file for \"\n              + blk + \" ignoring ...\");\n        }\n        DataChecksum checksum \u003d header.getChecksum();\n        long firstChunkOffset \u003d startOffset\n            - (startOffset % checksum.getBytesPerChecksum());\n        localBlockReader \u003d new BlockReaderLocalLegacy(conf, file, blk, token,\n            startOffset, length, pathinfo, checksum, true, dataIn,\n            firstChunkOffset, checksumIn);\n      } else {\n        localBlockReader \u003d new BlockReaderLocalLegacy(conf, file, blk, token,\n            startOffset, length, pathinfo, dataIn);\n      }\n    } catch (IOException e) {\n      // remove from cache\n      localDatanodeInfo.removeBlockLocalPathInfo(blk);\n      DFSClient.LOG.warn(\"BlockReaderLocalLegacy: Removing \" + blk\n          + \" from cache because local file \" + pathinfo.getBlockPath()\n          + \" could not be opened.\");\n      throw e;\n    } finally {\n      if (localBlockReader \u003d\u003d null) {\n        if (dataIn !\u003d null) {\n          dataIn.close();\n        }\n        if (checksumIn !\u003d null) {\n          checksumIn.close();\n        }\n      }\n    }\n    return localBlockReader;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocalLegacy.java",
      "extendedDetails": {}
    },
    "beb0d25d2a7ba5004c6aabd105546ba9a9fec9be": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-5810. Unify mmap cache and short-circuit file descriptor cache (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1567720 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "12/02/14 11:08 AM",
      "commitName": "beb0d25d2a7ba5004c6aabd105546ba9a9fec9be",
      "commitAuthor": "Colin McCabe",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-5810. Unify mmap cache and short-circuit file descriptor cache (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1567720 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "12/02/14 11:08 AM",
          "commitName": "beb0d25d2a7ba5004c6aabd105546ba9a9fec9be",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "06/02/14 7:45 AM",
          "commitNameOld": "ab96a0838dafbfea77382135914feadbfd03cf53",
          "commitAuthorOld": "Kihwal Lee",
          "daysBetweenCommits": 6.14,
          "commitsBetweenForRepo": 45,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,79 +1,81 @@\n-  static BlockReaderLocalLegacy newBlockReader(DFSClient dfsClient,\n-      String file, ExtendedBlock blk, Token\u003cBlockTokenIdentifier\u003e token,\n-      DatanodeInfo node, long startOffset, long length)\n-      throws IOException {\n-    final DFSClient.Conf conf \u003d dfsClient.getConf();\n-\n+  static BlockReaderLocalLegacy newBlockReader(DFSClient.Conf conf,\n+      UserGroupInformation userGroupInformation,\n+      Configuration configuration, String file, ExtendedBlock blk,\n+      Token\u003cBlockTokenIdentifier\u003e token, DatanodeInfo node, \n+      long startOffset, long length) throws IOException {\n     LocalDatanodeInfo localDatanodeInfo \u003d getLocalDatanodeInfo(node\n         .getIpcPort());\n     // check the cache first\n     BlockLocalPathInfo pathinfo \u003d localDatanodeInfo.getBlockLocalPathInfo(blk);\n     if (pathinfo \u003d\u003d null) {\n-      pathinfo \u003d getBlockPathInfo(dfsClient.ugi, blk, node,\n-          dfsClient.getConfiguration(), dfsClient.getHdfsTimeout(), token,\n+      if (userGroupInformation \u003d\u003d null) {\n+        userGroupInformation \u003d UserGroupInformation.getCurrentUser();\n+      }\n+      pathinfo \u003d getBlockPathInfo(userGroupInformation, blk, node,\n+          configuration, conf.hdfsTimeout, token,\n           conf.connectToDnViaHostname);\n     }\n \n     // check to see if the file exists. It may so happen that the\n     // HDFS file has been deleted and this block-lookup is occurring\n     // on behalf of a new HDFS file. This time, the block file could\n     // be residing in a different portion of the fs.data.dir directory.\n     // In this case, we remove this entry from the cache. The next\n     // call to this method will re-populate the cache.\n     FileInputStream dataIn \u003d null;\n     FileInputStream checksumIn \u003d null;\n     BlockReaderLocalLegacy localBlockReader \u003d null;\n     boolean skipChecksumCheck \u003d conf.skipShortCircuitChecksums;\n     try {\n       // get a local file system\n       File blkfile \u003d new File(pathinfo.getBlockPath());\n       dataIn \u003d new FileInputStream(blkfile);\n \n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"New BlockReaderLocalLegacy for file \" + blkfile + \" of size \"\n             + blkfile.length() + \" startOffset \" + startOffset + \" length \"\n             + length + \" short circuit checksum \" + !skipChecksumCheck);\n       }\n \n       if (!skipChecksumCheck) {\n         // get the metadata file\n         File metafile \u003d new File(pathinfo.getMetaPath());\n         checksumIn \u003d new FileInputStream(metafile);\n \n         // read and handle the common header here. For now just a version\n         BlockMetadataHeader header \u003d BlockMetadataHeader\n             .readHeader(new DataInputStream(checksumIn));\n         short version \u003d header.getVersion();\n         if (version !\u003d BlockMetadataHeader.VERSION) {\n           LOG.warn(\"Wrong version (\" + version + \") for metadata file for \"\n               + blk + \" ignoring ...\");\n         }\n         DataChecksum checksum \u003d header.getChecksum();\n         long firstChunkOffset \u003d startOffset\n             - (startOffset % checksum.getBytesPerChecksum());\n         localBlockReader \u003d new BlockReaderLocalLegacy(conf, file, blk, token,\n             startOffset, length, pathinfo, checksum, true, dataIn,\n             firstChunkOffset, checksumIn);\n       } else {\n         localBlockReader \u003d new BlockReaderLocalLegacy(conf, file, blk, token,\n             startOffset, length, pathinfo, dataIn);\n       }\n     } catch (IOException e) {\n       // remove from cache\n       localDatanodeInfo.removeBlockLocalPathInfo(blk);\n       DFSClient.LOG.warn(\"BlockReaderLocalLegacy: Removing \" + blk\n           + \" from cache because local file \" + pathinfo.getBlockPath()\n           + \" could not be opened.\");\n       throw e;\n     } finally {\n       if (localBlockReader \u003d\u003d null) {\n         if (dataIn !\u003d null) {\n           dataIn.close();\n         }\n         if (checksumIn !\u003d null) {\n           checksumIn.close();\n         }\n       }\n     }\n     return localBlockReader;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static BlockReaderLocalLegacy newBlockReader(DFSClient.Conf conf,\n      UserGroupInformation userGroupInformation,\n      Configuration configuration, String file, ExtendedBlock blk,\n      Token\u003cBlockTokenIdentifier\u003e token, DatanodeInfo node, \n      long startOffset, long length) throws IOException {\n    LocalDatanodeInfo localDatanodeInfo \u003d getLocalDatanodeInfo(node\n        .getIpcPort());\n    // check the cache first\n    BlockLocalPathInfo pathinfo \u003d localDatanodeInfo.getBlockLocalPathInfo(blk);\n    if (pathinfo \u003d\u003d null) {\n      if (userGroupInformation \u003d\u003d null) {\n        userGroupInformation \u003d UserGroupInformation.getCurrentUser();\n      }\n      pathinfo \u003d getBlockPathInfo(userGroupInformation, blk, node,\n          configuration, conf.hdfsTimeout, token,\n          conf.connectToDnViaHostname);\n    }\n\n    // check to see if the file exists. It may so happen that the\n    // HDFS file has been deleted and this block-lookup is occurring\n    // on behalf of a new HDFS file. This time, the block file could\n    // be residing in a different portion of the fs.data.dir directory.\n    // In this case, we remove this entry from the cache. The next\n    // call to this method will re-populate the cache.\n    FileInputStream dataIn \u003d null;\n    FileInputStream checksumIn \u003d null;\n    BlockReaderLocalLegacy localBlockReader \u003d null;\n    boolean skipChecksumCheck \u003d conf.skipShortCircuitChecksums;\n    try {\n      // get a local file system\n      File blkfile \u003d new File(pathinfo.getBlockPath());\n      dataIn \u003d new FileInputStream(blkfile);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"New BlockReaderLocalLegacy for file \" + blkfile + \" of size \"\n            + blkfile.length() + \" startOffset \" + startOffset + \" length \"\n            + length + \" short circuit checksum \" + !skipChecksumCheck);\n      }\n\n      if (!skipChecksumCheck) {\n        // get the metadata file\n        File metafile \u003d new File(pathinfo.getMetaPath());\n        checksumIn \u003d new FileInputStream(metafile);\n\n        // read and handle the common header here. For now just a version\n        BlockMetadataHeader header \u003d BlockMetadataHeader\n            .readHeader(new DataInputStream(checksumIn));\n        short version \u003d header.getVersion();\n        if (version !\u003d BlockMetadataHeader.VERSION) {\n          LOG.warn(\"Wrong version (\" + version + \") for metadata file for \"\n              + blk + \" ignoring ...\");\n        }\n        DataChecksum checksum \u003d header.getChecksum();\n        long firstChunkOffset \u003d startOffset\n            - (startOffset % checksum.getBytesPerChecksum());\n        localBlockReader \u003d new BlockReaderLocalLegacy(conf, file, blk, token,\n            startOffset, length, pathinfo, checksum, true, dataIn,\n            firstChunkOffset, checksumIn);\n      } else {\n        localBlockReader \u003d new BlockReaderLocalLegacy(conf, file, blk, token,\n            startOffset, length, pathinfo, dataIn);\n      }\n    } catch (IOException e) {\n      // remove from cache\n      localDatanodeInfo.removeBlockLocalPathInfo(blk);\n      DFSClient.LOG.warn(\"BlockReaderLocalLegacy: Removing \" + blk\n          + \" from cache because local file \" + pathinfo.getBlockPath()\n          + \" could not be opened.\");\n      throw e;\n    } finally {\n      if (localBlockReader \u003d\u003d null) {\n        if (dataIn !\u003d null) {\n          dataIn.close();\n        }\n        if (checksumIn !\u003d null) {\n          checksumIn.close();\n        }\n      }\n    }\n    return localBlockReader;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocalLegacy.java",
          "extendedDetails": {
            "oldValue": "[dfsClient-DFSClient, file-String, blk-ExtendedBlock, token-Token\u003cBlockTokenIdentifier\u003e, node-DatanodeInfo, startOffset-long, length-long]",
            "newValue": "[conf-DFSClient.Conf, userGroupInformation-UserGroupInformation, configuration-Configuration, file-String, blk-ExtendedBlock, token-Token\u003cBlockTokenIdentifier\u003e, node-DatanodeInfo, startOffset-long, length-long]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-5810. Unify mmap cache and short-circuit file descriptor cache (cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1567720 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "12/02/14 11:08 AM",
          "commitName": "beb0d25d2a7ba5004c6aabd105546ba9a9fec9be",
          "commitAuthor": "Colin McCabe",
          "commitDateOld": "06/02/14 7:45 AM",
          "commitNameOld": "ab96a0838dafbfea77382135914feadbfd03cf53",
          "commitAuthorOld": "Kihwal Lee",
          "daysBetweenCommits": 6.14,
          "commitsBetweenForRepo": 45,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,79 +1,81 @@\n-  static BlockReaderLocalLegacy newBlockReader(DFSClient dfsClient,\n-      String file, ExtendedBlock blk, Token\u003cBlockTokenIdentifier\u003e token,\n-      DatanodeInfo node, long startOffset, long length)\n-      throws IOException {\n-    final DFSClient.Conf conf \u003d dfsClient.getConf();\n-\n+  static BlockReaderLocalLegacy newBlockReader(DFSClient.Conf conf,\n+      UserGroupInformation userGroupInformation,\n+      Configuration configuration, String file, ExtendedBlock blk,\n+      Token\u003cBlockTokenIdentifier\u003e token, DatanodeInfo node, \n+      long startOffset, long length) throws IOException {\n     LocalDatanodeInfo localDatanodeInfo \u003d getLocalDatanodeInfo(node\n         .getIpcPort());\n     // check the cache first\n     BlockLocalPathInfo pathinfo \u003d localDatanodeInfo.getBlockLocalPathInfo(blk);\n     if (pathinfo \u003d\u003d null) {\n-      pathinfo \u003d getBlockPathInfo(dfsClient.ugi, blk, node,\n-          dfsClient.getConfiguration(), dfsClient.getHdfsTimeout(), token,\n+      if (userGroupInformation \u003d\u003d null) {\n+        userGroupInformation \u003d UserGroupInformation.getCurrentUser();\n+      }\n+      pathinfo \u003d getBlockPathInfo(userGroupInformation, blk, node,\n+          configuration, conf.hdfsTimeout, token,\n           conf.connectToDnViaHostname);\n     }\n \n     // check to see if the file exists. It may so happen that the\n     // HDFS file has been deleted and this block-lookup is occurring\n     // on behalf of a new HDFS file. This time, the block file could\n     // be residing in a different portion of the fs.data.dir directory.\n     // In this case, we remove this entry from the cache. The next\n     // call to this method will re-populate the cache.\n     FileInputStream dataIn \u003d null;\n     FileInputStream checksumIn \u003d null;\n     BlockReaderLocalLegacy localBlockReader \u003d null;\n     boolean skipChecksumCheck \u003d conf.skipShortCircuitChecksums;\n     try {\n       // get a local file system\n       File blkfile \u003d new File(pathinfo.getBlockPath());\n       dataIn \u003d new FileInputStream(blkfile);\n \n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"New BlockReaderLocalLegacy for file \" + blkfile + \" of size \"\n             + blkfile.length() + \" startOffset \" + startOffset + \" length \"\n             + length + \" short circuit checksum \" + !skipChecksumCheck);\n       }\n \n       if (!skipChecksumCheck) {\n         // get the metadata file\n         File metafile \u003d new File(pathinfo.getMetaPath());\n         checksumIn \u003d new FileInputStream(metafile);\n \n         // read and handle the common header here. For now just a version\n         BlockMetadataHeader header \u003d BlockMetadataHeader\n             .readHeader(new DataInputStream(checksumIn));\n         short version \u003d header.getVersion();\n         if (version !\u003d BlockMetadataHeader.VERSION) {\n           LOG.warn(\"Wrong version (\" + version + \") for metadata file for \"\n               + blk + \" ignoring ...\");\n         }\n         DataChecksum checksum \u003d header.getChecksum();\n         long firstChunkOffset \u003d startOffset\n             - (startOffset % checksum.getBytesPerChecksum());\n         localBlockReader \u003d new BlockReaderLocalLegacy(conf, file, blk, token,\n             startOffset, length, pathinfo, checksum, true, dataIn,\n             firstChunkOffset, checksumIn);\n       } else {\n         localBlockReader \u003d new BlockReaderLocalLegacy(conf, file, blk, token,\n             startOffset, length, pathinfo, dataIn);\n       }\n     } catch (IOException e) {\n       // remove from cache\n       localDatanodeInfo.removeBlockLocalPathInfo(blk);\n       DFSClient.LOG.warn(\"BlockReaderLocalLegacy: Removing \" + blk\n           + \" from cache because local file \" + pathinfo.getBlockPath()\n           + \" could not be opened.\");\n       throw e;\n     } finally {\n       if (localBlockReader \u003d\u003d null) {\n         if (dataIn !\u003d null) {\n           dataIn.close();\n         }\n         if (checksumIn !\u003d null) {\n           checksumIn.close();\n         }\n       }\n     }\n     return localBlockReader;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static BlockReaderLocalLegacy newBlockReader(DFSClient.Conf conf,\n      UserGroupInformation userGroupInformation,\n      Configuration configuration, String file, ExtendedBlock blk,\n      Token\u003cBlockTokenIdentifier\u003e token, DatanodeInfo node, \n      long startOffset, long length) throws IOException {\n    LocalDatanodeInfo localDatanodeInfo \u003d getLocalDatanodeInfo(node\n        .getIpcPort());\n    // check the cache first\n    BlockLocalPathInfo pathinfo \u003d localDatanodeInfo.getBlockLocalPathInfo(blk);\n    if (pathinfo \u003d\u003d null) {\n      if (userGroupInformation \u003d\u003d null) {\n        userGroupInformation \u003d UserGroupInformation.getCurrentUser();\n      }\n      pathinfo \u003d getBlockPathInfo(userGroupInformation, blk, node,\n          configuration, conf.hdfsTimeout, token,\n          conf.connectToDnViaHostname);\n    }\n\n    // check to see if the file exists. It may so happen that the\n    // HDFS file has been deleted and this block-lookup is occurring\n    // on behalf of a new HDFS file. This time, the block file could\n    // be residing in a different portion of the fs.data.dir directory.\n    // In this case, we remove this entry from the cache. The next\n    // call to this method will re-populate the cache.\n    FileInputStream dataIn \u003d null;\n    FileInputStream checksumIn \u003d null;\n    BlockReaderLocalLegacy localBlockReader \u003d null;\n    boolean skipChecksumCheck \u003d conf.skipShortCircuitChecksums;\n    try {\n      // get a local file system\n      File blkfile \u003d new File(pathinfo.getBlockPath());\n      dataIn \u003d new FileInputStream(blkfile);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"New BlockReaderLocalLegacy for file \" + blkfile + \" of size \"\n            + blkfile.length() + \" startOffset \" + startOffset + \" length \"\n            + length + \" short circuit checksum \" + !skipChecksumCheck);\n      }\n\n      if (!skipChecksumCheck) {\n        // get the metadata file\n        File metafile \u003d new File(pathinfo.getMetaPath());\n        checksumIn \u003d new FileInputStream(metafile);\n\n        // read and handle the common header here. For now just a version\n        BlockMetadataHeader header \u003d BlockMetadataHeader\n            .readHeader(new DataInputStream(checksumIn));\n        short version \u003d header.getVersion();\n        if (version !\u003d BlockMetadataHeader.VERSION) {\n          LOG.warn(\"Wrong version (\" + version + \") for metadata file for \"\n              + blk + \" ignoring ...\");\n        }\n        DataChecksum checksum \u003d header.getChecksum();\n        long firstChunkOffset \u003d startOffset\n            - (startOffset % checksum.getBytesPerChecksum());\n        localBlockReader \u003d new BlockReaderLocalLegacy(conf, file, blk, token,\n            startOffset, length, pathinfo, checksum, true, dataIn,\n            firstChunkOffset, checksumIn);\n      } else {\n        localBlockReader \u003d new BlockReaderLocalLegacy(conf, file, blk, token,\n            startOffset, length, pathinfo, dataIn);\n      }\n    } catch (IOException e) {\n      // remove from cache\n      localDatanodeInfo.removeBlockLocalPathInfo(blk);\n      DFSClient.LOG.warn(\"BlockReaderLocalLegacy: Removing \" + blk\n          + \" from cache because local file \" + pathinfo.getBlockPath()\n          + \" could not be opened.\");\n      throw e;\n    } finally {\n      if (localBlockReader \u003d\u003d null) {\n        if (dataIn !\u003d null) {\n          dataIn.close();\n        }\n        if (checksumIn !\u003d null) {\n          checksumIn.close();\n        }\n      }\n    }\n    return localBlockReader;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocalLegacy.java",
          "extendedDetails": {}
        }
      ]
    },
    "c68b1d1b31e304c27e419e810ded0fc97e435ea6": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "HDFS-4914. Use DFSClient.Conf instead of Configuration.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1494854 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "19/06/13 9:43 PM",
      "commitName": "c68b1d1b31e304c27e419e810ded0fc97e435ea6",
      "commitAuthor": "Tsz-wo Sze",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "HDFS-4914. Use DFSClient.Conf instead of Configuration.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1494854 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "19/06/13 9:43 PM",
          "commitName": "c68b1d1b31e304c27e419e810ded0fc97e435ea6",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "10/05/13 10:58 AM",
          "commitNameOld": "4ed1fc58c0683bbcd5c4c211ea162ed37bf7dc4f",
          "commitAuthorOld": "Aaron Myers",
          "daysBetweenCommits": 40.45,
          "commitsBetweenForRepo": 278,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,78 +1,79 @@\n-  static BlockReaderLocalLegacy newBlockReader(UserGroupInformation ugi,\n-      Configuration conf, String file, ExtendedBlock blk,\n-      Token\u003cBlockTokenIdentifier\u003e token, DatanodeInfo node, int socketTimeout,\n-      long startOffset, long length, boolean connectToDnViaHostname)\n+  static BlockReaderLocalLegacy newBlockReader(DFSClient dfsClient,\n+      String file, ExtendedBlock blk, Token\u003cBlockTokenIdentifier\u003e token,\n+      DatanodeInfo node, long startOffset, long length)\n       throws IOException {\n+    final DFSClient.Conf conf \u003d dfsClient.getConf();\n \n     LocalDatanodeInfo localDatanodeInfo \u003d getLocalDatanodeInfo(node\n         .getIpcPort());\n     // check the cache first\n     BlockLocalPathInfo pathinfo \u003d localDatanodeInfo.getBlockLocalPathInfo(blk);\n     if (pathinfo \u003d\u003d null) {\n-      pathinfo \u003d getBlockPathInfo(ugi, blk, node, conf, socketTimeout, token,\n-          connectToDnViaHostname);\n+      pathinfo \u003d getBlockPathInfo(dfsClient.ugi, blk, node,\n+          dfsClient.getConfiguration(), dfsClient.getHdfsTimeout(), token,\n+          conf.connectToDnViaHostname);\n     }\n \n     // check to see if the file exists. It may so happen that the\n     // HDFS file has been deleted and this block-lookup is occurring\n     // on behalf of a new HDFS file. This time, the block file could\n     // be residing in a different portion of the fs.data.dir directory.\n     // In this case, we remove this entry from the cache. The next\n     // call to this method will re-populate the cache.\n     FileInputStream dataIn \u003d null;\n     FileInputStream checksumIn \u003d null;\n     BlockReaderLocalLegacy localBlockReader \u003d null;\n-    boolean skipChecksumCheck \u003d skipChecksumCheck(conf);\n+    boolean skipChecksumCheck \u003d conf.skipShortCircuitChecksums;\n     try {\n       // get a local file system\n       File blkfile \u003d new File(pathinfo.getBlockPath());\n       dataIn \u003d new FileInputStream(blkfile);\n \n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"New BlockReaderLocalLegacy for file \" + blkfile + \" of size \"\n             + blkfile.length() + \" startOffset \" + startOffset + \" length \"\n             + length + \" short circuit checksum \" + !skipChecksumCheck);\n       }\n \n       if (!skipChecksumCheck) {\n         // get the metadata file\n         File metafile \u003d new File(pathinfo.getMetaPath());\n         checksumIn \u003d new FileInputStream(metafile);\n \n         // read and handle the common header here. For now just a version\n         BlockMetadataHeader header \u003d BlockMetadataHeader\n             .readHeader(new DataInputStream(checksumIn));\n         short version \u003d header.getVersion();\n         if (version !\u003d BlockMetadataHeader.VERSION) {\n           LOG.warn(\"Wrong version (\" + version + \") for metadata file for \"\n               + blk + \" ignoring ...\");\n         }\n         DataChecksum checksum \u003d header.getChecksum();\n         long firstChunkOffset \u003d startOffset\n             - (startOffset % checksum.getBytesPerChecksum());\n         localBlockReader \u003d new BlockReaderLocalLegacy(conf, file, blk, token,\n             startOffset, length, pathinfo, checksum, true, dataIn,\n             firstChunkOffset, checksumIn);\n       } else {\n         localBlockReader \u003d new BlockReaderLocalLegacy(conf, file, blk, token,\n             startOffset, length, pathinfo, dataIn);\n       }\n     } catch (IOException e) {\n       // remove from cache\n       localDatanodeInfo.removeBlockLocalPathInfo(blk);\n       DFSClient.LOG.warn(\"BlockReaderLocalLegacy: Removing \" + blk\n           + \" from cache because local file \" + pathinfo.getBlockPath()\n           + \" could not be opened.\");\n       throw e;\n     } finally {\n       if (localBlockReader \u003d\u003d null) {\n         if (dataIn !\u003d null) {\n           dataIn.close();\n         }\n         if (checksumIn !\u003d null) {\n           checksumIn.close();\n         }\n       }\n     }\n     return localBlockReader;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static BlockReaderLocalLegacy newBlockReader(DFSClient dfsClient,\n      String file, ExtendedBlock blk, Token\u003cBlockTokenIdentifier\u003e token,\n      DatanodeInfo node, long startOffset, long length)\n      throws IOException {\n    final DFSClient.Conf conf \u003d dfsClient.getConf();\n\n    LocalDatanodeInfo localDatanodeInfo \u003d getLocalDatanodeInfo(node\n        .getIpcPort());\n    // check the cache first\n    BlockLocalPathInfo pathinfo \u003d localDatanodeInfo.getBlockLocalPathInfo(blk);\n    if (pathinfo \u003d\u003d null) {\n      pathinfo \u003d getBlockPathInfo(dfsClient.ugi, blk, node,\n          dfsClient.getConfiguration(), dfsClient.getHdfsTimeout(), token,\n          conf.connectToDnViaHostname);\n    }\n\n    // check to see if the file exists. It may so happen that the\n    // HDFS file has been deleted and this block-lookup is occurring\n    // on behalf of a new HDFS file. This time, the block file could\n    // be residing in a different portion of the fs.data.dir directory.\n    // In this case, we remove this entry from the cache. The next\n    // call to this method will re-populate the cache.\n    FileInputStream dataIn \u003d null;\n    FileInputStream checksumIn \u003d null;\n    BlockReaderLocalLegacy localBlockReader \u003d null;\n    boolean skipChecksumCheck \u003d conf.skipShortCircuitChecksums;\n    try {\n      // get a local file system\n      File blkfile \u003d new File(pathinfo.getBlockPath());\n      dataIn \u003d new FileInputStream(blkfile);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"New BlockReaderLocalLegacy for file \" + blkfile + \" of size \"\n            + blkfile.length() + \" startOffset \" + startOffset + \" length \"\n            + length + \" short circuit checksum \" + !skipChecksumCheck);\n      }\n\n      if (!skipChecksumCheck) {\n        // get the metadata file\n        File metafile \u003d new File(pathinfo.getMetaPath());\n        checksumIn \u003d new FileInputStream(metafile);\n\n        // read and handle the common header here. For now just a version\n        BlockMetadataHeader header \u003d BlockMetadataHeader\n            .readHeader(new DataInputStream(checksumIn));\n        short version \u003d header.getVersion();\n        if (version !\u003d BlockMetadataHeader.VERSION) {\n          LOG.warn(\"Wrong version (\" + version + \") for metadata file for \"\n              + blk + \" ignoring ...\");\n        }\n        DataChecksum checksum \u003d header.getChecksum();\n        long firstChunkOffset \u003d startOffset\n            - (startOffset % checksum.getBytesPerChecksum());\n        localBlockReader \u003d new BlockReaderLocalLegacy(conf, file, blk, token,\n            startOffset, length, pathinfo, checksum, true, dataIn,\n            firstChunkOffset, checksumIn);\n      } else {\n        localBlockReader \u003d new BlockReaderLocalLegacy(conf, file, blk, token,\n            startOffset, length, pathinfo, dataIn);\n      }\n    } catch (IOException e) {\n      // remove from cache\n      localDatanodeInfo.removeBlockLocalPathInfo(blk);\n      DFSClient.LOG.warn(\"BlockReaderLocalLegacy: Removing \" + blk\n          + \" from cache because local file \" + pathinfo.getBlockPath()\n          + \" could not be opened.\");\n      throw e;\n    } finally {\n      if (localBlockReader \u003d\u003d null) {\n        if (dataIn !\u003d null) {\n          dataIn.close();\n        }\n        if (checksumIn !\u003d null) {\n          checksumIn.close();\n        }\n      }\n    }\n    return localBlockReader;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocalLegacy.java",
          "extendedDetails": {
            "oldValue": "[ugi-UserGroupInformation, conf-Configuration, file-String, blk-ExtendedBlock, token-Token\u003cBlockTokenIdentifier\u003e, node-DatanodeInfo, socketTimeout-int, startOffset-long, length-long, connectToDnViaHostname-boolean]",
            "newValue": "[dfsClient-DFSClient, file-String, blk-ExtendedBlock, token-Token\u003cBlockTokenIdentifier\u003e, node-DatanodeInfo, startOffset-long, length-long]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "HDFS-4914. Use DFSClient.Conf instead of Configuration.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1494854 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "19/06/13 9:43 PM",
          "commitName": "c68b1d1b31e304c27e419e810ded0fc97e435ea6",
          "commitAuthor": "Tsz-wo Sze",
          "commitDateOld": "10/05/13 10:58 AM",
          "commitNameOld": "4ed1fc58c0683bbcd5c4c211ea162ed37bf7dc4f",
          "commitAuthorOld": "Aaron Myers",
          "daysBetweenCommits": 40.45,
          "commitsBetweenForRepo": 278,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,78 +1,79 @@\n-  static BlockReaderLocalLegacy newBlockReader(UserGroupInformation ugi,\n-      Configuration conf, String file, ExtendedBlock blk,\n-      Token\u003cBlockTokenIdentifier\u003e token, DatanodeInfo node, int socketTimeout,\n-      long startOffset, long length, boolean connectToDnViaHostname)\n+  static BlockReaderLocalLegacy newBlockReader(DFSClient dfsClient,\n+      String file, ExtendedBlock blk, Token\u003cBlockTokenIdentifier\u003e token,\n+      DatanodeInfo node, long startOffset, long length)\n       throws IOException {\n+    final DFSClient.Conf conf \u003d dfsClient.getConf();\n \n     LocalDatanodeInfo localDatanodeInfo \u003d getLocalDatanodeInfo(node\n         .getIpcPort());\n     // check the cache first\n     BlockLocalPathInfo pathinfo \u003d localDatanodeInfo.getBlockLocalPathInfo(blk);\n     if (pathinfo \u003d\u003d null) {\n-      pathinfo \u003d getBlockPathInfo(ugi, blk, node, conf, socketTimeout, token,\n-          connectToDnViaHostname);\n+      pathinfo \u003d getBlockPathInfo(dfsClient.ugi, blk, node,\n+          dfsClient.getConfiguration(), dfsClient.getHdfsTimeout(), token,\n+          conf.connectToDnViaHostname);\n     }\n \n     // check to see if the file exists. It may so happen that the\n     // HDFS file has been deleted and this block-lookup is occurring\n     // on behalf of a new HDFS file. This time, the block file could\n     // be residing in a different portion of the fs.data.dir directory.\n     // In this case, we remove this entry from the cache. The next\n     // call to this method will re-populate the cache.\n     FileInputStream dataIn \u003d null;\n     FileInputStream checksumIn \u003d null;\n     BlockReaderLocalLegacy localBlockReader \u003d null;\n-    boolean skipChecksumCheck \u003d skipChecksumCheck(conf);\n+    boolean skipChecksumCheck \u003d conf.skipShortCircuitChecksums;\n     try {\n       // get a local file system\n       File blkfile \u003d new File(pathinfo.getBlockPath());\n       dataIn \u003d new FileInputStream(blkfile);\n \n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"New BlockReaderLocalLegacy for file \" + blkfile + \" of size \"\n             + blkfile.length() + \" startOffset \" + startOffset + \" length \"\n             + length + \" short circuit checksum \" + !skipChecksumCheck);\n       }\n \n       if (!skipChecksumCheck) {\n         // get the metadata file\n         File metafile \u003d new File(pathinfo.getMetaPath());\n         checksumIn \u003d new FileInputStream(metafile);\n \n         // read and handle the common header here. For now just a version\n         BlockMetadataHeader header \u003d BlockMetadataHeader\n             .readHeader(new DataInputStream(checksumIn));\n         short version \u003d header.getVersion();\n         if (version !\u003d BlockMetadataHeader.VERSION) {\n           LOG.warn(\"Wrong version (\" + version + \") for metadata file for \"\n               + blk + \" ignoring ...\");\n         }\n         DataChecksum checksum \u003d header.getChecksum();\n         long firstChunkOffset \u003d startOffset\n             - (startOffset % checksum.getBytesPerChecksum());\n         localBlockReader \u003d new BlockReaderLocalLegacy(conf, file, blk, token,\n             startOffset, length, pathinfo, checksum, true, dataIn,\n             firstChunkOffset, checksumIn);\n       } else {\n         localBlockReader \u003d new BlockReaderLocalLegacy(conf, file, blk, token,\n             startOffset, length, pathinfo, dataIn);\n       }\n     } catch (IOException e) {\n       // remove from cache\n       localDatanodeInfo.removeBlockLocalPathInfo(blk);\n       DFSClient.LOG.warn(\"BlockReaderLocalLegacy: Removing \" + blk\n           + \" from cache because local file \" + pathinfo.getBlockPath()\n           + \" could not be opened.\");\n       throw e;\n     } finally {\n       if (localBlockReader \u003d\u003d null) {\n         if (dataIn !\u003d null) {\n           dataIn.close();\n         }\n         if (checksumIn !\u003d null) {\n           checksumIn.close();\n         }\n       }\n     }\n     return localBlockReader;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static BlockReaderLocalLegacy newBlockReader(DFSClient dfsClient,\n      String file, ExtendedBlock blk, Token\u003cBlockTokenIdentifier\u003e token,\n      DatanodeInfo node, long startOffset, long length)\n      throws IOException {\n    final DFSClient.Conf conf \u003d dfsClient.getConf();\n\n    LocalDatanodeInfo localDatanodeInfo \u003d getLocalDatanodeInfo(node\n        .getIpcPort());\n    // check the cache first\n    BlockLocalPathInfo pathinfo \u003d localDatanodeInfo.getBlockLocalPathInfo(blk);\n    if (pathinfo \u003d\u003d null) {\n      pathinfo \u003d getBlockPathInfo(dfsClient.ugi, blk, node,\n          dfsClient.getConfiguration(), dfsClient.getHdfsTimeout(), token,\n          conf.connectToDnViaHostname);\n    }\n\n    // check to see if the file exists. It may so happen that the\n    // HDFS file has been deleted and this block-lookup is occurring\n    // on behalf of a new HDFS file. This time, the block file could\n    // be residing in a different portion of the fs.data.dir directory.\n    // In this case, we remove this entry from the cache. The next\n    // call to this method will re-populate the cache.\n    FileInputStream dataIn \u003d null;\n    FileInputStream checksumIn \u003d null;\n    BlockReaderLocalLegacy localBlockReader \u003d null;\n    boolean skipChecksumCheck \u003d conf.skipShortCircuitChecksums;\n    try {\n      // get a local file system\n      File blkfile \u003d new File(pathinfo.getBlockPath());\n      dataIn \u003d new FileInputStream(blkfile);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"New BlockReaderLocalLegacy for file \" + blkfile + \" of size \"\n            + blkfile.length() + \" startOffset \" + startOffset + \" length \"\n            + length + \" short circuit checksum \" + !skipChecksumCheck);\n      }\n\n      if (!skipChecksumCheck) {\n        // get the metadata file\n        File metafile \u003d new File(pathinfo.getMetaPath());\n        checksumIn \u003d new FileInputStream(metafile);\n\n        // read and handle the common header here. For now just a version\n        BlockMetadataHeader header \u003d BlockMetadataHeader\n            .readHeader(new DataInputStream(checksumIn));\n        short version \u003d header.getVersion();\n        if (version !\u003d BlockMetadataHeader.VERSION) {\n          LOG.warn(\"Wrong version (\" + version + \") for metadata file for \"\n              + blk + \" ignoring ...\");\n        }\n        DataChecksum checksum \u003d header.getChecksum();\n        long firstChunkOffset \u003d startOffset\n            - (startOffset % checksum.getBytesPerChecksum());\n        localBlockReader \u003d new BlockReaderLocalLegacy(conf, file, blk, token,\n            startOffset, length, pathinfo, checksum, true, dataIn,\n            firstChunkOffset, checksumIn);\n      } else {\n        localBlockReader \u003d new BlockReaderLocalLegacy(conf, file, blk, token,\n            startOffset, length, pathinfo, dataIn);\n      }\n    } catch (IOException e) {\n      // remove from cache\n      localDatanodeInfo.removeBlockLocalPathInfo(blk);\n      DFSClient.LOG.warn(\"BlockReaderLocalLegacy: Removing \" + blk\n          + \" from cache because local file \" + pathinfo.getBlockPath()\n          + \" could not be opened.\");\n      throw e;\n    } finally {\n      if (localBlockReader \u003d\u003d null) {\n        if (dataIn !\u003d null) {\n          dataIn.close();\n        }\n        if (checksumIn !\u003d null) {\n          checksumIn.close();\n        }\n      }\n    }\n    return localBlockReader;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocalLegacy.java",
          "extendedDetails": {}
        }
      ]
    },
    "bbb24fbf5d220fbe137d43651ba3802a9806b1a3": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "Merge trunk into branch.\n\nConflicts resolved:\nC       hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestShortCircuitLocalRead.java\n!     C hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/SocketCache.java\nC       hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java\nC       hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java\nC       hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java\nC       hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java\n\n(thanks to Colin for help resolving)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-347@1462652 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "29/03/13 2:33 PM",
      "commitName": "bbb24fbf5d220fbe137d43651ba3802a9806b1a3",
      "commitAuthor": "Todd Lipcon",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "Merge trunk into branch.\n\nConflicts resolved:\nC       hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestShortCircuitLocalRead.java\n!     C hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/SocketCache.java\nC       hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java\nC       hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java\nC       hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java\nC       hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java\n\n(thanks to Colin for help resolving)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-347@1462652 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "29/03/13 2:33 PM",
          "commitName": "bbb24fbf5d220fbe137d43651ba3802a9806b1a3",
          "commitAuthor": "Todd Lipcon",
          "commitDateOld": "27/03/13 12:28 PM",
          "commitNameOld": "694a6721316aea14c1244447974231abc8dff0cb",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 2.09,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,77 +1,78 @@\n-  static BlockReaderLocalLegacy newBlockReader(Configuration conf, String file,\n-      ExtendedBlock blk, Token\u003cBlockTokenIdentifier\u003e token, DatanodeInfo node,\n-      int socketTimeout, long startOffset, long length,\n-      boolean connectToDnViaHostname) throws IOException {\n+  static BlockReaderLocalLegacy newBlockReader(UserGroupInformation ugi,\n+      Configuration conf, String file, ExtendedBlock blk,\n+      Token\u003cBlockTokenIdentifier\u003e token, DatanodeInfo node, int socketTimeout,\n+      long startOffset, long length, boolean connectToDnViaHostname)\n+      throws IOException {\n \n     LocalDatanodeInfo localDatanodeInfo \u003d getLocalDatanodeInfo(node\n         .getIpcPort());\n     // check the cache first\n     BlockLocalPathInfo pathinfo \u003d localDatanodeInfo.getBlockLocalPathInfo(blk);\n     if (pathinfo \u003d\u003d null) {\n-      pathinfo \u003d getBlockPathInfo(blk, node, conf, socketTimeout, token,\n+      pathinfo \u003d getBlockPathInfo(ugi, blk, node, conf, socketTimeout, token,\n           connectToDnViaHostname);\n     }\n \n     // check to see if the file exists. It may so happen that the\n     // HDFS file has been deleted and this block-lookup is occurring\n     // on behalf of a new HDFS file. This time, the block file could\n     // be residing in a different portion of the fs.data.dir directory.\n     // In this case, we remove this entry from the cache. The next\n     // call to this method will re-populate the cache.\n     FileInputStream dataIn \u003d null;\n     FileInputStream checksumIn \u003d null;\n     BlockReaderLocalLegacy localBlockReader \u003d null;\n     boolean skipChecksumCheck \u003d skipChecksumCheck(conf);\n     try {\n       // get a local file system\n       File blkfile \u003d new File(pathinfo.getBlockPath());\n       dataIn \u003d new FileInputStream(blkfile);\n \n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"New BlockReaderLocalLegacy for file \" + blkfile + \" of size \"\n             + blkfile.length() + \" startOffset \" + startOffset + \" length \"\n             + length + \" short circuit checksum \" + !skipChecksumCheck);\n       }\n \n       if (!skipChecksumCheck) {\n         // get the metadata file\n         File metafile \u003d new File(pathinfo.getMetaPath());\n         checksumIn \u003d new FileInputStream(metafile);\n \n         // read and handle the common header here. For now just a version\n         BlockMetadataHeader header \u003d BlockMetadataHeader\n             .readHeader(new DataInputStream(checksumIn));\n         short version \u003d header.getVersion();\n         if (version !\u003d BlockMetadataHeader.VERSION) {\n           LOG.warn(\"Wrong version (\" + version + \") for metadata file for \"\n               + blk + \" ignoring ...\");\n         }\n         DataChecksum checksum \u003d header.getChecksum();\n         long firstChunkOffset \u003d startOffset\n             - (startOffset % checksum.getBytesPerChecksum());\n         localBlockReader \u003d new BlockReaderLocalLegacy(conf, file, blk, token,\n             startOffset, length, pathinfo, checksum, true, dataIn,\n             firstChunkOffset, checksumIn);\n       } else {\n         localBlockReader \u003d new BlockReaderLocalLegacy(conf, file, blk, token,\n             startOffset, length, pathinfo, dataIn);\n       }\n     } catch (IOException e) {\n       // remove from cache\n       localDatanodeInfo.removeBlockLocalPathInfo(blk);\n       DFSClient.LOG.warn(\"BlockReaderLocalLegacy: Removing \" + blk\n           + \" from cache because local file \" + pathinfo.getBlockPath()\n           + \" could not be opened.\");\n       throw e;\n     } finally {\n       if (localBlockReader \u003d\u003d null) {\n         if (dataIn !\u003d null) {\n           dataIn.close();\n         }\n         if (checksumIn !\u003d null) {\n           checksumIn.close();\n         }\n       }\n     }\n     return localBlockReader;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static BlockReaderLocalLegacy newBlockReader(UserGroupInformation ugi,\n      Configuration conf, String file, ExtendedBlock blk,\n      Token\u003cBlockTokenIdentifier\u003e token, DatanodeInfo node, int socketTimeout,\n      long startOffset, long length, boolean connectToDnViaHostname)\n      throws IOException {\n\n    LocalDatanodeInfo localDatanodeInfo \u003d getLocalDatanodeInfo(node\n        .getIpcPort());\n    // check the cache first\n    BlockLocalPathInfo pathinfo \u003d localDatanodeInfo.getBlockLocalPathInfo(blk);\n    if (pathinfo \u003d\u003d null) {\n      pathinfo \u003d getBlockPathInfo(ugi, blk, node, conf, socketTimeout, token,\n          connectToDnViaHostname);\n    }\n\n    // check to see if the file exists. It may so happen that the\n    // HDFS file has been deleted and this block-lookup is occurring\n    // on behalf of a new HDFS file. This time, the block file could\n    // be residing in a different portion of the fs.data.dir directory.\n    // In this case, we remove this entry from the cache. The next\n    // call to this method will re-populate the cache.\n    FileInputStream dataIn \u003d null;\n    FileInputStream checksumIn \u003d null;\n    BlockReaderLocalLegacy localBlockReader \u003d null;\n    boolean skipChecksumCheck \u003d skipChecksumCheck(conf);\n    try {\n      // get a local file system\n      File blkfile \u003d new File(pathinfo.getBlockPath());\n      dataIn \u003d new FileInputStream(blkfile);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"New BlockReaderLocalLegacy for file \" + blkfile + \" of size \"\n            + blkfile.length() + \" startOffset \" + startOffset + \" length \"\n            + length + \" short circuit checksum \" + !skipChecksumCheck);\n      }\n\n      if (!skipChecksumCheck) {\n        // get the metadata file\n        File metafile \u003d new File(pathinfo.getMetaPath());\n        checksumIn \u003d new FileInputStream(metafile);\n\n        // read and handle the common header here. For now just a version\n        BlockMetadataHeader header \u003d BlockMetadataHeader\n            .readHeader(new DataInputStream(checksumIn));\n        short version \u003d header.getVersion();\n        if (version !\u003d BlockMetadataHeader.VERSION) {\n          LOG.warn(\"Wrong version (\" + version + \") for metadata file for \"\n              + blk + \" ignoring ...\");\n        }\n        DataChecksum checksum \u003d header.getChecksum();\n        long firstChunkOffset \u003d startOffset\n            - (startOffset % checksum.getBytesPerChecksum());\n        localBlockReader \u003d new BlockReaderLocalLegacy(conf, file, blk, token,\n            startOffset, length, pathinfo, checksum, true, dataIn,\n            firstChunkOffset, checksumIn);\n      } else {\n        localBlockReader \u003d new BlockReaderLocalLegacy(conf, file, blk, token,\n            startOffset, length, pathinfo, dataIn);\n      }\n    } catch (IOException e) {\n      // remove from cache\n      localDatanodeInfo.removeBlockLocalPathInfo(blk);\n      DFSClient.LOG.warn(\"BlockReaderLocalLegacy: Removing \" + blk\n          + \" from cache because local file \" + pathinfo.getBlockPath()\n          + \" could not be opened.\");\n      throw e;\n    } finally {\n      if (localBlockReader \u003d\u003d null) {\n        if (dataIn !\u003d null) {\n          dataIn.close();\n        }\n        if (checksumIn !\u003d null) {\n          checksumIn.close();\n        }\n      }\n    }\n    return localBlockReader;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocalLegacy.java",
          "extendedDetails": {
            "oldValue": "[conf-Configuration, file-String, blk-ExtendedBlock, token-Token\u003cBlockTokenIdentifier\u003e, node-DatanodeInfo, socketTimeout-int, startOffset-long, length-long, connectToDnViaHostname-boolean]",
            "newValue": "[ugi-UserGroupInformation, conf-Configuration, file-String, blk-ExtendedBlock, token-Token\u003cBlockTokenIdentifier\u003e, node-DatanodeInfo, socketTimeout-int, startOffset-long, length-long, connectToDnViaHostname-boolean]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "Merge trunk into branch.\n\nConflicts resolved:\nC       hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestShortCircuitLocalRead.java\n!     C hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/SocketCache.java\nC       hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java\nC       hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java\nC       hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java\nC       hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocal.java\n\n(thanks to Colin for help resolving)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-347@1462652 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "29/03/13 2:33 PM",
          "commitName": "bbb24fbf5d220fbe137d43651ba3802a9806b1a3",
          "commitAuthor": "Todd Lipcon",
          "commitDateOld": "27/03/13 12:28 PM",
          "commitNameOld": "694a6721316aea14c1244447974231abc8dff0cb",
          "commitAuthorOld": "Todd Lipcon",
          "daysBetweenCommits": 2.09,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,77 +1,78 @@\n-  static BlockReaderLocalLegacy newBlockReader(Configuration conf, String file,\n-      ExtendedBlock blk, Token\u003cBlockTokenIdentifier\u003e token, DatanodeInfo node,\n-      int socketTimeout, long startOffset, long length,\n-      boolean connectToDnViaHostname) throws IOException {\n+  static BlockReaderLocalLegacy newBlockReader(UserGroupInformation ugi,\n+      Configuration conf, String file, ExtendedBlock blk,\n+      Token\u003cBlockTokenIdentifier\u003e token, DatanodeInfo node, int socketTimeout,\n+      long startOffset, long length, boolean connectToDnViaHostname)\n+      throws IOException {\n \n     LocalDatanodeInfo localDatanodeInfo \u003d getLocalDatanodeInfo(node\n         .getIpcPort());\n     // check the cache first\n     BlockLocalPathInfo pathinfo \u003d localDatanodeInfo.getBlockLocalPathInfo(blk);\n     if (pathinfo \u003d\u003d null) {\n-      pathinfo \u003d getBlockPathInfo(blk, node, conf, socketTimeout, token,\n+      pathinfo \u003d getBlockPathInfo(ugi, blk, node, conf, socketTimeout, token,\n           connectToDnViaHostname);\n     }\n \n     // check to see if the file exists. It may so happen that the\n     // HDFS file has been deleted and this block-lookup is occurring\n     // on behalf of a new HDFS file. This time, the block file could\n     // be residing in a different portion of the fs.data.dir directory.\n     // In this case, we remove this entry from the cache. The next\n     // call to this method will re-populate the cache.\n     FileInputStream dataIn \u003d null;\n     FileInputStream checksumIn \u003d null;\n     BlockReaderLocalLegacy localBlockReader \u003d null;\n     boolean skipChecksumCheck \u003d skipChecksumCheck(conf);\n     try {\n       // get a local file system\n       File blkfile \u003d new File(pathinfo.getBlockPath());\n       dataIn \u003d new FileInputStream(blkfile);\n \n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"New BlockReaderLocalLegacy for file \" + blkfile + \" of size \"\n             + blkfile.length() + \" startOffset \" + startOffset + \" length \"\n             + length + \" short circuit checksum \" + !skipChecksumCheck);\n       }\n \n       if (!skipChecksumCheck) {\n         // get the metadata file\n         File metafile \u003d new File(pathinfo.getMetaPath());\n         checksumIn \u003d new FileInputStream(metafile);\n \n         // read and handle the common header here. For now just a version\n         BlockMetadataHeader header \u003d BlockMetadataHeader\n             .readHeader(new DataInputStream(checksumIn));\n         short version \u003d header.getVersion();\n         if (version !\u003d BlockMetadataHeader.VERSION) {\n           LOG.warn(\"Wrong version (\" + version + \") for metadata file for \"\n               + blk + \" ignoring ...\");\n         }\n         DataChecksum checksum \u003d header.getChecksum();\n         long firstChunkOffset \u003d startOffset\n             - (startOffset % checksum.getBytesPerChecksum());\n         localBlockReader \u003d new BlockReaderLocalLegacy(conf, file, blk, token,\n             startOffset, length, pathinfo, checksum, true, dataIn,\n             firstChunkOffset, checksumIn);\n       } else {\n         localBlockReader \u003d new BlockReaderLocalLegacy(conf, file, blk, token,\n             startOffset, length, pathinfo, dataIn);\n       }\n     } catch (IOException e) {\n       // remove from cache\n       localDatanodeInfo.removeBlockLocalPathInfo(blk);\n       DFSClient.LOG.warn(\"BlockReaderLocalLegacy: Removing \" + blk\n           + \" from cache because local file \" + pathinfo.getBlockPath()\n           + \" could not be opened.\");\n       throw e;\n     } finally {\n       if (localBlockReader \u003d\u003d null) {\n         if (dataIn !\u003d null) {\n           dataIn.close();\n         }\n         if (checksumIn !\u003d null) {\n           checksumIn.close();\n         }\n       }\n     }\n     return localBlockReader;\n   }\n\\ No newline at end of file\n",
          "actualSource": "  static BlockReaderLocalLegacy newBlockReader(UserGroupInformation ugi,\n      Configuration conf, String file, ExtendedBlock blk,\n      Token\u003cBlockTokenIdentifier\u003e token, DatanodeInfo node, int socketTimeout,\n      long startOffset, long length, boolean connectToDnViaHostname)\n      throws IOException {\n\n    LocalDatanodeInfo localDatanodeInfo \u003d getLocalDatanodeInfo(node\n        .getIpcPort());\n    // check the cache first\n    BlockLocalPathInfo pathinfo \u003d localDatanodeInfo.getBlockLocalPathInfo(blk);\n    if (pathinfo \u003d\u003d null) {\n      pathinfo \u003d getBlockPathInfo(ugi, blk, node, conf, socketTimeout, token,\n          connectToDnViaHostname);\n    }\n\n    // check to see if the file exists. It may so happen that the\n    // HDFS file has been deleted and this block-lookup is occurring\n    // on behalf of a new HDFS file. This time, the block file could\n    // be residing in a different portion of the fs.data.dir directory.\n    // In this case, we remove this entry from the cache. The next\n    // call to this method will re-populate the cache.\n    FileInputStream dataIn \u003d null;\n    FileInputStream checksumIn \u003d null;\n    BlockReaderLocalLegacy localBlockReader \u003d null;\n    boolean skipChecksumCheck \u003d skipChecksumCheck(conf);\n    try {\n      // get a local file system\n      File blkfile \u003d new File(pathinfo.getBlockPath());\n      dataIn \u003d new FileInputStream(blkfile);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"New BlockReaderLocalLegacy for file \" + blkfile + \" of size \"\n            + blkfile.length() + \" startOffset \" + startOffset + \" length \"\n            + length + \" short circuit checksum \" + !skipChecksumCheck);\n      }\n\n      if (!skipChecksumCheck) {\n        // get the metadata file\n        File metafile \u003d new File(pathinfo.getMetaPath());\n        checksumIn \u003d new FileInputStream(metafile);\n\n        // read and handle the common header here. For now just a version\n        BlockMetadataHeader header \u003d BlockMetadataHeader\n            .readHeader(new DataInputStream(checksumIn));\n        short version \u003d header.getVersion();\n        if (version !\u003d BlockMetadataHeader.VERSION) {\n          LOG.warn(\"Wrong version (\" + version + \") for metadata file for \"\n              + blk + \" ignoring ...\");\n        }\n        DataChecksum checksum \u003d header.getChecksum();\n        long firstChunkOffset \u003d startOffset\n            - (startOffset % checksum.getBytesPerChecksum());\n        localBlockReader \u003d new BlockReaderLocalLegacy(conf, file, blk, token,\n            startOffset, length, pathinfo, checksum, true, dataIn,\n            firstChunkOffset, checksumIn);\n      } else {\n        localBlockReader \u003d new BlockReaderLocalLegacy(conf, file, blk, token,\n            startOffset, length, pathinfo, dataIn);\n      }\n    } catch (IOException e) {\n      // remove from cache\n      localDatanodeInfo.removeBlockLocalPathInfo(blk);\n      DFSClient.LOG.warn(\"BlockReaderLocalLegacy: Removing \" + blk\n          + \" from cache because local file \" + pathinfo.getBlockPath()\n          + \" could not be opened.\");\n      throw e;\n    } finally {\n      if (localBlockReader \u003d\u003d null) {\n        if (dataIn !\u003d null) {\n          dataIn.close();\n        }\n        if (checksumIn !\u003d null) {\n          checksumIn.close();\n        }\n      }\n    }\n    return localBlockReader;\n  }",
          "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocalLegacy.java",
          "extendedDetails": {}
        }
      ]
    },
    "694a6721316aea14c1244447974231abc8dff0cb": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-4538. Allow use of legacy blockreader. Contributed by Colin Patrick McCabe.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-347@1461818 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/03/13 12:28 PM",
      "commitName": "694a6721316aea14c1244447974231abc8dff0cb",
      "commitAuthor": "Todd Lipcon",
      "diff": "@@ -0,0 +1,77 @@\n+  static BlockReaderLocalLegacy newBlockReader(Configuration conf, String file,\n+      ExtendedBlock blk, Token\u003cBlockTokenIdentifier\u003e token, DatanodeInfo node,\n+      int socketTimeout, long startOffset, long length,\n+      boolean connectToDnViaHostname) throws IOException {\n+\n+    LocalDatanodeInfo localDatanodeInfo \u003d getLocalDatanodeInfo(node\n+        .getIpcPort());\n+    // check the cache first\n+    BlockLocalPathInfo pathinfo \u003d localDatanodeInfo.getBlockLocalPathInfo(blk);\n+    if (pathinfo \u003d\u003d null) {\n+      pathinfo \u003d getBlockPathInfo(blk, node, conf, socketTimeout, token,\n+          connectToDnViaHostname);\n+    }\n+\n+    // check to see if the file exists. It may so happen that the\n+    // HDFS file has been deleted and this block-lookup is occurring\n+    // on behalf of a new HDFS file. This time, the block file could\n+    // be residing in a different portion of the fs.data.dir directory.\n+    // In this case, we remove this entry from the cache. The next\n+    // call to this method will re-populate the cache.\n+    FileInputStream dataIn \u003d null;\n+    FileInputStream checksumIn \u003d null;\n+    BlockReaderLocalLegacy localBlockReader \u003d null;\n+    boolean skipChecksumCheck \u003d skipChecksumCheck(conf);\n+    try {\n+      // get a local file system\n+      File blkfile \u003d new File(pathinfo.getBlockPath());\n+      dataIn \u003d new FileInputStream(blkfile);\n+\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"New BlockReaderLocalLegacy for file \" + blkfile + \" of size \"\n+            + blkfile.length() + \" startOffset \" + startOffset + \" length \"\n+            + length + \" short circuit checksum \" + !skipChecksumCheck);\n+      }\n+\n+      if (!skipChecksumCheck) {\n+        // get the metadata file\n+        File metafile \u003d new File(pathinfo.getMetaPath());\n+        checksumIn \u003d new FileInputStream(metafile);\n+\n+        // read and handle the common header here. For now just a version\n+        BlockMetadataHeader header \u003d BlockMetadataHeader\n+            .readHeader(new DataInputStream(checksumIn));\n+        short version \u003d header.getVersion();\n+        if (version !\u003d BlockMetadataHeader.VERSION) {\n+          LOG.warn(\"Wrong version (\" + version + \") for metadata file for \"\n+              + blk + \" ignoring ...\");\n+        }\n+        DataChecksum checksum \u003d header.getChecksum();\n+        long firstChunkOffset \u003d startOffset\n+            - (startOffset % checksum.getBytesPerChecksum());\n+        localBlockReader \u003d new BlockReaderLocalLegacy(conf, file, blk, token,\n+            startOffset, length, pathinfo, checksum, true, dataIn,\n+            firstChunkOffset, checksumIn);\n+      } else {\n+        localBlockReader \u003d new BlockReaderLocalLegacy(conf, file, blk, token,\n+            startOffset, length, pathinfo, dataIn);\n+      }\n+    } catch (IOException e) {\n+      // remove from cache\n+      localDatanodeInfo.removeBlockLocalPathInfo(blk);\n+      DFSClient.LOG.warn(\"BlockReaderLocalLegacy: Removing \" + blk\n+          + \" from cache because local file \" + pathinfo.getBlockPath()\n+          + \" could not be opened.\");\n+      throw e;\n+    } finally {\n+      if (localBlockReader \u003d\u003d null) {\n+        if (dataIn !\u003d null) {\n+          dataIn.close();\n+        }\n+        if (checksumIn !\u003d null) {\n+          checksumIn.close();\n+        }\n+      }\n+    }\n+    return localBlockReader;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  static BlockReaderLocalLegacy newBlockReader(Configuration conf, String file,\n      ExtendedBlock blk, Token\u003cBlockTokenIdentifier\u003e token, DatanodeInfo node,\n      int socketTimeout, long startOffset, long length,\n      boolean connectToDnViaHostname) throws IOException {\n\n    LocalDatanodeInfo localDatanodeInfo \u003d getLocalDatanodeInfo(node\n        .getIpcPort());\n    // check the cache first\n    BlockLocalPathInfo pathinfo \u003d localDatanodeInfo.getBlockLocalPathInfo(blk);\n    if (pathinfo \u003d\u003d null) {\n      pathinfo \u003d getBlockPathInfo(blk, node, conf, socketTimeout, token,\n          connectToDnViaHostname);\n    }\n\n    // check to see if the file exists. It may so happen that the\n    // HDFS file has been deleted and this block-lookup is occurring\n    // on behalf of a new HDFS file. This time, the block file could\n    // be residing in a different portion of the fs.data.dir directory.\n    // In this case, we remove this entry from the cache. The next\n    // call to this method will re-populate the cache.\n    FileInputStream dataIn \u003d null;\n    FileInputStream checksumIn \u003d null;\n    BlockReaderLocalLegacy localBlockReader \u003d null;\n    boolean skipChecksumCheck \u003d skipChecksumCheck(conf);\n    try {\n      // get a local file system\n      File blkfile \u003d new File(pathinfo.getBlockPath());\n      dataIn \u003d new FileInputStream(blkfile);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"New BlockReaderLocalLegacy for file \" + blkfile + \" of size \"\n            + blkfile.length() + \" startOffset \" + startOffset + \" length \"\n            + length + \" short circuit checksum \" + !skipChecksumCheck);\n      }\n\n      if (!skipChecksumCheck) {\n        // get the metadata file\n        File metafile \u003d new File(pathinfo.getMetaPath());\n        checksumIn \u003d new FileInputStream(metafile);\n\n        // read and handle the common header here. For now just a version\n        BlockMetadataHeader header \u003d BlockMetadataHeader\n            .readHeader(new DataInputStream(checksumIn));\n        short version \u003d header.getVersion();\n        if (version !\u003d BlockMetadataHeader.VERSION) {\n          LOG.warn(\"Wrong version (\" + version + \") for metadata file for \"\n              + blk + \" ignoring ...\");\n        }\n        DataChecksum checksum \u003d header.getChecksum();\n        long firstChunkOffset \u003d startOffset\n            - (startOffset % checksum.getBytesPerChecksum());\n        localBlockReader \u003d new BlockReaderLocalLegacy(conf, file, blk, token,\n            startOffset, length, pathinfo, checksum, true, dataIn,\n            firstChunkOffset, checksumIn);\n      } else {\n        localBlockReader \u003d new BlockReaderLocalLegacy(conf, file, blk, token,\n            startOffset, length, pathinfo, dataIn);\n      }\n    } catch (IOException e) {\n      // remove from cache\n      localDatanodeInfo.removeBlockLocalPathInfo(blk);\n      DFSClient.LOG.warn(\"BlockReaderLocalLegacy: Removing \" + blk\n          + \" from cache because local file \" + pathinfo.getBlockPath()\n          + \" could not be opened.\");\n      throw e;\n    } finally {\n      if (localBlockReader \u003d\u003d null) {\n        if (dataIn !\u003d null) {\n          dataIn.close();\n        }\n        if (checksumIn !\u003d null) {\n          checksumIn.close();\n        }\n      }\n    }\n    return localBlockReader;\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/BlockReaderLocalLegacy.java"
    }
  }
}