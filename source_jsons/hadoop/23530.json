{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "HistoryFileManager.java",
  "functionName": "clean",
  "functionId": "clean",
  "sourceFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HistoryFileManager.java",
  "functionStartLine": 1184,
  "functionEndLine": 1223,
  "numCommitsSeen": 45,
  "timeTaken": 2079,
  "changeHistory": [
    "5ffb54694b52657f3b7de4560474ab740734e1b2",
    "84cec3c805867cf0c880c9ecb9fc220733032bc9",
    "8fa3ebd13451a243510eed5c2f3dd43cdf605a77",
    "7d04a96027ad75877b41b7cd8f67455dd13159d7",
    "cbb5f6109097a77f18f5fb0ba62ac132b8fa980f"
  ],
  "changeHistoryShort": {
    "5ffb54694b52657f3b7de4560474ab740734e1b2": "Ybodychange",
    "84cec3c805867cf0c880c9ecb9fc220733032bc9": "Ybodychange",
    "8fa3ebd13451a243510eed5c2f3dd43cdf605a77": "Ybodychange",
    "7d04a96027ad75877b41b7cd8f67455dd13159d7": "Ymultichange(Yparameterchange,Ybodychange)",
    "cbb5f6109097a77f18f5fb0ba62ac132b8fa980f": "Yintroduced"
  },
  "changeHistoryDetails": {
    "5ffb54694b52657f3b7de4560474ab740734e1b2": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-6684. High contention on scanning of user directory under immediate_done in Job History Server. Contributed by Haibo Chen\n",
      "commitDate": "10/05/16 9:03 AM",
      "commitName": "5ffb54694b52657f3b7de4560474ab740734e1b2",
      "commitAuthor": "Jason Lowe",
      "commitDateOld": "20/04/16 7:02 PM",
      "commitNameOld": "1e48eefe5800975ea0c4295c9911ae3f572ed37d",
      "commitAuthorOld": "Jian He",
      "daysBetweenCommits": 19.58,
      "commitsBetweenForRepo": 113,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,40 +1,40 @@\n   void clean() throws IOException {\n     long cutoff \u003d System.currentTimeMillis() - maxHistoryAge;\n     boolean halted \u003d false;\n     List\u003cFileStatus\u003e serialDirList \u003d getHistoryDirsForCleaning(cutoff);\n     // Sort in ascending order. Relies on YYYY/MM/DD/Serial\n     Collections.sort(serialDirList);\n     for (FileStatus serialDir : serialDirList) {\n       List\u003cFileStatus\u003e historyFileList \u003d scanDirectoryForHistoryFiles(\n           serialDir.getPath(), doneDirFc);\n       for (FileStatus historyFile : historyFileList) {\n         JobIndexInfo jobIndexInfo \u003d FileNameIndexUtils.getIndexInfo(historyFile\n             .getPath().getName());\n         long effectiveTimestamp \u003d getEffectiveTimestamp(\n             jobIndexInfo.getFinishTime(), historyFile);\n         if (effectiveTimestamp \u003c\u003d cutoff) {\n           HistoryFileInfo fileInfo \u003d this.jobListCache.get(jobIndexInfo\n               .getJobId());\n           if (fileInfo \u003d\u003d null) {\n             String confFileName \u003d JobHistoryUtils\n                 .getIntermediateConfFileName(jobIndexInfo.getJobId());\n \n-            fileInfo \u003d new HistoryFileInfo(historyFile.getPath(), new Path(\n+            fileInfo \u003d createHistoryFileInfo(historyFile.getPath(), new Path(\n                 historyFile.getPath().getParent(), confFileName), null,\n                 jobIndexInfo, true);\n           }\n           deleteJobFromDone(fileInfo);\n         } else {\n           halted \u003d true;\n           break;\n         }\n       }\n       if (!halted) {\n         deleteDir(serialDir);\n         removeDirectoryFromSerialNumberIndex(serialDir.getPath());\n         existingDoneSubdirs.remove(serialDir.getPath());\n       } else {\n         break; // Don\u0027t scan any more directories.\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void clean() throws IOException {\n    long cutoff \u003d System.currentTimeMillis() - maxHistoryAge;\n    boolean halted \u003d false;\n    List\u003cFileStatus\u003e serialDirList \u003d getHistoryDirsForCleaning(cutoff);\n    // Sort in ascending order. Relies on YYYY/MM/DD/Serial\n    Collections.sort(serialDirList);\n    for (FileStatus serialDir : serialDirList) {\n      List\u003cFileStatus\u003e historyFileList \u003d scanDirectoryForHistoryFiles(\n          serialDir.getPath(), doneDirFc);\n      for (FileStatus historyFile : historyFileList) {\n        JobIndexInfo jobIndexInfo \u003d FileNameIndexUtils.getIndexInfo(historyFile\n            .getPath().getName());\n        long effectiveTimestamp \u003d getEffectiveTimestamp(\n            jobIndexInfo.getFinishTime(), historyFile);\n        if (effectiveTimestamp \u003c\u003d cutoff) {\n          HistoryFileInfo fileInfo \u003d this.jobListCache.get(jobIndexInfo\n              .getJobId());\n          if (fileInfo \u003d\u003d null) {\n            String confFileName \u003d JobHistoryUtils\n                .getIntermediateConfFileName(jobIndexInfo.getJobId());\n\n            fileInfo \u003d createHistoryFileInfo(historyFile.getPath(), new Path(\n                historyFile.getPath().getParent(), confFileName), null,\n                jobIndexInfo, true);\n          }\n          deleteJobFromDone(fileInfo);\n        } else {\n          halted \u003d true;\n          break;\n        }\n      }\n      if (!halted) {\n        deleteDir(serialDir);\n        removeDirectoryFromSerialNumberIndex(serialDir.getPath());\n        existingDoneSubdirs.remove(serialDir.getPath());\n      } else {\n        break; // Don\u0027t scan any more directories.\n      }\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HistoryFileManager.java",
      "extendedDetails": {}
    },
    "84cec3c805867cf0c880c9ecb9fc220733032bc9": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-4680. Job history cleaner should only check timestamps of files in old enough directories (Robert Kanter via Sandy Ryza)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1536558 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "28/10/13 4:48 PM",
      "commitName": "84cec3c805867cf0c880c9ecb9fc220733032bc9",
      "commitAuthor": "Sanford Ryza",
      "commitDateOld": "01/08/13 12:57 PM",
      "commitNameOld": "bfe5a528d8511d3ae54041d9a5a3d8acbd15f352",
      "commitAuthorOld": "Jason Darrell Lowe",
      "daysBetweenCommits": 88.16,
      "commitsBetweenForRepo": 512,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,43 +1,40 @@\n   void clean() throws IOException {\n-    // TODO this should be replaced by something that knows about the directory\n-    // structure and will put less of a load on HDFS.\n     long cutoff \u003d System.currentTimeMillis() - maxHistoryAge;\n     boolean halted \u003d false;\n-    // TODO Delete YYYY/MM/DD directories.\n-    List\u003cFileStatus\u003e serialDirList \u003d findTimestampedDirectories();\n+    List\u003cFileStatus\u003e serialDirList \u003d getHistoryDirsForCleaning(cutoff);\n     // Sort in ascending order. Relies on YYYY/MM/DD/Serial\n     Collections.sort(serialDirList);\n     for (FileStatus serialDir : serialDirList) {\n       List\u003cFileStatus\u003e historyFileList \u003d scanDirectoryForHistoryFiles(\n           serialDir.getPath(), doneDirFc);\n       for (FileStatus historyFile : historyFileList) {\n         JobIndexInfo jobIndexInfo \u003d FileNameIndexUtils.getIndexInfo(historyFile\n             .getPath().getName());\n         long effectiveTimestamp \u003d getEffectiveTimestamp(\n             jobIndexInfo.getFinishTime(), historyFile);\n         if (effectiveTimestamp \u003c\u003d cutoff) {\n           HistoryFileInfo fileInfo \u003d this.jobListCache.get(jobIndexInfo\n               .getJobId());\n           if (fileInfo \u003d\u003d null) {\n             String confFileName \u003d JobHistoryUtils\n                 .getIntermediateConfFileName(jobIndexInfo.getJobId());\n \n             fileInfo \u003d new HistoryFileInfo(historyFile.getPath(), new Path(\n                 historyFile.getPath().getParent(), confFileName), null,\n                 jobIndexInfo, true);\n           }\n           deleteJobFromDone(fileInfo);\n         } else {\n           halted \u003d true;\n           break;\n         }\n       }\n       if (!halted) {\n         deleteDir(serialDir);\n         removeDirectoryFromSerialNumberIndex(serialDir.getPath());\n         existingDoneSubdirs.remove(serialDir.getPath());\n       } else {\n         break; // Don\u0027t scan any more directories.\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void clean() throws IOException {\n    long cutoff \u003d System.currentTimeMillis() - maxHistoryAge;\n    boolean halted \u003d false;\n    List\u003cFileStatus\u003e serialDirList \u003d getHistoryDirsForCleaning(cutoff);\n    // Sort in ascending order. Relies on YYYY/MM/DD/Serial\n    Collections.sort(serialDirList);\n    for (FileStatus serialDir : serialDirList) {\n      List\u003cFileStatus\u003e historyFileList \u003d scanDirectoryForHistoryFiles(\n          serialDir.getPath(), doneDirFc);\n      for (FileStatus historyFile : historyFileList) {\n        JobIndexInfo jobIndexInfo \u003d FileNameIndexUtils.getIndexInfo(historyFile\n            .getPath().getName());\n        long effectiveTimestamp \u003d getEffectiveTimestamp(\n            jobIndexInfo.getFinishTime(), historyFile);\n        if (effectiveTimestamp \u003c\u003d cutoff) {\n          HistoryFileInfo fileInfo \u003d this.jobListCache.get(jobIndexInfo\n              .getJobId());\n          if (fileInfo \u003d\u003d null) {\n            String confFileName \u003d JobHistoryUtils\n                .getIntermediateConfFileName(jobIndexInfo.getJobId());\n\n            fileInfo \u003d new HistoryFileInfo(historyFile.getPath(), new Path(\n                historyFile.getPath().getParent(), confFileName), null,\n                jobIndexInfo, true);\n          }\n          deleteJobFromDone(fileInfo);\n        } else {\n          halted \u003d true;\n          break;\n        }\n      }\n      if (!halted) {\n        deleteDir(serialDir);\n        removeDirectoryFromSerialNumberIndex(serialDir.getPath());\n        existingDoneSubdirs.remove(serialDir.getPath());\n      } else {\n        break; // Don\u0027t scan any more directories.\n      }\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HistoryFileManager.java",
      "extendedDetails": {}
    },
    "8fa3ebd13451a243510eed5c2f3dd43cdf605a77": {
      "type": "Ybodychange",
      "commitMessage": "MAPREDUCE-5386. Ability to refresh history server job retention and job cleaner settings. Contributed by Ashwin Shankar\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1507135 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "25/07/13 2:27 PM",
      "commitName": "8fa3ebd13451a243510eed5c2f3dd43cdf605a77",
      "commitAuthor": "Jason Darrell Lowe",
      "commitDateOld": "16/06/13 11:39 PM",
      "commitNameOld": "b9efe6bd4a1277b4067ecde715a7713a85968886",
      "commitAuthorOld": "Vinod Kumar Vavilapalli",
      "daysBetweenCommits": 38.62,
      "commitsBetweenForRepo": 218,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,43 +1,43 @@\n   void clean() throws IOException {\n     // TODO this should be replaced by something that knows about the directory\n     // structure and will put less of a load on HDFS.\n     long cutoff \u003d System.currentTimeMillis() - maxHistoryAge;\n     boolean halted \u003d false;\n     // TODO Delete YYYY/MM/DD directories.\n     List\u003cFileStatus\u003e serialDirList \u003d findTimestampedDirectories();\n     // Sort in ascending order. Relies on YYYY/MM/DD/Serial\n     Collections.sort(serialDirList);\n     for (FileStatus serialDir : serialDirList) {\n       List\u003cFileStatus\u003e historyFileList \u003d scanDirectoryForHistoryFiles(\n           serialDir.getPath(), doneDirFc);\n       for (FileStatus historyFile : historyFileList) {\n         JobIndexInfo jobIndexInfo \u003d FileNameIndexUtils.getIndexInfo(historyFile\n             .getPath().getName());\n         long effectiveTimestamp \u003d getEffectiveTimestamp(\n             jobIndexInfo.getFinishTime(), historyFile);\n         if (effectiveTimestamp \u003c\u003d cutoff) {\n           HistoryFileInfo fileInfo \u003d this.jobListCache.get(jobIndexInfo\n               .getJobId());\n           if (fileInfo \u003d\u003d null) {\n             String confFileName \u003d JobHistoryUtils\n                 .getIntermediateConfFileName(jobIndexInfo.getJobId());\n \n             fileInfo \u003d new HistoryFileInfo(historyFile.getPath(), new Path(\n                 historyFile.getPath().getParent(), confFileName), null,\n                 jobIndexInfo, true);\n           }\n           deleteJobFromDone(fileInfo);\n         } else {\n           halted \u003d true;\n           break;\n         }\n       }\n       if (!halted) {\n-        doneDirFc.delete(doneDirFc.makeQualified(serialDir.getPath()), true);\n+        deleteDir(serialDir);\n         removeDirectoryFromSerialNumberIndex(serialDir.getPath());\n         existingDoneSubdirs.remove(serialDir.getPath());\n       } else {\n         break; // Don\u0027t scan any more directories.\n       }\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  void clean() throws IOException {\n    // TODO this should be replaced by something that knows about the directory\n    // structure and will put less of a load on HDFS.\n    long cutoff \u003d System.currentTimeMillis() - maxHistoryAge;\n    boolean halted \u003d false;\n    // TODO Delete YYYY/MM/DD directories.\n    List\u003cFileStatus\u003e serialDirList \u003d findTimestampedDirectories();\n    // Sort in ascending order. Relies on YYYY/MM/DD/Serial\n    Collections.sort(serialDirList);\n    for (FileStatus serialDir : serialDirList) {\n      List\u003cFileStatus\u003e historyFileList \u003d scanDirectoryForHistoryFiles(\n          serialDir.getPath(), doneDirFc);\n      for (FileStatus historyFile : historyFileList) {\n        JobIndexInfo jobIndexInfo \u003d FileNameIndexUtils.getIndexInfo(historyFile\n            .getPath().getName());\n        long effectiveTimestamp \u003d getEffectiveTimestamp(\n            jobIndexInfo.getFinishTime(), historyFile);\n        if (effectiveTimestamp \u003c\u003d cutoff) {\n          HistoryFileInfo fileInfo \u003d this.jobListCache.get(jobIndexInfo\n              .getJobId());\n          if (fileInfo \u003d\u003d null) {\n            String confFileName \u003d JobHistoryUtils\n                .getIntermediateConfFileName(jobIndexInfo.getJobId());\n\n            fileInfo \u003d new HistoryFileInfo(historyFile.getPath(), new Path(\n                historyFile.getPath().getParent(), confFileName), null,\n                jobIndexInfo, true);\n          }\n          deleteJobFromDone(fileInfo);\n        } else {\n          halted \u003d true;\n          break;\n        }\n      }\n      if (!halted) {\n        deleteDir(serialDir);\n        removeDirectoryFromSerialNumberIndex(serialDir.getPath());\n        existingDoneSubdirs.remove(serialDir.getPath());\n      } else {\n        break; // Don\u0027t scan any more directories.\n      }\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HistoryFileManager.java",
      "extendedDetails": {}
    },
    "7d04a96027ad75877b41b7cd8f67455dd13159d7": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "MAPREDUCE-3972. Fix locking and exception issues in JobHistory server. (Contributed by Robert Joseph Evans)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1327354 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "17/04/12 6:59 PM",
      "commitName": "7d04a96027ad75877b41b7cd8f67455dd13159d7",
      "commitAuthor": "Siddharth Seth",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "MAPREDUCE-3972. Fix locking and exception issues in JobHistory server. (Contributed by Robert Joseph Evans)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1327354 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "17/04/12 6:59 PM",
          "commitName": "7d04a96027ad75877b41b7cd8f67455dd13159d7",
          "commitAuthor": "Siddharth Seth",
          "commitDateOld": "10/04/12 11:11 AM",
          "commitNameOld": "cbb5f6109097a77f18f5fb0ba62ac132b8fa980f",
          "commitAuthorOld": "Thomas Graves",
          "daysBetweenCommits": 7.32,
          "commitsBetweenForRepo": 59,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,40 +1,43 @@\n-  void clean(long cutoff, HistoryStorage storage) throws IOException {\n+  void clean() throws IOException {\n     // TODO this should be replaced by something that knows about the directory\n     // structure and will put less of a load on HDFS.\n+    long cutoff \u003d System.currentTimeMillis() - maxHistoryAge;\n     boolean halted \u003d false;\n     // TODO Delete YYYY/MM/DD directories.\n     List\u003cFileStatus\u003e serialDirList \u003d findTimestampedDirectories();\n     // Sort in ascending order. Relies on YYYY/MM/DD/Serial\n     Collections.sort(serialDirList);\n     for (FileStatus serialDir : serialDirList) {\n       List\u003cFileStatus\u003e historyFileList \u003d scanDirectoryForHistoryFiles(\n           serialDir.getPath(), doneDirFc);\n       for (FileStatus historyFile : historyFileList) {\n         JobIndexInfo jobIndexInfo \u003d FileNameIndexUtils.getIndexInfo(historyFile\n             .getPath().getName());\n         long effectiveTimestamp \u003d getEffectiveTimestamp(\n             jobIndexInfo.getFinishTime(), historyFile);\n         if (effectiveTimestamp \u003c\u003d cutoff) {\n-          String confFileName \u003d JobHistoryUtils\n-              .getIntermediateConfFileName(jobIndexInfo.getJobId());\n-          MetaInfo metaInfo \u003d new MetaInfo(historyFile.getPath(), new Path(\n-              historyFile.getPath().getParent(), confFileName), null,\n-              jobIndexInfo);\n-          storage.jobRemovedFromHDFS(metaInfo.getJobId());\n-          deleteJobFromDone(metaInfo);\n+          HistoryFileInfo fileInfo \u003d this.jobListCache.get(jobIndexInfo\n+              .getJobId());\n+          if (fileInfo \u003d\u003d null) {\n+            String confFileName \u003d JobHistoryUtils\n+                .getIntermediateConfFileName(jobIndexInfo.getJobId());\n+\n+            fileInfo \u003d new HistoryFileInfo(historyFile.getPath(), new Path(\n+                historyFile.getPath().getParent(), confFileName), null,\n+                jobIndexInfo, true);\n+          }\n+          deleteJobFromDone(fileInfo);\n         } else {\n           halted \u003d true;\n           break;\n         }\n       }\n       if (!halted) {\n         doneDirFc.delete(doneDirFc.makeQualified(serialDir.getPath()), true);\n         removeDirectoryFromSerialNumberIndex(serialDir.getPath());\n-        synchronized (existingDoneSubdirs) {\n-          existingDoneSubdirs.remove(serialDir.getPath());\n-        }\n+        existingDoneSubdirs.remove(serialDir.getPath());\n       } else {\n         break; // Don\u0027t scan any more directories.\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void clean() throws IOException {\n    // TODO this should be replaced by something that knows about the directory\n    // structure and will put less of a load on HDFS.\n    long cutoff \u003d System.currentTimeMillis() - maxHistoryAge;\n    boolean halted \u003d false;\n    // TODO Delete YYYY/MM/DD directories.\n    List\u003cFileStatus\u003e serialDirList \u003d findTimestampedDirectories();\n    // Sort in ascending order. Relies on YYYY/MM/DD/Serial\n    Collections.sort(serialDirList);\n    for (FileStatus serialDir : serialDirList) {\n      List\u003cFileStatus\u003e historyFileList \u003d scanDirectoryForHistoryFiles(\n          serialDir.getPath(), doneDirFc);\n      for (FileStatus historyFile : historyFileList) {\n        JobIndexInfo jobIndexInfo \u003d FileNameIndexUtils.getIndexInfo(historyFile\n            .getPath().getName());\n        long effectiveTimestamp \u003d getEffectiveTimestamp(\n            jobIndexInfo.getFinishTime(), historyFile);\n        if (effectiveTimestamp \u003c\u003d cutoff) {\n          HistoryFileInfo fileInfo \u003d this.jobListCache.get(jobIndexInfo\n              .getJobId());\n          if (fileInfo \u003d\u003d null) {\n            String confFileName \u003d JobHistoryUtils\n                .getIntermediateConfFileName(jobIndexInfo.getJobId());\n\n            fileInfo \u003d new HistoryFileInfo(historyFile.getPath(), new Path(\n                historyFile.getPath().getParent(), confFileName), null,\n                jobIndexInfo, true);\n          }\n          deleteJobFromDone(fileInfo);\n        } else {\n          halted \u003d true;\n          break;\n        }\n      }\n      if (!halted) {\n        doneDirFc.delete(doneDirFc.makeQualified(serialDir.getPath()), true);\n        removeDirectoryFromSerialNumberIndex(serialDir.getPath());\n        existingDoneSubdirs.remove(serialDir.getPath());\n      } else {\n        break; // Don\u0027t scan any more directories.\n      }\n    }\n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HistoryFileManager.java",
          "extendedDetails": {
            "oldValue": "[cutoff-long, storage-HistoryStorage]",
            "newValue": "[]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "MAPREDUCE-3972. Fix locking and exception issues in JobHistory server. (Contributed by Robert Joseph Evans)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1327354 13f79535-47bb-0310-9956-ffa450edef68\n",
          "commitDate": "17/04/12 6:59 PM",
          "commitName": "7d04a96027ad75877b41b7cd8f67455dd13159d7",
          "commitAuthor": "Siddharth Seth",
          "commitDateOld": "10/04/12 11:11 AM",
          "commitNameOld": "cbb5f6109097a77f18f5fb0ba62ac132b8fa980f",
          "commitAuthorOld": "Thomas Graves",
          "daysBetweenCommits": 7.32,
          "commitsBetweenForRepo": 59,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,40 +1,43 @@\n-  void clean(long cutoff, HistoryStorage storage) throws IOException {\n+  void clean() throws IOException {\n     // TODO this should be replaced by something that knows about the directory\n     // structure and will put less of a load on HDFS.\n+    long cutoff \u003d System.currentTimeMillis() - maxHistoryAge;\n     boolean halted \u003d false;\n     // TODO Delete YYYY/MM/DD directories.\n     List\u003cFileStatus\u003e serialDirList \u003d findTimestampedDirectories();\n     // Sort in ascending order. Relies on YYYY/MM/DD/Serial\n     Collections.sort(serialDirList);\n     for (FileStatus serialDir : serialDirList) {\n       List\u003cFileStatus\u003e historyFileList \u003d scanDirectoryForHistoryFiles(\n           serialDir.getPath(), doneDirFc);\n       for (FileStatus historyFile : historyFileList) {\n         JobIndexInfo jobIndexInfo \u003d FileNameIndexUtils.getIndexInfo(historyFile\n             .getPath().getName());\n         long effectiveTimestamp \u003d getEffectiveTimestamp(\n             jobIndexInfo.getFinishTime(), historyFile);\n         if (effectiveTimestamp \u003c\u003d cutoff) {\n-          String confFileName \u003d JobHistoryUtils\n-              .getIntermediateConfFileName(jobIndexInfo.getJobId());\n-          MetaInfo metaInfo \u003d new MetaInfo(historyFile.getPath(), new Path(\n-              historyFile.getPath().getParent(), confFileName), null,\n-              jobIndexInfo);\n-          storage.jobRemovedFromHDFS(metaInfo.getJobId());\n-          deleteJobFromDone(metaInfo);\n+          HistoryFileInfo fileInfo \u003d this.jobListCache.get(jobIndexInfo\n+              .getJobId());\n+          if (fileInfo \u003d\u003d null) {\n+            String confFileName \u003d JobHistoryUtils\n+                .getIntermediateConfFileName(jobIndexInfo.getJobId());\n+\n+            fileInfo \u003d new HistoryFileInfo(historyFile.getPath(), new Path(\n+                historyFile.getPath().getParent(), confFileName), null,\n+                jobIndexInfo, true);\n+          }\n+          deleteJobFromDone(fileInfo);\n         } else {\n           halted \u003d true;\n           break;\n         }\n       }\n       if (!halted) {\n         doneDirFc.delete(doneDirFc.makeQualified(serialDir.getPath()), true);\n         removeDirectoryFromSerialNumberIndex(serialDir.getPath());\n-        synchronized (existingDoneSubdirs) {\n-          existingDoneSubdirs.remove(serialDir.getPath());\n-        }\n+        existingDoneSubdirs.remove(serialDir.getPath());\n       } else {\n         break; // Don\u0027t scan any more directories.\n       }\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  void clean() throws IOException {\n    // TODO this should be replaced by something that knows about the directory\n    // structure and will put less of a load on HDFS.\n    long cutoff \u003d System.currentTimeMillis() - maxHistoryAge;\n    boolean halted \u003d false;\n    // TODO Delete YYYY/MM/DD directories.\n    List\u003cFileStatus\u003e serialDirList \u003d findTimestampedDirectories();\n    // Sort in ascending order. Relies on YYYY/MM/DD/Serial\n    Collections.sort(serialDirList);\n    for (FileStatus serialDir : serialDirList) {\n      List\u003cFileStatus\u003e historyFileList \u003d scanDirectoryForHistoryFiles(\n          serialDir.getPath(), doneDirFc);\n      for (FileStatus historyFile : historyFileList) {\n        JobIndexInfo jobIndexInfo \u003d FileNameIndexUtils.getIndexInfo(historyFile\n            .getPath().getName());\n        long effectiveTimestamp \u003d getEffectiveTimestamp(\n            jobIndexInfo.getFinishTime(), historyFile);\n        if (effectiveTimestamp \u003c\u003d cutoff) {\n          HistoryFileInfo fileInfo \u003d this.jobListCache.get(jobIndexInfo\n              .getJobId());\n          if (fileInfo \u003d\u003d null) {\n            String confFileName \u003d JobHistoryUtils\n                .getIntermediateConfFileName(jobIndexInfo.getJobId());\n\n            fileInfo \u003d new HistoryFileInfo(historyFile.getPath(), new Path(\n                historyFile.getPath().getParent(), confFileName), null,\n                jobIndexInfo, true);\n          }\n          deleteJobFromDone(fileInfo);\n        } else {\n          halted \u003d true;\n          break;\n        }\n      }\n      if (!halted) {\n        doneDirFc.delete(doneDirFc.makeQualified(serialDir.getPath()), true);\n        removeDirectoryFromSerialNumberIndex(serialDir.getPath());\n        existingDoneSubdirs.remove(serialDir.getPath());\n      } else {\n        break; // Don\u0027t scan any more directories.\n      }\n    }\n  }",
          "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HistoryFileManager.java",
          "extendedDetails": {}
        }
      ]
    },
    "cbb5f6109097a77f18f5fb0ba62ac132b8fa980f": {
      "type": "Yintroduced",
      "commitMessage": "MAPREDUCE-4059. The history server should have a separate pluggable storage/query interface. (Robert Evans via tgraves).\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1311896 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "10/04/12 11:11 AM",
      "commitName": "cbb5f6109097a77f18f5fb0ba62ac132b8fa980f",
      "commitAuthor": "Thomas Graves",
      "diff": "@@ -0,0 +1,40 @@\n+  void clean(long cutoff, HistoryStorage storage) throws IOException {\n+    // TODO this should be replaced by something that knows about the directory\n+    // structure and will put less of a load on HDFS.\n+    boolean halted \u003d false;\n+    // TODO Delete YYYY/MM/DD directories.\n+    List\u003cFileStatus\u003e serialDirList \u003d findTimestampedDirectories();\n+    // Sort in ascending order. Relies on YYYY/MM/DD/Serial\n+    Collections.sort(serialDirList);\n+    for (FileStatus serialDir : serialDirList) {\n+      List\u003cFileStatus\u003e historyFileList \u003d scanDirectoryForHistoryFiles(\n+          serialDir.getPath(), doneDirFc);\n+      for (FileStatus historyFile : historyFileList) {\n+        JobIndexInfo jobIndexInfo \u003d FileNameIndexUtils.getIndexInfo(historyFile\n+            .getPath().getName());\n+        long effectiveTimestamp \u003d getEffectiveTimestamp(\n+            jobIndexInfo.getFinishTime(), historyFile);\n+        if (effectiveTimestamp \u003c\u003d cutoff) {\n+          String confFileName \u003d JobHistoryUtils\n+              .getIntermediateConfFileName(jobIndexInfo.getJobId());\n+          MetaInfo metaInfo \u003d new MetaInfo(historyFile.getPath(), new Path(\n+              historyFile.getPath().getParent(), confFileName), null,\n+              jobIndexInfo);\n+          storage.jobRemovedFromHDFS(metaInfo.getJobId());\n+          deleteJobFromDone(metaInfo);\n+        } else {\n+          halted \u003d true;\n+          break;\n+        }\n+      }\n+      if (!halted) {\n+        doneDirFc.delete(doneDirFc.makeQualified(serialDir.getPath()), true);\n+        removeDirectoryFromSerialNumberIndex(serialDir.getPath());\n+        synchronized (existingDoneSubdirs) {\n+          existingDoneSubdirs.remove(serialDir.getPath());\n+        }\n+      } else {\n+        break; // Don\u0027t scan any more directories.\n+      }\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  void clean(long cutoff, HistoryStorage storage) throws IOException {\n    // TODO this should be replaced by something that knows about the directory\n    // structure and will put less of a load on HDFS.\n    boolean halted \u003d false;\n    // TODO Delete YYYY/MM/DD directories.\n    List\u003cFileStatus\u003e serialDirList \u003d findTimestampedDirectories();\n    // Sort in ascending order. Relies on YYYY/MM/DD/Serial\n    Collections.sort(serialDirList);\n    for (FileStatus serialDir : serialDirList) {\n      List\u003cFileStatus\u003e historyFileList \u003d scanDirectoryForHistoryFiles(\n          serialDir.getPath(), doneDirFc);\n      for (FileStatus historyFile : historyFileList) {\n        JobIndexInfo jobIndexInfo \u003d FileNameIndexUtils.getIndexInfo(historyFile\n            .getPath().getName());\n        long effectiveTimestamp \u003d getEffectiveTimestamp(\n            jobIndexInfo.getFinishTime(), historyFile);\n        if (effectiveTimestamp \u003c\u003d cutoff) {\n          String confFileName \u003d JobHistoryUtils\n              .getIntermediateConfFileName(jobIndexInfo.getJobId());\n          MetaInfo metaInfo \u003d new MetaInfo(historyFile.getPath(), new Path(\n              historyFile.getPath().getParent(), confFileName), null,\n              jobIndexInfo);\n          storage.jobRemovedFromHDFS(metaInfo.getJobId());\n          deleteJobFromDone(metaInfo);\n        } else {\n          halted \u003d true;\n          break;\n        }\n      }\n      if (!halted) {\n        doneDirFc.delete(doneDirFc.makeQualified(serialDir.getPath()), true);\n        removeDirectoryFromSerialNumberIndex(serialDir.getPath());\n        synchronized (existingDoneSubdirs) {\n          existingDoneSubdirs.remove(serialDir.getPath());\n        }\n      } else {\n        break; // Don\u0027t scan any more directories.\n      }\n    }\n  }",
      "path": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HistoryFileManager.java"
    }
  }
}