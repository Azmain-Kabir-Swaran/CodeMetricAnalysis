{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "LpSolver.java",
  "functionName": "solve",
  "functionId": "solve___jobHistory-Map__RecurrenceId,List__ResourceSkyline____(modifiers-final)",
  "sourceFilePath": "hadoop-tools/hadoop-resourceestimator/src/main/java/org/apache/hadoop/resourceestimator/solver/impl/LpSolver.java",
  "functionStartLine": 211,
  "functionEndLine": 335,
  "numCommitsSeen": 1,
  "timeTaken": 376,
  "changeHistory": [
    "625039ef20e6011ab360131d70582a6e4bf2ec1d"
  ],
  "changeHistoryShort": {
    "625039ef20e6011ab360131d70582a6e4bf2ec1d": "Yintroduced"
  },
  "changeHistoryDetails": {
    "625039ef20e6011ab360131d70582a6e4bf2ec1d": {
      "type": "Yintroduced",
      "commitMessage": "HADOOP-14840. Tool to estimate resource requirements of an application pipeline based on prior executions. (Rui Li via Subru).\n",
      "commitDate": "25/10/17 3:51 PM",
      "commitName": "625039ef20e6011ab360131d70582a6e4bf2ec1d",
      "commitAuthor": "Subru Krishnan",
      "diff": "@@ -0,0 +1,125 @@\n+  @Override public final RLESparseResourceAllocation solve(\n+      final Map\u003cRecurrenceId, List\u003cResourceSkyline\u003e\u003e jobHistory)\n+      throws SolverException, SkylineStoreException {\n+    // TODO: addHistory timeout support for this function, and ideally we should\n+    // return the confidence\n+    // level associated with the predicted resource.\n+    preprocessor.validate(jobHistory, timeInterval);\n+    final List\u003cResourceSkyline\u003e resourceSkylines \u003d\n+        preprocessor.aggregateSkylines(jobHistory, minJobRuns);\n+    final int numJobs \u003d resourceSkylines.size();\n+    final int jobLen \u003d getJobLen(resourceSkylines, numJobs);\n+\n+    /** Create variables. */\n+    final ExpressionsBasedModel lpModel \u003d new ExpressionsBasedModel();\n+\n+    Variable[] oa \u003d new Variable[jobLen * numJobs];\n+    Variable[] ua \u003d new Variable[jobLen * numJobs];\n+    Variable[] uaPredict \u003d new Variable[jobLen * numJobs];\n+    Variable[] x \u003d new Variable[jobLen];\n+    for (int i \u003d 0; i \u003c jobLen * numJobs; i++) {\n+      oa[i] \u003d new Variable(\"oa\" + i).lower(BigDecimal.valueOf(0));\n+      ua[i] \u003d new Variable(\"ua\" + i).lower(BigDecimal.valueOf(0));\n+      uaPredict[i] \u003d new Variable(\"uaPredict\" + i).lower(BigDecimal.valueOf(0));\n+    }\n+    for (int i \u003d 0; i \u003c jobLen; i++) {\n+      x[i] \u003d new Variable(\"x\").lower(BigDecimal.valueOf(0));\n+    }\n+    lpModel.addVariables(x);\n+    lpModel.addVariables(oa);\n+    lpModel.addVariables(ua);\n+    lpModel.addVariables(uaPredict);\n+    Variable eps \u003d new Variable(\"epsilon\").lower(BigDecimal.valueOf(0));\n+    lpModel.addVariable(eps);\n+\n+    /** Set constraints. */\n+    int indexJobITimeK \u003d 0;\n+    double cJobI \u003d 0;\n+    double cJobITimeK \u003d 0;\n+    ResourceSkyline resourceSkyline;\n+    int[] containerNums;\n+    // 1. sum(job_i){sum(timeK){1/cJobI * uaPredict_job_i_timeK}} \u003c\u003d numJobs\n+    // * eps\n+    Expression regularizationConstraint \u003d\n+        lpModel.addExpression(\"regularization\");\n+    regularizationConstraint.set(eps, -numJobs);\n+    regularizationConstraint.upper(BigDecimal.valueOf(0)); // \u003c\u003d 0\n+    for (int indexJobI \u003d 0;\n+         indexJobI \u003c resourceSkylines.size(); indexJobI++) {\n+      resourceSkyline \u003d resourceSkylines.get(indexJobI);\n+      // the # of containers consumed by job i in discretized time intervals\n+      containerNums \u003d preprocessor\n+          .getDiscreteSkyline(resourceSkyline.getSkylineList(), timeInterval,\n+              resourceSkyline.getContainerSpec().getMemorySize(), jobLen);\n+      // the aggregated # of containers consumed by job i during its lifespan\n+      cJobI \u003d 0;\n+      for (int i \u003d 0; i \u003c containerNums.length; i++) {\n+        cJobI \u003d cJobI + containerNums[i];\n+      }\n+      for (int timeK \u003d 0; timeK \u003c jobLen; timeK++) {\n+        indexJobITimeK \u003d indexJobI * jobLen + timeK;\n+        // the # of containers consumed by job i in the k-th time interval\n+        cJobITimeK \u003d containerNums[timeK];\n+        regularizationConstraint\n+            .set(uaPredict[indexJobITimeK], 1 / cJobI);\n+        generateOverAllocationConstraints(lpModel, cJobITimeK, oa, x,\n+            indexJobITimeK, timeK);\n+        generateUnderAllocationConstraints(lpModel, cJobITimeK, uaPredict,\n+            ua, x, indexJobITimeK, timeK);\n+      }\n+    }\n+\n+    /** Set objective. */\n+    Expression objective \u003d lpModel.addExpression(\"objective\");\n+    generateObjective(objective, numJobs, jobLen, oa, ua, eps);\n+\n+    /** Solve the model. */\n+    final Result lpResult \u003d lpModel.minimise();\n+    final TreeMap\u003cLong, Resource\u003e treeMap \u003d new TreeMap\u003c\u003e();\n+    RLESparseResourceAllocation result \u003d\n+        new RLESparseResourceAllocation(treeMap,\n+            new DefaultResourceCalculator());\n+    ReservationInterval riAdd;\n+    Resource containerSpec \u003d resourceSkylines.get(0).getContainerSpec();\n+    String pipelineId \u003d\n+        ((RecurrenceId) jobHistory.keySet().toArray()[0]).getPipelineId();\n+    Resource resource;\n+    for (int indexTimeK \u003d 0; indexTimeK \u003c jobLen; indexTimeK++) {\n+      riAdd \u003d new ReservationInterval(indexTimeK * timeInterval,\n+          (indexTimeK + 1) * timeInterval);\n+      resource \u003d Resource.newInstance(\n+          containerSpec.getMemorySize() * (int) lpResult\n+              .doubleValue(indexTimeK),\n+          containerSpec.getVirtualCores() * (int) lpResult\n+              .doubleValue(indexTimeK));\n+      result.addInterval(riAdd, resource);\n+      LOGGER.debug(\"time interval: {}, container: {}.\", indexTimeK,\n+          lpResult.doubleValue(indexTimeK));\n+    }\n+\n+    predictionSkylineStore.addEstimation(pipelineId, result);\n+\n+    /**\n+     * TODO: 1. We can calculate the estimated error (over-allocation,\n+     * under-allocation) of our prediction which could be used to generate\n+     * confidence level for our prediction; 2. Also, we can modify our model to\n+     * take job input data size (and maybe stage info) into consideration; 3. We\n+     * can also try to generate such conclusion: our prediction under-allocates\n+     * X amount of resources from time 0 to time 100 compared with 95% of\n+     * history runs; 4. We can build framework-specific versions of estimator\n+     * (such as scope/spark/hive, etc.) and provides more specific suggestions.\n+     * For example, we may say: for spark job i, its task size is X GB while the\n+     * container memory allocation is Y GB; as a result, its shuffling stage is\n+     * 20% slower than ideal case due to the disk spilling operations, etc. 5.\n+     * If we have more information of jobs (other than ResourceSkyline), we may\n+     * have such conclusion: job i is 20% slower than 90% of history runs, and\n+     * it is because part of its tasks are running together with job j\u0027s tasks.\n+     * In this case, we not only predict the amount of resource needed for job\n+     * i, but also how to place the resource requirements to clusters; 6. We may\n+     * monitor job progress, and dynamically increase/decrease container\n+     * allocations to satisfy job deadline while minimizing the cost; 7. We may\n+     * allow users to specify a budget (say $100 per job run), and optimize the\n+     * resource allocation under the budget constraints. 8. ...\n+     */\n+    return result;\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  @Override public final RLESparseResourceAllocation solve(\n      final Map\u003cRecurrenceId, List\u003cResourceSkyline\u003e\u003e jobHistory)\n      throws SolverException, SkylineStoreException {\n    // TODO: addHistory timeout support for this function, and ideally we should\n    // return the confidence\n    // level associated with the predicted resource.\n    preprocessor.validate(jobHistory, timeInterval);\n    final List\u003cResourceSkyline\u003e resourceSkylines \u003d\n        preprocessor.aggregateSkylines(jobHistory, minJobRuns);\n    final int numJobs \u003d resourceSkylines.size();\n    final int jobLen \u003d getJobLen(resourceSkylines, numJobs);\n\n    /** Create variables. */\n    final ExpressionsBasedModel lpModel \u003d new ExpressionsBasedModel();\n\n    Variable[] oa \u003d new Variable[jobLen * numJobs];\n    Variable[] ua \u003d new Variable[jobLen * numJobs];\n    Variable[] uaPredict \u003d new Variable[jobLen * numJobs];\n    Variable[] x \u003d new Variable[jobLen];\n    for (int i \u003d 0; i \u003c jobLen * numJobs; i++) {\n      oa[i] \u003d new Variable(\"oa\" + i).lower(BigDecimal.valueOf(0));\n      ua[i] \u003d new Variable(\"ua\" + i).lower(BigDecimal.valueOf(0));\n      uaPredict[i] \u003d new Variable(\"uaPredict\" + i).lower(BigDecimal.valueOf(0));\n    }\n    for (int i \u003d 0; i \u003c jobLen; i++) {\n      x[i] \u003d new Variable(\"x\").lower(BigDecimal.valueOf(0));\n    }\n    lpModel.addVariables(x);\n    lpModel.addVariables(oa);\n    lpModel.addVariables(ua);\n    lpModel.addVariables(uaPredict);\n    Variable eps \u003d new Variable(\"epsilon\").lower(BigDecimal.valueOf(0));\n    lpModel.addVariable(eps);\n\n    /** Set constraints. */\n    int indexJobITimeK \u003d 0;\n    double cJobI \u003d 0;\n    double cJobITimeK \u003d 0;\n    ResourceSkyline resourceSkyline;\n    int[] containerNums;\n    // 1. sum(job_i){sum(timeK){1/cJobI * uaPredict_job_i_timeK}} \u003c\u003d numJobs\n    // * eps\n    Expression regularizationConstraint \u003d\n        lpModel.addExpression(\"regularization\");\n    regularizationConstraint.set(eps, -numJobs);\n    regularizationConstraint.upper(BigDecimal.valueOf(0)); // \u003c\u003d 0\n    for (int indexJobI \u003d 0;\n         indexJobI \u003c resourceSkylines.size(); indexJobI++) {\n      resourceSkyline \u003d resourceSkylines.get(indexJobI);\n      // the # of containers consumed by job i in discretized time intervals\n      containerNums \u003d preprocessor\n          .getDiscreteSkyline(resourceSkyline.getSkylineList(), timeInterval,\n              resourceSkyline.getContainerSpec().getMemorySize(), jobLen);\n      // the aggregated # of containers consumed by job i during its lifespan\n      cJobI \u003d 0;\n      for (int i \u003d 0; i \u003c containerNums.length; i++) {\n        cJobI \u003d cJobI + containerNums[i];\n      }\n      for (int timeK \u003d 0; timeK \u003c jobLen; timeK++) {\n        indexJobITimeK \u003d indexJobI * jobLen + timeK;\n        // the # of containers consumed by job i in the k-th time interval\n        cJobITimeK \u003d containerNums[timeK];\n        regularizationConstraint\n            .set(uaPredict[indexJobITimeK], 1 / cJobI);\n        generateOverAllocationConstraints(lpModel, cJobITimeK, oa, x,\n            indexJobITimeK, timeK);\n        generateUnderAllocationConstraints(lpModel, cJobITimeK, uaPredict,\n            ua, x, indexJobITimeK, timeK);\n      }\n    }\n\n    /** Set objective. */\n    Expression objective \u003d lpModel.addExpression(\"objective\");\n    generateObjective(objective, numJobs, jobLen, oa, ua, eps);\n\n    /** Solve the model. */\n    final Result lpResult \u003d lpModel.minimise();\n    final TreeMap\u003cLong, Resource\u003e treeMap \u003d new TreeMap\u003c\u003e();\n    RLESparseResourceAllocation result \u003d\n        new RLESparseResourceAllocation(treeMap,\n            new DefaultResourceCalculator());\n    ReservationInterval riAdd;\n    Resource containerSpec \u003d resourceSkylines.get(0).getContainerSpec();\n    String pipelineId \u003d\n        ((RecurrenceId) jobHistory.keySet().toArray()[0]).getPipelineId();\n    Resource resource;\n    for (int indexTimeK \u003d 0; indexTimeK \u003c jobLen; indexTimeK++) {\n      riAdd \u003d new ReservationInterval(indexTimeK * timeInterval,\n          (indexTimeK + 1) * timeInterval);\n      resource \u003d Resource.newInstance(\n          containerSpec.getMemorySize() * (int) lpResult\n              .doubleValue(indexTimeK),\n          containerSpec.getVirtualCores() * (int) lpResult\n              .doubleValue(indexTimeK));\n      result.addInterval(riAdd, resource);\n      LOGGER.debug(\"time interval: {}, container: {}.\", indexTimeK,\n          lpResult.doubleValue(indexTimeK));\n    }\n\n    predictionSkylineStore.addEstimation(pipelineId, result);\n\n    /**\n     * TODO: 1. We can calculate the estimated error (over-allocation,\n     * under-allocation) of our prediction which could be used to generate\n     * confidence level for our prediction; 2. Also, we can modify our model to\n     * take job input data size (and maybe stage info) into consideration; 3. We\n     * can also try to generate such conclusion: our prediction under-allocates\n     * X amount of resources from time 0 to time 100 compared with 95% of\n     * history runs; 4. We can build framework-specific versions of estimator\n     * (such as scope/spark/hive, etc.) and provides more specific suggestions.\n     * For example, we may say: for spark job i, its task size is X GB while the\n     * container memory allocation is Y GB; as a result, its shuffling stage is\n     * 20% slower than ideal case due to the disk spilling operations, etc. 5.\n     * If we have more information of jobs (other than ResourceSkyline), we may\n     * have such conclusion: job i is 20% slower than 90% of history runs, and\n     * it is because part of its tasks are running together with job j\u0027s tasks.\n     * In this case, we not only predict the amount of resource needed for job\n     * i, but also how to place the resource requirements to clusters; 6. We may\n     * monitor job progress, and dynamically increase/decrease container\n     * allocations to satisfy job deadline while minimizing the cost; 7. We may\n     * allow users to specify a budget (say $100 per job run), and optimize the\n     * resource allocation under the budget constraints. 8. ...\n     */\n    return result;\n  }",
      "path": "hadoop-tools/hadoop-resourceestimator/src/main/java/org/apache/hadoop/resourceestimator/solver/impl/LpSolver.java"
    }
  }
}