{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "AbstractPreemptableResourceCalculator.java",
  "functionName": "computeFixpointAllocation",
  "functionId": "computeFixpointAllocation___totGuarant-Resource__qAlloc-Collection__TempQueuePerPartition____unassigned-Resource__ignoreGuarantee-boolean",
  "sourceFilePath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/AbstractPreemptableResourceCalculator.java",
  "functionStartLine": 132,
  "functionEndLine": 235,
  "numCommitsSeen": 48,
  "timeTaken": 7832,
  "changeHistory": [
    "291194302cc1a875d6d94ea93cf1184a3f1fc2cc",
    "8d5509c68156faaa6641f4e747fc9ff80adccf88",
    "514794e1a5a39ca61de3981d53a05547ae17f5e4",
    "90dd3a8148468ac37a3f2173ad8d45e38bfcb0c9",
    "bb62e0592566b2fcae7136b30972aad2d3ac55b0",
    "60e4116bf1d00afed91010e57357fe54057e4e39",
    "ae14e5d07f1b6702a5160637438028bb03d9387e",
    "fa7a43529d529f0006c8033c2003f15b9b93f103",
    "7e8c9beb4156dcaeb3a11e60aaa06d2370626913",
    "d497f6ea2be559aa31ed76f37ae949dbfabe2a51",
    "4b130821995a3cfe20c71e38e0f63294085c0491",
    "45b42676f9333ed4fa05355ccb4e1f91a9556525"
  ],
  "changeHistoryShort": {
    "291194302cc1a875d6d94ea93cf1184a3f1fc2cc": "Ybodychange",
    "8d5509c68156faaa6641f4e747fc9ff80adccf88": "Ybodychange",
    "514794e1a5a39ca61de3981d53a05547ae17f5e4": "Ybodychange",
    "90dd3a8148468ac37a3f2173ad8d45e38bfcb0c9": "Ymultichange(Ymovefromfile,Ymodifierchange,Ybodychange,Yparameterchange)",
    "bb62e0592566b2fcae7136b30972aad2d3ac55b0": "Ybodychange",
    "60e4116bf1d00afed91010e57357fe54057e4e39": "Ymovefromfile",
    "ae14e5d07f1b6702a5160637438028bb03d9387e": "Ybodychange",
    "fa7a43529d529f0006c8033c2003f15b9b93f103": "Ybodychange",
    "7e8c9beb4156dcaeb3a11e60aaa06d2370626913": "Ybodychange",
    "d497f6ea2be559aa31ed76f37ae949dbfabe2a51": "Ymultichange(Yparameterchange,Ybodychange)",
    "4b130821995a3cfe20c71e38e0f63294085c0491": "Ybodychange",
    "45b42676f9333ed4fa05355ccb4e1f91a9556525": "Yintroduced"
  },
  "changeHistoryDetails": {
    "291194302cc1a875d6d94ea93cf1184a3f1fc2cc": {
      "type": "Ybodychange",
      "commitMessage": "YARN-8379. Improve balancing resources in already satisfied queues by using Capacity Scheduler preemption. Contributed by Zian Chen.\n",
      "commitDate": "28/06/18 10:23 AM",
      "commitName": "291194302cc1a875d6d94ea93cf1184a3f1fc2cc",
      "commitAuthor": "Sunil G",
      "commitDateOld": "25/05/18 9:06 AM",
      "commitNameOld": "8d5509c68156faaa6641f4e747fc9ff80adccf88",
      "commitAuthorOld": "Eric E Payne",
      "daysBetweenCommits": 34.05,
      "commitsBetweenForRepo": 228,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,103 +1,104 @@\n   protected void computeFixpointAllocation(Resource totGuarant,\n       Collection\u003cTempQueuePerPartition\u003e qAlloc, Resource unassigned,\n       boolean ignoreGuarantee) {\n     // Prior to assigning the unused resources, process each queue as follows:\n     // If current \u003e guaranteed, idealAssigned \u003d guaranteed + untouchable extra\n     // Else idealAssigned \u003d current;\n     // Subtract idealAssigned resources from unassigned.\n     // If the queue has all of its needs met (that is, if\n     // idealAssigned \u003e\u003d current + pending), remove it from consideration.\n     // Sort queues from most under-guaranteed to most over-guaranteed.\n     TQComparator tqComparator \u003d new TQComparator(rc, totGuarant);\n     PriorityQueue\u003cTempQueuePerPartition\u003e orderedByNeed \u003d new PriorityQueue\u003c\u003e(10,\n         tqComparator);\n     for (Iterator\u003cTempQueuePerPartition\u003e i \u003d qAlloc.iterator(); i.hasNext(); ) {\n       TempQueuePerPartition q \u003d i.next();\n       Resource used \u003d q.getUsed();\n \n       Resource initIdealAssigned;\n       if (Resources.greaterThan(rc, totGuarant, used, q.getGuaranteed())) {\n         initIdealAssigned \u003d Resources.add(\n             Resources.componentwiseMin(q.getGuaranteed(), q.getUsed()),\n             q.untouchableExtra);\n       } else{\n         initIdealAssigned \u003d Resources.clone(used);\n       }\n \n       // perform initial assignment\n       initIdealAssignment(totGuarant, q, initIdealAssigned);\n \n       Resources.subtractFrom(unassigned, q.idealAssigned);\n \n       // If idealAssigned \u003c (allocated + used + pending), q needs more\n       // resources, so\n       // add it to the list of underserved queues, ordered by need.\n       Resource curPlusPend \u003d Resources.add(q.getUsed(), q.pending);\n       if (Resources.lessThan(rc, totGuarant, q.idealAssigned, curPlusPend)) {\n         orderedByNeed.add(q);\n       }\n     }\n \n     // assign all cluster resources until no more demand, or no resources are\n     // left\n     while (!orderedByNeed.isEmpty() \u0026\u0026 Resources.greaterThan(rc, totGuarant,\n         unassigned, Resources.none())) {\n       // we compute normalizedGuarantees capacity based on currently active\n       // queues\n       resetCapacity(unassigned, orderedByNeed, ignoreGuarantee);\n \n       // For each underserved queue (or set of queues if multiple are equally\n       // underserved), offer its share of the unassigned resources based on its\n       // normalized guarantee. After the offer, if the queue is not satisfied,\n       // place it back in the ordered list of queues, recalculating its place\n       // in the order of most under-guaranteed to most over-guaranteed. In this\n       // way, the most underserved queue(s) are always given resources first.\n       Collection\u003cTempQueuePerPartition\u003e underserved \u003d getMostUnderservedQueues(\n           orderedByNeed, tqComparator);\n \n       // This value will be used in every round to calculate ideal allocation.\n       // So make a copy to avoid it changed during calculation.\n       Resource dupUnassignedForTheRound \u003d Resources.clone(unassigned);\n \n       for (Iterator\u003cTempQueuePerPartition\u003e i \u003d underserved.iterator(); i\n           .hasNext();) {\n         if (!rc.isAnyMajorResourceAboveZero(unassigned)) {\n           break;\n         }\n \n         TempQueuePerPartition sub \u003d i.next();\n \n         // How much resource we offer to the queue (to increase its ideal_alloc\n         Resource wQavail \u003d Resources.multiplyAndNormalizeUp(rc,\n             dupUnassignedForTheRound,\n             sub.normalizedGuarantee, this.stepFactor);\n \n         // Make sure it is not beyond unassigned\n         wQavail \u003d Resources.componentwiseMin(wQavail, unassigned);\n \n         Resource wQidle \u003d sub.offer(wQavail, rc, totGuarant,\n-            isReservedPreemptionCandidatesSelector);\n+            isReservedPreemptionCandidatesSelector,\n+            allowQueuesBalanceAfterAllQueuesSatisfied);\n         Resource wQdone \u003d Resources.subtract(wQavail, wQidle);\n \n         if (Resources.greaterThan(rc, totGuarant, wQdone, Resources.none())) {\n           // The queue is still asking for more. Put it back in the priority\n           // queue, recalculating its order based on need.\n           orderedByNeed.add(sub);\n         }\n \n         Resources.subtractFrom(unassigned, wQdone);\n \n         // Make sure unassigned is always larger than 0\n         unassigned \u003d Resources.componentwiseMax(unassigned, Resources.none());\n       }\n     }\n \n     // Sometimes its possible that, all queues are properly served. So intra\n     // queue preemption will not try for any preemption. How ever there are\n     // chances that within a queue, there are some imbalances. Hence make sure\n     // all queues are added to list.\n     while (!orderedByNeed.isEmpty()) {\n       TempQueuePerPartition q1 \u003d orderedByNeed.remove();\n       context.addPartitionToUnderServedQueues(q1.queueName, q1.partition);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void computeFixpointAllocation(Resource totGuarant,\n      Collection\u003cTempQueuePerPartition\u003e qAlloc, Resource unassigned,\n      boolean ignoreGuarantee) {\n    // Prior to assigning the unused resources, process each queue as follows:\n    // If current \u003e guaranteed, idealAssigned \u003d guaranteed + untouchable extra\n    // Else idealAssigned \u003d current;\n    // Subtract idealAssigned resources from unassigned.\n    // If the queue has all of its needs met (that is, if\n    // idealAssigned \u003e\u003d current + pending), remove it from consideration.\n    // Sort queues from most under-guaranteed to most over-guaranteed.\n    TQComparator tqComparator \u003d new TQComparator(rc, totGuarant);\n    PriorityQueue\u003cTempQueuePerPartition\u003e orderedByNeed \u003d new PriorityQueue\u003c\u003e(10,\n        tqComparator);\n    for (Iterator\u003cTempQueuePerPartition\u003e i \u003d qAlloc.iterator(); i.hasNext(); ) {\n      TempQueuePerPartition q \u003d i.next();\n      Resource used \u003d q.getUsed();\n\n      Resource initIdealAssigned;\n      if (Resources.greaterThan(rc, totGuarant, used, q.getGuaranteed())) {\n        initIdealAssigned \u003d Resources.add(\n            Resources.componentwiseMin(q.getGuaranteed(), q.getUsed()),\n            q.untouchableExtra);\n      } else{\n        initIdealAssigned \u003d Resources.clone(used);\n      }\n\n      // perform initial assignment\n      initIdealAssignment(totGuarant, q, initIdealAssigned);\n\n      Resources.subtractFrom(unassigned, q.idealAssigned);\n\n      // If idealAssigned \u003c (allocated + used + pending), q needs more\n      // resources, so\n      // add it to the list of underserved queues, ordered by need.\n      Resource curPlusPend \u003d Resources.add(q.getUsed(), q.pending);\n      if (Resources.lessThan(rc, totGuarant, q.idealAssigned, curPlusPend)) {\n        orderedByNeed.add(q);\n      }\n    }\n\n    // assign all cluster resources until no more demand, or no resources are\n    // left\n    while (!orderedByNeed.isEmpty() \u0026\u0026 Resources.greaterThan(rc, totGuarant,\n        unassigned, Resources.none())) {\n      // we compute normalizedGuarantees capacity based on currently active\n      // queues\n      resetCapacity(unassigned, orderedByNeed, ignoreGuarantee);\n\n      // For each underserved queue (or set of queues if multiple are equally\n      // underserved), offer its share of the unassigned resources based on its\n      // normalized guarantee. After the offer, if the queue is not satisfied,\n      // place it back in the ordered list of queues, recalculating its place\n      // in the order of most under-guaranteed to most over-guaranteed. In this\n      // way, the most underserved queue(s) are always given resources first.\n      Collection\u003cTempQueuePerPartition\u003e underserved \u003d getMostUnderservedQueues(\n          orderedByNeed, tqComparator);\n\n      // This value will be used in every round to calculate ideal allocation.\n      // So make a copy to avoid it changed during calculation.\n      Resource dupUnassignedForTheRound \u003d Resources.clone(unassigned);\n\n      for (Iterator\u003cTempQueuePerPartition\u003e i \u003d underserved.iterator(); i\n          .hasNext();) {\n        if (!rc.isAnyMajorResourceAboveZero(unassigned)) {\n          break;\n        }\n\n        TempQueuePerPartition sub \u003d i.next();\n\n        // How much resource we offer to the queue (to increase its ideal_alloc\n        Resource wQavail \u003d Resources.multiplyAndNormalizeUp(rc,\n            dupUnassignedForTheRound,\n            sub.normalizedGuarantee, this.stepFactor);\n\n        // Make sure it is not beyond unassigned\n        wQavail \u003d Resources.componentwiseMin(wQavail, unassigned);\n\n        Resource wQidle \u003d sub.offer(wQavail, rc, totGuarant,\n            isReservedPreemptionCandidatesSelector,\n            allowQueuesBalanceAfterAllQueuesSatisfied);\n        Resource wQdone \u003d Resources.subtract(wQavail, wQidle);\n\n        if (Resources.greaterThan(rc, totGuarant, wQdone, Resources.none())) {\n          // The queue is still asking for more. Put it back in the priority\n          // queue, recalculating its order based on need.\n          orderedByNeed.add(sub);\n        }\n\n        Resources.subtractFrom(unassigned, wQdone);\n\n        // Make sure unassigned is always larger than 0\n        unassigned \u003d Resources.componentwiseMax(unassigned, Resources.none());\n      }\n    }\n\n    // Sometimes its possible that, all queues are properly served. So intra\n    // queue preemption will not try for any preemption. How ever there are\n    // chances that within a queue, there are some imbalances. Hence make sure\n    // all queues are added to list.\n    while (!orderedByNeed.isEmpty()) {\n      TempQueuePerPartition q1 \u003d orderedByNeed.remove();\n      context.addPartitionToUnderServedQueues(q1.queueName, q1.partition);\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/AbstractPreemptableResourceCalculator.java",
      "extendedDetails": {}
    },
    "8d5509c68156faaa6641f4e747fc9ff80adccf88": {
      "type": "Ybodychange",
      "commitMessage": "YARN-8292: Fix the dominant resource preemption cannot happen when some of the resource vector becomes negative. Contributed by Wangda Tan.\n",
      "commitDate": "25/05/18 9:06 AM",
      "commitName": "8d5509c68156faaa6641f4e747fc9ff80adccf88",
      "commitAuthor": "Eric E Payne",
      "commitDateOld": "22/02/18 6:12 PM",
      "commitNameOld": "514794e1a5a39ca61de3981d53a05547ae17f5e4",
      "commitAuthorOld": "Carlo Curino",
      "daysBetweenCommits": 91.58,
      "commitsBetweenForRepo": 1293,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,85 +1,103 @@\n   protected void computeFixpointAllocation(Resource totGuarant,\n       Collection\u003cTempQueuePerPartition\u003e qAlloc, Resource unassigned,\n       boolean ignoreGuarantee) {\n     // Prior to assigning the unused resources, process each queue as follows:\n     // If current \u003e guaranteed, idealAssigned \u003d guaranteed + untouchable extra\n     // Else idealAssigned \u003d current;\n     // Subtract idealAssigned resources from unassigned.\n     // If the queue has all of its needs met (that is, if\n     // idealAssigned \u003e\u003d current + pending), remove it from consideration.\n     // Sort queues from most under-guaranteed to most over-guaranteed.\n     TQComparator tqComparator \u003d new TQComparator(rc, totGuarant);\n     PriorityQueue\u003cTempQueuePerPartition\u003e orderedByNeed \u003d new PriorityQueue\u003c\u003e(10,\n         tqComparator);\n-    for (Iterator\u003cTempQueuePerPartition\u003e i \u003d qAlloc.iterator(); i.hasNext();) {\n+    for (Iterator\u003cTempQueuePerPartition\u003e i \u003d qAlloc.iterator(); i.hasNext(); ) {\n       TempQueuePerPartition q \u003d i.next();\n       Resource used \u003d q.getUsed();\n \n       Resource initIdealAssigned;\n       if (Resources.greaterThan(rc, totGuarant, used, q.getGuaranteed())) {\n-        initIdealAssigned \u003d\n-            Resources.add(q.getGuaranteed(), q.untouchableExtra);\n-      } else {\n+        initIdealAssigned \u003d Resources.add(\n+            Resources.componentwiseMin(q.getGuaranteed(), q.getUsed()),\n+            q.untouchableExtra);\n+      } else{\n         initIdealAssigned \u003d Resources.clone(used);\n       }\n \n       // perform initial assignment\n       initIdealAssignment(totGuarant, q, initIdealAssigned);\n \n-\n       Resources.subtractFrom(unassigned, q.idealAssigned);\n+\n       // If idealAssigned \u003c (allocated + used + pending), q needs more\n       // resources, so\n       // add it to the list of underserved queues, ordered by need.\n       Resource curPlusPend \u003d Resources.add(q.getUsed(), q.pending);\n       if (Resources.lessThan(rc, totGuarant, q.idealAssigned, curPlusPend)) {\n         orderedByNeed.add(q);\n       }\n     }\n \n     // assign all cluster resources until no more demand, or no resources are\n     // left\n     while (!orderedByNeed.isEmpty() \u0026\u0026 Resources.greaterThan(rc, totGuarant,\n         unassigned, Resources.none())) {\n-      Resource wQassigned \u003d Resource.newInstance(0, 0);\n       // we compute normalizedGuarantees capacity based on currently active\n       // queues\n       resetCapacity(unassigned, orderedByNeed, ignoreGuarantee);\n \n       // For each underserved queue (or set of queues if multiple are equally\n       // underserved), offer its share of the unassigned resources based on its\n       // normalized guarantee. After the offer, if the queue is not satisfied,\n       // place it back in the ordered list of queues, recalculating its place\n       // in the order of most under-guaranteed to most over-guaranteed. In this\n       // way, the most underserved queue(s) are always given resources first.\n       Collection\u003cTempQueuePerPartition\u003e underserved \u003d getMostUnderservedQueues(\n           orderedByNeed, tqComparator);\n \n+      // This value will be used in every round to calculate ideal allocation.\n+      // So make a copy to avoid it changed during calculation.\n+      Resource dupUnassignedForTheRound \u003d Resources.clone(unassigned);\n+\n       for (Iterator\u003cTempQueuePerPartition\u003e i \u003d underserved.iterator(); i\n           .hasNext();) {\n+        if (!rc.isAnyMajorResourceAboveZero(unassigned)) {\n+          break;\n+        }\n+\n         TempQueuePerPartition sub \u003d i.next();\n-        Resource wQavail \u003d Resources.multiplyAndNormalizeUp(rc, unassigned,\n-            sub.normalizedGuarantee, Resource.newInstance(1, 1));\n+\n+        // How much resource we offer to the queue (to increase its ideal_alloc\n+        Resource wQavail \u003d Resources.multiplyAndNormalizeUp(rc,\n+            dupUnassignedForTheRound,\n+            sub.normalizedGuarantee, this.stepFactor);\n+\n+        // Make sure it is not beyond unassigned\n+        wQavail \u003d Resources.componentwiseMin(wQavail, unassigned);\n+\n         Resource wQidle \u003d sub.offer(wQavail, rc, totGuarant,\n             isReservedPreemptionCandidatesSelector);\n         Resource wQdone \u003d Resources.subtract(wQavail, wQidle);\n \n         if (Resources.greaterThan(rc, totGuarant, wQdone, Resources.none())) {\n           // The queue is still asking for more. Put it back in the priority\n           // queue, recalculating its order based on need.\n           orderedByNeed.add(sub);\n         }\n-        Resources.addTo(wQassigned, wQdone);\n+\n+        Resources.subtractFrom(unassigned, wQdone);\n+\n+        // Make sure unassigned is always larger than 0\n+        unassigned \u003d Resources.componentwiseMax(unassigned, Resources.none());\n       }\n-      Resources.subtractFrom(unassigned, wQassigned);\n     }\n \n     // Sometimes its possible that, all queues are properly served. So intra\n     // queue preemption will not try for any preemption. How ever there are\n     // chances that within a queue, there are some imbalances. Hence make sure\n     // all queues are added to list.\n     while (!orderedByNeed.isEmpty()) {\n       TempQueuePerPartition q1 \u003d orderedByNeed.remove();\n       context.addPartitionToUnderServedQueues(q1.queueName, q1.partition);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void computeFixpointAllocation(Resource totGuarant,\n      Collection\u003cTempQueuePerPartition\u003e qAlloc, Resource unassigned,\n      boolean ignoreGuarantee) {\n    // Prior to assigning the unused resources, process each queue as follows:\n    // If current \u003e guaranteed, idealAssigned \u003d guaranteed + untouchable extra\n    // Else idealAssigned \u003d current;\n    // Subtract idealAssigned resources from unassigned.\n    // If the queue has all of its needs met (that is, if\n    // idealAssigned \u003e\u003d current + pending), remove it from consideration.\n    // Sort queues from most under-guaranteed to most over-guaranteed.\n    TQComparator tqComparator \u003d new TQComparator(rc, totGuarant);\n    PriorityQueue\u003cTempQueuePerPartition\u003e orderedByNeed \u003d new PriorityQueue\u003c\u003e(10,\n        tqComparator);\n    for (Iterator\u003cTempQueuePerPartition\u003e i \u003d qAlloc.iterator(); i.hasNext(); ) {\n      TempQueuePerPartition q \u003d i.next();\n      Resource used \u003d q.getUsed();\n\n      Resource initIdealAssigned;\n      if (Resources.greaterThan(rc, totGuarant, used, q.getGuaranteed())) {\n        initIdealAssigned \u003d Resources.add(\n            Resources.componentwiseMin(q.getGuaranteed(), q.getUsed()),\n            q.untouchableExtra);\n      } else{\n        initIdealAssigned \u003d Resources.clone(used);\n      }\n\n      // perform initial assignment\n      initIdealAssignment(totGuarant, q, initIdealAssigned);\n\n      Resources.subtractFrom(unassigned, q.idealAssigned);\n\n      // If idealAssigned \u003c (allocated + used + pending), q needs more\n      // resources, so\n      // add it to the list of underserved queues, ordered by need.\n      Resource curPlusPend \u003d Resources.add(q.getUsed(), q.pending);\n      if (Resources.lessThan(rc, totGuarant, q.idealAssigned, curPlusPend)) {\n        orderedByNeed.add(q);\n      }\n    }\n\n    // assign all cluster resources until no more demand, or no resources are\n    // left\n    while (!orderedByNeed.isEmpty() \u0026\u0026 Resources.greaterThan(rc, totGuarant,\n        unassigned, Resources.none())) {\n      // we compute normalizedGuarantees capacity based on currently active\n      // queues\n      resetCapacity(unassigned, orderedByNeed, ignoreGuarantee);\n\n      // For each underserved queue (or set of queues if multiple are equally\n      // underserved), offer its share of the unassigned resources based on its\n      // normalized guarantee. After the offer, if the queue is not satisfied,\n      // place it back in the ordered list of queues, recalculating its place\n      // in the order of most under-guaranteed to most over-guaranteed. In this\n      // way, the most underserved queue(s) are always given resources first.\n      Collection\u003cTempQueuePerPartition\u003e underserved \u003d getMostUnderservedQueues(\n          orderedByNeed, tqComparator);\n\n      // This value will be used in every round to calculate ideal allocation.\n      // So make a copy to avoid it changed during calculation.\n      Resource dupUnassignedForTheRound \u003d Resources.clone(unassigned);\n\n      for (Iterator\u003cTempQueuePerPartition\u003e i \u003d underserved.iterator(); i\n          .hasNext();) {\n        if (!rc.isAnyMajorResourceAboveZero(unassigned)) {\n          break;\n        }\n\n        TempQueuePerPartition sub \u003d i.next();\n\n        // How much resource we offer to the queue (to increase its ideal_alloc\n        Resource wQavail \u003d Resources.multiplyAndNormalizeUp(rc,\n            dupUnassignedForTheRound,\n            sub.normalizedGuarantee, this.stepFactor);\n\n        // Make sure it is not beyond unassigned\n        wQavail \u003d Resources.componentwiseMin(wQavail, unassigned);\n\n        Resource wQidle \u003d sub.offer(wQavail, rc, totGuarant,\n            isReservedPreemptionCandidatesSelector);\n        Resource wQdone \u003d Resources.subtract(wQavail, wQidle);\n\n        if (Resources.greaterThan(rc, totGuarant, wQdone, Resources.none())) {\n          // The queue is still asking for more. Put it back in the priority\n          // queue, recalculating its order based on need.\n          orderedByNeed.add(sub);\n        }\n\n        Resources.subtractFrom(unassigned, wQdone);\n\n        // Make sure unassigned is always larger than 0\n        unassigned \u003d Resources.componentwiseMax(unassigned, Resources.none());\n      }\n    }\n\n    // Sometimes its possible that, all queues are properly served. So intra\n    // queue preemption will not try for any preemption. How ever there are\n    // chances that within a queue, there are some imbalances. Hence make sure\n    // all queues are added to list.\n    while (!orderedByNeed.isEmpty()) {\n      TempQueuePerPartition q1 \u003d orderedByNeed.remove();\n      context.addPartitionToUnderServedQueues(q1.queueName, q1.partition);\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/AbstractPreemptableResourceCalculator.java",
      "extendedDetails": {}
    },
    "514794e1a5a39ca61de3981d53a05547ae17f5e4": {
      "type": "Ybodychange",
      "commitMessage": "YARN-7934. [GQ] Refactor preemption calculators to allow overriding for Federation Global Algos. (Contributed by curino)\n",
      "commitDate": "22/02/18 6:12 PM",
      "commitName": "514794e1a5a39ca61de3981d53a05547ae17f5e4",
      "commitAuthor": "Carlo Curino",
      "commitDateOld": "07/12/17 6:56 PM",
      "commitNameOld": "034b312d9f19024d2eabd377210d17d4080ef70e",
      "commitAuthorOld": "Wangda Tan",
      "daysBetweenCommits": 76.97,
      "commitsBetweenForRepo": 451,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,78 +1,85 @@\n   protected void computeFixpointAllocation(Resource totGuarant,\n       Collection\u003cTempQueuePerPartition\u003e qAlloc, Resource unassigned,\n       boolean ignoreGuarantee) {\n     // Prior to assigning the unused resources, process each queue as follows:\n     // If current \u003e guaranteed, idealAssigned \u003d guaranteed + untouchable extra\n     // Else idealAssigned \u003d current;\n     // Subtract idealAssigned resources from unassigned.\n     // If the queue has all of its needs met (that is, if\n     // idealAssigned \u003e\u003d current + pending), remove it from consideration.\n     // Sort queues from most under-guaranteed to most over-guaranteed.\n     TQComparator tqComparator \u003d new TQComparator(rc, totGuarant);\n     PriorityQueue\u003cTempQueuePerPartition\u003e orderedByNeed \u003d new PriorityQueue\u003c\u003e(10,\n         tqComparator);\n     for (Iterator\u003cTempQueuePerPartition\u003e i \u003d qAlloc.iterator(); i.hasNext();) {\n       TempQueuePerPartition q \u003d i.next();\n       Resource used \u003d q.getUsed();\n \n+      Resource initIdealAssigned;\n       if (Resources.greaterThan(rc, totGuarant, used, q.getGuaranteed())) {\n-        q.idealAssigned \u003d Resources.add(q.getGuaranteed(), q.untouchableExtra);\n+        initIdealAssigned \u003d\n+            Resources.add(q.getGuaranteed(), q.untouchableExtra);\n       } else {\n-        q.idealAssigned \u003d Resources.clone(used);\n+        initIdealAssigned \u003d Resources.clone(used);\n       }\n+\n+      // perform initial assignment\n+      initIdealAssignment(totGuarant, q, initIdealAssigned);\n+\n+\n       Resources.subtractFrom(unassigned, q.idealAssigned);\n       // If idealAssigned \u003c (allocated + used + pending), q needs more\n       // resources, so\n       // add it to the list of underserved queues, ordered by need.\n       Resource curPlusPend \u003d Resources.add(q.getUsed(), q.pending);\n       if (Resources.lessThan(rc, totGuarant, q.idealAssigned, curPlusPend)) {\n         orderedByNeed.add(q);\n       }\n     }\n \n     // assign all cluster resources until no more demand, or no resources are\n     // left\n     while (!orderedByNeed.isEmpty() \u0026\u0026 Resources.greaterThan(rc, totGuarant,\n         unassigned, Resources.none())) {\n       Resource wQassigned \u003d Resource.newInstance(0, 0);\n       // we compute normalizedGuarantees capacity based on currently active\n       // queues\n       resetCapacity(unassigned, orderedByNeed, ignoreGuarantee);\n \n       // For each underserved queue (or set of queues if multiple are equally\n       // underserved), offer its share of the unassigned resources based on its\n       // normalized guarantee. After the offer, if the queue is not satisfied,\n       // place it back in the ordered list of queues, recalculating its place\n       // in the order of most under-guaranteed to most over-guaranteed. In this\n       // way, the most underserved queue(s) are always given resources first.\n       Collection\u003cTempQueuePerPartition\u003e underserved \u003d getMostUnderservedQueues(\n           orderedByNeed, tqComparator);\n \n       for (Iterator\u003cTempQueuePerPartition\u003e i \u003d underserved.iterator(); i\n           .hasNext();) {\n         TempQueuePerPartition sub \u003d i.next();\n         Resource wQavail \u003d Resources.multiplyAndNormalizeUp(rc, unassigned,\n             sub.normalizedGuarantee, Resource.newInstance(1, 1));\n         Resource wQidle \u003d sub.offer(wQavail, rc, totGuarant,\n             isReservedPreemptionCandidatesSelector);\n         Resource wQdone \u003d Resources.subtract(wQavail, wQidle);\n \n         if (Resources.greaterThan(rc, totGuarant, wQdone, Resources.none())) {\n           // The queue is still asking for more. Put it back in the priority\n           // queue, recalculating its order based on need.\n           orderedByNeed.add(sub);\n         }\n         Resources.addTo(wQassigned, wQdone);\n       }\n       Resources.subtractFrom(unassigned, wQassigned);\n     }\n \n     // Sometimes its possible that, all queues are properly served. So intra\n     // queue preemption will not try for any preemption. How ever there are\n     // chances that within a queue, there are some imbalances. Hence make sure\n     // all queues are added to list.\n     while (!orderedByNeed.isEmpty()) {\n       TempQueuePerPartition q1 \u003d orderedByNeed.remove();\n       context.addPartitionToUnderServedQueues(q1.queueName, q1.partition);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  protected void computeFixpointAllocation(Resource totGuarant,\n      Collection\u003cTempQueuePerPartition\u003e qAlloc, Resource unassigned,\n      boolean ignoreGuarantee) {\n    // Prior to assigning the unused resources, process each queue as follows:\n    // If current \u003e guaranteed, idealAssigned \u003d guaranteed + untouchable extra\n    // Else idealAssigned \u003d current;\n    // Subtract idealAssigned resources from unassigned.\n    // If the queue has all of its needs met (that is, if\n    // idealAssigned \u003e\u003d current + pending), remove it from consideration.\n    // Sort queues from most under-guaranteed to most over-guaranteed.\n    TQComparator tqComparator \u003d new TQComparator(rc, totGuarant);\n    PriorityQueue\u003cTempQueuePerPartition\u003e orderedByNeed \u003d new PriorityQueue\u003c\u003e(10,\n        tqComparator);\n    for (Iterator\u003cTempQueuePerPartition\u003e i \u003d qAlloc.iterator(); i.hasNext();) {\n      TempQueuePerPartition q \u003d i.next();\n      Resource used \u003d q.getUsed();\n\n      Resource initIdealAssigned;\n      if (Resources.greaterThan(rc, totGuarant, used, q.getGuaranteed())) {\n        initIdealAssigned \u003d\n            Resources.add(q.getGuaranteed(), q.untouchableExtra);\n      } else {\n        initIdealAssigned \u003d Resources.clone(used);\n      }\n\n      // perform initial assignment\n      initIdealAssignment(totGuarant, q, initIdealAssigned);\n\n\n      Resources.subtractFrom(unassigned, q.idealAssigned);\n      // If idealAssigned \u003c (allocated + used + pending), q needs more\n      // resources, so\n      // add it to the list of underserved queues, ordered by need.\n      Resource curPlusPend \u003d Resources.add(q.getUsed(), q.pending);\n      if (Resources.lessThan(rc, totGuarant, q.idealAssigned, curPlusPend)) {\n        orderedByNeed.add(q);\n      }\n    }\n\n    // assign all cluster resources until no more demand, or no resources are\n    // left\n    while (!orderedByNeed.isEmpty() \u0026\u0026 Resources.greaterThan(rc, totGuarant,\n        unassigned, Resources.none())) {\n      Resource wQassigned \u003d Resource.newInstance(0, 0);\n      // we compute normalizedGuarantees capacity based on currently active\n      // queues\n      resetCapacity(unassigned, orderedByNeed, ignoreGuarantee);\n\n      // For each underserved queue (or set of queues if multiple are equally\n      // underserved), offer its share of the unassigned resources based on its\n      // normalized guarantee. After the offer, if the queue is not satisfied,\n      // place it back in the ordered list of queues, recalculating its place\n      // in the order of most under-guaranteed to most over-guaranteed. In this\n      // way, the most underserved queue(s) are always given resources first.\n      Collection\u003cTempQueuePerPartition\u003e underserved \u003d getMostUnderservedQueues(\n          orderedByNeed, tqComparator);\n\n      for (Iterator\u003cTempQueuePerPartition\u003e i \u003d underserved.iterator(); i\n          .hasNext();) {\n        TempQueuePerPartition sub \u003d i.next();\n        Resource wQavail \u003d Resources.multiplyAndNormalizeUp(rc, unassigned,\n            sub.normalizedGuarantee, Resource.newInstance(1, 1));\n        Resource wQidle \u003d sub.offer(wQavail, rc, totGuarant,\n            isReservedPreemptionCandidatesSelector);\n        Resource wQdone \u003d Resources.subtract(wQavail, wQidle);\n\n        if (Resources.greaterThan(rc, totGuarant, wQdone, Resources.none())) {\n          // The queue is still asking for more. Put it back in the priority\n          // queue, recalculating its order based on need.\n          orderedByNeed.add(sub);\n        }\n        Resources.addTo(wQassigned, wQdone);\n      }\n      Resources.subtractFrom(unassigned, wQassigned);\n    }\n\n    // Sometimes its possible that, all queues are properly served. So intra\n    // queue preemption will not try for any preemption. How ever there are\n    // chances that within a queue, there are some imbalances. Hence make sure\n    // all queues are added to list.\n    while (!orderedByNeed.isEmpty()) {\n      TempQueuePerPartition q1 \u003d orderedByNeed.remove();\n      context.addPartitionToUnderServedQueues(q1.queueName, q1.partition);\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/AbstractPreemptableResourceCalculator.java",
      "extendedDetails": {}
    },
    "90dd3a8148468ac37a3f2173ad8d45e38bfcb0c9": {
      "type": "Ymultichange(Ymovefromfile,Ymodifierchange,Ybodychange,Yparameterchange)",
      "commitMessage": "YARN-2009. CapacityScheduler: Add intra-queue preemption for app priority support. (Sunil G via wangda)\n",
      "commitDate": "31/10/16 3:18 PM",
      "commitName": "90dd3a8148468ac37a3f2173ad8d45e38bfcb0c9",
      "commitAuthor": "Wangda Tan",
      "subchanges": [
        {
          "type": "Ymovefromfile",
          "commitMessage": "YARN-2009. CapacityScheduler: Add intra-queue preemption for app priority support. (Sunil G via wangda)\n",
          "commitDate": "31/10/16 3:18 PM",
          "commitName": "90dd3a8148468ac37a3f2173ad8d45e38bfcb0c9",
          "commitAuthor": "Wangda Tan",
          "commitDateOld": "31/10/16 2:47 PM",
          "commitNameOld": "773c60bd7bd00651dc3016799b424b9bd2233eb3",
          "commitAuthorOld": "Daniel Templeton",
          "daysBetweenCommits": 0.02,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,69 +1,77 @@\n-  private void computeFixpointAllocation(ResourceCalculator rc,\n-      Resource tot_guarant, Collection\u003cTempQueuePerPartition\u003e qAlloc,\n-      Resource unassigned, boolean ignoreGuarantee) {\n+  protected void computeFixpointAllocation(Resource totGuarant,\n+      Collection\u003cTempQueuePerPartition\u003e qAlloc, Resource unassigned,\n+      boolean ignoreGuarantee) {\n     // Prior to assigning the unused resources, process each queue as follows:\n     // If current \u003e guaranteed, idealAssigned \u003d guaranteed + untouchable extra\n     // Else idealAssigned \u003d current;\n     // Subtract idealAssigned resources from unassigned.\n     // If the queue has all of its needs met (that is, if\n     // idealAssigned \u003e\u003d current + pending), remove it from consideration.\n     // Sort queues from most under-guaranteed to most over-guaranteed.\n-    TQComparator tqComparator \u003d new TQComparator(rc, tot_guarant);\n+    TQComparator tqComparator \u003d new TQComparator(rc, totGuarant);\n     PriorityQueue\u003cTempQueuePerPartition\u003e orderedByNeed \u003d new PriorityQueue\u003c\u003e(10,\n         tqComparator);\n     for (Iterator\u003cTempQueuePerPartition\u003e i \u003d qAlloc.iterator(); i.hasNext();) {\n       TempQueuePerPartition q \u003d i.next();\n       Resource used \u003d q.getUsed();\n \n-      if (Resources.greaterThan(rc, tot_guarant, used,\n-          q.getGuaranteed())) {\n-        q.idealAssigned \u003d Resources.add(\n-            q.getGuaranteed(), q.untouchableExtra);\n+      if (Resources.greaterThan(rc, totGuarant, used, q.getGuaranteed())) {\n+        q.idealAssigned \u003d Resources.add(q.getGuaranteed(), q.untouchableExtra);\n       } else {\n         q.idealAssigned \u003d Resources.clone(used);\n       }\n       Resources.subtractFrom(unassigned, q.idealAssigned);\n-      // If idealAssigned \u003c (allocated + used + pending), q needs more resources, so\n+      // If idealAssigned \u003c (allocated + used + pending), q needs more\n+      // resources, so\n       // add it to the list of underserved queues, ordered by need.\n       Resource curPlusPend \u003d Resources.add(q.getUsed(), q.pending);\n-      if (Resources.lessThan(rc, tot_guarant, q.idealAssigned, curPlusPend)) {\n+      if (Resources.lessThan(rc, totGuarant, q.idealAssigned, curPlusPend)) {\n         orderedByNeed.add(q);\n       }\n     }\n \n-    //assign all cluster resources until no more demand, or no resources are left\n-    while (!orderedByNeed.isEmpty()\n-        \u0026\u0026 Resources.greaterThan(rc,tot_guarant, unassigned,Resources.none())) {\n+    // assign all cluster resources until no more demand, or no resources are\n+    // left\n+    while (!orderedByNeed.isEmpty() \u0026\u0026 Resources.greaterThan(rc, totGuarant,\n+        unassigned, Resources.none())) {\n       Resource wQassigned \u003d Resource.newInstance(0, 0);\n       // we compute normalizedGuarantees capacity based on currently active\n       // queues\n-      resetCapacity(rc, unassigned, orderedByNeed, ignoreGuarantee);\n+      resetCapacity(unassigned, orderedByNeed, ignoreGuarantee);\n \n       // For each underserved queue (or set of queues if multiple are equally\n       // underserved), offer its share of the unassigned resources based on its\n       // normalized guarantee. After the offer, if the queue is not satisfied,\n       // place it back in the ordered list of queues, recalculating its place\n       // in the order of most under-guaranteed to most over-guaranteed. In this\n       // way, the most underserved queue(s) are always given resources first.\n-      Collection\u003cTempQueuePerPartition\u003e underserved \u003d\n-          getMostUnderservedQueues(orderedByNeed, tqComparator);\n+      Collection\u003cTempQueuePerPartition\u003e underserved \u003d getMostUnderservedQueues(\n+          orderedByNeed, tqComparator);\n       for (Iterator\u003cTempQueuePerPartition\u003e i \u003d underserved.iterator(); i\n           .hasNext();) {\n         TempQueuePerPartition sub \u003d i.next();\n-        Resource wQavail \u003d Resources.multiplyAndNormalizeUp(rc,\n-            unassigned, sub.normalizedGuarantee, Resource.newInstance(1, 1));\n-        Resource wQidle \u003d sub.offer(wQavail, rc, tot_guarant,\n+        Resource wQavail \u003d Resources.multiplyAndNormalizeUp(rc, unassigned,\n+            sub.normalizedGuarantee, Resource.newInstance(1, 1));\n+        Resource wQidle \u003d sub.offer(wQavail, rc, totGuarant,\n             isReservedPreemptionCandidatesSelector);\n         Resource wQdone \u003d Resources.subtract(wQavail, wQidle);\n \n-        if (Resources.greaterThan(rc, tot_guarant,\n-            wQdone, Resources.none())) {\n+        if (Resources.greaterThan(rc, totGuarant, wQdone, Resources.none())) {\n           // The queue is still asking for more. Put it back in the priority\n           // queue, recalculating its order based on need.\n           orderedByNeed.add(sub);\n         }\n         Resources.addTo(wQassigned, wQdone);\n       }\n       Resources.subtractFrom(unassigned, wQassigned);\n     }\n+\n+    // Sometimes its possible that, all queues are properly served. So intra\n+    // queue preemption will not try for any preemption. How ever there are\n+    // chances that within a queue, there are some imbalances. Hence make sure\n+    // all queues are added to list.\n+    while (!orderedByNeed.isEmpty()) {\n+      TempQueuePerPartition q1 \u003d orderedByNeed.remove();\n+      context.addPartitionToUnderServedQueues(q1.queueName, q1.partition);\n+    }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected void computeFixpointAllocation(Resource totGuarant,\n      Collection\u003cTempQueuePerPartition\u003e qAlloc, Resource unassigned,\n      boolean ignoreGuarantee) {\n    // Prior to assigning the unused resources, process each queue as follows:\n    // If current \u003e guaranteed, idealAssigned \u003d guaranteed + untouchable extra\n    // Else idealAssigned \u003d current;\n    // Subtract idealAssigned resources from unassigned.\n    // If the queue has all of its needs met (that is, if\n    // idealAssigned \u003e\u003d current + pending), remove it from consideration.\n    // Sort queues from most under-guaranteed to most over-guaranteed.\n    TQComparator tqComparator \u003d new TQComparator(rc, totGuarant);\n    PriorityQueue\u003cTempQueuePerPartition\u003e orderedByNeed \u003d new PriorityQueue\u003c\u003e(10,\n        tqComparator);\n    for (Iterator\u003cTempQueuePerPartition\u003e i \u003d qAlloc.iterator(); i.hasNext();) {\n      TempQueuePerPartition q \u003d i.next();\n      Resource used \u003d q.getUsed();\n\n      if (Resources.greaterThan(rc, totGuarant, used, q.getGuaranteed())) {\n        q.idealAssigned \u003d Resources.add(q.getGuaranteed(), q.untouchableExtra);\n      } else {\n        q.idealAssigned \u003d Resources.clone(used);\n      }\n      Resources.subtractFrom(unassigned, q.idealAssigned);\n      // If idealAssigned \u003c (allocated + used + pending), q needs more\n      // resources, so\n      // add it to the list of underserved queues, ordered by need.\n      Resource curPlusPend \u003d Resources.add(q.getUsed(), q.pending);\n      if (Resources.lessThan(rc, totGuarant, q.idealAssigned, curPlusPend)) {\n        orderedByNeed.add(q);\n      }\n    }\n\n    // assign all cluster resources until no more demand, or no resources are\n    // left\n    while (!orderedByNeed.isEmpty() \u0026\u0026 Resources.greaterThan(rc, totGuarant,\n        unassigned, Resources.none())) {\n      Resource wQassigned \u003d Resource.newInstance(0, 0);\n      // we compute normalizedGuarantees capacity based on currently active\n      // queues\n      resetCapacity(unassigned, orderedByNeed, ignoreGuarantee);\n\n      // For each underserved queue (or set of queues if multiple are equally\n      // underserved), offer its share of the unassigned resources based on its\n      // normalized guarantee. After the offer, if the queue is not satisfied,\n      // place it back in the ordered list of queues, recalculating its place\n      // in the order of most under-guaranteed to most over-guaranteed. In this\n      // way, the most underserved queue(s) are always given resources first.\n      Collection\u003cTempQueuePerPartition\u003e underserved \u003d getMostUnderservedQueues(\n          orderedByNeed, tqComparator);\n      for (Iterator\u003cTempQueuePerPartition\u003e i \u003d underserved.iterator(); i\n          .hasNext();) {\n        TempQueuePerPartition sub \u003d i.next();\n        Resource wQavail \u003d Resources.multiplyAndNormalizeUp(rc, unassigned,\n            sub.normalizedGuarantee, Resource.newInstance(1, 1));\n        Resource wQidle \u003d sub.offer(wQavail, rc, totGuarant,\n            isReservedPreemptionCandidatesSelector);\n        Resource wQdone \u003d Resources.subtract(wQavail, wQidle);\n\n        if (Resources.greaterThan(rc, totGuarant, wQdone, Resources.none())) {\n          // The queue is still asking for more. Put it back in the priority\n          // queue, recalculating its order based on need.\n          orderedByNeed.add(sub);\n        }\n        Resources.addTo(wQassigned, wQdone);\n      }\n      Resources.subtractFrom(unassigned, wQassigned);\n    }\n\n    // Sometimes its possible that, all queues are properly served. So intra\n    // queue preemption will not try for any preemption. How ever there are\n    // chances that within a queue, there are some imbalances. Hence make sure\n    // all queues are added to list.\n    while (!orderedByNeed.isEmpty()) {\n      TempQueuePerPartition q1 \u003d orderedByNeed.remove();\n      context.addPartitionToUnderServedQueues(q1.queueName, q1.partition);\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/AbstractPreemptableResourceCalculator.java",
          "extendedDetails": {
            "oldPath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/PreemptableResourceCalculator.java",
            "newPath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/AbstractPreemptableResourceCalculator.java",
            "oldMethodName": "computeFixpointAllocation",
            "newMethodName": "computeFixpointAllocation"
          }
        },
        {
          "type": "Ymodifierchange",
          "commitMessage": "YARN-2009. CapacityScheduler: Add intra-queue preemption for app priority support. (Sunil G via wangda)\n",
          "commitDate": "31/10/16 3:18 PM",
          "commitName": "90dd3a8148468ac37a3f2173ad8d45e38bfcb0c9",
          "commitAuthor": "Wangda Tan",
          "commitDateOld": "31/10/16 2:47 PM",
          "commitNameOld": "773c60bd7bd00651dc3016799b424b9bd2233eb3",
          "commitAuthorOld": "Daniel Templeton",
          "daysBetweenCommits": 0.02,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,69 +1,77 @@\n-  private void computeFixpointAllocation(ResourceCalculator rc,\n-      Resource tot_guarant, Collection\u003cTempQueuePerPartition\u003e qAlloc,\n-      Resource unassigned, boolean ignoreGuarantee) {\n+  protected void computeFixpointAllocation(Resource totGuarant,\n+      Collection\u003cTempQueuePerPartition\u003e qAlloc, Resource unassigned,\n+      boolean ignoreGuarantee) {\n     // Prior to assigning the unused resources, process each queue as follows:\n     // If current \u003e guaranteed, idealAssigned \u003d guaranteed + untouchable extra\n     // Else idealAssigned \u003d current;\n     // Subtract idealAssigned resources from unassigned.\n     // If the queue has all of its needs met (that is, if\n     // idealAssigned \u003e\u003d current + pending), remove it from consideration.\n     // Sort queues from most under-guaranteed to most over-guaranteed.\n-    TQComparator tqComparator \u003d new TQComparator(rc, tot_guarant);\n+    TQComparator tqComparator \u003d new TQComparator(rc, totGuarant);\n     PriorityQueue\u003cTempQueuePerPartition\u003e orderedByNeed \u003d new PriorityQueue\u003c\u003e(10,\n         tqComparator);\n     for (Iterator\u003cTempQueuePerPartition\u003e i \u003d qAlloc.iterator(); i.hasNext();) {\n       TempQueuePerPartition q \u003d i.next();\n       Resource used \u003d q.getUsed();\n \n-      if (Resources.greaterThan(rc, tot_guarant, used,\n-          q.getGuaranteed())) {\n-        q.idealAssigned \u003d Resources.add(\n-            q.getGuaranteed(), q.untouchableExtra);\n+      if (Resources.greaterThan(rc, totGuarant, used, q.getGuaranteed())) {\n+        q.idealAssigned \u003d Resources.add(q.getGuaranteed(), q.untouchableExtra);\n       } else {\n         q.idealAssigned \u003d Resources.clone(used);\n       }\n       Resources.subtractFrom(unassigned, q.idealAssigned);\n-      // If idealAssigned \u003c (allocated + used + pending), q needs more resources, so\n+      // If idealAssigned \u003c (allocated + used + pending), q needs more\n+      // resources, so\n       // add it to the list of underserved queues, ordered by need.\n       Resource curPlusPend \u003d Resources.add(q.getUsed(), q.pending);\n-      if (Resources.lessThan(rc, tot_guarant, q.idealAssigned, curPlusPend)) {\n+      if (Resources.lessThan(rc, totGuarant, q.idealAssigned, curPlusPend)) {\n         orderedByNeed.add(q);\n       }\n     }\n \n-    //assign all cluster resources until no more demand, or no resources are left\n-    while (!orderedByNeed.isEmpty()\n-        \u0026\u0026 Resources.greaterThan(rc,tot_guarant, unassigned,Resources.none())) {\n+    // assign all cluster resources until no more demand, or no resources are\n+    // left\n+    while (!orderedByNeed.isEmpty() \u0026\u0026 Resources.greaterThan(rc, totGuarant,\n+        unassigned, Resources.none())) {\n       Resource wQassigned \u003d Resource.newInstance(0, 0);\n       // we compute normalizedGuarantees capacity based on currently active\n       // queues\n-      resetCapacity(rc, unassigned, orderedByNeed, ignoreGuarantee);\n+      resetCapacity(unassigned, orderedByNeed, ignoreGuarantee);\n \n       // For each underserved queue (or set of queues if multiple are equally\n       // underserved), offer its share of the unassigned resources based on its\n       // normalized guarantee. After the offer, if the queue is not satisfied,\n       // place it back in the ordered list of queues, recalculating its place\n       // in the order of most under-guaranteed to most over-guaranteed. In this\n       // way, the most underserved queue(s) are always given resources first.\n-      Collection\u003cTempQueuePerPartition\u003e underserved \u003d\n-          getMostUnderservedQueues(orderedByNeed, tqComparator);\n+      Collection\u003cTempQueuePerPartition\u003e underserved \u003d getMostUnderservedQueues(\n+          orderedByNeed, tqComparator);\n       for (Iterator\u003cTempQueuePerPartition\u003e i \u003d underserved.iterator(); i\n           .hasNext();) {\n         TempQueuePerPartition sub \u003d i.next();\n-        Resource wQavail \u003d Resources.multiplyAndNormalizeUp(rc,\n-            unassigned, sub.normalizedGuarantee, Resource.newInstance(1, 1));\n-        Resource wQidle \u003d sub.offer(wQavail, rc, tot_guarant,\n+        Resource wQavail \u003d Resources.multiplyAndNormalizeUp(rc, unassigned,\n+            sub.normalizedGuarantee, Resource.newInstance(1, 1));\n+        Resource wQidle \u003d sub.offer(wQavail, rc, totGuarant,\n             isReservedPreemptionCandidatesSelector);\n         Resource wQdone \u003d Resources.subtract(wQavail, wQidle);\n \n-        if (Resources.greaterThan(rc, tot_guarant,\n-            wQdone, Resources.none())) {\n+        if (Resources.greaterThan(rc, totGuarant, wQdone, Resources.none())) {\n           // The queue is still asking for more. Put it back in the priority\n           // queue, recalculating its order based on need.\n           orderedByNeed.add(sub);\n         }\n         Resources.addTo(wQassigned, wQdone);\n       }\n       Resources.subtractFrom(unassigned, wQassigned);\n     }\n+\n+    // Sometimes its possible that, all queues are properly served. So intra\n+    // queue preemption will not try for any preemption. How ever there are\n+    // chances that within a queue, there are some imbalances. Hence make sure\n+    // all queues are added to list.\n+    while (!orderedByNeed.isEmpty()) {\n+      TempQueuePerPartition q1 \u003d orderedByNeed.remove();\n+      context.addPartitionToUnderServedQueues(q1.queueName, q1.partition);\n+    }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected void computeFixpointAllocation(Resource totGuarant,\n      Collection\u003cTempQueuePerPartition\u003e qAlloc, Resource unassigned,\n      boolean ignoreGuarantee) {\n    // Prior to assigning the unused resources, process each queue as follows:\n    // If current \u003e guaranteed, idealAssigned \u003d guaranteed + untouchable extra\n    // Else idealAssigned \u003d current;\n    // Subtract idealAssigned resources from unassigned.\n    // If the queue has all of its needs met (that is, if\n    // idealAssigned \u003e\u003d current + pending), remove it from consideration.\n    // Sort queues from most under-guaranteed to most over-guaranteed.\n    TQComparator tqComparator \u003d new TQComparator(rc, totGuarant);\n    PriorityQueue\u003cTempQueuePerPartition\u003e orderedByNeed \u003d new PriorityQueue\u003c\u003e(10,\n        tqComparator);\n    for (Iterator\u003cTempQueuePerPartition\u003e i \u003d qAlloc.iterator(); i.hasNext();) {\n      TempQueuePerPartition q \u003d i.next();\n      Resource used \u003d q.getUsed();\n\n      if (Resources.greaterThan(rc, totGuarant, used, q.getGuaranteed())) {\n        q.idealAssigned \u003d Resources.add(q.getGuaranteed(), q.untouchableExtra);\n      } else {\n        q.idealAssigned \u003d Resources.clone(used);\n      }\n      Resources.subtractFrom(unassigned, q.idealAssigned);\n      // If idealAssigned \u003c (allocated + used + pending), q needs more\n      // resources, so\n      // add it to the list of underserved queues, ordered by need.\n      Resource curPlusPend \u003d Resources.add(q.getUsed(), q.pending);\n      if (Resources.lessThan(rc, totGuarant, q.idealAssigned, curPlusPend)) {\n        orderedByNeed.add(q);\n      }\n    }\n\n    // assign all cluster resources until no more demand, or no resources are\n    // left\n    while (!orderedByNeed.isEmpty() \u0026\u0026 Resources.greaterThan(rc, totGuarant,\n        unassigned, Resources.none())) {\n      Resource wQassigned \u003d Resource.newInstance(0, 0);\n      // we compute normalizedGuarantees capacity based on currently active\n      // queues\n      resetCapacity(unassigned, orderedByNeed, ignoreGuarantee);\n\n      // For each underserved queue (or set of queues if multiple are equally\n      // underserved), offer its share of the unassigned resources based on its\n      // normalized guarantee. After the offer, if the queue is not satisfied,\n      // place it back in the ordered list of queues, recalculating its place\n      // in the order of most under-guaranteed to most over-guaranteed. In this\n      // way, the most underserved queue(s) are always given resources first.\n      Collection\u003cTempQueuePerPartition\u003e underserved \u003d getMostUnderservedQueues(\n          orderedByNeed, tqComparator);\n      for (Iterator\u003cTempQueuePerPartition\u003e i \u003d underserved.iterator(); i\n          .hasNext();) {\n        TempQueuePerPartition sub \u003d i.next();\n        Resource wQavail \u003d Resources.multiplyAndNormalizeUp(rc, unassigned,\n            sub.normalizedGuarantee, Resource.newInstance(1, 1));\n        Resource wQidle \u003d sub.offer(wQavail, rc, totGuarant,\n            isReservedPreemptionCandidatesSelector);\n        Resource wQdone \u003d Resources.subtract(wQavail, wQidle);\n\n        if (Resources.greaterThan(rc, totGuarant, wQdone, Resources.none())) {\n          // The queue is still asking for more. Put it back in the priority\n          // queue, recalculating its order based on need.\n          orderedByNeed.add(sub);\n        }\n        Resources.addTo(wQassigned, wQdone);\n      }\n      Resources.subtractFrom(unassigned, wQassigned);\n    }\n\n    // Sometimes its possible that, all queues are properly served. So intra\n    // queue preemption will not try for any preemption. How ever there are\n    // chances that within a queue, there are some imbalances. Hence make sure\n    // all queues are added to list.\n    while (!orderedByNeed.isEmpty()) {\n      TempQueuePerPartition q1 \u003d orderedByNeed.remove();\n      context.addPartitionToUnderServedQueues(q1.queueName, q1.partition);\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/AbstractPreemptableResourceCalculator.java",
          "extendedDetails": {
            "oldValue": "[private]",
            "newValue": "[protected]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "YARN-2009. CapacityScheduler: Add intra-queue preemption for app priority support. (Sunil G via wangda)\n",
          "commitDate": "31/10/16 3:18 PM",
          "commitName": "90dd3a8148468ac37a3f2173ad8d45e38bfcb0c9",
          "commitAuthor": "Wangda Tan",
          "commitDateOld": "31/10/16 2:47 PM",
          "commitNameOld": "773c60bd7bd00651dc3016799b424b9bd2233eb3",
          "commitAuthorOld": "Daniel Templeton",
          "daysBetweenCommits": 0.02,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,69 +1,77 @@\n-  private void computeFixpointAllocation(ResourceCalculator rc,\n-      Resource tot_guarant, Collection\u003cTempQueuePerPartition\u003e qAlloc,\n-      Resource unassigned, boolean ignoreGuarantee) {\n+  protected void computeFixpointAllocation(Resource totGuarant,\n+      Collection\u003cTempQueuePerPartition\u003e qAlloc, Resource unassigned,\n+      boolean ignoreGuarantee) {\n     // Prior to assigning the unused resources, process each queue as follows:\n     // If current \u003e guaranteed, idealAssigned \u003d guaranteed + untouchable extra\n     // Else idealAssigned \u003d current;\n     // Subtract idealAssigned resources from unassigned.\n     // If the queue has all of its needs met (that is, if\n     // idealAssigned \u003e\u003d current + pending), remove it from consideration.\n     // Sort queues from most under-guaranteed to most over-guaranteed.\n-    TQComparator tqComparator \u003d new TQComparator(rc, tot_guarant);\n+    TQComparator tqComparator \u003d new TQComparator(rc, totGuarant);\n     PriorityQueue\u003cTempQueuePerPartition\u003e orderedByNeed \u003d new PriorityQueue\u003c\u003e(10,\n         tqComparator);\n     for (Iterator\u003cTempQueuePerPartition\u003e i \u003d qAlloc.iterator(); i.hasNext();) {\n       TempQueuePerPartition q \u003d i.next();\n       Resource used \u003d q.getUsed();\n \n-      if (Resources.greaterThan(rc, tot_guarant, used,\n-          q.getGuaranteed())) {\n-        q.idealAssigned \u003d Resources.add(\n-            q.getGuaranteed(), q.untouchableExtra);\n+      if (Resources.greaterThan(rc, totGuarant, used, q.getGuaranteed())) {\n+        q.idealAssigned \u003d Resources.add(q.getGuaranteed(), q.untouchableExtra);\n       } else {\n         q.idealAssigned \u003d Resources.clone(used);\n       }\n       Resources.subtractFrom(unassigned, q.idealAssigned);\n-      // If idealAssigned \u003c (allocated + used + pending), q needs more resources, so\n+      // If idealAssigned \u003c (allocated + used + pending), q needs more\n+      // resources, so\n       // add it to the list of underserved queues, ordered by need.\n       Resource curPlusPend \u003d Resources.add(q.getUsed(), q.pending);\n-      if (Resources.lessThan(rc, tot_guarant, q.idealAssigned, curPlusPend)) {\n+      if (Resources.lessThan(rc, totGuarant, q.idealAssigned, curPlusPend)) {\n         orderedByNeed.add(q);\n       }\n     }\n \n-    //assign all cluster resources until no more demand, or no resources are left\n-    while (!orderedByNeed.isEmpty()\n-        \u0026\u0026 Resources.greaterThan(rc,tot_guarant, unassigned,Resources.none())) {\n+    // assign all cluster resources until no more demand, or no resources are\n+    // left\n+    while (!orderedByNeed.isEmpty() \u0026\u0026 Resources.greaterThan(rc, totGuarant,\n+        unassigned, Resources.none())) {\n       Resource wQassigned \u003d Resource.newInstance(0, 0);\n       // we compute normalizedGuarantees capacity based on currently active\n       // queues\n-      resetCapacity(rc, unassigned, orderedByNeed, ignoreGuarantee);\n+      resetCapacity(unassigned, orderedByNeed, ignoreGuarantee);\n \n       // For each underserved queue (or set of queues if multiple are equally\n       // underserved), offer its share of the unassigned resources based on its\n       // normalized guarantee. After the offer, if the queue is not satisfied,\n       // place it back in the ordered list of queues, recalculating its place\n       // in the order of most under-guaranteed to most over-guaranteed. In this\n       // way, the most underserved queue(s) are always given resources first.\n-      Collection\u003cTempQueuePerPartition\u003e underserved \u003d\n-          getMostUnderservedQueues(orderedByNeed, tqComparator);\n+      Collection\u003cTempQueuePerPartition\u003e underserved \u003d getMostUnderservedQueues(\n+          orderedByNeed, tqComparator);\n       for (Iterator\u003cTempQueuePerPartition\u003e i \u003d underserved.iterator(); i\n           .hasNext();) {\n         TempQueuePerPartition sub \u003d i.next();\n-        Resource wQavail \u003d Resources.multiplyAndNormalizeUp(rc,\n-            unassigned, sub.normalizedGuarantee, Resource.newInstance(1, 1));\n-        Resource wQidle \u003d sub.offer(wQavail, rc, tot_guarant,\n+        Resource wQavail \u003d Resources.multiplyAndNormalizeUp(rc, unassigned,\n+            sub.normalizedGuarantee, Resource.newInstance(1, 1));\n+        Resource wQidle \u003d sub.offer(wQavail, rc, totGuarant,\n             isReservedPreemptionCandidatesSelector);\n         Resource wQdone \u003d Resources.subtract(wQavail, wQidle);\n \n-        if (Resources.greaterThan(rc, tot_guarant,\n-            wQdone, Resources.none())) {\n+        if (Resources.greaterThan(rc, totGuarant, wQdone, Resources.none())) {\n           // The queue is still asking for more. Put it back in the priority\n           // queue, recalculating its order based on need.\n           orderedByNeed.add(sub);\n         }\n         Resources.addTo(wQassigned, wQdone);\n       }\n       Resources.subtractFrom(unassigned, wQassigned);\n     }\n+\n+    // Sometimes its possible that, all queues are properly served. So intra\n+    // queue preemption will not try for any preemption. How ever there are\n+    // chances that within a queue, there are some imbalances. Hence make sure\n+    // all queues are added to list.\n+    while (!orderedByNeed.isEmpty()) {\n+      TempQueuePerPartition q1 \u003d orderedByNeed.remove();\n+      context.addPartitionToUnderServedQueues(q1.queueName, q1.partition);\n+    }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected void computeFixpointAllocation(Resource totGuarant,\n      Collection\u003cTempQueuePerPartition\u003e qAlloc, Resource unassigned,\n      boolean ignoreGuarantee) {\n    // Prior to assigning the unused resources, process each queue as follows:\n    // If current \u003e guaranteed, idealAssigned \u003d guaranteed + untouchable extra\n    // Else idealAssigned \u003d current;\n    // Subtract idealAssigned resources from unassigned.\n    // If the queue has all of its needs met (that is, if\n    // idealAssigned \u003e\u003d current + pending), remove it from consideration.\n    // Sort queues from most under-guaranteed to most over-guaranteed.\n    TQComparator tqComparator \u003d new TQComparator(rc, totGuarant);\n    PriorityQueue\u003cTempQueuePerPartition\u003e orderedByNeed \u003d new PriorityQueue\u003c\u003e(10,\n        tqComparator);\n    for (Iterator\u003cTempQueuePerPartition\u003e i \u003d qAlloc.iterator(); i.hasNext();) {\n      TempQueuePerPartition q \u003d i.next();\n      Resource used \u003d q.getUsed();\n\n      if (Resources.greaterThan(rc, totGuarant, used, q.getGuaranteed())) {\n        q.idealAssigned \u003d Resources.add(q.getGuaranteed(), q.untouchableExtra);\n      } else {\n        q.idealAssigned \u003d Resources.clone(used);\n      }\n      Resources.subtractFrom(unassigned, q.idealAssigned);\n      // If idealAssigned \u003c (allocated + used + pending), q needs more\n      // resources, so\n      // add it to the list of underserved queues, ordered by need.\n      Resource curPlusPend \u003d Resources.add(q.getUsed(), q.pending);\n      if (Resources.lessThan(rc, totGuarant, q.idealAssigned, curPlusPend)) {\n        orderedByNeed.add(q);\n      }\n    }\n\n    // assign all cluster resources until no more demand, or no resources are\n    // left\n    while (!orderedByNeed.isEmpty() \u0026\u0026 Resources.greaterThan(rc, totGuarant,\n        unassigned, Resources.none())) {\n      Resource wQassigned \u003d Resource.newInstance(0, 0);\n      // we compute normalizedGuarantees capacity based on currently active\n      // queues\n      resetCapacity(unassigned, orderedByNeed, ignoreGuarantee);\n\n      // For each underserved queue (or set of queues if multiple are equally\n      // underserved), offer its share of the unassigned resources based on its\n      // normalized guarantee. After the offer, if the queue is not satisfied,\n      // place it back in the ordered list of queues, recalculating its place\n      // in the order of most under-guaranteed to most over-guaranteed. In this\n      // way, the most underserved queue(s) are always given resources first.\n      Collection\u003cTempQueuePerPartition\u003e underserved \u003d getMostUnderservedQueues(\n          orderedByNeed, tqComparator);\n      for (Iterator\u003cTempQueuePerPartition\u003e i \u003d underserved.iterator(); i\n          .hasNext();) {\n        TempQueuePerPartition sub \u003d i.next();\n        Resource wQavail \u003d Resources.multiplyAndNormalizeUp(rc, unassigned,\n            sub.normalizedGuarantee, Resource.newInstance(1, 1));\n        Resource wQidle \u003d sub.offer(wQavail, rc, totGuarant,\n            isReservedPreemptionCandidatesSelector);\n        Resource wQdone \u003d Resources.subtract(wQavail, wQidle);\n\n        if (Resources.greaterThan(rc, totGuarant, wQdone, Resources.none())) {\n          // The queue is still asking for more. Put it back in the priority\n          // queue, recalculating its order based on need.\n          orderedByNeed.add(sub);\n        }\n        Resources.addTo(wQassigned, wQdone);\n      }\n      Resources.subtractFrom(unassigned, wQassigned);\n    }\n\n    // Sometimes its possible that, all queues are properly served. So intra\n    // queue preemption will not try for any preemption. How ever there are\n    // chances that within a queue, there are some imbalances. Hence make sure\n    // all queues are added to list.\n    while (!orderedByNeed.isEmpty()) {\n      TempQueuePerPartition q1 \u003d orderedByNeed.remove();\n      context.addPartitionToUnderServedQueues(q1.queueName, q1.partition);\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/AbstractPreemptableResourceCalculator.java",
          "extendedDetails": {}
        },
        {
          "type": "Yparameterchange",
          "commitMessage": "YARN-2009. CapacityScheduler: Add intra-queue preemption for app priority support. (Sunil G via wangda)\n",
          "commitDate": "31/10/16 3:18 PM",
          "commitName": "90dd3a8148468ac37a3f2173ad8d45e38bfcb0c9",
          "commitAuthor": "Wangda Tan",
          "commitDateOld": "31/10/16 2:47 PM",
          "commitNameOld": "773c60bd7bd00651dc3016799b424b9bd2233eb3",
          "commitAuthorOld": "Daniel Templeton",
          "daysBetweenCommits": 0.02,
          "commitsBetweenForRepo": 1,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,69 +1,77 @@\n-  private void computeFixpointAllocation(ResourceCalculator rc,\n-      Resource tot_guarant, Collection\u003cTempQueuePerPartition\u003e qAlloc,\n-      Resource unassigned, boolean ignoreGuarantee) {\n+  protected void computeFixpointAllocation(Resource totGuarant,\n+      Collection\u003cTempQueuePerPartition\u003e qAlloc, Resource unassigned,\n+      boolean ignoreGuarantee) {\n     // Prior to assigning the unused resources, process each queue as follows:\n     // If current \u003e guaranteed, idealAssigned \u003d guaranteed + untouchable extra\n     // Else idealAssigned \u003d current;\n     // Subtract idealAssigned resources from unassigned.\n     // If the queue has all of its needs met (that is, if\n     // idealAssigned \u003e\u003d current + pending), remove it from consideration.\n     // Sort queues from most under-guaranteed to most over-guaranteed.\n-    TQComparator tqComparator \u003d new TQComparator(rc, tot_guarant);\n+    TQComparator tqComparator \u003d new TQComparator(rc, totGuarant);\n     PriorityQueue\u003cTempQueuePerPartition\u003e orderedByNeed \u003d new PriorityQueue\u003c\u003e(10,\n         tqComparator);\n     for (Iterator\u003cTempQueuePerPartition\u003e i \u003d qAlloc.iterator(); i.hasNext();) {\n       TempQueuePerPartition q \u003d i.next();\n       Resource used \u003d q.getUsed();\n \n-      if (Resources.greaterThan(rc, tot_guarant, used,\n-          q.getGuaranteed())) {\n-        q.idealAssigned \u003d Resources.add(\n-            q.getGuaranteed(), q.untouchableExtra);\n+      if (Resources.greaterThan(rc, totGuarant, used, q.getGuaranteed())) {\n+        q.idealAssigned \u003d Resources.add(q.getGuaranteed(), q.untouchableExtra);\n       } else {\n         q.idealAssigned \u003d Resources.clone(used);\n       }\n       Resources.subtractFrom(unassigned, q.idealAssigned);\n-      // If idealAssigned \u003c (allocated + used + pending), q needs more resources, so\n+      // If idealAssigned \u003c (allocated + used + pending), q needs more\n+      // resources, so\n       // add it to the list of underserved queues, ordered by need.\n       Resource curPlusPend \u003d Resources.add(q.getUsed(), q.pending);\n-      if (Resources.lessThan(rc, tot_guarant, q.idealAssigned, curPlusPend)) {\n+      if (Resources.lessThan(rc, totGuarant, q.idealAssigned, curPlusPend)) {\n         orderedByNeed.add(q);\n       }\n     }\n \n-    //assign all cluster resources until no more demand, or no resources are left\n-    while (!orderedByNeed.isEmpty()\n-        \u0026\u0026 Resources.greaterThan(rc,tot_guarant, unassigned,Resources.none())) {\n+    // assign all cluster resources until no more demand, or no resources are\n+    // left\n+    while (!orderedByNeed.isEmpty() \u0026\u0026 Resources.greaterThan(rc, totGuarant,\n+        unassigned, Resources.none())) {\n       Resource wQassigned \u003d Resource.newInstance(0, 0);\n       // we compute normalizedGuarantees capacity based on currently active\n       // queues\n-      resetCapacity(rc, unassigned, orderedByNeed, ignoreGuarantee);\n+      resetCapacity(unassigned, orderedByNeed, ignoreGuarantee);\n \n       // For each underserved queue (or set of queues if multiple are equally\n       // underserved), offer its share of the unassigned resources based on its\n       // normalized guarantee. After the offer, if the queue is not satisfied,\n       // place it back in the ordered list of queues, recalculating its place\n       // in the order of most under-guaranteed to most over-guaranteed. In this\n       // way, the most underserved queue(s) are always given resources first.\n-      Collection\u003cTempQueuePerPartition\u003e underserved \u003d\n-          getMostUnderservedQueues(orderedByNeed, tqComparator);\n+      Collection\u003cTempQueuePerPartition\u003e underserved \u003d getMostUnderservedQueues(\n+          orderedByNeed, tqComparator);\n       for (Iterator\u003cTempQueuePerPartition\u003e i \u003d underserved.iterator(); i\n           .hasNext();) {\n         TempQueuePerPartition sub \u003d i.next();\n-        Resource wQavail \u003d Resources.multiplyAndNormalizeUp(rc,\n-            unassigned, sub.normalizedGuarantee, Resource.newInstance(1, 1));\n-        Resource wQidle \u003d sub.offer(wQavail, rc, tot_guarant,\n+        Resource wQavail \u003d Resources.multiplyAndNormalizeUp(rc, unassigned,\n+            sub.normalizedGuarantee, Resource.newInstance(1, 1));\n+        Resource wQidle \u003d sub.offer(wQavail, rc, totGuarant,\n             isReservedPreemptionCandidatesSelector);\n         Resource wQdone \u003d Resources.subtract(wQavail, wQidle);\n \n-        if (Resources.greaterThan(rc, tot_guarant,\n-            wQdone, Resources.none())) {\n+        if (Resources.greaterThan(rc, totGuarant, wQdone, Resources.none())) {\n           // The queue is still asking for more. Put it back in the priority\n           // queue, recalculating its order based on need.\n           orderedByNeed.add(sub);\n         }\n         Resources.addTo(wQassigned, wQdone);\n       }\n       Resources.subtractFrom(unassigned, wQassigned);\n     }\n+\n+    // Sometimes its possible that, all queues are properly served. So intra\n+    // queue preemption will not try for any preemption. How ever there are\n+    // chances that within a queue, there are some imbalances. Hence make sure\n+    // all queues are added to list.\n+    while (!orderedByNeed.isEmpty()) {\n+      TempQueuePerPartition q1 \u003d orderedByNeed.remove();\n+      context.addPartitionToUnderServedQueues(q1.queueName, q1.partition);\n+    }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  protected void computeFixpointAllocation(Resource totGuarant,\n      Collection\u003cTempQueuePerPartition\u003e qAlloc, Resource unassigned,\n      boolean ignoreGuarantee) {\n    // Prior to assigning the unused resources, process each queue as follows:\n    // If current \u003e guaranteed, idealAssigned \u003d guaranteed + untouchable extra\n    // Else idealAssigned \u003d current;\n    // Subtract idealAssigned resources from unassigned.\n    // If the queue has all of its needs met (that is, if\n    // idealAssigned \u003e\u003d current + pending), remove it from consideration.\n    // Sort queues from most under-guaranteed to most over-guaranteed.\n    TQComparator tqComparator \u003d new TQComparator(rc, totGuarant);\n    PriorityQueue\u003cTempQueuePerPartition\u003e orderedByNeed \u003d new PriorityQueue\u003c\u003e(10,\n        tqComparator);\n    for (Iterator\u003cTempQueuePerPartition\u003e i \u003d qAlloc.iterator(); i.hasNext();) {\n      TempQueuePerPartition q \u003d i.next();\n      Resource used \u003d q.getUsed();\n\n      if (Resources.greaterThan(rc, totGuarant, used, q.getGuaranteed())) {\n        q.idealAssigned \u003d Resources.add(q.getGuaranteed(), q.untouchableExtra);\n      } else {\n        q.idealAssigned \u003d Resources.clone(used);\n      }\n      Resources.subtractFrom(unassigned, q.idealAssigned);\n      // If idealAssigned \u003c (allocated + used + pending), q needs more\n      // resources, so\n      // add it to the list of underserved queues, ordered by need.\n      Resource curPlusPend \u003d Resources.add(q.getUsed(), q.pending);\n      if (Resources.lessThan(rc, totGuarant, q.idealAssigned, curPlusPend)) {\n        orderedByNeed.add(q);\n      }\n    }\n\n    // assign all cluster resources until no more demand, or no resources are\n    // left\n    while (!orderedByNeed.isEmpty() \u0026\u0026 Resources.greaterThan(rc, totGuarant,\n        unassigned, Resources.none())) {\n      Resource wQassigned \u003d Resource.newInstance(0, 0);\n      // we compute normalizedGuarantees capacity based on currently active\n      // queues\n      resetCapacity(unassigned, orderedByNeed, ignoreGuarantee);\n\n      // For each underserved queue (or set of queues if multiple are equally\n      // underserved), offer its share of the unassigned resources based on its\n      // normalized guarantee. After the offer, if the queue is not satisfied,\n      // place it back in the ordered list of queues, recalculating its place\n      // in the order of most under-guaranteed to most over-guaranteed. In this\n      // way, the most underserved queue(s) are always given resources first.\n      Collection\u003cTempQueuePerPartition\u003e underserved \u003d getMostUnderservedQueues(\n          orderedByNeed, tqComparator);\n      for (Iterator\u003cTempQueuePerPartition\u003e i \u003d underserved.iterator(); i\n          .hasNext();) {\n        TempQueuePerPartition sub \u003d i.next();\n        Resource wQavail \u003d Resources.multiplyAndNormalizeUp(rc, unassigned,\n            sub.normalizedGuarantee, Resource.newInstance(1, 1));\n        Resource wQidle \u003d sub.offer(wQavail, rc, totGuarant,\n            isReservedPreemptionCandidatesSelector);\n        Resource wQdone \u003d Resources.subtract(wQavail, wQidle);\n\n        if (Resources.greaterThan(rc, totGuarant, wQdone, Resources.none())) {\n          // The queue is still asking for more. Put it back in the priority\n          // queue, recalculating its order based on need.\n          orderedByNeed.add(sub);\n        }\n        Resources.addTo(wQassigned, wQdone);\n      }\n      Resources.subtractFrom(unassigned, wQassigned);\n    }\n\n    // Sometimes its possible that, all queues are properly served. So intra\n    // queue preemption will not try for any preemption. How ever there are\n    // chances that within a queue, there are some imbalances. Hence make sure\n    // all queues are added to list.\n    while (!orderedByNeed.isEmpty()) {\n      TempQueuePerPartition q1 \u003d orderedByNeed.remove();\n      context.addPartitionToUnderServedQueues(q1.queueName, q1.partition);\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/AbstractPreemptableResourceCalculator.java",
          "extendedDetails": {
            "oldValue": "[rc-ResourceCalculator, tot_guarant-Resource, qAlloc-Collection\u003cTempQueuePerPartition\u003e, unassigned-Resource, ignoreGuarantee-boolean]",
            "newValue": "[totGuarant-Resource, qAlloc-Collection\u003cTempQueuePerPartition\u003e, unassigned-Resource, ignoreGuarantee-boolean]"
          }
        }
      ]
    },
    "bb62e0592566b2fcae7136b30972aad2d3ac55b0": {
      "type": "Ybodychange",
      "commitMessage": "YARN-4390. Do surgical preemption based on reserved container in CapacityScheduler. Contributed by Wangda Tan\n",
      "commitDate": "05/05/16 12:56 PM",
      "commitName": "bb62e0592566b2fcae7136b30972aad2d3ac55b0",
      "commitAuthor": "Jian He",
      "commitDateOld": "30/03/16 12:43 PM",
      "commitNameOld": "60e4116bf1d00afed91010e57357fe54057e4e39",
      "commitAuthorOld": "Jian He",
      "daysBetweenCommits": 36.01,
      "commitsBetweenForRepo": 225,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,64 +1,69 @@\n   private void computeFixpointAllocation(ResourceCalculator rc,\n       Resource tot_guarant, Collection\u003cTempQueuePerPartition\u003e qAlloc,\n       Resource unassigned, boolean ignoreGuarantee) {\n     // Prior to assigning the unused resources, process each queue as follows:\n     // If current \u003e guaranteed, idealAssigned \u003d guaranteed + untouchable extra\n     // Else idealAssigned \u003d current;\n     // Subtract idealAssigned resources from unassigned.\n     // If the queue has all of its needs met (that is, if\n     // idealAssigned \u003e\u003d current + pending), remove it from consideration.\n     // Sort queues from most under-guaranteed to most over-guaranteed.\n     TQComparator tqComparator \u003d new TQComparator(rc, tot_guarant);\n     PriorityQueue\u003cTempQueuePerPartition\u003e orderedByNeed \u003d new PriorityQueue\u003c\u003e(10,\n         tqComparator);\n     for (Iterator\u003cTempQueuePerPartition\u003e i \u003d qAlloc.iterator(); i.hasNext();) {\n       TempQueuePerPartition q \u003d i.next();\n-      if (Resources.greaterThan(rc, tot_guarant, q.current, q.guaranteed)) {\n-        q.idealAssigned \u003d Resources.add(q.guaranteed, q.untouchableExtra);\n+      Resource used \u003d q.getUsed();\n+\n+      if (Resources.greaterThan(rc, tot_guarant, used,\n+          q.getGuaranteed())) {\n+        q.idealAssigned \u003d Resources.add(\n+            q.getGuaranteed(), q.untouchableExtra);\n       } else {\n-        q.idealAssigned \u003d Resources.clone(q.current);\n+        q.idealAssigned \u003d Resources.clone(used);\n       }\n       Resources.subtractFrom(unassigned, q.idealAssigned);\n-      // If idealAssigned \u003c (current + pending), q needs more resources, so\n+      // If idealAssigned \u003c (allocated + used + pending), q needs more resources, so\n       // add it to the list of underserved queues, ordered by need.\n-      Resource curPlusPend \u003d Resources.add(q.current, q.pending);\n+      Resource curPlusPend \u003d Resources.add(q.getUsed(), q.pending);\n       if (Resources.lessThan(rc, tot_guarant, q.idealAssigned, curPlusPend)) {\n         orderedByNeed.add(q);\n       }\n     }\n \n     //assign all cluster resources until no more demand, or no resources are left\n     while (!orderedByNeed.isEmpty()\n         \u0026\u0026 Resources.greaterThan(rc,tot_guarant, unassigned,Resources.none())) {\n       Resource wQassigned \u003d Resource.newInstance(0, 0);\n       // we compute normalizedGuarantees capacity based on currently active\n       // queues\n       resetCapacity(rc, unassigned, orderedByNeed, ignoreGuarantee);\n \n       // For each underserved queue (or set of queues if multiple are equally\n       // underserved), offer its share of the unassigned resources based on its\n       // normalized guarantee. After the offer, if the queue is not satisfied,\n       // place it back in the ordered list of queues, recalculating its place\n       // in the order of most under-guaranteed to most over-guaranteed. In this\n       // way, the most underserved queue(s) are always given resources first.\n       Collection\u003cTempQueuePerPartition\u003e underserved \u003d\n           getMostUnderservedQueues(orderedByNeed, tqComparator);\n       for (Iterator\u003cTempQueuePerPartition\u003e i \u003d underserved.iterator(); i\n           .hasNext();) {\n         TempQueuePerPartition sub \u003d i.next();\n         Resource wQavail \u003d Resources.multiplyAndNormalizeUp(rc,\n             unassigned, sub.normalizedGuarantee, Resource.newInstance(1, 1));\n-        Resource wQidle \u003d sub.offer(wQavail, rc, tot_guarant);\n+        Resource wQidle \u003d sub.offer(wQavail, rc, tot_guarant,\n+            isReservedPreemptionCandidatesSelector);\n         Resource wQdone \u003d Resources.subtract(wQavail, wQidle);\n \n         if (Resources.greaterThan(rc, tot_guarant,\n             wQdone, Resources.none())) {\n           // The queue is still asking for more. Put it back in the priority\n           // queue, recalculating its order based on need.\n           orderedByNeed.add(sub);\n         }\n         Resources.addTo(wQassigned, wQdone);\n       }\n       Resources.subtractFrom(unassigned, wQassigned);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void computeFixpointAllocation(ResourceCalculator rc,\n      Resource tot_guarant, Collection\u003cTempQueuePerPartition\u003e qAlloc,\n      Resource unassigned, boolean ignoreGuarantee) {\n    // Prior to assigning the unused resources, process each queue as follows:\n    // If current \u003e guaranteed, idealAssigned \u003d guaranteed + untouchable extra\n    // Else idealAssigned \u003d current;\n    // Subtract idealAssigned resources from unassigned.\n    // If the queue has all of its needs met (that is, if\n    // idealAssigned \u003e\u003d current + pending), remove it from consideration.\n    // Sort queues from most under-guaranteed to most over-guaranteed.\n    TQComparator tqComparator \u003d new TQComparator(rc, tot_guarant);\n    PriorityQueue\u003cTempQueuePerPartition\u003e orderedByNeed \u003d new PriorityQueue\u003c\u003e(10,\n        tqComparator);\n    for (Iterator\u003cTempQueuePerPartition\u003e i \u003d qAlloc.iterator(); i.hasNext();) {\n      TempQueuePerPartition q \u003d i.next();\n      Resource used \u003d q.getUsed();\n\n      if (Resources.greaterThan(rc, tot_guarant, used,\n          q.getGuaranteed())) {\n        q.idealAssigned \u003d Resources.add(\n            q.getGuaranteed(), q.untouchableExtra);\n      } else {\n        q.idealAssigned \u003d Resources.clone(used);\n      }\n      Resources.subtractFrom(unassigned, q.idealAssigned);\n      // If idealAssigned \u003c (allocated + used + pending), q needs more resources, so\n      // add it to the list of underserved queues, ordered by need.\n      Resource curPlusPend \u003d Resources.add(q.getUsed(), q.pending);\n      if (Resources.lessThan(rc, tot_guarant, q.idealAssigned, curPlusPend)) {\n        orderedByNeed.add(q);\n      }\n    }\n\n    //assign all cluster resources until no more demand, or no resources are left\n    while (!orderedByNeed.isEmpty()\n        \u0026\u0026 Resources.greaterThan(rc,tot_guarant, unassigned,Resources.none())) {\n      Resource wQassigned \u003d Resource.newInstance(0, 0);\n      // we compute normalizedGuarantees capacity based on currently active\n      // queues\n      resetCapacity(rc, unassigned, orderedByNeed, ignoreGuarantee);\n\n      // For each underserved queue (or set of queues if multiple are equally\n      // underserved), offer its share of the unassigned resources based on its\n      // normalized guarantee. After the offer, if the queue is not satisfied,\n      // place it back in the ordered list of queues, recalculating its place\n      // in the order of most under-guaranteed to most over-guaranteed. In this\n      // way, the most underserved queue(s) are always given resources first.\n      Collection\u003cTempQueuePerPartition\u003e underserved \u003d\n          getMostUnderservedQueues(orderedByNeed, tqComparator);\n      for (Iterator\u003cTempQueuePerPartition\u003e i \u003d underserved.iterator(); i\n          .hasNext();) {\n        TempQueuePerPartition sub \u003d i.next();\n        Resource wQavail \u003d Resources.multiplyAndNormalizeUp(rc,\n            unassigned, sub.normalizedGuarantee, Resource.newInstance(1, 1));\n        Resource wQidle \u003d sub.offer(wQavail, rc, tot_guarant,\n            isReservedPreemptionCandidatesSelector);\n        Resource wQdone \u003d Resources.subtract(wQavail, wQidle);\n\n        if (Resources.greaterThan(rc, tot_guarant,\n            wQdone, Resources.none())) {\n          // The queue is still asking for more. Put it back in the priority\n          // queue, recalculating its order based on need.\n          orderedByNeed.add(sub);\n        }\n        Resources.addTo(wQassigned, wQdone);\n      }\n      Resources.subtractFrom(unassigned, wQassigned);\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/PreemptableResourceCalculator.java",
      "extendedDetails": {}
    },
    "60e4116bf1d00afed91010e57357fe54057e4e39": {
      "type": "Ymovefromfile",
      "commitMessage": "YARN-4822. Refactor existing Preemption Policy of CS for easier adding new approach to select preemption candidates. Contributed by Wangda Tan\n",
      "commitDate": "30/03/16 12:43 PM",
      "commitName": "60e4116bf1d00afed91010e57357fe54057e4e39",
      "commitAuthor": "Jian He",
      "commitDateOld": "29/03/16 11:22 PM",
      "commitNameOld": "09d63d5a192b5d6b172f94ff6c94da348fd49ea6",
      "commitAuthorOld": "Vinayakumar B",
      "daysBetweenCommits": 0.56,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,64 +1,64 @@\n   private void computeFixpointAllocation(ResourceCalculator rc,\n       Resource tot_guarant, Collection\u003cTempQueuePerPartition\u003e qAlloc,\n       Resource unassigned, boolean ignoreGuarantee) {\n     // Prior to assigning the unused resources, process each queue as follows:\n     // If current \u003e guaranteed, idealAssigned \u003d guaranteed + untouchable extra\n     // Else idealAssigned \u003d current;\n     // Subtract idealAssigned resources from unassigned.\n-    // If the queue has all of its needs met (that is, if \n+    // If the queue has all of its needs met (that is, if\n     // idealAssigned \u003e\u003d current + pending), remove it from consideration.\n     // Sort queues from most under-guaranteed to most over-guaranteed.\n     TQComparator tqComparator \u003d new TQComparator(rc, tot_guarant);\n     PriorityQueue\u003cTempQueuePerPartition\u003e orderedByNeed \u003d new PriorityQueue\u003c\u003e(10,\n         tqComparator);\n     for (Iterator\u003cTempQueuePerPartition\u003e i \u003d qAlloc.iterator(); i.hasNext();) {\n       TempQueuePerPartition q \u003d i.next();\n       if (Resources.greaterThan(rc, tot_guarant, q.current, q.guaranteed)) {\n         q.idealAssigned \u003d Resources.add(q.guaranteed, q.untouchableExtra);\n       } else {\n         q.idealAssigned \u003d Resources.clone(q.current);\n       }\n       Resources.subtractFrom(unassigned, q.idealAssigned);\n       // If idealAssigned \u003c (current + pending), q needs more resources, so\n       // add it to the list of underserved queues, ordered by need.\n       Resource curPlusPend \u003d Resources.add(q.current, q.pending);\n       if (Resources.lessThan(rc, tot_guarant, q.idealAssigned, curPlusPend)) {\n         orderedByNeed.add(q);\n       }\n     }\n \n     //assign all cluster resources until no more demand, or no resources are left\n     while (!orderedByNeed.isEmpty()\n-       \u0026\u0026 Resources.greaterThan(rc,tot_guarant, unassigned,Resources.none())) {\n+        \u0026\u0026 Resources.greaterThan(rc,tot_guarant, unassigned,Resources.none())) {\n       Resource wQassigned \u003d Resource.newInstance(0, 0);\n       // we compute normalizedGuarantees capacity based on currently active\n       // queues\n       resetCapacity(rc, unassigned, orderedByNeed, ignoreGuarantee);\n \n       // For each underserved queue (or set of queues if multiple are equally\n       // underserved), offer its share of the unassigned resources based on its\n       // normalized guarantee. After the offer, if the queue is not satisfied,\n       // place it back in the ordered list of queues, recalculating its place\n       // in the order of most under-guaranteed to most over-guaranteed. In this\n       // way, the most underserved queue(s) are always given resources first.\n       Collection\u003cTempQueuePerPartition\u003e underserved \u003d\n           getMostUnderservedQueues(orderedByNeed, tqComparator);\n       for (Iterator\u003cTempQueuePerPartition\u003e i \u003d underserved.iterator(); i\n           .hasNext();) {\n         TempQueuePerPartition sub \u003d i.next();\n         Resource wQavail \u003d Resources.multiplyAndNormalizeUp(rc,\n             unassigned, sub.normalizedGuarantee, Resource.newInstance(1, 1));\n         Resource wQidle \u003d sub.offer(wQavail, rc, tot_guarant);\n         Resource wQdone \u003d Resources.subtract(wQavail, wQidle);\n \n         if (Resources.greaterThan(rc, tot_guarant,\n-              wQdone, Resources.none())) {\n+            wQdone, Resources.none())) {\n           // The queue is still asking for more. Put it back in the priority\n           // queue, recalculating its order based on need.\n           orderedByNeed.add(sub);\n         }\n         Resources.addTo(wQassigned, wQdone);\n       }\n       Resources.subtractFrom(unassigned, wQassigned);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void computeFixpointAllocation(ResourceCalculator rc,\n      Resource tot_guarant, Collection\u003cTempQueuePerPartition\u003e qAlloc,\n      Resource unassigned, boolean ignoreGuarantee) {\n    // Prior to assigning the unused resources, process each queue as follows:\n    // If current \u003e guaranteed, idealAssigned \u003d guaranteed + untouchable extra\n    // Else idealAssigned \u003d current;\n    // Subtract idealAssigned resources from unassigned.\n    // If the queue has all of its needs met (that is, if\n    // idealAssigned \u003e\u003d current + pending), remove it from consideration.\n    // Sort queues from most under-guaranteed to most over-guaranteed.\n    TQComparator tqComparator \u003d new TQComparator(rc, tot_guarant);\n    PriorityQueue\u003cTempQueuePerPartition\u003e orderedByNeed \u003d new PriorityQueue\u003c\u003e(10,\n        tqComparator);\n    for (Iterator\u003cTempQueuePerPartition\u003e i \u003d qAlloc.iterator(); i.hasNext();) {\n      TempQueuePerPartition q \u003d i.next();\n      if (Resources.greaterThan(rc, tot_guarant, q.current, q.guaranteed)) {\n        q.idealAssigned \u003d Resources.add(q.guaranteed, q.untouchableExtra);\n      } else {\n        q.idealAssigned \u003d Resources.clone(q.current);\n      }\n      Resources.subtractFrom(unassigned, q.idealAssigned);\n      // If idealAssigned \u003c (current + pending), q needs more resources, so\n      // add it to the list of underserved queues, ordered by need.\n      Resource curPlusPend \u003d Resources.add(q.current, q.pending);\n      if (Resources.lessThan(rc, tot_guarant, q.idealAssigned, curPlusPend)) {\n        orderedByNeed.add(q);\n      }\n    }\n\n    //assign all cluster resources until no more demand, or no resources are left\n    while (!orderedByNeed.isEmpty()\n        \u0026\u0026 Resources.greaterThan(rc,tot_guarant, unassigned,Resources.none())) {\n      Resource wQassigned \u003d Resource.newInstance(0, 0);\n      // we compute normalizedGuarantees capacity based on currently active\n      // queues\n      resetCapacity(rc, unassigned, orderedByNeed, ignoreGuarantee);\n\n      // For each underserved queue (or set of queues if multiple are equally\n      // underserved), offer its share of the unassigned resources based on its\n      // normalized guarantee. After the offer, if the queue is not satisfied,\n      // place it back in the ordered list of queues, recalculating its place\n      // in the order of most under-guaranteed to most over-guaranteed. In this\n      // way, the most underserved queue(s) are always given resources first.\n      Collection\u003cTempQueuePerPartition\u003e underserved \u003d\n          getMostUnderservedQueues(orderedByNeed, tqComparator);\n      for (Iterator\u003cTempQueuePerPartition\u003e i \u003d underserved.iterator(); i\n          .hasNext();) {\n        TempQueuePerPartition sub \u003d i.next();\n        Resource wQavail \u003d Resources.multiplyAndNormalizeUp(rc,\n            unassigned, sub.normalizedGuarantee, Resource.newInstance(1, 1));\n        Resource wQidle \u003d sub.offer(wQavail, rc, tot_guarant);\n        Resource wQdone \u003d Resources.subtract(wQavail, wQidle);\n\n        if (Resources.greaterThan(rc, tot_guarant,\n            wQdone, Resources.none())) {\n          // The queue is still asking for more. Put it back in the priority\n          // queue, recalculating its order based on need.\n          orderedByNeed.add(sub);\n        }\n        Resources.addTo(wQassigned, wQdone);\n      }\n      Resources.subtractFrom(unassigned, wQassigned);\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/PreemptableResourceCalculator.java",
      "extendedDetails": {
        "oldPath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/ProportionalCapacityPreemptionPolicy.java",
        "newPath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/PreemptableResourceCalculator.java",
        "oldMethodName": "computeFixpointAllocation",
        "newMethodName": "computeFixpointAllocation"
      }
    },
    "ae14e5d07f1b6702a5160637438028bb03d9387e": {
      "type": "Ybodychange",
      "commitMessage": "YARN-4108. CapacityScheduler: Improve preemption to only kill containers that would satisfy the incoming request. (Wangda Tan)\n\n(cherry picked from commit 7e8c9beb4156dcaeb3a11e60aaa06d2370626913)\n",
      "commitDate": "16/03/16 5:02 PM",
      "commitName": "ae14e5d07f1b6702a5160637438028bb03d9387e",
      "commitAuthor": "Wangda Tan",
      "commitDateOld": "16/03/16 5:02 PM",
      "commitNameOld": "fa7a43529d529f0006c8033c2003f15b9b93f103",
      "commitAuthorOld": "Wangda Tan",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,64 +1,64 @@\n   private void computeFixpointAllocation(ResourceCalculator rc,\n       Resource tot_guarant, Collection\u003cTempQueuePerPartition\u003e qAlloc,\n       Resource unassigned, boolean ignoreGuarantee) {\n     // Prior to assigning the unused resources, process each queue as follows:\n     // If current \u003e guaranteed, idealAssigned \u003d guaranteed + untouchable extra\n     // Else idealAssigned \u003d current;\n     // Subtract idealAssigned resources from unassigned.\n     // If the queue has all of its needs met (that is, if \n     // idealAssigned \u003e\u003d current + pending), remove it from consideration.\n     // Sort queues from most under-guaranteed to most over-guaranteed.\n     TQComparator tqComparator \u003d new TQComparator(rc, tot_guarant);\n-    PriorityQueue\u003cTempQueuePerPartition\u003e orderedByNeed \u003d\n-        new PriorityQueue\u003cTempQueuePerPartition\u003e(10, tqComparator);\n+    PriorityQueue\u003cTempQueuePerPartition\u003e orderedByNeed \u003d new PriorityQueue\u003c\u003e(10,\n+        tqComparator);\n     for (Iterator\u003cTempQueuePerPartition\u003e i \u003d qAlloc.iterator(); i.hasNext();) {\n       TempQueuePerPartition q \u003d i.next();\n       if (Resources.greaterThan(rc, tot_guarant, q.current, q.guaranteed)) {\n         q.idealAssigned \u003d Resources.add(q.guaranteed, q.untouchableExtra);\n       } else {\n         q.idealAssigned \u003d Resources.clone(q.current);\n       }\n       Resources.subtractFrom(unassigned, q.idealAssigned);\n       // If idealAssigned \u003c (current + pending), q needs more resources, so\n       // add it to the list of underserved queues, ordered by need.\n       Resource curPlusPend \u003d Resources.add(q.current, q.pending);\n       if (Resources.lessThan(rc, tot_guarant, q.idealAssigned, curPlusPend)) {\n         orderedByNeed.add(q);\n       }\n     }\n \n     //assign all cluster resources until no more demand, or no resources are left\n     while (!orderedByNeed.isEmpty()\n        \u0026\u0026 Resources.greaterThan(rc,tot_guarant, unassigned,Resources.none())) {\n       Resource wQassigned \u003d Resource.newInstance(0, 0);\n       // we compute normalizedGuarantees capacity based on currently active\n       // queues\n       resetCapacity(rc, unassigned, orderedByNeed, ignoreGuarantee);\n \n       // For each underserved queue (or set of queues if multiple are equally\n       // underserved), offer its share of the unassigned resources based on its\n       // normalized guarantee. After the offer, if the queue is not satisfied,\n       // place it back in the ordered list of queues, recalculating its place\n       // in the order of most under-guaranteed to most over-guaranteed. In this\n       // way, the most underserved queue(s) are always given resources first.\n       Collection\u003cTempQueuePerPartition\u003e underserved \u003d\n           getMostUnderservedQueues(orderedByNeed, tqComparator);\n       for (Iterator\u003cTempQueuePerPartition\u003e i \u003d underserved.iterator(); i\n           .hasNext();) {\n         TempQueuePerPartition sub \u003d i.next();\n         Resource wQavail \u003d Resources.multiplyAndNormalizeUp(rc,\n             unassigned, sub.normalizedGuarantee, Resource.newInstance(1, 1));\n         Resource wQidle \u003d sub.offer(wQavail, rc, tot_guarant);\n         Resource wQdone \u003d Resources.subtract(wQavail, wQidle);\n \n         if (Resources.greaterThan(rc, tot_guarant,\n               wQdone, Resources.none())) {\n           // The queue is still asking for more. Put it back in the priority\n           // queue, recalculating its order based on need.\n           orderedByNeed.add(sub);\n         }\n         Resources.addTo(wQassigned, wQdone);\n       }\n       Resources.subtractFrom(unassigned, wQassigned);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void computeFixpointAllocation(ResourceCalculator rc,\n      Resource tot_guarant, Collection\u003cTempQueuePerPartition\u003e qAlloc,\n      Resource unassigned, boolean ignoreGuarantee) {\n    // Prior to assigning the unused resources, process each queue as follows:\n    // If current \u003e guaranteed, idealAssigned \u003d guaranteed + untouchable extra\n    // Else idealAssigned \u003d current;\n    // Subtract idealAssigned resources from unassigned.\n    // If the queue has all of its needs met (that is, if \n    // idealAssigned \u003e\u003d current + pending), remove it from consideration.\n    // Sort queues from most under-guaranteed to most over-guaranteed.\n    TQComparator tqComparator \u003d new TQComparator(rc, tot_guarant);\n    PriorityQueue\u003cTempQueuePerPartition\u003e orderedByNeed \u003d new PriorityQueue\u003c\u003e(10,\n        tqComparator);\n    for (Iterator\u003cTempQueuePerPartition\u003e i \u003d qAlloc.iterator(); i.hasNext();) {\n      TempQueuePerPartition q \u003d i.next();\n      if (Resources.greaterThan(rc, tot_guarant, q.current, q.guaranteed)) {\n        q.idealAssigned \u003d Resources.add(q.guaranteed, q.untouchableExtra);\n      } else {\n        q.idealAssigned \u003d Resources.clone(q.current);\n      }\n      Resources.subtractFrom(unassigned, q.idealAssigned);\n      // If idealAssigned \u003c (current + pending), q needs more resources, so\n      // add it to the list of underserved queues, ordered by need.\n      Resource curPlusPend \u003d Resources.add(q.current, q.pending);\n      if (Resources.lessThan(rc, tot_guarant, q.idealAssigned, curPlusPend)) {\n        orderedByNeed.add(q);\n      }\n    }\n\n    //assign all cluster resources until no more demand, or no resources are left\n    while (!orderedByNeed.isEmpty()\n       \u0026\u0026 Resources.greaterThan(rc,tot_guarant, unassigned,Resources.none())) {\n      Resource wQassigned \u003d Resource.newInstance(0, 0);\n      // we compute normalizedGuarantees capacity based on currently active\n      // queues\n      resetCapacity(rc, unassigned, orderedByNeed, ignoreGuarantee);\n\n      // For each underserved queue (or set of queues if multiple are equally\n      // underserved), offer its share of the unassigned resources based on its\n      // normalized guarantee. After the offer, if the queue is not satisfied,\n      // place it back in the ordered list of queues, recalculating its place\n      // in the order of most under-guaranteed to most over-guaranteed. In this\n      // way, the most underserved queue(s) are always given resources first.\n      Collection\u003cTempQueuePerPartition\u003e underserved \u003d\n          getMostUnderservedQueues(orderedByNeed, tqComparator);\n      for (Iterator\u003cTempQueuePerPartition\u003e i \u003d underserved.iterator(); i\n          .hasNext();) {\n        TempQueuePerPartition sub \u003d i.next();\n        Resource wQavail \u003d Resources.multiplyAndNormalizeUp(rc,\n            unassigned, sub.normalizedGuarantee, Resource.newInstance(1, 1));\n        Resource wQidle \u003d sub.offer(wQavail, rc, tot_guarant);\n        Resource wQdone \u003d Resources.subtract(wQavail, wQidle);\n\n        if (Resources.greaterThan(rc, tot_guarant,\n              wQdone, Resources.none())) {\n          // The queue is still asking for more. Put it back in the priority\n          // queue, recalculating its order based on need.\n          orderedByNeed.add(sub);\n        }\n        Resources.addTo(wQassigned, wQdone);\n      }\n      Resources.subtractFrom(unassigned, wQassigned);\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/ProportionalCapacityPreemptionPolicy.java",
      "extendedDetails": {}
    },
    "fa7a43529d529f0006c8033c2003f15b9b93f103": {
      "type": "Ybodychange",
      "commitMessage": "Revert \"CapacityScheduler: Improve preemption to only kill containers that would satisfy the incoming request. (Wangda Tan)\"\n\nThis reverts commit 7e8c9beb4156dcaeb3a11e60aaa06d2370626913.\n",
      "commitDate": "16/03/16 5:02 PM",
      "commitName": "fa7a43529d529f0006c8033c2003f15b9b93f103",
      "commitAuthor": "Wangda Tan",
      "commitDateOld": "16/03/16 4:59 PM",
      "commitNameOld": "7e8c9beb4156dcaeb3a11e60aaa06d2370626913",
      "commitAuthorOld": "Wangda Tan",
      "daysBetweenCommits": 0.0,
      "commitsBetweenForRepo": 1,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,64 +1,64 @@\n   private void computeFixpointAllocation(ResourceCalculator rc,\n       Resource tot_guarant, Collection\u003cTempQueuePerPartition\u003e qAlloc,\n       Resource unassigned, boolean ignoreGuarantee) {\n     // Prior to assigning the unused resources, process each queue as follows:\n     // If current \u003e guaranteed, idealAssigned \u003d guaranteed + untouchable extra\n     // Else idealAssigned \u003d current;\n     // Subtract idealAssigned resources from unassigned.\n     // If the queue has all of its needs met (that is, if \n     // idealAssigned \u003e\u003d current + pending), remove it from consideration.\n     // Sort queues from most under-guaranteed to most over-guaranteed.\n     TQComparator tqComparator \u003d new TQComparator(rc, tot_guarant);\n-    PriorityQueue\u003cTempQueuePerPartition\u003e orderedByNeed \u003d new PriorityQueue\u003c\u003e(10,\n-        tqComparator);\n+    PriorityQueue\u003cTempQueuePerPartition\u003e orderedByNeed \u003d\n+        new PriorityQueue\u003cTempQueuePerPartition\u003e(10, tqComparator);\n     for (Iterator\u003cTempQueuePerPartition\u003e i \u003d qAlloc.iterator(); i.hasNext();) {\n       TempQueuePerPartition q \u003d i.next();\n       if (Resources.greaterThan(rc, tot_guarant, q.current, q.guaranteed)) {\n         q.idealAssigned \u003d Resources.add(q.guaranteed, q.untouchableExtra);\n       } else {\n         q.idealAssigned \u003d Resources.clone(q.current);\n       }\n       Resources.subtractFrom(unassigned, q.idealAssigned);\n       // If idealAssigned \u003c (current + pending), q needs more resources, so\n       // add it to the list of underserved queues, ordered by need.\n       Resource curPlusPend \u003d Resources.add(q.current, q.pending);\n       if (Resources.lessThan(rc, tot_guarant, q.idealAssigned, curPlusPend)) {\n         orderedByNeed.add(q);\n       }\n     }\n \n     //assign all cluster resources until no more demand, or no resources are left\n     while (!orderedByNeed.isEmpty()\n        \u0026\u0026 Resources.greaterThan(rc,tot_guarant, unassigned,Resources.none())) {\n       Resource wQassigned \u003d Resource.newInstance(0, 0);\n       // we compute normalizedGuarantees capacity based on currently active\n       // queues\n       resetCapacity(rc, unassigned, orderedByNeed, ignoreGuarantee);\n \n       // For each underserved queue (or set of queues if multiple are equally\n       // underserved), offer its share of the unassigned resources based on its\n       // normalized guarantee. After the offer, if the queue is not satisfied,\n       // place it back in the ordered list of queues, recalculating its place\n       // in the order of most under-guaranteed to most over-guaranteed. In this\n       // way, the most underserved queue(s) are always given resources first.\n       Collection\u003cTempQueuePerPartition\u003e underserved \u003d\n           getMostUnderservedQueues(orderedByNeed, tqComparator);\n       for (Iterator\u003cTempQueuePerPartition\u003e i \u003d underserved.iterator(); i\n           .hasNext();) {\n         TempQueuePerPartition sub \u003d i.next();\n         Resource wQavail \u003d Resources.multiplyAndNormalizeUp(rc,\n             unassigned, sub.normalizedGuarantee, Resource.newInstance(1, 1));\n         Resource wQidle \u003d sub.offer(wQavail, rc, tot_guarant);\n         Resource wQdone \u003d Resources.subtract(wQavail, wQidle);\n \n         if (Resources.greaterThan(rc, tot_guarant,\n               wQdone, Resources.none())) {\n           // The queue is still asking for more. Put it back in the priority\n           // queue, recalculating its order based on need.\n           orderedByNeed.add(sub);\n         }\n         Resources.addTo(wQassigned, wQdone);\n       }\n       Resources.subtractFrom(unassigned, wQassigned);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void computeFixpointAllocation(ResourceCalculator rc,\n      Resource tot_guarant, Collection\u003cTempQueuePerPartition\u003e qAlloc,\n      Resource unassigned, boolean ignoreGuarantee) {\n    // Prior to assigning the unused resources, process each queue as follows:\n    // If current \u003e guaranteed, idealAssigned \u003d guaranteed + untouchable extra\n    // Else idealAssigned \u003d current;\n    // Subtract idealAssigned resources from unassigned.\n    // If the queue has all of its needs met (that is, if \n    // idealAssigned \u003e\u003d current + pending), remove it from consideration.\n    // Sort queues from most under-guaranteed to most over-guaranteed.\n    TQComparator tqComparator \u003d new TQComparator(rc, tot_guarant);\n    PriorityQueue\u003cTempQueuePerPartition\u003e orderedByNeed \u003d\n        new PriorityQueue\u003cTempQueuePerPartition\u003e(10, tqComparator);\n    for (Iterator\u003cTempQueuePerPartition\u003e i \u003d qAlloc.iterator(); i.hasNext();) {\n      TempQueuePerPartition q \u003d i.next();\n      if (Resources.greaterThan(rc, tot_guarant, q.current, q.guaranteed)) {\n        q.idealAssigned \u003d Resources.add(q.guaranteed, q.untouchableExtra);\n      } else {\n        q.idealAssigned \u003d Resources.clone(q.current);\n      }\n      Resources.subtractFrom(unassigned, q.idealAssigned);\n      // If idealAssigned \u003c (current + pending), q needs more resources, so\n      // add it to the list of underserved queues, ordered by need.\n      Resource curPlusPend \u003d Resources.add(q.current, q.pending);\n      if (Resources.lessThan(rc, tot_guarant, q.idealAssigned, curPlusPend)) {\n        orderedByNeed.add(q);\n      }\n    }\n\n    //assign all cluster resources until no more demand, or no resources are left\n    while (!orderedByNeed.isEmpty()\n       \u0026\u0026 Resources.greaterThan(rc,tot_guarant, unassigned,Resources.none())) {\n      Resource wQassigned \u003d Resource.newInstance(0, 0);\n      // we compute normalizedGuarantees capacity based on currently active\n      // queues\n      resetCapacity(rc, unassigned, orderedByNeed, ignoreGuarantee);\n\n      // For each underserved queue (or set of queues if multiple are equally\n      // underserved), offer its share of the unassigned resources based on its\n      // normalized guarantee. After the offer, if the queue is not satisfied,\n      // place it back in the ordered list of queues, recalculating its place\n      // in the order of most under-guaranteed to most over-guaranteed. In this\n      // way, the most underserved queue(s) are always given resources first.\n      Collection\u003cTempQueuePerPartition\u003e underserved \u003d\n          getMostUnderservedQueues(orderedByNeed, tqComparator);\n      for (Iterator\u003cTempQueuePerPartition\u003e i \u003d underserved.iterator(); i\n          .hasNext();) {\n        TempQueuePerPartition sub \u003d i.next();\n        Resource wQavail \u003d Resources.multiplyAndNormalizeUp(rc,\n            unassigned, sub.normalizedGuarantee, Resource.newInstance(1, 1));\n        Resource wQidle \u003d sub.offer(wQavail, rc, tot_guarant);\n        Resource wQdone \u003d Resources.subtract(wQavail, wQidle);\n\n        if (Resources.greaterThan(rc, tot_guarant,\n              wQdone, Resources.none())) {\n          // The queue is still asking for more. Put it back in the priority\n          // queue, recalculating its order based on need.\n          orderedByNeed.add(sub);\n        }\n        Resources.addTo(wQassigned, wQdone);\n      }\n      Resources.subtractFrom(unassigned, wQassigned);\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/ProportionalCapacityPreemptionPolicy.java",
      "extendedDetails": {}
    },
    "7e8c9beb4156dcaeb3a11e60aaa06d2370626913": {
      "type": "Ybodychange",
      "commitMessage": "CapacityScheduler: Improve preemption to only kill containers that would satisfy the incoming request. (Wangda Tan)\n",
      "commitDate": "16/03/16 4:59 PM",
      "commitName": "7e8c9beb4156dcaeb3a11e60aaa06d2370626913",
      "commitAuthor": "Wangda Tan",
      "commitDateOld": "18/01/16 5:30 PM",
      "commitNameOld": "a44ce3f14fd940601f984fbf7980aa6fdc8f23b7",
      "commitAuthorOld": "Wangda Tan",
      "daysBetweenCommits": 57.94,
      "commitsBetweenForRepo": 394,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,64 +1,64 @@\n   private void computeFixpointAllocation(ResourceCalculator rc,\n       Resource tot_guarant, Collection\u003cTempQueuePerPartition\u003e qAlloc,\n       Resource unassigned, boolean ignoreGuarantee) {\n     // Prior to assigning the unused resources, process each queue as follows:\n     // If current \u003e guaranteed, idealAssigned \u003d guaranteed + untouchable extra\n     // Else idealAssigned \u003d current;\n     // Subtract idealAssigned resources from unassigned.\n     // If the queue has all of its needs met (that is, if \n     // idealAssigned \u003e\u003d current + pending), remove it from consideration.\n     // Sort queues from most under-guaranteed to most over-guaranteed.\n     TQComparator tqComparator \u003d new TQComparator(rc, tot_guarant);\n-    PriorityQueue\u003cTempQueuePerPartition\u003e orderedByNeed \u003d\n-        new PriorityQueue\u003cTempQueuePerPartition\u003e(10, tqComparator);\n+    PriorityQueue\u003cTempQueuePerPartition\u003e orderedByNeed \u003d new PriorityQueue\u003c\u003e(10,\n+        tqComparator);\n     for (Iterator\u003cTempQueuePerPartition\u003e i \u003d qAlloc.iterator(); i.hasNext();) {\n       TempQueuePerPartition q \u003d i.next();\n       if (Resources.greaterThan(rc, tot_guarant, q.current, q.guaranteed)) {\n         q.idealAssigned \u003d Resources.add(q.guaranteed, q.untouchableExtra);\n       } else {\n         q.idealAssigned \u003d Resources.clone(q.current);\n       }\n       Resources.subtractFrom(unassigned, q.idealAssigned);\n       // If idealAssigned \u003c (current + pending), q needs more resources, so\n       // add it to the list of underserved queues, ordered by need.\n       Resource curPlusPend \u003d Resources.add(q.current, q.pending);\n       if (Resources.lessThan(rc, tot_guarant, q.idealAssigned, curPlusPend)) {\n         orderedByNeed.add(q);\n       }\n     }\n \n     //assign all cluster resources until no more demand, or no resources are left\n     while (!orderedByNeed.isEmpty()\n        \u0026\u0026 Resources.greaterThan(rc,tot_guarant, unassigned,Resources.none())) {\n       Resource wQassigned \u003d Resource.newInstance(0, 0);\n       // we compute normalizedGuarantees capacity based on currently active\n       // queues\n       resetCapacity(rc, unassigned, orderedByNeed, ignoreGuarantee);\n \n       // For each underserved queue (or set of queues if multiple are equally\n       // underserved), offer its share of the unassigned resources based on its\n       // normalized guarantee. After the offer, if the queue is not satisfied,\n       // place it back in the ordered list of queues, recalculating its place\n       // in the order of most under-guaranteed to most over-guaranteed. In this\n       // way, the most underserved queue(s) are always given resources first.\n       Collection\u003cTempQueuePerPartition\u003e underserved \u003d\n           getMostUnderservedQueues(orderedByNeed, tqComparator);\n       for (Iterator\u003cTempQueuePerPartition\u003e i \u003d underserved.iterator(); i\n           .hasNext();) {\n         TempQueuePerPartition sub \u003d i.next();\n         Resource wQavail \u003d Resources.multiplyAndNormalizeUp(rc,\n             unassigned, sub.normalizedGuarantee, Resource.newInstance(1, 1));\n         Resource wQidle \u003d sub.offer(wQavail, rc, tot_guarant);\n         Resource wQdone \u003d Resources.subtract(wQavail, wQidle);\n \n         if (Resources.greaterThan(rc, tot_guarant,\n               wQdone, Resources.none())) {\n           // The queue is still asking for more. Put it back in the priority\n           // queue, recalculating its order based on need.\n           orderedByNeed.add(sub);\n         }\n         Resources.addTo(wQassigned, wQdone);\n       }\n       Resources.subtractFrom(unassigned, wQassigned);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void computeFixpointAllocation(ResourceCalculator rc,\n      Resource tot_guarant, Collection\u003cTempQueuePerPartition\u003e qAlloc,\n      Resource unassigned, boolean ignoreGuarantee) {\n    // Prior to assigning the unused resources, process each queue as follows:\n    // If current \u003e guaranteed, idealAssigned \u003d guaranteed + untouchable extra\n    // Else idealAssigned \u003d current;\n    // Subtract idealAssigned resources from unassigned.\n    // If the queue has all of its needs met (that is, if \n    // idealAssigned \u003e\u003d current + pending), remove it from consideration.\n    // Sort queues from most under-guaranteed to most over-guaranteed.\n    TQComparator tqComparator \u003d new TQComparator(rc, tot_guarant);\n    PriorityQueue\u003cTempQueuePerPartition\u003e orderedByNeed \u003d new PriorityQueue\u003c\u003e(10,\n        tqComparator);\n    for (Iterator\u003cTempQueuePerPartition\u003e i \u003d qAlloc.iterator(); i.hasNext();) {\n      TempQueuePerPartition q \u003d i.next();\n      if (Resources.greaterThan(rc, tot_guarant, q.current, q.guaranteed)) {\n        q.idealAssigned \u003d Resources.add(q.guaranteed, q.untouchableExtra);\n      } else {\n        q.idealAssigned \u003d Resources.clone(q.current);\n      }\n      Resources.subtractFrom(unassigned, q.idealAssigned);\n      // If idealAssigned \u003c (current + pending), q needs more resources, so\n      // add it to the list of underserved queues, ordered by need.\n      Resource curPlusPend \u003d Resources.add(q.current, q.pending);\n      if (Resources.lessThan(rc, tot_guarant, q.idealAssigned, curPlusPend)) {\n        orderedByNeed.add(q);\n      }\n    }\n\n    //assign all cluster resources until no more demand, or no resources are left\n    while (!orderedByNeed.isEmpty()\n       \u0026\u0026 Resources.greaterThan(rc,tot_guarant, unassigned,Resources.none())) {\n      Resource wQassigned \u003d Resource.newInstance(0, 0);\n      // we compute normalizedGuarantees capacity based on currently active\n      // queues\n      resetCapacity(rc, unassigned, orderedByNeed, ignoreGuarantee);\n\n      // For each underserved queue (or set of queues if multiple are equally\n      // underserved), offer its share of the unassigned resources based on its\n      // normalized guarantee. After the offer, if the queue is not satisfied,\n      // place it back in the ordered list of queues, recalculating its place\n      // in the order of most under-guaranteed to most over-guaranteed. In this\n      // way, the most underserved queue(s) are always given resources first.\n      Collection\u003cTempQueuePerPartition\u003e underserved \u003d\n          getMostUnderservedQueues(orderedByNeed, tqComparator);\n      for (Iterator\u003cTempQueuePerPartition\u003e i \u003d underserved.iterator(); i\n          .hasNext();) {\n        TempQueuePerPartition sub \u003d i.next();\n        Resource wQavail \u003d Resources.multiplyAndNormalizeUp(rc,\n            unassigned, sub.normalizedGuarantee, Resource.newInstance(1, 1));\n        Resource wQidle \u003d sub.offer(wQavail, rc, tot_guarant);\n        Resource wQdone \u003d Resources.subtract(wQavail, wQidle);\n\n        if (Resources.greaterThan(rc, tot_guarant,\n              wQdone, Resources.none())) {\n          // The queue is still asking for more. Put it back in the priority\n          // queue, recalculating its order based on need.\n          orderedByNeed.add(sub);\n        }\n        Resources.addTo(wQassigned, wQdone);\n      }\n      Resources.subtractFrom(unassigned, wQassigned);\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/ProportionalCapacityPreemptionPolicy.java",
      "extendedDetails": {}
    },
    "d497f6ea2be559aa31ed76f37ae949dbfabe2a51": {
      "type": "Ymultichange(Yparameterchange,Ybodychange)",
      "commitMessage": "YARN-2498. Respect labels in preemption policy of capacity scheduler for inter-queue preemption. Contributed by Wangda Tan\n",
      "commitDate": "24/04/15 5:03 PM",
      "commitName": "d497f6ea2be559aa31ed76f37ae949dbfabe2a51",
      "commitAuthor": "Jian He",
      "subchanges": [
        {
          "type": "Yparameterchange",
          "commitMessage": "YARN-2498. Respect labels in preemption policy of capacity scheduler for inter-queue preemption. Contributed by Wangda Tan\n",
          "commitDate": "24/04/15 5:03 PM",
          "commitName": "d497f6ea2be559aa31ed76f37ae949dbfabe2a51",
          "commitAuthor": "Jian He",
          "commitDateOld": "20/04/15 5:12 PM",
          "commitNameOld": "44872b76fcc0ddfbc7b0a4e54eef50fe8708e0f5",
          "commitAuthorOld": "Wangda Tan",
          "daysBetweenCommits": 3.99,
          "commitsBetweenForRepo": 60,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,63 +1,64 @@\n   private void computeFixpointAllocation(ResourceCalculator rc,\n-      Resource tot_guarant, Collection\u003cTempQueue\u003e qAlloc, Resource unassigned, \n-      boolean ignoreGuarantee) {\n+      Resource tot_guarant, Collection\u003cTempQueuePerPartition\u003e qAlloc,\n+      Resource unassigned, boolean ignoreGuarantee) {\n     // Prior to assigning the unused resources, process each queue as follows:\n     // If current \u003e guaranteed, idealAssigned \u003d guaranteed + untouchable extra\n     // Else idealAssigned \u003d current;\n     // Subtract idealAssigned resources from unassigned.\n     // If the queue has all of its needs met (that is, if \n     // idealAssigned \u003e\u003d current + pending), remove it from consideration.\n     // Sort queues from most under-guaranteed to most over-guaranteed.\n     TQComparator tqComparator \u003d new TQComparator(rc, tot_guarant);\n-    PriorityQueue\u003cTempQueue\u003e orderedByNeed \u003d\n-                                 new PriorityQueue\u003cTempQueue\u003e(10,tqComparator);\n-    for (Iterator\u003cTempQueue\u003e i \u003d qAlloc.iterator(); i.hasNext();) {\n-      TempQueue q \u003d i.next();\n+    PriorityQueue\u003cTempQueuePerPartition\u003e orderedByNeed \u003d\n+        new PriorityQueue\u003cTempQueuePerPartition\u003e(10, tqComparator);\n+    for (Iterator\u003cTempQueuePerPartition\u003e i \u003d qAlloc.iterator(); i.hasNext();) {\n+      TempQueuePerPartition q \u003d i.next();\n       if (Resources.greaterThan(rc, tot_guarant, q.current, q.guaranteed)) {\n         q.idealAssigned \u003d Resources.add(q.guaranteed, q.untouchableExtra);\n       } else {\n         q.idealAssigned \u003d Resources.clone(q.current);\n       }\n       Resources.subtractFrom(unassigned, q.idealAssigned);\n       // If idealAssigned \u003c (current + pending), q needs more resources, so\n       // add it to the list of underserved queues, ordered by need.\n       Resource curPlusPend \u003d Resources.add(q.current, q.pending);\n       if (Resources.lessThan(rc, tot_guarant, q.idealAssigned, curPlusPend)) {\n         orderedByNeed.add(q);\n       }\n     }\n \n     //assign all cluster resources until no more demand, or no resources are left\n     while (!orderedByNeed.isEmpty()\n        \u0026\u0026 Resources.greaterThan(rc,tot_guarant, unassigned,Resources.none())) {\n       Resource wQassigned \u003d Resource.newInstance(0, 0);\n       // we compute normalizedGuarantees capacity based on currently active\n       // queues\n       resetCapacity(rc, unassigned, orderedByNeed, ignoreGuarantee);\n \n       // For each underserved queue (or set of queues if multiple are equally\n       // underserved), offer its share of the unassigned resources based on its\n       // normalized guarantee. After the offer, if the queue is not satisfied,\n       // place it back in the ordered list of queues, recalculating its place\n       // in the order of most under-guaranteed to most over-guaranteed. In this\n       // way, the most underserved queue(s) are always given resources first.\n-      Collection\u003cTempQueue\u003e underserved \u003d\n+      Collection\u003cTempQueuePerPartition\u003e underserved \u003d\n           getMostUnderservedQueues(orderedByNeed, tqComparator);\n-      for (Iterator\u003cTempQueue\u003e i \u003d underserved.iterator(); i.hasNext();) {\n-        TempQueue sub \u003d i.next();\n+      for (Iterator\u003cTempQueuePerPartition\u003e i \u003d underserved.iterator(); i\n+          .hasNext();) {\n+        TempQueuePerPartition sub \u003d i.next();\n         Resource wQavail \u003d Resources.multiplyAndNormalizeUp(rc,\n             unassigned, sub.normalizedGuarantee, Resource.newInstance(1, 1));\n         Resource wQidle \u003d sub.offer(wQavail, rc, tot_guarant);\n         Resource wQdone \u003d Resources.subtract(wQavail, wQidle);\n \n         if (Resources.greaterThan(rc, tot_guarant,\n               wQdone, Resources.none())) {\n           // The queue is still asking for more. Put it back in the priority\n           // queue, recalculating its order based on need.\n           orderedByNeed.add(sub);\n         }\n         Resources.addTo(wQassigned, wQdone);\n       }\n       Resources.subtractFrom(unassigned, wQassigned);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void computeFixpointAllocation(ResourceCalculator rc,\n      Resource tot_guarant, Collection\u003cTempQueuePerPartition\u003e qAlloc,\n      Resource unassigned, boolean ignoreGuarantee) {\n    // Prior to assigning the unused resources, process each queue as follows:\n    // If current \u003e guaranteed, idealAssigned \u003d guaranteed + untouchable extra\n    // Else idealAssigned \u003d current;\n    // Subtract idealAssigned resources from unassigned.\n    // If the queue has all of its needs met (that is, if \n    // idealAssigned \u003e\u003d current + pending), remove it from consideration.\n    // Sort queues from most under-guaranteed to most over-guaranteed.\n    TQComparator tqComparator \u003d new TQComparator(rc, tot_guarant);\n    PriorityQueue\u003cTempQueuePerPartition\u003e orderedByNeed \u003d\n        new PriorityQueue\u003cTempQueuePerPartition\u003e(10, tqComparator);\n    for (Iterator\u003cTempQueuePerPartition\u003e i \u003d qAlloc.iterator(); i.hasNext();) {\n      TempQueuePerPartition q \u003d i.next();\n      if (Resources.greaterThan(rc, tot_guarant, q.current, q.guaranteed)) {\n        q.idealAssigned \u003d Resources.add(q.guaranteed, q.untouchableExtra);\n      } else {\n        q.idealAssigned \u003d Resources.clone(q.current);\n      }\n      Resources.subtractFrom(unassigned, q.idealAssigned);\n      // If idealAssigned \u003c (current + pending), q needs more resources, so\n      // add it to the list of underserved queues, ordered by need.\n      Resource curPlusPend \u003d Resources.add(q.current, q.pending);\n      if (Resources.lessThan(rc, tot_guarant, q.idealAssigned, curPlusPend)) {\n        orderedByNeed.add(q);\n      }\n    }\n\n    //assign all cluster resources until no more demand, or no resources are left\n    while (!orderedByNeed.isEmpty()\n       \u0026\u0026 Resources.greaterThan(rc,tot_guarant, unassigned,Resources.none())) {\n      Resource wQassigned \u003d Resource.newInstance(0, 0);\n      // we compute normalizedGuarantees capacity based on currently active\n      // queues\n      resetCapacity(rc, unassigned, orderedByNeed, ignoreGuarantee);\n\n      // For each underserved queue (or set of queues if multiple are equally\n      // underserved), offer its share of the unassigned resources based on its\n      // normalized guarantee. After the offer, if the queue is not satisfied,\n      // place it back in the ordered list of queues, recalculating its place\n      // in the order of most under-guaranteed to most over-guaranteed. In this\n      // way, the most underserved queue(s) are always given resources first.\n      Collection\u003cTempQueuePerPartition\u003e underserved \u003d\n          getMostUnderservedQueues(orderedByNeed, tqComparator);\n      for (Iterator\u003cTempQueuePerPartition\u003e i \u003d underserved.iterator(); i\n          .hasNext();) {\n        TempQueuePerPartition sub \u003d i.next();\n        Resource wQavail \u003d Resources.multiplyAndNormalizeUp(rc,\n            unassigned, sub.normalizedGuarantee, Resource.newInstance(1, 1));\n        Resource wQidle \u003d sub.offer(wQavail, rc, tot_guarant);\n        Resource wQdone \u003d Resources.subtract(wQavail, wQidle);\n\n        if (Resources.greaterThan(rc, tot_guarant,\n              wQdone, Resources.none())) {\n          // The queue is still asking for more. Put it back in the priority\n          // queue, recalculating its order based on need.\n          orderedByNeed.add(sub);\n        }\n        Resources.addTo(wQassigned, wQdone);\n      }\n      Resources.subtractFrom(unassigned, wQassigned);\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/ProportionalCapacityPreemptionPolicy.java",
          "extendedDetails": {
            "oldValue": "[rc-ResourceCalculator, tot_guarant-Resource, qAlloc-Collection\u003cTempQueue\u003e, unassigned-Resource, ignoreGuarantee-boolean]",
            "newValue": "[rc-ResourceCalculator, tot_guarant-Resource, qAlloc-Collection\u003cTempQueuePerPartition\u003e, unassigned-Resource, ignoreGuarantee-boolean]"
          }
        },
        {
          "type": "Ybodychange",
          "commitMessage": "YARN-2498. Respect labels in preemption policy of capacity scheduler for inter-queue preemption. Contributed by Wangda Tan\n",
          "commitDate": "24/04/15 5:03 PM",
          "commitName": "d497f6ea2be559aa31ed76f37ae949dbfabe2a51",
          "commitAuthor": "Jian He",
          "commitDateOld": "20/04/15 5:12 PM",
          "commitNameOld": "44872b76fcc0ddfbc7b0a4e54eef50fe8708e0f5",
          "commitAuthorOld": "Wangda Tan",
          "daysBetweenCommits": 3.99,
          "commitsBetweenForRepo": 60,
          "commitsBetweenForFile": 1,
          "diff": "@@ -1,63 +1,64 @@\n   private void computeFixpointAllocation(ResourceCalculator rc,\n-      Resource tot_guarant, Collection\u003cTempQueue\u003e qAlloc, Resource unassigned, \n-      boolean ignoreGuarantee) {\n+      Resource tot_guarant, Collection\u003cTempQueuePerPartition\u003e qAlloc,\n+      Resource unassigned, boolean ignoreGuarantee) {\n     // Prior to assigning the unused resources, process each queue as follows:\n     // If current \u003e guaranteed, idealAssigned \u003d guaranteed + untouchable extra\n     // Else idealAssigned \u003d current;\n     // Subtract idealAssigned resources from unassigned.\n     // If the queue has all of its needs met (that is, if \n     // idealAssigned \u003e\u003d current + pending), remove it from consideration.\n     // Sort queues from most under-guaranteed to most over-guaranteed.\n     TQComparator tqComparator \u003d new TQComparator(rc, tot_guarant);\n-    PriorityQueue\u003cTempQueue\u003e orderedByNeed \u003d\n-                                 new PriorityQueue\u003cTempQueue\u003e(10,tqComparator);\n-    for (Iterator\u003cTempQueue\u003e i \u003d qAlloc.iterator(); i.hasNext();) {\n-      TempQueue q \u003d i.next();\n+    PriorityQueue\u003cTempQueuePerPartition\u003e orderedByNeed \u003d\n+        new PriorityQueue\u003cTempQueuePerPartition\u003e(10, tqComparator);\n+    for (Iterator\u003cTempQueuePerPartition\u003e i \u003d qAlloc.iterator(); i.hasNext();) {\n+      TempQueuePerPartition q \u003d i.next();\n       if (Resources.greaterThan(rc, tot_guarant, q.current, q.guaranteed)) {\n         q.idealAssigned \u003d Resources.add(q.guaranteed, q.untouchableExtra);\n       } else {\n         q.idealAssigned \u003d Resources.clone(q.current);\n       }\n       Resources.subtractFrom(unassigned, q.idealAssigned);\n       // If idealAssigned \u003c (current + pending), q needs more resources, so\n       // add it to the list of underserved queues, ordered by need.\n       Resource curPlusPend \u003d Resources.add(q.current, q.pending);\n       if (Resources.lessThan(rc, tot_guarant, q.idealAssigned, curPlusPend)) {\n         orderedByNeed.add(q);\n       }\n     }\n \n     //assign all cluster resources until no more demand, or no resources are left\n     while (!orderedByNeed.isEmpty()\n        \u0026\u0026 Resources.greaterThan(rc,tot_guarant, unassigned,Resources.none())) {\n       Resource wQassigned \u003d Resource.newInstance(0, 0);\n       // we compute normalizedGuarantees capacity based on currently active\n       // queues\n       resetCapacity(rc, unassigned, orderedByNeed, ignoreGuarantee);\n \n       // For each underserved queue (or set of queues if multiple are equally\n       // underserved), offer its share of the unassigned resources based on its\n       // normalized guarantee. After the offer, if the queue is not satisfied,\n       // place it back in the ordered list of queues, recalculating its place\n       // in the order of most under-guaranteed to most over-guaranteed. In this\n       // way, the most underserved queue(s) are always given resources first.\n-      Collection\u003cTempQueue\u003e underserved \u003d\n+      Collection\u003cTempQueuePerPartition\u003e underserved \u003d\n           getMostUnderservedQueues(orderedByNeed, tqComparator);\n-      for (Iterator\u003cTempQueue\u003e i \u003d underserved.iterator(); i.hasNext();) {\n-        TempQueue sub \u003d i.next();\n+      for (Iterator\u003cTempQueuePerPartition\u003e i \u003d underserved.iterator(); i\n+          .hasNext();) {\n+        TempQueuePerPartition sub \u003d i.next();\n         Resource wQavail \u003d Resources.multiplyAndNormalizeUp(rc,\n             unassigned, sub.normalizedGuarantee, Resource.newInstance(1, 1));\n         Resource wQidle \u003d sub.offer(wQavail, rc, tot_guarant);\n         Resource wQdone \u003d Resources.subtract(wQavail, wQidle);\n \n         if (Resources.greaterThan(rc, tot_guarant,\n               wQdone, Resources.none())) {\n           // The queue is still asking for more. Put it back in the priority\n           // queue, recalculating its order based on need.\n           orderedByNeed.add(sub);\n         }\n         Resources.addTo(wQassigned, wQdone);\n       }\n       Resources.subtractFrom(unassigned, wQassigned);\n     }\n   }\n\\ No newline at end of file\n",
          "actualSource": "  private void computeFixpointAllocation(ResourceCalculator rc,\n      Resource tot_guarant, Collection\u003cTempQueuePerPartition\u003e qAlloc,\n      Resource unassigned, boolean ignoreGuarantee) {\n    // Prior to assigning the unused resources, process each queue as follows:\n    // If current \u003e guaranteed, idealAssigned \u003d guaranteed + untouchable extra\n    // Else idealAssigned \u003d current;\n    // Subtract idealAssigned resources from unassigned.\n    // If the queue has all of its needs met (that is, if \n    // idealAssigned \u003e\u003d current + pending), remove it from consideration.\n    // Sort queues from most under-guaranteed to most over-guaranteed.\n    TQComparator tqComparator \u003d new TQComparator(rc, tot_guarant);\n    PriorityQueue\u003cTempQueuePerPartition\u003e orderedByNeed \u003d\n        new PriorityQueue\u003cTempQueuePerPartition\u003e(10, tqComparator);\n    for (Iterator\u003cTempQueuePerPartition\u003e i \u003d qAlloc.iterator(); i.hasNext();) {\n      TempQueuePerPartition q \u003d i.next();\n      if (Resources.greaterThan(rc, tot_guarant, q.current, q.guaranteed)) {\n        q.idealAssigned \u003d Resources.add(q.guaranteed, q.untouchableExtra);\n      } else {\n        q.idealAssigned \u003d Resources.clone(q.current);\n      }\n      Resources.subtractFrom(unassigned, q.idealAssigned);\n      // If idealAssigned \u003c (current + pending), q needs more resources, so\n      // add it to the list of underserved queues, ordered by need.\n      Resource curPlusPend \u003d Resources.add(q.current, q.pending);\n      if (Resources.lessThan(rc, tot_guarant, q.idealAssigned, curPlusPend)) {\n        orderedByNeed.add(q);\n      }\n    }\n\n    //assign all cluster resources until no more demand, or no resources are left\n    while (!orderedByNeed.isEmpty()\n       \u0026\u0026 Resources.greaterThan(rc,tot_guarant, unassigned,Resources.none())) {\n      Resource wQassigned \u003d Resource.newInstance(0, 0);\n      // we compute normalizedGuarantees capacity based on currently active\n      // queues\n      resetCapacity(rc, unassigned, orderedByNeed, ignoreGuarantee);\n\n      // For each underserved queue (or set of queues if multiple are equally\n      // underserved), offer its share of the unassigned resources based on its\n      // normalized guarantee. After the offer, if the queue is not satisfied,\n      // place it back in the ordered list of queues, recalculating its place\n      // in the order of most under-guaranteed to most over-guaranteed. In this\n      // way, the most underserved queue(s) are always given resources first.\n      Collection\u003cTempQueuePerPartition\u003e underserved \u003d\n          getMostUnderservedQueues(orderedByNeed, tqComparator);\n      for (Iterator\u003cTempQueuePerPartition\u003e i \u003d underserved.iterator(); i\n          .hasNext();) {\n        TempQueuePerPartition sub \u003d i.next();\n        Resource wQavail \u003d Resources.multiplyAndNormalizeUp(rc,\n            unassigned, sub.normalizedGuarantee, Resource.newInstance(1, 1));\n        Resource wQidle \u003d sub.offer(wQavail, rc, tot_guarant);\n        Resource wQdone \u003d Resources.subtract(wQavail, wQidle);\n\n        if (Resources.greaterThan(rc, tot_guarant,\n              wQdone, Resources.none())) {\n          // The queue is still asking for more. Put it back in the priority\n          // queue, recalculating its order based on need.\n          orderedByNeed.add(sub);\n        }\n        Resources.addTo(wQassigned, wQdone);\n      }\n      Resources.subtractFrom(unassigned, wQassigned);\n    }\n  }",
          "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/ProportionalCapacityPreemptionPolicy.java",
          "extendedDetails": {}
        }
      ]
    },
    "4b130821995a3cfe20c71e38e0f63294085c0491": {
      "type": "Ybodychange",
      "commitMessage": "YARN-2056. Disable preemption at Queue level. Contributed by Eric Payne\n",
      "commitDate": "05/12/14 1:06 PM",
      "commitName": "4b130821995a3cfe20c71e38e0f63294085c0491",
      "commitAuthor": "Jason Lowe",
      "commitDateOld": "12/09/14 10:33 AM",
      "commitNameOld": "3122daa80261b466e309e88d88d1e2c030525e3f",
      "commitAuthorOld": "Jian He",
      "daysBetweenCommits": 84.15,
      "commitsBetweenForRepo": 775,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,31 +1,63 @@\n   private void computeFixpointAllocation(ResourceCalculator rc,\n       Resource tot_guarant, Collection\u003cTempQueue\u003e qAlloc, Resource unassigned, \n       boolean ignoreGuarantee) {\n-    //assign all cluster resources until no more demand, or no resources are left\n-    while (!qAlloc.isEmpty() \u0026\u0026 Resources.greaterThan(rc, tot_guarant,\n-          unassigned, Resources.none())) {\n-      Resource wQassigned \u003d Resource.newInstance(0, 0);\n+    // Prior to assigning the unused resources, process each queue as follows:\n+    // If current \u003e guaranteed, idealAssigned \u003d guaranteed + untouchable extra\n+    // Else idealAssigned \u003d current;\n+    // Subtract idealAssigned resources from unassigned.\n+    // If the queue has all of its needs met (that is, if \n+    // idealAssigned \u003e\u003d current + pending), remove it from consideration.\n+    // Sort queues from most under-guaranteed to most over-guaranteed.\n+    TQComparator tqComparator \u003d new TQComparator(rc, tot_guarant);\n+    PriorityQueue\u003cTempQueue\u003e orderedByNeed \u003d\n+                                 new PriorityQueue\u003cTempQueue\u003e(10,tqComparator);\n+    for (Iterator\u003cTempQueue\u003e i \u003d qAlloc.iterator(); i.hasNext();) {\n+      TempQueue q \u003d i.next();\n+      if (Resources.greaterThan(rc, tot_guarant, q.current, q.guaranteed)) {\n+        q.idealAssigned \u003d Resources.add(q.guaranteed, q.untouchableExtra);\n+      } else {\n+        q.idealAssigned \u003d Resources.clone(q.current);\n+      }\n+      Resources.subtractFrom(unassigned, q.idealAssigned);\n+      // If idealAssigned \u003c (current + pending), q needs more resources, so\n+      // add it to the list of underserved queues, ordered by need.\n+      Resource curPlusPend \u003d Resources.add(q.current, q.pending);\n+      if (Resources.lessThan(rc, tot_guarant, q.idealAssigned, curPlusPend)) {\n+        orderedByNeed.add(q);\n+      }\n+    }\n \n+    //assign all cluster resources until no more demand, or no resources are left\n+    while (!orderedByNeed.isEmpty()\n+       \u0026\u0026 Resources.greaterThan(rc,tot_guarant, unassigned,Resources.none())) {\n+      Resource wQassigned \u003d Resource.newInstance(0, 0);\n       // we compute normalizedGuarantees capacity based on currently active\n       // queues\n-      resetCapacity(rc, unassigned, qAlloc, ignoreGuarantee);\n-      \n-      // offer for each queue their capacity first and in following invocations\n-      // their share of over-capacity\n-      for (Iterator\u003cTempQueue\u003e i \u003d qAlloc.iterator(); i.hasNext();) {\n+      resetCapacity(rc, unassigned, orderedByNeed, ignoreGuarantee);\n+\n+      // For each underserved queue (or set of queues if multiple are equally\n+      // underserved), offer its share of the unassigned resources based on its\n+      // normalized guarantee. After the offer, if the queue is not satisfied,\n+      // place it back in the ordered list of queues, recalculating its place\n+      // in the order of most under-guaranteed to most over-guaranteed. In this\n+      // way, the most underserved queue(s) are always given resources first.\n+      Collection\u003cTempQueue\u003e underserved \u003d\n+          getMostUnderservedQueues(orderedByNeed, tqComparator);\n+      for (Iterator\u003cTempQueue\u003e i \u003d underserved.iterator(); i.hasNext();) {\n         TempQueue sub \u003d i.next();\n-        Resource wQavail \u003d\n-          Resources.multiply(unassigned, sub.normalizedGuarantee);\n+        Resource wQavail \u003d Resources.multiplyAndNormalizeUp(rc,\n+            unassigned, sub.normalizedGuarantee, Resource.newInstance(1, 1));\n         Resource wQidle \u003d sub.offer(wQavail, rc, tot_guarant);\n         Resource wQdone \u003d Resources.subtract(wQavail, wQidle);\n-        // if the queue returned a value \u003e 0 it means it is fully satisfied\n-        // and it is removed from the list of active queues qAlloc\n-        if (!Resources.greaterThan(rc, tot_guarant,\n+\n+        if (Resources.greaterThan(rc, tot_guarant,\n               wQdone, Resources.none())) {\n-          i.remove();\n+          // The queue is still asking for more. Put it back in the priority\n+          // queue, recalculating its order based on need.\n+          orderedByNeed.add(sub);\n         }\n         Resources.addTo(wQassigned, wQdone);\n       }\n       Resources.subtractFrom(unassigned, wQassigned);\n     }\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void computeFixpointAllocation(ResourceCalculator rc,\n      Resource tot_guarant, Collection\u003cTempQueue\u003e qAlloc, Resource unassigned, \n      boolean ignoreGuarantee) {\n    // Prior to assigning the unused resources, process each queue as follows:\n    // If current \u003e guaranteed, idealAssigned \u003d guaranteed + untouchable extra\n    // Else idealAssigned \u003d current;\n    // Subtract idealAssigned resources from unassigned.\n    // If the queue has all of its needs met (that is, if \n    // idealAssigned \u003e\u003d current + pending), remove it from consideration.\n    // Sort queues from most under-guaranteed to most over-guaranteed.\n    TQComparator tqComparator \u003d new TQComparator(rc, tot_guarant);\n    PriorityQueue\u003cTempQueue\u003e orderedByNeed \u003d\n                                 new PriorityQueue\u003cTempQueue\u003e(10,tqComparator);\n    for (Iterator\u003cTempQueue\u003e i \u003d qAlloc.iterator(); i.hasNext();) {\n      TempQueue q \u003d i.next();\n      if (Resources.greaterThan(rc, tot_guarant, q.current, q.guaranteed)) {\n        q.idealAssigned \u003d Resources.add(q.guaranteed, q.untouchableExtra);\n      } else {\n        q.idealAssigned \u003d Resources.clone(q.current);\n      }\n      Resources.subtractFrom(unassigned, q.idealAssigned);\n      // If idealAssigned \u003c (current + pending), q needs more resources, so\n      // add it to the list of underserved queues, ordered by need.\n      Resource curPlusPend \u003d Resources.add(q.current, q.pending);\n      if (Resources.lessThan(rc, tot_guarant, q.idealAssigned, curPlusPend)) {\n        orderedByNeed.add(q);\n      }\n    }\n\n    //assign all cluster resources until no more demand, or no resources are left\n    while (!orderedByNeed.isEmpty()\n       \u0026\u0026 Resources.greaterThan(rc,tot_guarant, unassigned,Resources.none())) {\n      Resource wQassigned \u003d Resource.newInstance(0, 0);\n      // we compute normalizedGuarantees capacity based on currently active\n      // queues\n      resetCapacity(rc, unassigned, orderedByNeed, ignoreGuarantee);\n\n      // For each underserved queue (or set of queues if multiple are equally\n      // underserved), offer its share of the unassigned resources based on its\n      // normalized guarantee. After the offer, if the queue is not satisfied,\n      // place it back in the ordered list of queues, recalculating its place\n      // in the order of most under-guaranteed to most over-guaranteed. In this\n      // way, the most underserved queue(s) are always given resources first.\n      Collection\u003cTempQueue\u003e underserved \u003d\n          getMostUnderservedQueues(orderedByNeed, tqComparator);\n      for (Iterator\u003cTempQueue\u003e i \u003d underserved.iterator(); i.hasNext();) {\n        TempQueue sub \u003d i.next();\n        Resource wQavail \u003d Resources.multiplyAndNormalizeUp(rc,\n            unassigned, sub.normalizedGuarantee, Resource.newInstance(1, 1));\n        Resource wQidle \u003d sub.offer(wQavail, rc, tot_guarant);\n        Resource wQdone \u003d Resources.subtract(wQavail, wQidle);\n\n        if (Resources.greaterThan(rc, tot_guarant,\n              wQdone, Resources.none())) {\n          // The queue is still asking for more. Put it back in the priority\n          // queue, recalculating its order based on need.\n          orderedByNeed.add(sub);\n        }\n        Resources.addTo(wQassigned, wQdone);\n      }\n      Resources.subtractFrom(unassigned, wQassigned);\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/ProportionalCapacityPreemptionPolicy.java",
      "extendedDetails": {}
    },
    "45b42676f9333ed4fa05355ccb4e1f91a9556525": {
      "type": "Yintroduced",
      "commitMessage": "YARN-1957. Consider the max capacity of the queue when computing the ideal\ncapacity for preemption. Contributed by Carlo Curino\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1594414 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "13/05/14 4:15 PM",
      "commitName": "45b42676f9333ed4fa05355ccb4e1f91a9556525",
      "commitAuthor": "Christopher Douglas",
      "diff": "@@ -0,0 +1,31 @@\n+  private void computeFixpointAllocation(ResourceCalculator rc,\n+      Resource tot_guarant, Collection\u003cTempQueue\u003e qAlloc, Resource unassigned, \n+      boolean ignoreGuarantee) {\n+    //assign all cluster resources until no more demand, or no resources are left\n+    while (!qAlloc.isEmpty() \u0026\u0026 Resources.greaterThan(rc, tot_guarant,\n+          unassigned, Resources.none())) {\n+      Resource wQassigned \u003d Resource.newInstance(0, 0);\n+\n+      // we compute normalizedGuarantees capacity based on currently active\n+      // queues\n+      resetCapacity(rc, unassigned, qAlloc, ignoreGuarantee);\n+      \n+      // offer for each queue their capacity first and in following invocations\n+      // their share of over-capacity\n+      for (Iterator\u003cTempQueue\u003e i \u003d qAlloc.iterator(); i.hasNext();) {\n+        TempQueue sub \u003d i.next();\n+        Resource wQavail \u003d\n+          Resources.multiply(unassigned, sub.normalizedGuarantee);\n+        Resource wQidle \u003d sub.offer(wQavail, rc, tot_guarant);\n+        Resource wQdone \u003d Resources.subtract(wQavail, wQidle);\n+        // if the queue returned a value \u003e 0 it means it is fully satisfied\n+        // and it is removed from the list of active queues qAlloc\n+        if (!Resources.greaterThan(rc, tot_guarant,\n+              wQdone, Resources.none())) {\n+          i.remove();\n+        }\n+        Resources.addTo(wQassigned, wQdone);\n+      }\n+      Resources.subtractFrom(unassigned, wQassigned);\n+    }\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void computeFixpointAllocation(ResourceCalculator rc,\n      Resource tot_guarant, Collection\u003cTempQueue\u003e qAlloc, Resource unassigned, \n      boolean ignoreGuarantee) {\n    //assign all cluster resources until no more demand, or no resources are left\n    while (!qAlloc.isEmpty() \u0026\u0026 Resources.greaterThan(rc, tot_guarant,\n          unassigned, Resources.none())) {\n      Resource wQassigned \u003d Resource.newInstance(0, 0);\n\n      // we compute normalizedGuarantees capacity based on currently active\n      // queues\n      resetCapacity(rc, unassigned, qAlloc, ignoreGuarantee);\n      \n      // offer for each queue their capacity first and in following invocations\n      // their share of over-capacity\n      for (Iterator\u003cTempQueue\u003e i \u003d qAlloc.iterator(); i.hasNext();) {\n        TempQueue sub \u003d i.next();\n        Resource wQavail \u003d\n          Resources.multiply(unassigned, sub.normalizedGuarantee);\n        Resource wQidle \u003d sub.offer(wQavail, rc, tot_guarant);\n        Resource wQdone \u003d Resources.subtract(wQavail, wQidle);\n        // if the queue returned a value \u003e 0 it means it is fully satisfied\n        // and it is removed from the list of active queues qAlloc\n        if (!Resources.greaterThan(rc, tot_guarant,\n              wQdone, Resources.none())) {\n          i.remove();\n        }\n        Resources.addTo(wQassigned, wQdone);\n      }\n      Resources.subtractFrom(unassigned, wQassigned);\n    }\n  }",
      "path": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/ProportionalCapacityPreemptionPolicy.java"
    }
  }
}