{
  "origin": "codeshovel",
  "repositoryName": "hadoop",
  "repositoryPath": "/home/shaiful/research/codeshovel/codeshovel-projects/hadoop/.git",
  "startCommitName": "HEAD",
  "sourceFileName": "OpenFileCtx.java",
  "functionName": "processOverWrite",
  "functionId": "processOverWrite___dfsClient-DFSClient__request-WRITE3Request__channel-Channel__xid-int__iug-IdMappingServiceProvider",
  "sourceFilePath": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java",
  "functionStartLine": 590,
  "functionEndLine": 613,
  "numCommitsSeen": 62,
  "timeTaken": 2106,
  "changeHistory": [
    "f20dc0d5770d3876954faf0a6e8dcce6539ffc23",
    "72a556d3b0def0ab4e4509528cc513f6df06b084",
    "875aa797caee96572162ff59bc50cf97d1195348",
    "caa4abd30cfc4361c7bc9f212a9092840d7c3b53",
    "027419832c1125d707b45ce852032d704ab79d88",
    "28e3d09230971b32f74284311931525cb7ad1b7c"
  ],
  "changeHistoryShort": {
    "f20dc0d5770d3876954faf0a6e8dcce6539ffc23": "Ybodychange",
    "72a556d3b0def0ab4e4509528cc513f6df06b084": "Yparameterchange",
    "875aa797caee96572162ff59bc50cf97d1195348": "Ybodychange",
    "caa4abd30cfc4361c7bc9f212a9092840d7c3b53": "Ybodychange",
    "027419832c1125d707b45ce852032d704ab79d88": "Ybodychange",
    "28e3d09230971b32f74284311931525cb7ad1b7c": "Yintroduced"
  },
  "changeHistoryDetails": {
    "f20dc0d5770d3876954faf0a6e8dcce6539ffc23": {
      "type": "Ybodychange",
      "commitMessage": "HADOOP-10571. Use Log.*(Object, Throwable) overload to log exceptions.\nContributed by Andras Bokor.\n",
      "commitDate": "14/02/18 8:20 AM",
      "commitName": "f20dc0d5770d3876954faf0a6e8dcce6539ffc23",
      "commitAuthor": "Steve Loughran",
      "commitDateOld": "10/10/17 10:38 AM",
      "commitNameOld": "d6602b5f39833611b4afa4581552f6c4c37e23a8",
      "commitAuthorOld": "Jitendra Pandey",
      "daysBetweenCommits": 126.95,
      "commitsBetweenForRepo": 833,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,26 +1,24 @@\n   private void processOverWrite(DFSClient dfsClient, WRITE3Request request,\n       Channel channel, int xid, IdMappingServiceProvider iug) {\n     WccData wccData \u003d new WccData(latestAttr.getWccAttr(), null);\n     long offset \u003d request.getOffset();\n     int count \u003d request.getCount();\n     WriteStableHow stableHow \u003d request.getStableHow();\n     WRITE3Response response;\n     long cachedOffset \u003d nextOffset.get();\n     if (offset + count \u003e cachedOffset) {\n       LOG.warn(\"Treat this jumbo write as a real random write, no support.\");\n       response \u003d new WRITE3Response(Nfs3Status.NFS3ERR_INVAL, wccData, 0,\n           WriteStableHow.UNSTABLE, Nfs3Constant.WRITE_COMMIT_VERF);\n     } else {\n-      if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"Process perfectOverWrite\");\n-      }\n+      LOG.debug(\"Process perfectOverWrite\");\n       // TODO: let executor handle perfect overwrite\n       response \u003d processPerfectOverWrite(dfsClient, offset, count, stableHow,\n           request.getData().array(),\n           Nfs3Utils.getFileIdPath(request.getHandle()), wccData, iug);\n     }\n     updateLastAccessTime();\n     Nfs3Utils.writeChannel(channel,\n         response.serialize(new XDR(), xid, new VerifierNone()),\n         xid);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void processOverWrite(DFSClient dfsClient, WRITE3Request request,\n      Channel channel, int xid, IdMappingServiceProvider iug) {\n    WccData wccData \u003d new WccData(latestAttr.getWccAttr(), null);\n    long offset \u003d request.getOffset();\n    int count \u003d request.getCount();\n    WriteStableHow stableHow \u003d request.getStableHow();\n    WRITE3Response response;\n    long cachedOffset \u003d nextOffset.get();\n    if (offset + count \u003e cachedOffset) {\n      LOG.warn(\"Treat this jumbo write as a real random write, no support.\");\n      response \u003d new WRITE3Response(Nfs3Status.NFS3ERR_INVAL, wccData, 0,\n          WriteStableHow.UNSTABLE, Nfs3Constant.WRITE_COMMIT_VERF);\n    } else {\n      LOG.debug(\"Process perfectOverWrite\");\n      // TODO: let executor handle perfect overwrite\n      response \u003d processPerfectOverWrite(dfsClient, offset, count, stableHow,\n          request.getData().array(),\n          Nfs3Utils.getFileIdPath(request.getHandle()), wccData, iug);\n    }\n    updateLastAccessTime();\n    Nfs3Utils.writeChannel(channel,\n        response.serialize(new XDR(), xid, new VerifierNone()),\n        xid);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java",
      "extendedDetails": {}
    },
    "72a556d3b0def0ab4e4509528cc513f6df06b084": {
      "type": "Yparameterchange",
      "commitMessage": "HADOOP-11195. Move Id-Name mapping in NFS to the hadoop-common area for better maintenance. Contributed by Yongjun Zhang\n",
      "commitDate": "29/10/14 11:05 AM",
      "commitName": "72a556d3b0def0ab4e4509528cc513f6df06b084",
      "commitAuthor": "Brandon Li",
      "commitDateOld": "22/10/14 9:27 PM",
      "commitNameOld": "d71d40a63d198991077d5babd70be5e9787a53f1",
      "commitAuthorOld": "Brandon Li",
      "daysBetweenCommits": 6.57,
      "commitsBetweenForRepo": 67,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,26 +1,26 @@\n   private void processOverWrite(DFSClient dfsClient, WRITE3Request request,\n-      Channel channel, int xid, IdUserGroup iug) {\n+      Channel channel, int xid, IdMappingServiceProvider iug) {\n     WccData wccData \u003d new WccData(latestAttr.getWccAttr(), null);\n     long offset \u003d request.getOffset();\n     int count \u003d request.getCount();\n     WriteStableHow stableHow \u003d request.getStableHow();\n     WRITE3Response response;\n     long cachedOffset \u003d nextOffset.get();\n     if (offset + count \u003e cachedOffset) {\n       LOG.warn(\"Treat this jumbo write as a real random write, no support.\");\n       response \u003d new WRITE3Response(Nfs3Status.NFS3ERR_INVAL, wccData, 0,\n           WriteStableHow.UNSTABLE, Nfs3Constant.WRITE_COMMIT_VERF);\n     } else {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Process perfectOverWrite\");\n       }\n       // TODO: let executor handle perfect overwrite\n       response \u003d processPerfectOverWrite(dfsClient, offset, count, stableHow,\n           request.getData().array(),\n           Nfs3Utils.getFileIdPath(request.getHandle()), wccData, iug);\n     }\n     updateLastAccessTime();\n     Nfs3Utils.writeChannel(channel,\n         response.serialize(new XDR(), xid, new VerifierNone()),\n         xid);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void processOverWrite(DFSClient dfsClient, WRITE3Request request,\n      Channel channel, int xid, IdMappingServiceProvider iug) {\n    WccData wccData \u003d new WccData(latestAttr.getWccAttr(), null);\n    long offset \u003d request.getOffset();\n    int count \u003d request.getCount();\n    WriteStableHow stableHow \u003d request.getStableHow();\n    WRITE3Response response;\n    long cachedOffset \u003d nextOffset.get();\n    if (offset + count \u003e cachedOffset) {\n      LOG.warn(\"Treat this jumbo write as a real random write, no support.\");\n      response \u003d new WRITE3Response(Nfs3Status.NFS3ERR_INVAL, wccData, 0,\n          WriteStableHow.UNSTABLE, Nfs3Constant.WRITE_COMMIT_VERF);\n    } else {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Process perfectOverWrite\");\n      }\n      // TODO: let executor handle perfect overwrite\n      response \u003d processPerfectOverWrite(dfsClient, offset, count, stableHow,\n          request.getData().array(),\n          Nfs3Utils.getFileIdPath(request.getHandle()), wccData, iug);\n    }\n    updateLastAccessTime();\n    Nfs3Utils.writeChannel(channel,\n        response.serialize(new XDR(), xid, new VerifierNone()),\n        xid);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java",
      "extendedDetails": {
        "oldValue": "[dfsClient-DFSClient, request-WRITE3Request, channel-Channel, xid-int, iug-IdUserGroup]",
        "newValue": "[dfsClient-DFSClient, request-WRITE3Request, channel-Channel, xid-int, iug-IdMappingServiceProvider]"
      }
    },
    "875aa797caee96572162ff59bc50cf97d1195348": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-6894. Add XDR parser method for each NFS response. Contributed by Brandon Li.\n",
      "commitDate": "01/10/14 1:18 PM",
      "commitName": "875aa797caee96572162ff59bc50cf97d1195348",
      "commitAuthor": "Haohui Mai",
      "commitDateOld": "18/09/14 2:57 PM",
      "commitNameOld": "70be56d093022de9953e14a92dfa1a146bd9a290",
      "commitAuthorOld": "Andrew Wang",
      "daysBetweenCommits": 12.93,
      "commitsBetweenForRepo": 154,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,26 +1,26 @@\n   private void processOverWrite(DFSClient dfsClient, WRITE3Request request,\n       Channel channel, int xid, IdUserGroup iug) {\n     WccData wccData \u003d new WccData(latestAttr.getWccAttr(), null);\n     long offset \u003d request.getOffset();\n     int count \u003d request.getCount();\n     WriteStableHow stableHow \u003d request.getStableHow();\n     WRITE3Response response;\n     long cachedOffset \u003d nextOffset.get();\n     if (offset + count \u003e cachedOffset) {\n       LOG.warn(\"Treat this jumbo write as a real random write, no support.\");\n       response \u003d new WRITE3Response(Nfs3Status.NFS3ERR_INVAL, wccData, 0,\n           WriteStableHow.UNSTABLE, Nfs3Constant.WRITE_COMMIT_VERF);\n     } else {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Process perfectOverWrite\");\n       }\n       // TODO: let executor handle perfect overwrite\n       response \u003d processPerfectOverWrite(dfsClient, offset, count, stableHow,\n           request.getData().array(),\n           Nfs3Utils.getFileIdPath(request.getHandle()), wccData, iug);\n     }\n     updateLastAccessTime();\n     Nfs3Utils.writeChannel(channel,\n-        response.writeHeaderAndResponse(new XDR(), xid, new VerifierNone()),\n+        response.serialize(new XDR(), xid, new VerifierNone()),\n         xid);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void processOverWrite(DFSClient dfsClient, WRITE3Request request,\n      Channel channel, int xid, IdUserGroup iug) {\n    WccData wccData \u003d new WccData(latestAttr.getWccAttr(), null);\n    long offset \u003d request.getOffset();\n    int count \u003d request.getCount();\n    WriteStableHow stableHow \u003d request.getStableHow();\n    WRITE3Response response;\n    long cachedOffset \u003d nextOffset.get();\n    if (offset + count \u003e cachedOffset) {\n      LOG.warn(\"Treat this jumbo write as a real random write, no support.\");\n      response \u003d new WRITE3Response(Nfs3Status.NFS3ERR_INVAL, wccData, 0,\n          WriteStableHow.UNSTABLE, Nfs3Constant.WRITE_COMMIT_VERF);\n    } else {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Process perfectOverWrite\");\n      }\n      // TODO: let executor handle perfect overwrite\n      response \u003d processPerfectOverWrite(dfsClient, offset, count, stableHow,\n          request.getData().array(),\n          Nfs3Utils.getFileIdPath(request.getHandle()), wccData, iug);\n    }\n    updateLastAccessTime();\n    Nfs3Utils.writeChannel(channel,\n        response.serialize(new XDR(), xid, new VerifierNone()),\n        xid);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java",
      "extendedDetails": {}
    },
    "caa4abd30cfc4361c7bc9f212a9092840d7c3b53": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5259. Support client which combines appended data with old data before sends it to NFS server. Contributed by Brandon Li\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1529730 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "06/10/13 7:57 PM",
      "commitName": "caa4abd30cfc4361c7bc9f212a9092840d7c3b53",
      "commitAuthor": "Brandon Li",
      "commitDateOld": "27/09/13 2:28 PM",
      "commitNameOld": "027419832c1125d707b45ce852032d704ab79d88",
      "commitAuthorOld": "Brandon Li",
      "daysBetweenCommits": 9.23,
      "commitsBetweenForRepo": 77,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,27 +1,26 @@\n   private void processOverWrite(DFSClient dfsClient, WRITE3Request request,\n       Channel channel, int xid, IdUserGroup iug) {\n     WccData wccData \u003d new WccData(latestAttr.getWccAttr(), null);\n     long offset \u003d request.getOffset();\n     int count \u003d request.getCount();\n     WriteStableHow stableHow \u003d request.getStableHow();\n     WRITE3Response response;\n     long cachedOffset \u003d nextOffset.get();\n     if (offset + count \u003e cachedOffset) {\n-      LOG.warn(\"Haven\u0027t noticed any partial overwrite for a sequential file\"\n-          + \" write requests. Treat it as a real random write, no support.\");\n+      LOG.warn(\"Treat this jumbo write as a real random write, no support.\");\n       response \u003d new WRITE3Response(Nfs3Status.NFS3ERR_INVAL, wccData, 0,\n           WriteStableHow.UNSTABLE, Nfs3Constant.WRITE_COMMIT_VERF);\n     } else {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Process perfectOverWrite\");\n       }\n       // TODO: let executor handle perfect overwrite\n       response \u003d processPerfectOverWrite(dfsClient, offset, count, stableHow,\n           request.getData().array(),\n           Nfs3Utils.getFileIdPath(request.getHandle()), wccData, iug);\n     }\n     updateLastAccessTime();\n     Nfs3Utils.writeChannel(channel,\n         response.writeHeaderAndResponse(new XDR(), xid, new VerifierNone()),\n         xid);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void processOverWrite(DFSClient dfsClient, WRITE3Request request,\n      Channel channel, int xid, IdUserGroup iug) {\n    WccData wccData \u003d new WccData(latestAttr.getWccAttr(), null);\n    long offset \u003d request.getOffset();\n    int count \u003d request.getCount();\n    WriteStableHow stableHow \u003d request.getStableHow();\n    WRITE3Response response;\n    long cachedOffset \u003d nextOffset.get();\n    if (offset + count \u003e cachedOffset) {\n      LOG.warn(\"Treat this jumbo write as a real random write, no support.\");\n      response \u003d new WRITE3Response(Nfs3Status.NFS3ERR_INVAL, wccData, 0,\n          WriteStableHow.UNSTABLE, Nfs3Constant.WRITE_COMMIT_VERF);\n    } else {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Process perfectOverWrite\");\n      }\n      // TODO: let executor handle perfect overwrite\n      response \u003d processPerfectOverWrite(dfsClient, offset, count, stableHow,\n          request.getData().array(),\n          Nfs3Utils.getFileIdPath(request.getHandle()), wccData, iug);\n    }\n    updateLastAccessTime();\n    Nfs3Utils.writeChannel(channel,\n        response.writeHeaderAndResponse(new XDR(), xid, new VerifierNone()),\n        xid);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java",
      "extendedDetails": {}
    },
    "027419832c1125d707b45ce852032d704ab79d88": {
      "type": "Ybodychange",
      "commitMessage": "HDFS-5268. NFS write commit verifier is not set in a few places. Contributed by Brandon Li\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1527087 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "27/09/13 2:28 PM",
      "commitName": "027419832c1125d707b45ce852032d704ab79d88",
      "commitAuthor": "Brandon Li",
      "commitDateOld": "23/09/13 9:11 PM",
      "commitNameOld": "e3088e4aef8cdfc0841858e7851f7276fab9b24b",
      "commitAuthorOld": "Brandon Li",
      "daysBetweenCommits": 3.72,
      "commitsBetweenForRepo": 32,
      "commitsBetweenForFile": 1,
      "diff": "@@ -1,27 +1,27 @@\n   private void processOverWrite(DFSClient dfsClient, WRITE3Request request,\n       Channel channel, int xid, IdUserGroup iug) {\n     WccData wccData \u003d new WccData(latestAttr.getWccAttr(), null);\n     long offset \u003d request.getOffset();\n     int count \u003d request.getCount();\n     WriteStableHow stableHow \u003d request.getStableHow();\n     WRITE3Response response;\n     long cachedOffset \u003d nextOffset.get();\n     if (offset + count \u003e cachedOffset) {\n       LOG.warn(\"Haven\u0027t noticed any partial overwrite for a sequential file\"\n           + \" write requests. Treat it as a real random write, no support.\");\n       response \u003d new WRITE3Response(Nfs3Status.NFS3ERR_INVAL, wccData, 0,\n-          WriteStableHow.UNSTABLE, 0);\n+          WriteStableHow.UNSTABLE, Nfs3Constant.WRITE_COMMIT_VERF);\n     } else {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Process perfectOverWrite\");\n       }\n       // TODO: let executor handle perfect overwrite\n       response \u003d processPerfectOverWrite(dfsClient, offset, count, stableHow,\n           request.getData().array(),\n           Nfs3Utils.getFileIdPath(request.getHandle()), wccData, iug);\n     }\n     updateLastAccessTime();\n     Nfs3Utils.writeChannel(channel,\n         response.writeHeaderAndResponse(new XDR(), xid, new VerifierNone()),\n         xid);\n   }\n\\ No newline at end of file\n",
      "actualSource": "  private void processOverWrite(DFSClient dfsClient, WRITE3Request request,\n      Channel channel, int xid, IdUserGroup iug) {\n    WccData wccData \u003d new WccData(latestAttr.getWccAttr(), null);\n    long offset \u003d request.getOffset();\n    int count \u003d request.getCount();\n    WriteStableHow stableHow \u003d request.getStableHow();\n    WRITE3Response response;\n    long cachedOffset \u003d nextOffset.get();\n    if (offset + count \u003e cachedOffset) {\n      LOG.warn(\"Haven\u0027t noticed any partial overwrite for a sequential file\"\n          + \" write requests. Treat it as a real random write, no support.\");\n      response \u003d new WRITE3Response(Nfs3Status.NFS3ERR_INVAL, wccData, 0,\n          WriteStableHow.UNSTABLE, Nfs3Constant.WRITE_COMMIT_VERF);\n    } else {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Process perfectOverWrite\");\n      }\n      // TODO: let executor handle perfect overwrite\n      response \u003d processPerfectOverWrite(dfsClient, offset, count, stableHow,\n          request.getData().array(),\n          Nfs3Utils.getFileIdPath(request.getHandle()), wccData, iug);\n    }\n    updateLastAccessTime();\n    Nfs3Utils.writeChannel(channel,\n        response.writeHeaderAndResponse(new XDR(), xid, new VerifierNone()),\n        xid);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java",
      "extendedDetails": {}
    },
    "28e3d09230971b32f74284311931525cb7ad1b7c": {
      "type": "Yintroduced",
      "commitMessage": "HDFS-4971. Move IO operations out of locking in OpenFileCtx. Contributed by Jing Zhao and Brandon Li.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1525681 13f79535-47bb-0310-9956-ffa450edef68\n",
      "commitDate": "23/09/13 1:02 PM",
      "commitName": "28e3d09230971b32f74284311931525cb7ad1b7c",
      "commitAuthor": "Jing Zhao",
      "diff": "@@ -0,0 +1,27 @@\n+  private void processOverWrite(DFSClient dfsClient, WRITE3Request request,\n+      Channel channel, int xid, IdUserGroup iug) {\n+    WccData wccData \u003d new WccData(latestAttr.getWccAttr(), null);\n+    long offset \u003d request.getOffset();\n+    int count \u003d request.getCount();\n+    WriteStableHow stableHow \u003d request.getStableHow();\n+    WRITE3Response response;\n+    long cachedOffset \u003d nextOffset.get();\n+    if (offset + count \u003e cachedOffset) {\n+      LOG.warn(\"Haven\u0027t noticed any partial overwrite for a sequential file\"\n+          + \" write requests. Treat it as a real random write, no support.\");\n+      response \u003d new WRITE3Response(Nfs3Status.NFS3ERR_INVAL, wccData, 0,\n+          WriteStableHow.UNSTABLE, 0);\n+    } else {\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"Process perfectOverWrite\");\n+      }\n+      // TODO: let executor handle perfect overwrite\n+      response \u003d processPerfectOverWrite(dfsClient, offset, count, stableHow,\n+          request.getData().array(),\n+          Nfs3Utils.getFileIdPath(request.getHandle()), wccData, iug);\n+    }\n+    updateLastAccessTime();\n+    Nfs3Utils.writeChannel(channel,\n+        response.writeHeaderAndResponse(new XDR(), xid, new VerifierNone()),\n+        xid);\n+  }\n\\ No newline at end of file\n",
      "actualSource": "  private void processOverWrite(DFSClient dfsClient, WRITE3Request request,\n      Channel channel, int xid, IdUserGroup iug) {\n    WccData wccData \u003d new WccData(latestAttr.getWccAttr(), null);\n    long offset \u003d request.getOffset();\n    int count \u003d request.getCount();\n    WriteStableHow stableHow \u003d request.getStableHow();\n    WRITE3Response response;\n    long cachedOffset \u003d nextOffset.get();\n    if (offset + count \u003e cachedOffset) {\n      LOG.warn(\"Haven\u0027t noticed any partial overwrite for a sequential file\"\n          + \" write requests. Treat it as a real random write, no support.\");\n      response \u003d new WRITE3Response(Nfs3Status.NFS3ERR_INVAL, wccData, 0,\n          WriteStableHow.UNSTABLE, 0);\n    } else {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Process perfectOverWrite\");\n      }\n      // TODO: let executor handle perfect overwrite\n      response \u003d processPerfectOverWrite(dfsClient, offset, count, stableHow,\n          request.getData().array(),\n          Nfs3Utils.getFileIdPath(request.getHandle()), wccData, iug);\n    }\n    updateLastAccessTime();\n    Nfs3Utils.writeChannel(channel,\n        response.writeHeaderAndResponse(new XDR(), xid, new VerifierNone()),\n        xid);\n  }",
      "path": "hadoop-hdfs-project/hadoop-hdfs-nfs/src/main/java/org/apache/hadoop/hdfs/nfs/nfs3/OpenFileCtx.java"
    }
  }
}